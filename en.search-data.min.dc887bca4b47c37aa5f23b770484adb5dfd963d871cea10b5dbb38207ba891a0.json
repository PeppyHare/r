[{"id":0,"href":"/r/notes/UWAA545/","title":"Computational Methods For Plasmas","section":"Notes","content":" MHD Theory # Course notes from AA545 at the University of Washington, Seattle\nTable of Contents # Introduction Plasma Models Formulary "},{"id":1,"href":"/r/notes/UWAA560/","title":"Plasma Diagnostics","section":"Notes","content":" MHD Theory # Course notes from AA560 at the University of Washington, Seattle\nTable of Contents # Introduction Diagnostic Considerations Magnetic Field Diagnostics Electrostatic Diagnostics Index of Refraction Measurements Spectroscopic Measurements Student Lectures Zeeman Spectroscopy Formulary "},{"id":2,"href":"/r/notes/UWAA543/ch20-1/","title":"Introduction to Computational CFD","section":"Computational CFD","content":" 20.1 Introduction to Computational CFD # What is computational fluid dynamics? Put simply, it\u0026rsquo;s the use of computers to obtain a numerical solution to the governing equations of fluid dynamics. As always, the numerical solution is an approximation to the real solution to the equations, but additionally those equations are themselves only an approximation of the real physics.\nWe will be interested in speeds much less than \\( c \\) (\\( 3 \\cdot 10^8 m/s) \\) and distances much greater than the atomic radius \\( \\alpha_{bohr} = 5 \\cdot 10^{-11} m \\).\nClassical mechanics is governed entirely by Newton\u0026rsquo;s laws:\n\\[m \\ddot{x_i} = \\sum_j F_{ij}\\] Modeling using only classical mechanics can be and is done. For example, the PIC (\u0026ldquo;particle-in-cell\u0026rdquo;) method models individual particles. But often, even in PIC simulations, we approximate many particles (\\( \\geq 20^{20} )\\) by fewer superparticles \\( (\\approx 10^6) \\). Modeling with fewer particles leads to statistical errors, since noise is reduced with larger numbers of particles. When modeling long time scales, those statistical errors build up. PIC is only useful for timescales around \\( \\leq 10^{-6} \\) seconds.\nTo handle longer time scales and large numbers of particles, we can group the particles into elements in phase space \\( (\\vec v, \\vec x,t ) \\). The governing equation is the Boltzmann equation\n\\[\\pdv{f}{t} \u0026#43; \\vec v \\cdot \\pdv{f}{x} \u0026#43; \\frac{\\vec F}{m} \\cdot \\pdv{f}{\\vec v} = \\left. \\pdv{f}{t} \\right| _{\\text{collisions}}\\] where \\( f(\\vec v, \\vec x, t) \\) is the probability distribution function - the number of particles at location \\( \\vec x \\) with velocity \\( \\vec v \\) at time t. This lets us replace the massive discrete particle simulation with an easier continuous set of probability distribution components. However, the 3-dimensional problem has been turned into a 7-dimensional (3 space, 3 velocity, time) one, which is very expensive and slow.\nIf we assume local thermodynamic equilibrium (in other words, Maxwellian velocity distribution) we can assign a characteristic velocity to spatial (fluid) elements based on the Maxwellian distribution velocity. Formally, this means that we take moments of the Boltzmann equation and we define an equation of state for closure (determining a truncation of the higher-order moments).\nDoing so gives us the equations of fluid mechanics, the Navier-Stokes equations. More generally, for conducting fluids or plasmas this gives the magnetofluid equations. In plasmas in which we care about multiple species (ions, electrons), further unification is done by a single-fluid assumption, resulting in MHD equations. These assumptions give impressive agreement with real-world observed results.\n20.1.1 Applications # CFD is routinely used for\nThe engineering design of airplanes Boat hull design Automobiles Various manufacturing processes Computer chassis design Weather forecasting There are a number of unsolved \u0026ldquo;Grand challenges\u0026rdquo; in the CFD space\nNumerical Wind Tunnel eliminate the need for wind tunnel tests, give access to any Reynolds number cost savings Global Weather Model predict flooding anywhere snowstorms problem: \u0026ldquo;butterfly effect\u0026rdquo; Numerical Tokamak would help solve the problem of magnetic fusion for energy production avoid expensive experiments 20.1.2 Computer Hardware # By 2020 many useful simulations can be done on personal computers. Previously workstations served the purpose of small-scale but greater than personal computer applications. Supercomputers are now the main CFD target\nVector computers (Cray) Parallel computers (IBM, SGI, Cray\u0026hellip;): massively parallel, N \u0026gt; 10,000 Symmetric multiprocessors: combines parallel and vector computing. Many cores on a single CPU chip. Parallel computing issues quickly become very important for large-scale simulations that can not fit on a single node. How do you divide a problem so that many processors evenly share work? We can define parallel efficiency to mean N processors should perform the work N times faster. This efficiency is not always possible, and is algorithm dependent. To take advantage of parallel computing benefits, appropriate algorithms must be chosen.\n"},{"id":3,"href":"/r/notes/UWAA558/","title":"MHD Theory","section":"Notes","content":" MHD Theory # Course notes from AA558 at the University of Washington, Seattle\nTable of Contents # Introduction Kinetic Models Plasma Fluid Model Two-Fluid Model MHD MHD Boundary Conditions MHD Equilibrium Concepts Equilibrium of 1D Configurations Equilibrium of 2D Configurations Equilibrium of 3D Configurations MHD Stability Formulary "},{"id":4,"href":"/r/notes/UWAA557/ch10-0/","title":"Rules of thumb","section":"Physics of Fusion Plasmas","content":" Rules of thumb # \\[\\] Jarboe\u0026rsquo;s \u0026lsquo;Rules of Thumb\u0026rsquo; - basic associations and concepts to keep in mind when working with fusion plasmas\nRule 1 In event counting measurements the uncertainty in event rate is equal to the square root of the number of events detected. A trade off is made between signal resolution and time resolution Rule 2 The total E and M force on a volume of plasma can be computed from surface values through \\( \\vec F = \\int \\overline{T} \\cdot \\vec n \\dd V \\) Rule 3 Magnetic field has a tension and isotropic pressure\n\\[T = \\frac{B^2}{\\mu_0} \\qquad p = \\frac{B^2}{2 \\mu_0}\\] Rule 4 Plasma is frozen in the magnetic field when \\( \\eta = 0 \\) (zero resistivity). Rule 5\nWhen \\( \\vec{v}_{mag} \\) is the velocity of the magnetic field,\n\\[\\vec E = - \\vec{v}_{mag} \\cross \\vec B\\] Rule 6 Ambipolar diffusion requires the ions and electrons to leave the plasma at the same rate\n\\[n_{ew} v_{ew} = n_{iw} v_{iw}\\] Rule 7 At a given temperature the electron speed is 60 times the D ions requiring a four-e-folding in sheath voltage to make the electron and ion losses equal\n\\[V_{sheath} \\approx 4 T_e\\] Rule 8\nThe energy of the ions into the wall is:\n\\[W_{impact} \\approx 4 Z_i k T_e \u0026#43; \\frac{1}{2} k T_i\\] Rule 9\nA Langmuir probe can measure \\( T_e \\) \\( T_i \\) and \\( n \\). The probe perturbs the plasma and is good for unmagnetized plasma of \\( T \\) less than \\( 50 eV \\) and \\( n \\) less than \\( 10^{20} m^{-3} \\) . The probe is used for velocity in magnetized plasma where \\( v \\cross B \\) often dominates \\( E \\).\nRule 10 Particles interact within a Debye sphere. Outside \\( \\lambda_D \\) distance particles do not see each other due to shielding. Rule 11 Four collision frequencies\n\\( \\nu_{ei} \\) - Electron momentum loss rate on ions. Used in resistivity. \\( \\nu_{ee} \\) Electron energy exchange rate with electrons. In other words, if you do something to the electrons this is how long it will take to get back to Maxwellian. Same order as \\( \\nu_{ei} \\) \\( \\nu_{ii} \\) Ion energy exchange rate with ions \\( \\nu_{ie} \\) Electron energy exchange rate between electrons and ions. It\u0026rsquo;s about the same as ions slowing down in electrons: \\( \\approx \\frac{m_e}{m_i} \\nu_{ee} \\). For fusion to work, need confinement times longer than this time. "},{"id":5,"href":"/r/notes/UWAA545/01-syllabus/","title":"Syllabus","section":"Computational Methods For Plasmas","content":" \\[\\] Syllabus # Learn the basis of common computational methods used for plasma physics, specifically, methods applied to magnetic confinement, plasma thrusters, and astrophysics. Learn the regions of applicability of the various computational methods. Implement a computational method to simulate plasma kinetics using an electrostatic PIC approach. Simulate plasma dynamics using a PIC code and an MHD code. Motivation # As a motivation for studying computational methods for plasmas, consider the task of analytical methods for arriving at closed-form expressions of the plasma properties. The physical computations of plasmas generally limit analytical methods to very simple configurations and dynamics; it\u0026rsquo;s rather exciting to encounter such a situation, because they occur so infrequently. For example, 1-dimensional, axisymmetric, equilibrium configurations limited to linear stability. To model more complicated, realistic situations, we take a complementary approach using computational and experimental methods.\nComputational methods provide tools for understanding plasma physics. As we\u0026rsquo;ll see in the course, using a basic particle description is not usually possible, leading to the need to derive reduced models. We arrive at reduced models by applying approximations and assumptions to make the problem easier to solve. These approximations inherently limit the applicability and accuracy of the reduced model to whichever regime is appropriate, based on the limits of the approximation.\nExperimental methods also require approximations and assumptions and are likewise limited, but their limitations are usually different. It is the contrast between their assumptions that leads to this complementary approach between computational and experimental methods\nComputational Methods Experimental Methods Known physics, but incomplete (reduced, controllable) models Unknown physics, complete model (uncontrollable) Deterministic and reproducible Difficult to identify most important physical effects Completely diagnosable using nonperturbing diagnostics Diagnostics can be perturbing, incomplete (chord- or time-integrated, limited, indirect) Both approaches are necessary to provide a deeper understanding of the plasma. Many experiments today require computational simulations just to interpret the experimental data (e.g. NIF \\( \\rightarrow \\) Hydra, DIII-D \\( \\rightarrow \\) EFIT). Further into the course, we will talk about the formal methods we use to combine experimental and computational data, and to quantify the uncertainty in our model (UQ). Uncertainty quantification is used to perform verification and validation (V\u0026amp;V). Verification tells you that you\u0026rsquo;re solving the PDE\u0026rsquo;s correctly, and validation says that you have the correct PDE\u0026rsquo;s for the experiment.\nCourse Format # All submitted material should be publication quality, i.e. typeset, clearly organized and written. Students will write a 1D, electrostatic PIC code based on the Vlasov-Poisson plasma model. The code, a run script or instructions, and a report describing their code will be submitted. The PIC code will be used to perform several \u0026ldquo;computer mini-projects\u0026rdquo; which compare the numerical results to theory. The project reports should follow the format for PRL submissions (4 typeset pages including plots). Download the PRL template. Students will write an MHD fluid code that will calculate plasma equilibrium, linear stability, and nonlinear dynamics. They will apply their code to two equilibria and compare the numerical results to theoretical expectations. The computer projects will be presented in class at the end fo the quarter. Students will submit a report describing their code, applications, and comparison to theory at the end of the quarter. This report should also follow the PRL submission format. Students will be required to formulate and ask one question of each presenter. The question and response must be written and submitted by the questioner. Course topics # Plasma Models: particle model, kinetic model, multi-fluid model, single fluid (MHD) model, hybrid models Nonlinear Dynamics: PIC methods; continuum kinetics; gyrokinetic; multi-fluid; methods for MHD (div B issues, energy vs. pressure-based); finite element methods Transport: Monte Carlo methods; Fokker-Planck model Equilibrium: elliptic governing equation; elliptic numerical methods (direct vs. iterative, relaxation, CG, MG) Linear Stability; eigenvalue formulation (self-adjoint, Rayleigh quotient, relaxation, shooting method); time integration of governing equation. Uncertainty Quantification: verification and validation; combining computational and experimental data. "},{"id":6,"href":"/r/notes/UWAA558/01-syllabus/","title":"Syllabus","section":"MHD Theory","content":" Syllabus # The course topics planned for this section are (in rough order):\nParticle Model, Boltzmann-Maxwell Model, Magnetohydrodynamic (MHD) Model, Region of Validity, Common Assumptions, Ideal MHD Model, General Properties (Equilibrium, Boundary Conditions, Conservation Laws, \u0026ldquo;Frozen-In\u0026rdquo; Flux)\nIdeal MHD Equilibrium, Virial Theorem, Magnetic Flux Surfaces\nOne-Dimensional Equilibria, Theta-Pinch, Z-Pinch, Screw-Pinch, Safety Factor q\nTwo-Dimensional Equilibria, Toroidal Geometry, Grad-Shafranov Equation, Closed Flux Surfaces, Safety Factor q, Magnetic Shear, Magnetic Well, Shafranov Shift, Spheromak, Reversed Field Pinch (RFP), Tokamaks, Stellarators (Elmo Bumpy Torus)\nMHD Stability, General Concepts, Linearized MHD, Exponential (Linear) Stability, Force Operator and Properties, Variational Formulation, Energy Principle, Intuitive Form of delta W, Classification of Instabilities (internal/external, pressure-drive/current-driven, kink/interchange/ballooning)\nStability of One-Dimensional Equilibria, Modal Analysis, Rayleigh-Taylor, Theta-Pinch, Z-Pinch (Kadomtsev Condition), Screw-Pinch (Kruskal-Shafranov Condition, Suydam Criterion), RFP, \u0026ldquo;Straight\u0026rdquo; Tokamak\nStability of Two-Dimensional Equilibria, Tokamak, Mercier Criterion, Elmo Bumpy Torus\nResistive (Tearing) Instabilities, Stability of Non-static Equilibria, Nonlinear Stability Effects\nCourse Motivation # Plasma phenomena tend to be hard to treat because of the span of relevant scales. You have ions, electrons, and photons interacting through electromagnetic interactions. There is a tremendous variation in mass across species, which leads to a large span of both spatial and temporal scales. The species can interact through both short scale collisions and long range interactions through EM forces. In contrast, in normal gas dynamics you may consider only the short-scale interactions. As a consequence, we can describe dispersive plasma waves.\nFor comparison, remember in gas dynamics, the speed of sound is\n\\[\\frac{\\omega}{k} = v_s = \\pdv{\\omega}{k} \\qquad \\text{(gas)}\\] Here the phase velocity \\( \\frac{\\omega}{k} \\) is equal to the phase velocity \\( \\pdv{\\omega}{k} \\) . In a plasma, we can have non-linear dispersion relations in which the phase and group velocity are different.\n\\[\\frac{\\omega}{k} = v(\\omega, k) \\neq \\pdv{\\omega}{k} \\qquad \\text{(plasma)} \\] The number of particles we typically deal with in a laboratory plasma is roughly on the order of a mole of particles\n\\[\\text{particles} \\sim O(10^{23})\\] With long-range interactions, we have a combinatorial explosion of interacting particles! It is not possible to track individual particles at such a scale, so we need much simpler plasma descriptions. These are the plasma models we will discuss in the next chapter. When simplifying our models, we need to pay careful attention to the simplifications we are making because in general, inaccurate physics lead to incorrect conclusions.\n"},{"id":7,"href":"/r/notes/UWAA560/01-syllabus/","title":"Syllabus","section":"Plasma Diagnostics","content":" \\[\\] Syllabus # The goals of this course are:\nTo teach the function of diagnostic components, and the specific plasma mechanisms that are being measured. You will be come familiar with the analysis procedures required to get (often indirect) measurements. To appreciate uncertainty and error quantification in reporting experimental measurements. This course will use I.H. Hutchinson, Principles of Plasma Diagnostics as the primary text, with the standard R.J. Goldston and P.H. Rutherford, Introduction to Plasma Physics for reference.\nIntroduction # We\u0026rsquo;ll set the stage with a few quotes from giants in the field:\n\u0026ldquo;When you can measure what you are speaking about and express it in numbers, you know something about it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced to the stage of science.\u0026rdquo; - Lord Kelvin, 1883\n\u0026ldquo;An experiment is a question which science poses to nature, and a measurement is the recording of nature\u0026rsquo;s answer\u0026rdquo; - Max Planck, 1949\n\u0026ldquo;Measurements are meaningless without quantifying the uncertainty\u0026rdquo; - Uri Shumlak, 2021\nMany of the measurements we report are plasma parameters we extracted indirectly via other means, so proper propagation of error and uncertainty is particularly important.\nWhat makes plasma diagnostics particularly difficult is that we are measuring the physical properties of a plasma that range in temperature from \\( 1 eV \\) to \\( 10 keV \\) and also in number density from \\( 10^{13} cm^{-3} \\) to \\( 10^{19} cm^{-3} \\). We\u0026rsquo;ll find that some diagnostics are only appropriate in a certain portion of this range. Further complicating the situation, spatial scales can vary from \\( 10 \\mu m \\) to \\( 1m \\) and temporal scales can vary from \\( 10ns \\) to hundreds of seconds.\nInstruments inserted into the plasma (Langmuir probes, Mach probes, RPA\u0026rsquo;s, etc.) can alter the plasma by creating perturbations which alter the local plasma properties. High-energy density plasmas can even destroy the instruments.\nPlasma diagnostics extract plasma properties by measuring nuanced interactions between EM fields and detailed velocity distribution functions for the various relevant species. These species might be electrons, bulk ions (\\( H^+, D^+, He^{++}, Ar^+ \\)), and impurity ions (\\( C^+, O^+ \\)). The field of plasma diagnostics is vast, and new techniques are currently being developed. Specialized journals and conferences are dedicated to the topic. The goal of this course is to develop a foundation to understand common diagnostics and to understand more advanced diagnostic techniques.\n"},{"id":8,"href":"/r/notes/griffiths/ch1-1/","title":"Vector Algebra","section":"Griffiths Introduction to Electrodynamics","content":" 1.1 Vector Algebra # 1.1.1 Vector Operations # If you walk 4 miles due north and then 3 miles due east (Fig. 1.1), you will have gone a total of 7 miles, but you\u0026rsquo;re not 7 miles from where you set out-you\u0026rsquo;re only 5. We need an arithmetic to describe quantities like this, which evidently do not add in the ordinary way. The reason they don\u0026rsquo;t, of course, is that displacements (straight line segments going from one point to another) have direction as well as magnitude (length), and it is essential to take both into account when you combine them. Such objects are called vectors: velocity, acceleration, force and momentum are other examples. By contrast, quantities that have magnitude but no direction are called scalars: examples include mass, charge, density, and temperature.\nI shall use boldface (\\( \\vec{A} \\) , \\( \\vec{B} \\) , and so on) for vectors and ordinary type for scalars. The magnitude of a vector \\( \\vec{A} \\) is written \\( |\\vec{A}| \\) or, more simply, \\( A \\) . In diagrams, vectors are denoted by arrows: the length of the arrow is proportional to the magnitude of the vector, and the arrowhead indicates its direction. Minus \\( \\vec{A} \\) (\\( - \\vec{A} \\) ) is a vector with the same magnitude as A but of opposite direction (Fig. 1.2). Note that vectors have magnitude and direction but not location: a displacement of 4 miles due north from Washington is represented by the same vector as a displacement 4 miles north from Baltimore (neglecting, of course, the curvature of the earth). On a diagram, therefore, you can slide the arrow around at will, as long as you don\u0026rsquo;t change its length or direction.\nWe define four vector operations: addition and three kinds of multiplication.\n(i) Addition of two vectors.. Place the tail of \\( \\vec{B} \\) at the head of \\( \\vec{A} \\); the sum, \\( \\vec{A} + \\vec{B} \\), is the vector from the tail of \\( \\vec{A} \\) to the head of \\( \\vec{B} \\) (Fig 1.3). This rule generalizes the obvious procedure for combining two displacements. Addition is commutative:\n\\[\\vec{A} \u0026#43; \\vec{B} = \\vec{B} \u0026#43; \\vec{A}\\] 3 miles east followed by 4 miles north gets you to the same place as 4 miles north followed by 3 miles east. Addition is also associative:\n\\[(\\vec{A} \u0026#43; \\vec{B}) \u0026#43; \\vec{C} = \\vec{A} \u0026#43; (\\vec{B} \u0026#43; \\vec{C})\\] To subtract a vector, add its opposite (Fig. 1.4):\n\\[\\vec{A} - \\vec{B} = \\vec{A} \u0026#43; (- \\vec{B})\\] (ii) Multiplication by a scalar. Multiplication of a vector by a positive scalar a multiplies the magnitude but leaves the direction unchanged (Fig. 1.5). (If a is negative, the direction is reversed.) Scalar multiplication is distributive:\n\\[a(\\vec{A} \u0026#43; \\vec{B}) = a \\vec{A} \u0026#43; a \\vec{B}\\] (iii) Dot product of two vectors. The dot product of two vectors is defined by\n\\[\\vec{A} \\cdot \\vec{B} = A B \\cos \\theta \\tag{1.1}\\] where \\( \\theta \\) is the angle they form when placed tail-to-tail (Fig. 1.6). Note that \\( \\vec{A} \\cdot \\vec{B} \\) is itself a scalar (hence the alternative name scalar product). The dot product is commutative,\n\\[\\vec{A} \\cdot \\vec{B} = \\vec{B} \\cdot \\vec{A}\\] and distributive\n\\[\\vec{A} \\cdot (\\vec{B} \u0026#43; \\vec{C}) = \\vec{A} \\cdot \\vec{B} \u0026#43; \\vec{A} \\cdot \\vec{C} \\tag{1.2}\\] Geometrically, \\( \\vec{A} \\cdot \\vec{B} \\) is the product of A times the projection of B along A (or the product of B times the projection of A along B). If the two vectors are parallel, then \\( \\vec{A} \\cdot \\vec{B} = AB \\). In particular, for any vector A\n\\[\\vec{A} \\cdot \\vec{A} = A^2 \\tag{1.3}\\] If A and B are perpendicular, then \\( \\vec{A} \\cdot \\vec{B} = 0 \\)\nExample 1.1 # Q Let \\( \\vec{C} = \\vec{A} - \\vec{B} \\) (Fig 1.7), and calculate the dot product of \\( \\vec{C} \\) with itself. A \\[\\vec{C} \\cdot \\vec{C} = ( \\vec{A} - \\vec{B} ) \\cdot (\\vec{A} - \\vec{B}) = \\vec{A} \\cdot \\vec{A} - \\vec{A} \\cdot \\vec{B} - \\vec{B} \\cdot \\vec{A} \u0026#43; \\vec{B} \\cdot \\vec{B}\\] or\n\\[C^2 = A^2 \u0026#43; B^2 - 2AB\\cos \\theta\\] This is the law of cosines.\n(iv) Cross product of two vectors. The cross product of two vectors is defined by\n\\[\\vec{A} \\cross \\vec{B} = AB \\sin \\theta \\vu{n} \\tag{1.4}\\] where \\( \\vu{n} \\) is a unit vector (vector of magnitude 1) pointing perpendicular to the plane of A and B. (I shall use a hat \\( \\vu{} \\) to denote unit vectors.) Of course, there are two directions perpendicular to any plane: \u0026ldquo;in\u0026rdquo; and \u0026ldquo;out.\u0026rdquo; The ambiguity is resolved by the right-hand rule: let your fingers point in the direction of the first vector and curl around (via the smaller angle) toward the second; then your thumb indicates the direction of \\( \\vu{n} \\). (In Fig. 1.8, \\( \\vec{A} \\cross \\vec{B} \\) points into the page; \\( \\vec{B} \\cross \\vec{A} \\) points out of the page.) Note that \\( \\vec{A} \\cross \\vec{B} \\) is itself a vector (hence the alternative name vector product). The cross product is distributive\n\\[\\vec{A} \\cross ( \\vec{B} \u0026#43; \\vec{C}) = ( \\vec{A} \\cross \\vec{B}) \u0026#43; (\\vec{A} \\cross \\vec{C})\\] but not commutative. In fact,\n\\[(\\vec{B} \\cross \\vec{A}) = - (\\vec{A} \\cross \\vec{B}) \\tagl{1.6}\\] Geometrically, \\( | \\vec{A} \\cross \\vec{B} | \\) is the area of the parallelogram generated by \\( \\vec{A} \\) and \\( \\vec{B} \\) (Fig 1.8). If two vectors are parallel, their cross product is zero. In particular,\n\\[ \\vec{A} \\cross \\vec{A} = 0 \\] for any vector A.\n1.1.2: Vector Algebra: Component Form # In the previous section, I defined the four vector operations (addition, scalar multiplication, dot product, and cross product) in \u0026ldquo;abstract\u0026rdquo; form-that is, without reference to any particular coordinate system. In practice, it is often easier to set up Cartesian coordinates x, y, z and work with vector components. Let \\( \\vu{x} \\), \\( \\vu{y} \\) , and \\( \\vu{z} \\) be unit vectors parallel to the x, y, and z axes, respectively (Fig. 1.9(a)). An arbitrary vector A can be expanded in terms ofthese basis vectors (Fig. 1.9(b)):\n\\[\\vec{A} = A_x \\vu{x} \u0026#43; A_y \\vu{y} \u0026#43; A_z \\vu{z}\\] The numbers \\( A_x \\), \\( A_y \\), and \\( A_z \\) are the \u0026ldquo;components\u0026rdquo; of A; geometrically, they are the projections of A along the three coordinate axes (\\( A_x = \\vec{A} \\cdot \\vu{x}, A_y = \\vec{A} \\cdot \\vu{y}, A_z = \\vec{A} \\cdot \\vu{z} \\) ). We can now reformulate each of the four vector operations as a rule for manipulating components:\n\\[\\vec{A} \u0026#43; \\vec{B} = (A_x \\vu{x} \u0026#43; A_y \\vu{y} \u0026#43; A_z \\vu{z}) \u0026#43; (B_x \\vu{x} \u0026#43; B_y \\vu{y} \u0026#43; B_z \\vu{z}) \\\\ = (A_x \u0026#43; B_x) \\vu{x} \u0026#43; (A_y \u0026#43; B_y) \\vu{y} \u0026#43; (A_z \u0026#43; B_z) \\vu{z} \\tag{1.7}\\] Rule (i): To add vectors, add like components.\n\\[a\\vec{A} = (a A_x) \\vu{x} \u0026#43; (a A_y) \\vu{y} \u0026#43; (a A_z)\\vu{z} \\tag{1.8}\\] Rule (ii): To multiply by a scalar, multiply each component.\nBecause \\( \\vu{x}, \\vu{y} \\), and \\( \\vu{z} \\) are mutually perpendicular unit vectors\n\\[\\vu{x} \\cdot \\vu{x} = \\vu{y} \\cdot \\vu{y} = \\vu{z} \\cdot \\vu{z} = 1; \\qquad \\vu{x} \\cdot \\vu{y} = \\vu{x} \\cdot \\vu{z} = \\vu{y} \\cdot \\vu{z} = 0 \\tag{1.9}\\] Accordingly,\n\\[\\vec{A} \\cdot \\vec{B} = (A_x \\vu{x} \u0026#43; A_y \\vu{y} \u0026#43; A_z \\vu{z}) \\cdot (B_x \\vu{x} \u0026#43; B_y \\vu{y} \u0026#43; B_z \\vu{z}) \\\\ = A_x B_x \u0026#43; A_y B_y \u0026#43; A_z B_z \\tag{1.10}\\] Rule (iii): To calculate the dot product, multiply like components and add. In particular,\n\\[\\vec{A} \\cdot \\vec{A} = A_x ^2 \u0026#43; A_y ^2 \u0026#43; A_z ^2\\] so\n\\[A = \\sqrt{A_x ^2 \u0026#43; A_y ^2 \u0026#43; A_z ^2} \\tag{1.11}\\] Similarly,\n\\[\\begin{aligned} \\vu{x} \\cross \\vu{x} \u0026amp; = \u0026amp; \\vu{y} \\cross \\vu{y} \u0026amp; = \u0026amp; \\vu{z} \\cross \\vu{z} = 0 \\\\ \\vu{x} \\cross \\vu{y} \u0026amp; = \u0026amp; - \\vu{y} \\cross \\vu{x} \u0026amp; = \u0026amp; \\vu{z} \\\\ \\vu{y} \\cross \\vu{z} \u0026amp; = \u0026amp; - \\vu{z} \\cross \\vu{y} \u0026amp; = \u0026amp; \\vu{x} \\\\ \\vu{z} \\cross \\vu{x} \u0026amp; = \u0026amp; - \\vu{x} \\cross \\vu{z} \u0026amp; = \u0026amp; \\vu{y} \\end{aligned}\\] Therefore,\n\\[\\vec{A} \\cross \\vec{B} = (A_x \\vu{x} \u0026#43; A_y \\vu{y} \u0026#43; A_z \\vu{z}) \\cross (B_x \\vu{x} \u0026#43; B_y \\vu{y} \u0026#43; B_z \\vu{z}) \\\\ = (A_y B_z - A_z B_y) \\vu{x} \u0026#43; (A_z B_x - A_x B_z)\\vu{y} \u0026#43; (A_x B_y - A_y B_x) \\vu{z} \\tag{1.13}\\] This cumbersome expression can be written more neatly as a determinant:\n\\[\\vec{A} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} \u0026amp; \\vu{y} \u0026amp; \\vu{z} \\\\ A_x \u0026amp; A_y \u0026amp; A_z \\\\ B_x \u0026amp; B_y \u0026amp; B_z \\end{vmatrix}\\] Rule (iv): To calculate the cross product, form the determinant whose first row is \\( \\vu{x}, \\vu{y}, \\vu{z} \\), whose second row is A, and whose third row is B.\nExample 1.2 # Q Find the angle between the face diagonals of a cube. A Solution We might as well use a cube of side 1, and place it as shown in Fig 1.10, with one corner at the origin. The face diagonals \\( \\vec{A} \\) and \\( \\vec{B} \\) are\n\\[\\vec{A} = 1 \\vu{x} \u0026#43; 0 \\vu{y} \u0026#43; 1 \\vu{z}; \\qquad \\vec{B} = 0 \\vu{x} \u0026#43; 1 \\vu{y} \u0026#43; 1 \\vu{z}\\] So, in component form, \\[\\vec{A} \\cdot \\vec{B} = 1 \\cdot 0 \u0026#43; 0 \\cdot 1 \u0026#43; 1 \\cdot 1 = 1\\] On the other hand, in \u0026ldquo;abstract\u0026rdquo; form,\n\\[\\vec{A} \\cdot \\vec{B} = AB \\cos \\theta = \\sqrt{2} \\sqrt{2} \\cos \\theta = 2 \\cos \\theta\\] Therefore,\n\\[\\cos \\theta = 1/2 \\quad \\text{ or } \\quad \\theta = 60^{\\circ}\\] Of course, you can get the answer more easily by drawing in a diagonal across the top of the cube, completing the equilateral triangle. But in cases where the geometry is not so simple, this device of comparing the abstract and component forms of the dot product can be a very efficient means of finding angles.\n1.1.3: Triple Products # Since the cross product of two vectors is itself a vector, it can be dotted or crossed with a third vector to form a triple product.\n(i) Scalar triple product: \\( \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) \\). Geometrically, \\( |\\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) | \\) is the volume of the parallelpiped generated by A, B, and C, since \\( |\\vec{B} \\cross \\vec{C}| \\) is the area of the base, and \\( | \\vec{A} \\cos \\theta | \\) is the altitude (Fig. 1.12). Evidently,\n\\[\\vec{A} \\cdot(\\vec{B} \\cross \\vec{C}) = \\vec{B} \\cdot (\\vec{C} \\cross \\vec{A}) = \\vec{C} \\cdot (\\vec{A} \\cross \\vec{B}) \\tagl{1.15}\\] for they all correspond to the same figure. Note that \u0026ldquo;alphabetical\u0026rdquo; order is preserved - in view of \\( \\eqref{1.6} \\), the \u0026ldquo;nonalphabetical\u0026rdquo; triple products\n\\[\\vec{A} \\cdot(\\vec{C} \\cross \\vec{B}) = \\vec{B} \\cdot (\\vec{A} \\cross \\vec{C}) = \\vec{C} \\cdot (\\vec{B} \\cross \\vec{A}) \\] have the opposite sign. In component form,\n\\[\\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = \\begin{vmatrix} A_x \u0026amp; A_y \u0026amp; A_z \\\\ B_x \u0026amp; B_y \u0026amp; B_z \\\\ C_x \u0026amp; C_y \u0026amp; C_z \\end{vmatrix} \\tagl{1.16}\\] Note that the dot and cross can be interchanged:\n\\[\\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = (\\vec{A} \\cross \\vec{B}) \\cdot \\vec{C}\\] (this follows immediately from Eq. 1.15); however, the placement of the parentheses is critical: \\( (\\vec{A} \\cdot \\vec{B}) \\cdot \\vec{C} \\) is a meaningless expression - you can\u0026rsquo;t make a cross product from a scalar and a vector.\n(ii) Vector triple product: \\( \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) \\). The vector triple product can be simplified by the so-called BAC-CAB rule:\n\\[\\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) = \\vec{B}(\\vec{A} \\cdot \\vec{C}) - \\vec{C} (\\vec{A} \\cdot \\vec{B}) \\tagl{1.17}\\] Notice that\n\\[(\\vec{A} \\cross \\vec{B}) \\cross \\vec{C} = - \\vec{C} \\cross (\\vec{A} \\cross \\vec{B}) = -\\vec{A}(\\vec{B} \\cdot \\vec{C}) \u0026#43; \\vec{B}(\\vec{A} \\cdot \\vec{C})\\] is an entirely different vector (cross-products are not associative). All higher vector products can be similarly reduced, often by repeated application of \\( \\eqref{1.17} \\), so it is never necessary for an expression to contain more than one cross product in any term. For instance,\n\\[(\\vec{A} \\cross \\vec{B}) \\cdot (\\vec{C} \\cross \\vec{D}) = (\\vec{A} \\cdot \\vec{C})(\\vec{B} \\cdot \\vec{D}) - (\\vec{A} \\cdot \\vec{D}) (\\vec{B} \\cdot \\vec{C})\\] \\[\\vec{A} \\cross [\\vec{B} \\cross (\\vec{C} \\cross \\vec{D})] = \\vec{B}[ \\vec{A} \\cdot (\\vec{C} \\cross \\vec{D})] - (\\vec{A} \\cdot \\vec{B})(\\vec{C} \\cross \\vec{D}) \\tagl{1.18}\\] 1.1.4: Position, Displacement, and Separation Vectors # The location of a point in three dimensions can be described by listing its Cartesian coordinates (x, y, z). The vector to that point from the origin (\\( \\mathscr{O} \\)) is called the position vector (Fig 1.13):\n\\[\\vec{r} \\equiv x \\vu{x} \u0026#43; y \\vu{y} \u0026#43; z \\vu{z} \\tagl{1.19}\\] I will reserve the letter \\( \\vec{r} \\) for this purpose. Its magnitude,\n\\[r = \\sqrt{x^2 \u0026#43; y^2 \u0026#43; z^2} \\tagl{1.20}\\] is the distance from the origin, and\n\\[\\vu{r} = \\frac{\\vec{r}}{r} = \\frac{x \\vu{x} \u0026#43; y \\vu{y} \u0026#43; z \\vu{z}}{ \\sqrt{x^2 \u0026#43; y^2 \u0026#43; z^2}} \\tagl{1.21}\\] is a unit vector pointing radially outward. The infinitesimal displacement vector from \\( (x, y, z) \\) to \\( x + \\dd{x}, y + \\dd{y}, z + \\dd{z} \\) is\n\\[\\dd{\\vec{l}} = \\dd{x} \\vu{x} \u0026#43; \\dd{y} \\vu{y} \u0026#43; \\dd{z} \\vu{z} \\tagl{1.22}\\] (We could call this \\( \\dd{\\vec{r}} \\), since that\u0026rsquo;s what it is, but it is useful to have a special notation for infinitesimal displacements.)\nIn electrodynamics, one frequently encounters problems involving two points - typically a source point, \\( \\vec{r\u0026rsquo;} \\), where an electric charge is located, and a field point \\( \\vec{r} \\) at which you are calculating the electric or magnetic field (Fig 1.14). It pays to adopt right from the start some short-hand notation for the separation vector from the source point to the field point. I shall use for this purpose the letter \\( \\gr \\):\n\\[\\vec{\\gr} \\equiv \\vec{r} - \\vec{r\u0026#39;} \\tagl{1.23}\\] Its magnitude is\n\\[|\\gr| = | \\vu{r} - \\vu{r\u0026#39;} | \\tagl{1.24}\\] and a unit vector in the direction from \\( \\vec{r\u0026rsquo;} \\) to \\( \\vec{r} \\) is\n\\[\\vu{\\gr} = \\frac{\\gr}{|\\gr|} = \\frac{\\vec{r} - \\vec{r\u0026#39;}}{|\\vec{r} - \\vec{r\u0026#39;}|} \\tagl{1.25} \\] In Cartesian coordinates,\n\\[\\gr = (x - x\u0026#39;) \\vu{x} \u0026#43; (y-y\u0026#39;) \\vu{y} \u0026#43; (z-z\u0026#39;) \\vu{z} \\tagl{1.26}\\] \\[|\\gr| = \\sqrt{(x - x\u0026#39;)^2 \u0026#43; (y-y\u0026#39;)^2 \u0026#43; (z-z\u0026#39;)^2 } \\tagl{1.27}\\] \\[\\vu{\\gr} = \\frac{(x - x\u0026#39;) \\vu{x} \u0026#43; (y-y\u0026#39;) \\vu{y} \u0026#43; (z-z\u0026#39;) \\vu{z}}{\\sqrt{(x - x\u0026#39;)^2 \u0026#43; (y-y\u0026#39;)^2 \u0026#43; (z-z\u0026#39;)^2 }} \\] (from which you can appreciate the economy of the \\( \\gr \\) notation).\n1.1.5: How Vectors Transform # The definition of a vector as \u0026ldquo;a quantity with a magnitude and direction\u0026rdquo; is not altogether satisfactory: What precisely does \u0026ldquo;direction\u0026rdquo; mean? This may seem a pedantic question, but we shall soon encounter a species of derivative that looks rather like a vector, and we\u0026rsquo;ll want to know for sure whether it is one.\nYou might be inclined to say that a vector is anything that has three components that combine properly under addition. Well, how about this: We have a barrel of fruit that contains \\( N_x \\) pears, \\( N_y \\) apples, and \\( N_z \\) bananas. Is \\( \\vec{N} = N_x \\vu{x} + N_y \\vu{y} + N_z \\vu{z} \\) a vector? It has three components, and when you add another barrel with \\( M_x \\) pears, \\( M_y \\) apples, and \\( M_z \\) bananas the result is \\( N_x + M_x \\) pears, \\( N_y + M_y \\) apples, \\( N_z + M_z \\) bananas. So it does add like a vector. Yet it\u0026rsquo;s obviously not a vector, in the physicist\u0026rsquo;s sense of the word, because it doesn\u0026rsquo;t really have a direction. What exactly is wrong with it?\nThe answer is that \\( \\vec{N} \\) does not transform properly when you change coordinates. The coordinate frame we use to describe positions in space is of course entirely arbitrary, but there is a specific geometrical transformation law for converting vector components from one frame to another. Suppose, for instance, the \\( \\overline{x}, \\overline{y}, \\overline{z} \\) system is rotated by angle \\( \\phi \\), relative to \\( x, y, z \\), about the common \\( x = \\overline{x} \\) axes. From Fig. 1.15,\n\\[A_y = A \\cos \\theta, \\qquad A_z = A \\sin \\theta\\] while\n\\[\\begin{aligned} \\overline{A_y} \u0026amp; = A \\cos \\overline{\\theta} = A \\cos (\\theta - \\phi) = A (\\cos \\theta \\cos \\phi \u0026#43; \\sin \\theta \\sin \\phi) \\\\ \u0026amp; = \\cos \\phi A_y \u0026#43; \\sin \\phi A_z \\\\ \\overline{A_z} \u0026amp; = A \\sin \\overline{\\theta} = A \\sin (\\theta - \\phi) = A (\\sin \\theta \\cos \\phi - \\cos \\theta \\sin \\phi) \\\\ \u0026amp; = - \\sin \\phi A_y \u0026#43; \\cos \\phi A_z \\end{aligned}\\] We might express this conclusion in matrix notation:\n\\[\\begin{pmatrix} \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} \\cos \\phi \u0026amp; \\sin \\phi \\\\ - \\sin \\phi \u0026amp; \\cos \\phi \\end{pmatrix} \\begin{pmatrix} A_y \\\\ A_z \\end{pmatrix} \\tagl{1.29}\\] More generally, for rotation about an arbitrary axis in three dimensions, the transformation law takes the form\n\\[\\begin{pmatrix} \\overline{A_x} \\\\ \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} R_{xx} \u0026amp; R_{xy} \u0026amp; R_{xz} \\\\ R_{yx} \u0026amp; R_{yy} \u0026amp; R_{yz} \\\\ R_{zx} \u0026amp; R_{zy} \u0026amp; R_{zz} \\end{pmatrix} \\begin{pmatrix} A_x \\\\ A_y \\\\ A_z \\end{pmatrix} \\tagl{1.30}\\] or, more compactly,\n\\[\\overline{A_i} = \\sum_{j=1}^3 R_{ij} A_j \\tagl{1.31}\\] where index 1 stands for x, 2 for y, and 3 for z. The elements of the matrix R can be ascertained, for a given rotation, by the same sort of trigonometric arguments as we used for a rotation about the x axis. Now: Do the components of \\( \\vec{N} \\) transform this way? Of course not - it doesn\u0026rsquo;t matter what coordinates you use to represent positions in space; there are still just as many apples in the barrel. You can\u0026rsquo;t convert a pear into a banana by choosing a different set of axes, but you can turn in \\( A_x \\) into \\( \\overline{A_y} \\). Formally, then, a vector is any set of three components that transforms in the same manner as a displacement when you change coordinates. As always, displacement is the model for the behavior of vectors.\nBy the way, a (second-rank) tensor is a quantity with nine components, \\( T_{xx}, T_{xy}, T_{xz}, T_{yx}, \\ldots T_{zz} \\) which transform with two factors of \\( R \\):\n\\[\\begin{aligned} \\overline{T}_{xx} \u0026amp; = R_{xx}(R_{xx} T_{xx} \u0026#43; R_{xy} T_{xy} \u0026#43; R_{xz} T_{xz}) \\\\ \u0026amp; \u0026#43; R_{xy}(R_{xx} T_{yx} \u0026#43; R_{xy} T_{yy} \u0026#43; R_{xz} T_{yz}) \\\\ \u0026amp; \u0026#43; R_{xz}(R_{xx} T_{zx} \u0026#43; R_{xy} T_{zy} \u0026#43; R_{xz} T_{zz}), \\ldots \\end{aligned}\\] or, more compactly,\n\\[\\overline{T}_{ij} = \\sum_{k=1}^3 \\sum_{l=1} ^3 R_{ik} R_{jl} T_{kl} \\tagl{1.32}\\] In general, an n-th rank tensor has \\( n \\) indices and \\( 3^n \\) components, and transforms with \\( n \\) factors of \\( R \\). In this hierarchy, a vector is a tensor of rank 1, and a scalar is a tensor of rank zero.\n"},{"id":9,"href":"/r/notes/UWAA557/","title":"Physics of Fusion Plasmas","section":"Notes","content":" Physics of Fusion Plasmas # Course notes from AA557 at the University of Washington, Seattle\nRules of Thumb Review Statistical Mechanics Review of E\u0026amp;M Lagrange Multipliers Plasma Properties Wall-supported Plasma Collisions Oscillations "},{"id":10,"href":"/r/notes/UWAA543/","title":"Computational CFD","section":"Notes","content":" Computational CFD # Course notes from AA543 at the University of Washington, Seattle\nIntroduction to CFD Introduction Governing Equations Reduced Models Equation Types Panel Method Finite Differencing Finite Difference Algorithms Explicit Finite Difference Algorithms Finite Difference and Finite Volume Methods Implicit Algorithms Numerical Boundary Conditions "},{"id":11,"href":"/r/notes/griffiths/ch1-2/","title":"Differential Calculus","section":"Griffiths Introduction to Electrodynamics","content":" 1.2: Differential Calculus # 1.2.1: \u0026ldquo;Ordinary\u0026rdquo; Derivatives # Suppose we have a function of one variable, \\( f(x) \\). Question: what does the derivative \\( \\dv{f}{x} \\) do for us? Answer: It tells us how rapidly the function \\( f(x) \\) varies when we change the argument x by a tiny amount, \\( \\dd{x} \\)\n\\[\\dd{f} = \\left( \\dv{f}{x} \\right) \\dd{x} \\tagl{1.33}\\] In words: If we increment x by an infinitesimal amount \\( \\dd{x} \\), then \\( f \\) changes by an amount \\( \\dd{f} \\); the derivative is the proportionality factor. Foe example, in Fig. 1.17(a), the function varies slowly with x, and the derivative is correspondingly small. In Fig 1.17(b), f increases rapidly with x, and the derivative is large as you move away from \\( x = 0 \\). Geometrical interpretation: The derivative \\( \\dv{f}{x} \\) is the slope of the graph of f versus x.\n1.2.2: Gradient # Suppose, now, that we have a function of three variables-say, the temperature T (x, y, z) in this room. (Start out in one comer, and set up a system of axes; then for each point (x, y, z) in the room, T gives the temperature at that spot.) We want to generalize the notion of \u0026ldquo;derivative\u0026rdquo; to functions like T, which depend not on one but on three variables.\nA derivative is supposed to tell us how fast the function varies, if we move a little distance. But this time the situation is more complicated, because it depends on what direction we move: If we go straight up, then the temperature will prob- ably increase fairly rapidly, but if we move horizontally, it may not change much at all. In fact, the question \u0026ldquo;How fast does T vary?\u0026rdquo; has an infinite number of answers, one for each direction we might choose to explore.\nFortunately, the problem is not as bad as it looks. A theorem on partial derivatives states that\n\\[\\dd{T} = \\left( \\pdv{T}{x} \\right)\\dd{x} \u0026#43; \\left( \\pdv{T}{y} \\right) \\dd{y} \u0026#43; \\left( \\pdv{T}{z} \\right) \\dd{z} \\tagl{1.34}\\] This tells us how T changes when we alter all three variables by the infinitesimal amounts dx, dy, dz. Notice that we do not require an infinite number of derivatives - three will suffice: the partial derivatives along each of the three coordinate directions. \\( \\eqref{1.34} \\) is reminiscent of a dot product:\n\\[\\dd{T} = \\left( \\pdv{T}{x} \\vu{x} \u0026#43; \\pdv{T}{y} \\vu{y} \u0026#43; \\pdv{T}{z} \\vu{z} \\right)\\cdot ( \\dd{x} \\vu{x} \u0026#43; \\dd{y} \\vu{y} \u0026#43; \\dd{z} \\vu{z} \\\\ = (\\grad{T}) \\cdot (\\dd{\\vec{l}}) \\tagl{1.35}\\] where\n\\[\\grad{T} \\equiv \\pdv{T}{x} \\vu{x} \u0026#43; \\pdv{T}{y} \\vu{y} \u0026#43; \\pdv{T}{z} \\vu{z} \\tagl{1.36}\\] is the gradient of T. Note that \\( \\grad{T} \\) is a vector quantity, with three components; it is the generalized derivative we have been looking for. \\( \\eqref{1.35} \\) is the three-dimensional version of \\( \\eqref{1.33} \\). Geometrical interpretation of the Gradient: Like any vector, the gradient has magnitude and direction. To determine its geometrical meaning, let\u0026rsquo;s rewrite the dot product using Eq. 1.1\n\\[\\dd{T} = \\grad{T} \\cdot \\dd{\\vec{l}} = |\\grad{T}| |\\dd{\\vec{l}}| \\cos \\theta \\tagl{1.37}\\] where \\( \\theta \\) is the angle between \\( \\grad{T} \\) and \\( \\dd{\\vec{l}} \\). Now if we fix the magnitude \\( |\\dd{\\vec{l}}| \\) and search around in various directions, the maximum change in T evidently occurs when \\( \\theta = 0 \\) (for then \\( \\cos \\theta = 1 \\)). That is, for a fixed distance, dT is greatest when I move in the same direction as \\( \\grad{T} \\) . Thus:\nThe gradient \\( \\grad{T} \\) points in the direction of maximum increase of the function T. Moreover:\nThe magnitude \\( | \\grad{T} | \\) gives the slope (rate of increase) along this maximal direction Imagine you are standing on a hillside. Look all around you, and find the direction of steepest ascent. That is the direction of the gradient. Now measure the slope in that direction (rise over run). That is the magnitude of the gradient. (Here the function we\u0026rsquo;re talking about is the height of the hill, and the coordinates it depends on are positions-latitude and longitude, say. This function depends on only two variables, not three, but the geometrical meaning of the gradient is easier to grasp in two dimensions.) Notice from Eq. 1.37 that the direction of maximum descent is opposite to the direction of maximum ascent, while at right angles \\( (\\theta = 90^{\\circ}) \\) the slope is zero (the gradient is perpendicular to the contour lines). You can conceive of surfaces that do not have these properties, but they always have \u0026ldquo;kinks\u0026rdquo; in them, and correspond to non-differentiable functions.\nWhat would it mean for the gradient to vanish? If \\( \\grad{T} = 0 \\) at (x, y, z), then \\( \\dd{T} = 0 \\) for small displacements about the point (x, y, z). This is, then, a stationary point of the function T(x, y, z). It could be a maximum (a summit), a minimum (a valley), a saddle point (a pass), or a \u0026ldquo;shoulder.\u0026rdquo; This is analogous to the situation for functions of one variable, where a vanishing derivative signals a maximum, a minimum, or an inflection. In particular, if you want to locate the extrema of a function of three variables, set its gradient equal to zero.\nExample 1.3 # Q Find the gradient of \\( r = \\sqrt{x^2 + y^2 + z^2} \\) (the magnitude of the position vector) A \\[\\begin{aligned} \\grad{r} \u0026amp; = \\pdv{r}{x} \\vu{x} \u0026#43; \\pdv{r}{y} \\vu{y} \u0026#43; \\pdv{r}{z} \\vu{z} \\\\ \u0026amp; = \\frac{1}{2} \\frac{2x}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; z^2}}\\vu{x} \u0026#43; \\frac{1}{2} \\frac{2y}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; z^2}}\\vu{y} \u0026#43; \\frac{1}{2} \\frac{2z}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; z^2}}\\vu{z} \\\\ \u0026amp; = \\frac{x \\vu{x} \u0026#43; y \\vu{y} \u0026#43; z \\vu{z}}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; z^2}} = \\frac{\\vec{r}}{r} = \\vu{r} \\end{aligned}\\] Does this makes sense? Well, it says that the distance from the origin increases most rapidly in the radial direction, and that its rate of increase in that direction is 1\u0026hellip; just what you\u0026rsquo;d expect. 1.2.3: The Del Operator # The gradient has the formal appearance of a vector, \\( \\nabla \\), \u0026ldquo;multiplying\u0026rdquo; a scalar T:\n\\[\\grad{T} = \\left( \\vu{x} \\pdv{}{x} \u0026#43; \\vu{y} \\pdv{}{y} \u0026#43; \\vu{z} \\pdv{}{z} \\right) T \\tagl{1.38}\\] (For once, I write the unit vectors to the left, just so no one will think that this means \\( \\pdv{\\vu{x}}{x} \\) and so on, which would be zero since the coordinate directions are constant.) The term in parentheses is called del:\n\\[\\grad{} = \\vu{x} \\pdv{}{x} \u0026#43; \\vu{y} \\pdv{}{y} \u0026#43; \\vu{z} \\pdv{}{z} \\tagl{1.39}\\] Of course, del is not a vector, in the usual sense. Indeed, it doesn\u0026rsquo;t mean much until we provide it with a function to act upon. Furthermore, it does not \u0026ldquo;multiply\u0026rdquo; T; rather, it is an instruction to differentiate what follows. To be precise, then, we say that \\( \\grad{} \\) is a vector operator that acts upon T, not a vector that multiplies T.\nWith this qualification, though, \\( \\grad{} \\) mimics the behavior of an ordinary vector in virtually every way; almost anything that can be done with other vectors can also be done with \\( \\grad{} \\), if we merely translate \u0026ldquo;multiply\u0026rdquo; by \u0026ldquo;act upon.\u0026rdquo; So by all means take the vector appearance of \\( \\grad{} \\) seriously: it is a marvelous piece of notational simplification, as you will appreciate if you ever consult Maxwell\u0026rsquo;s original work on electromagnetism, written without the benefit of \\( \\grad{} \\).\nNow, an ordinary vector \\( \\vec{A} \\) can multiply in three ways:\nBy a scalar a: \\( \\vec{A}a \\) By a vector \\( \\vec{B} \\), via the dot product: \\( \\vec{A} \\cdot \\vec{B} \\) By a vector \\( \\vec{B} \\), via the cross product: \\( \\vec{A} \\cross \\vec{B} \\) Correspondingly, there are three ways the operator \\( \\grad{} \\) can act:\nOn a scalar function T: \\( \\grad{T} \\) (the gradient) On a vector function \\( \\vec{v} \\), via the dot product: \\( \\div{\\vec{v}} \\) (the divergence) On a vector function \\( \\vec{v} \\), via the cross product: \\( \\curl{\\vec{v}} \\) (the curl) We have already discussed the gradient. In the following sections we examine the other two vector derivatives: divergence and curl.\n1.2.4: The Divergence # From the definition of \\( \\grad{} \\) we construct the divergence:\n\\[\\begin{aligned} \\div{\\vec{v}} \u0026amp; = \\left( \\vu{x} \\pdv{}{x} \u0026#43; \\vu{y} \\pdv{}{y} \u0026#43; \\vu{z} \\pdv{}{z} \\right) \\cdot (v_x \\vu{x} \u0026#43; v_y \\vu{y} \u0026#43; v_z \\vu{z}) \\\\ \u0026amp; = \\pdv{v_x}{x} \u0026#43; \\pdv{v_y}{y} \u0026#43; \\pdv{v_z}{z} \\tagl{1.40} \\end{aligned}\\] Observe that the divergence of a vector function is itself a scalar.\nGeometrical interpretation: The name divergence is well chosen, for \\( \\div{\\vec{v}} \\) is a measure of how much the vector \\( \\vec{v} \\) spreads out (diverges) from the point in question. For example, the vector function in Fig. 1.18a has a large (positive) divergence (if the arrows pointed in, it would be a negative divergence), the function in Fig. 1.18b has zero divergence, and the function in Fig. 1.18c again has a positive divergence. (Please understand that \\( \\vec{v} \\) here is a function - there\u0026rsquo;s a different vector associated with every point in space. In the diagrams, of course, I can only draw the arrows at a few representative locations.)\nImagine standing at the edge of a pond. Sprinkle some sawdust or pine needles on the surface. If the material spreads out, then you dropped it at a point of positive divergence; if it collects together, you dropped it at a point of negative divergence. (The vector function \\( \\vec{v} \\) in this model is the velocity of the water at the surface - this is a two-dimensional example, but it helps give one a \u0026ldquo;feel\u0026rdquo; for what the divergence means. A point of positive divergence is a source, or \u0026ldquo;faucet\u0026rdquo;; a point of negative divergence is a sink, or \u0026ldquo;drain.\u0026rdquo;)\nExample 1.4 # Q Suppose the functions in Fig 1.18 are \\( \\vec{v_a} = \\vec{r} = x \\vu{x} + y \\vu{y} + z \\vu{z} \\), \\( \\vec{v_b} = \\vu{z} \\), and \\( \\vec{v_c} = z\\vu{z} \\). Calculate their divergences. A \\[\\div{\\vec{v_a}} = \\pdv{}{x} (x) \u0026#43; \\pdv{}{y} (x) \u0026#43; \\pdv{}{z} (z) = 1 \u0026#43; 1 \u0026#43; 1 = 3\\] As anticipated, this function has a positive divergence.\n\\[\\div{\\vec{v_b}} = \\pdv{}{x} (0) \u0026#43; \\pdv{}{0} (x) \u0026#43; \\pdv{}{z} (1) = 0 \u0026#43; 0 \u0026#43; 0 = 0\\] as expected.\n\\[\\div{\\vec{v_b}} = \\pdv{}{x} (0) \u0026#43; \\pdv{}{0} (x) \u0026#43; \\pdv{}{z} (z) = 0 \u0026#43; 0 \u0026#43; 1 = 1\\] 1.2.5: The Curl # From the definition of \\( \\grad{} \\) we construct the curl:\n\\[\\begin{aligned} \\curl{\\vec{v}} \u0026amp; = \\begin{vmatrix} \\vu{x} \u0026amp; \\vu{y} \u0026amp; \\vu{z} \\\\ \\pdv{}{x} \u0026amp; \\pdv{}{y} \u0026amp; \\pdv{}{z} \\\\ v_x \u0026amp; v_y \u0026amp; v_z \\end{vmatrix} \\\\ \u0026amp; \u0026#43; \\vu{x} \\left( \\pdv{v_z}{y} - \\pdv{v_y}{z} \\right) \u0026#43; \\vu{y} \\left( \\pdv{v_x}{z} - \\pdv{v_z}{x} \\right) \u0026#43; \\vu{z} \\left( \\pdv{v_y}{x} - \\pdv{v_x}{y} \\right) \\tagl{1.41} \\end{aligned}\\] Notice that the curl of a vector function is, like any cross product, a vector.\nGeometrical Interpretation: The name curl is also well chosen, for \\( \\curl{\\vec{v}} \\) is a measure of how much the vector \\( \\vec{v} \\) swirls around the point in question. Thus the three functions in Fig. 1.18 all have zero curl (as you can easily check for yourself), whereas the functions in Fig. 1.19 have a substantial curl, pointing in the z direction, as the natural right-hand rule would suggest. Imagine (again) you are standing at the edge of a pond. Float a small paddlewheel (a cork with toothpicks pointing out radially would do); if it starts to rotate, then you placed it at a point of nonzero curl. A whirlpool would be a region of large curl.\nExample 1.5 # Q Suppose the function sketched in Fig 1.19a is \\( \\vec{v_a} = -y \\vu{x} + x \\vu{y} \\), and that in Fig 1.19b is \\( \\vec{v_b} = x \\vu{y} \\). Calculate their curls. A \\[\\curl{\\vec{v_a}} = \\begin{vmatrix} \\vu{x} \u0026amp; \\vu{y} \u0026amp; \\vu{z} \\\\ \\pdv{}{x} \u0026amp; \\pdv{}{y} \u0026amp; \\pdv{}{z} \\\\ -y \u0026amp; x \u0026amp; 0 \\end{vmatrix} = 2 \\vu{z}\\] and\n\\[\\curl{\\vec{v_b}} = \\begin{vmatrix} \\vu{x} \u0026amp; \\vu{y} \u0026amp; \\vu{z} \\\\ \\pdv{}{x} \u0026amp; \\pdv{}{y} \u0026amp; \\pdv{}{z} \\\\ 0 \u0026amp; x \u0026amp; 0 \\end{vmatrix} = \\vu{z}\\] As expected, these curls point in the +z direction. (Incidentally, they both have zero divergence, as you might guess from the pictures: nothing is \u0026ldquo;spreading out\u0026rdquo;\u0026hellip; it just \u0026ldquo;swirls around.\u0026rdquo;)\n1.2.6: Product Rules # The calculation of ordinary derivatives is facilitated by a number of rules, such as the sum rule\n\\[\\dv{}{x} (f \u0026#43; g) = \\dv{f}{x} \u0026#43; \\dv{g}{x}\\] the rule for multiplying a constant:\n\\[\\dv{}{x} (kf) = k \\dv{f}{x}\\] the product rule:\n\\[\\dv{}{x}(fg) = f \\dv{g}{x} \u0026#43; g \\dv{f}{x}\\] and the quotient rule\n\\[\\dv{}{x} \\left( \\frac{f}{g} \\right) = \\frac{g \\dv{f}{x} - f \\dv{g}{x}}{g^2} \\] Similar relations hold for the vector derivatives. Thus,\n\\[\\grad{(f \u0026#43; g)} = \\grad{f} \u0026#43; \\grad{g}, \\qquad \\div{(\\vec{A} \u0026#43; \\vec{B})} = (\\div{\\vec{A}}) \u0026#43; (\\div{\\vec{B}})\\] \\[\\curl{(\\vec{A} \u0026#43; \\vec{B})} = (\\curl{\\vec{A}}) \u0026#43; (\\curl{\\vec{B}})\\] and\n\\[\\grad{(kf)} = k \\grad f, \\quad \\div{(k\\vec{A})} = k (\\div{\\vec{A}}), \\quad \\curl{(k\\vec{A})} = k(\\curl{\\vec{A}})\\] as you can check for yourself. The product rules are not quite so simple. There are two ways to construct a scalar as the product of two functions:\n\\[fg \\quad \\text{(product of two scalar functions),} \\\\ \\vec{A} \\cdot \\vec{B} \\quad \\text{(dot product of two vector functions),}\\] and two ways to make a vector:\n\\[f \\vec{A} \\quad \\text{(scalar times vector),} \\\\ \\vec{A} \\cross \\vec{B} \\quad \\text{(cross product of two vectors).}\\] Accordingly, there are six product rules, two for gradients:\n\\[\\grad{(fg)} = f \\grad{g} \u0026#43; g \\grad{f} \\tag{i}\\] \\[\\grad( \\vec{A} \\cdot \\vec{B}) = \\vec{A} \\cross (\\curl{\\vec{B}}) \u0026#43; \\vec{B} \\cross (\\curl{\\vec{A}}) \u0026#43; (\\vec{A} \\cdot \\grad{})\\vec{B} \u0026#43; (\\vec{B} \\cdot \\grad{}) \\vec{A} \\tag{ii}\\] two for divergences:\n\\[\\div{(f\\vec{A})} = f(\\div{\\vec{A}}) \u0026#43; \\vec{A} \\cdot (\\grad{f}) \\tag{iii}\\] \\[\\div{(\\vec{A} \\cross \\vec{B})} = \\vec{B} \\cdot (\\curl{\\vec{A}}) - \\vec{A} \\cdot (\\curl{\\vec{B}}) \\tag{iv}\\] and two for curls:\n\\[\\curl{(f\\vec{A})} = f(\\curl{\\vec{A}}) - \\vec{A} \\cross (\\grad{f}) \\tag{v}\\] \\[\\curl{(\\vec{A} \\cross \\vec{B})} = (\\vec{B} \\cdot \\grad{})\\vec{A} - (\\vec{A} \\cdot \\grad{}) \\vec{B} \u0026#43; \\vec{A}(\\div{\\vec{B}}) - \\vec{B}(\\div{\\vec{A}}) \\tag{vi}\\] If there\u0026rsquo;s anything in this chapter that\u0026rsquo;s worth memorizing, it is this set of identities. The proofs come straight from the product rule for ordinary derivatives. For instance,\n\\[\\begin{aligned} \\div{(f\\vec{A})} \u0026amp; = \\pdv{}{x} (f A_x) \u0026#43; \\pdv{}{y} (fA_y) \u0026#43; \\pdv{}{z}(f A_z) \\\\ \u0026amp; = \\left( \\pdv{f}{x} A_x \u0026#43; f \\pdv{A_x}{x} \\right) \u0026#43; \\left( \\pdv{f}{y} A_y \u0026#43; f \\pdv{A_y}{y} \\right) \u0026#43; \\left( \\pdv{f}{z}A_z \u0026#43; f \\pdv{A_z}{z} \\right) \\\\ \u0026amp; = (\\grad{f}) \\cdot \\vec{A} \u0026#43; f(\\div{\\vec{A}}) \\end{aligned}\\] It is also possible to formulate three quotient rules:\n\\[\\grad \\left( \\frac{f}{g} \\right) = \\frac{g \\grad{f} - f \\grad{g}}{g^2} \\] \\[\\div{\\left( \\frac{\\vec{A}}{g} \\right)} = \\frac{g(\\div{\\vec{A}}) - \\vec{A} \\cdot (\\grad{g})}{g^2} \\] \\[\\curl \\left( \\frac{\\vec{A}}{g} \\right) = \\frac{g(\\curl{\\vec{A}}) \u0026#43; \\vec{A} \\cross (\\grad{g})}{g^2} \\] However, since these can be obtained quickly from the corresponding product rules, there is no point in listing them separately.\n1.2.7: Second Derivatives # The gradient, the divergence, and the curl are the only first derivatives we can make with \\( \\grad \\); by applying \\( \\grad \\) twice, we can construct five species of second derivatives. The gradient \\( \\grad{T} \\) is a vector, so we can take the divergence and curl of it:\nDivergence of gradient: \\( \\div (\\grad{T}) \\) Curl of gradient: \\( \\curl (\\grad{T}) \\) The divergence \\( \\div{\\vec{v}} \\) is a scalar - all we can do is take its gradient:\nGradient of divergence: \\( \\grad (\\div{\\vec{v}}) \\) The curl \\( \\curl \\vec{v} \\) is a vector, so we can take its divergence and curl:\nDivergence of curl: \\( \\div (\\curl \\vec{v}) \\) Curl of curl: \\( \\curl (\\curl \\vec{v}) \\) This exhausts the possibilities, and in fact not all of them give anything new. Let\u0026rsquo;s consider them one at a time:\n\\[\\begin{aligned} \\div (\\grad{T}) \u0026amp; = \\left( \\vu{x} \\pdv{}{x} \u0026#43; \\vu{y} \\pdv{}{y} \u0026#43; \\vu{z} \\pdv{}{z} \\right) \\cdot \\left( \\pdv{T}{x} \\vu{x} \u0026#43; \\pdv{T}{y} \\vu{y} \u0026#43; \\pdv{T}{z} \\vu{z} \\right) \\\\ \u0026amp; = \\frac{\\partial ^2 T}{\\partial x^2} \u0026#43; \\frac{\\partial ^2 T}{\\partial y^2} \u0026#43; \\frac{\\partial ^2 T}{\\partial z^2} \\end{aligned} \\tagl{1.42}\\] This object, which we write as \\( \\laplacian T \\) for short, is called the Laplacian of T; we shall be studying it in great detail later on. Notice that the Laplacian of a scalar T is a scalar. Occasionally we shall speak of the laplacian of a vector, \\( \\laplacian \\vec{v} \\). By this we mean a vector quantity whose x-component is the Laplacian of \\( v_x \\), and so in:\n\\[\\laplacian \\vec{v} \\equiv (\\laplacian v_x) \\vu{x} \u0026#43; (\\laplacian v_y) \\vu{y} \u0026#43; (\\laplacian v_z) \\vu{z} \\tagl{1.43}\\] This is nothing more than a convenient extension of the meaning of \\( \\laplacian \\).\nThe curl of a gradient is always zero:\n\\[\\curl (\\grad{T}) = 0 \\tagl{1.44}\\] This is an important fact, which we shall use repeatedly; you can easily prove it from the definition of \\( \\grad \\), hinging on the equality of cross-derivatives:\n\\[\\pdv{}{x} \\left( \\pdv{T}{y} \\right) = \\pdv{}{y} \\left( \\pdv{T}{x} \\right) \\tagl{1.45}\\] \\( \\grad(\\div{\\vec{v}}) \\) seldom occurs in physical applications, and it has not been given any special name of its own - it\u0026rsquo;s just the gradient of the divergence. Notice that \\( \\grad (\\div \\vec{v}) \\) is not the same as the Laplacian of a vector: \\( \\laplacian \\vec{v} = (\\div \\grad) \\vec{v} \\neq \\grad( \\div \\vec{v}) \\).\nThe divergence of a curl, like the curl of a gradient, is always zero:\n\\[\\div (\\curl \\vec{v}) = 0 \\tagl{1.46}\\] You can prove this for yourself.\nAs you can check from the definition of \\( \\grad \\):\n\\[\\curl (\\curl \\vec{v}) = \\grad(\\div \\vec{v}) - \\laplacian \\vec{v} \\tagl{1.47}\\] So curl-of-curl gives nothing new; the first term is just gradient of divergence, and the second is the Laplacian (of a vector). (In fact, this is often used to define the Laplacian of a vector, in preference to \\( \\eqref{1.43} \\) which makes explicit reference to Cartesian coordinates.)\nReally, then, there are just two kinds of second derivatives: the Laplacian (which is of fundamental importance) and the gradient-of-divergence (which we seldom encounter). We could go through a similar ritual to work out third derivatives, but fortunately second derivatives suffice for practically all physical applications.\nA final word on vector differential calculus: It all flows from the operator \\( \\grad \\), and from taking seriously its vectorial character. Even if you remembered only the definition of \\( \\grad \\), you could easily reconstruct all the rest.\n"},{"id":12,"href":"/r/notes/UWAA560/02-diagnostic-considerations/","title":"General Diagnostic Considerations","section":"Plasma Diagnostics","content":" General Diagnostic Considerations # \\[\\] First, let us categorize some broad categories of items to consider when we think about plasma diagnostics by asking a few questions:\nWhich plasma properties do we want to measure?\nDensities for individual species, velocity, temperature (moment measurements). In the case where a plasma is not thermal (Maxwellian), we may need to measure species distribution function \\( f_s (\\vec v) \\) Plasma content, i.e. species concentrations, \\( Z_{eff} \\) (which is a determining factor in resistivity and radiation power) Plasma currents, current densities, electromagnetic fields. Note: some properties are inferred from other properties, e.g. \\[\\vec j = \\sum_s q_s n_s \\vec v_s = \\frac{1}{\\mu_0} \\curl \\vec B \\quad \\text{if} \\quad \\tau \\gg L/c\\] \\[\\div \\vec E = \\frac{1}{\\epsilon_0} \\sum_s q_s n_s\\] Global measurements: input power, radiative power, energy confinement time, thrust, fusion reaction rate/yield. What spatial resolution is needed?\nSome plasma diagnostics are spatially integrated, and some are chord-integrated, single-point measurements, 1D spatial resolution \\( f(r) \\), 2D \\( f(r, z) \\), 3D \\( f(x, y, z) \\), and 6D \\( f(\\vec x, \\vec v) \\) How many points in space are sufficient to resolve? 4, 10, 1000\u0026hellip; Typically, \u0026ldquo;spatially-resolved\u0026rdquo; means a measurement at a particular location (not volume-integrated or chord-integrated). What temporal resolution is needed?\nTime-integrated over pulse, once/twice during a pulse, gate time (how long is the shutter open?), continuous acquisition, sample rate and bandwidth \u0026ldquo;Temporally-resolved\u0026rdquo; includes all these except time-integrated measurements What other things do we need to take into consideration?\nDoes measurement require plasma access? Ports are usually small and expensive. What if the plasma is contained in a flux conserver, and what if the ports are far from the plasma? Some diagnostics require multiple ports and may be constrained to be coaxial (laser diagnostics) or orthogonal (scattering measurements). Data analysis: Many properties are inferred from measurements indirectly, data has to be processed to extract plasma properties. Analysis/processing often applies assumptions which can increase the uncertainty of the measurement. Suitability of detectors, lasers, and circuitry of detector itself. Often introduce sample rate, resolution, uncertainty limitations. Signal noise from detector and cabling Plasma reproducibility, shot-to-shot variability The limitations and challenges with making experimental measurements of plasmas can be compensated by computational modeling, which have different, and often complementary, limitations. NIF is a good example: each pulse has a dedicated set of simulations for how much energy is deposited, how the target implodes, etc. Measurements (neutron yield, X-ray measurements, etc.) are compared with computational models to find agreement and to understand what happened in the experiment itself.\nTypes of Plasma Diagnostics # Passive Diagnostics # Passive diagnostics do not interact with the plasma and are non-perturbative. Self-emission spectroscopy, imaging, and X-ray and neutron detectors are examples.\nActive Diagnostics # Active diagnostics are external means to interact with the plasma and measure its response. These are further divided by the extent of perturbation. For an example, laser scattering at lower power often doesn\u0026rsquo;t change the plasma properties appreciably, but Thompson scattering can require high enough power to significantly perturb the plasma.\nSafety Concerns # There are three main safety concerns when performing plasma experiments and diagnostics:\nHigh Voltage. We often plasmas by applying a high voltage to a neutral gas. Many diagnostics use high voltages, be it applied to detectors or in laser systems. Technically voltages as low as 30V can be lethal under the wrong circumstances. Lasers. Visible and infrared like \\( CO_2 \\) and \\( YAG \\). Classified by power level and frequency. Class 4 \\( Nd:YAG \\) and \\( HeNe \\) are commonly used in the lab. UV Light. Plasmas emit in broadband (Bremsstrahlung and line radiation mostly), including UV radiation (100nm - 400nm). Can cause \u0026ldquo;sun burns\u0026rdquo; and retina damage (\u0026ldquo;snow blindness\u0026rdquo;). Most things will block UV light, but often we want the UV radiation for diagnostics and use fused silica windows which transmit UV light. Calibration light/lamps also emit UV radiation. Noise and Signal Contamination # Many continuous time-resolved diagnostics use detector transducers, sensors, located near the plasma to produce an electrical signal. The signal is recorded by an oscilloscope or digitizer in this sort of setup:\n(Missing diagram)\nPlasmas are often formed through electrical discharges or RF waves, the plasmas often produce large electric fields, currents, and magnetic fields, sometimes with high fluctuation frequencies. How do we measure the plasma properties without contaminating the signals with noise, which is any unwanted signal that couples to the detector circuit.\nPlasma diagnostics are susceptible to two primary types of noise:\nElectrostatic - typically generated through capacitive coupling (Electro)magnetic - comes about from inductive coupling Capacitive Coupling # Proximity of sensor to plasma generates electrostatic current \\( I_{es} \\) through capacitive coupling.\n(Missing diagram)\nInductive Coupling # Inductive coupling, as the name implies, operates through the inductive principle. It can occur as a result of ground loops which can be of a large area.\n\\[\\dot \\Phi = \\int \\pdv{\\vec B}{t} \\cdot \\dd \\vec s = - \\oint \\vec E \\cdot \\dd l = - V_{emf}\\] Since the loop includes the entire setup, even a low \\( B \\) can produce large \\( V_{emf} \\), which is across the oscilloscope we\u0026rsquo;re reading. Now the plasma produces a magnetic field which couples to the detector circuit. The magnetic field generated by the plasma effectively acts as a transformer which produces a current in the measurement circuit. In contrast to capacitive coupling, with inductive coupling the \\( I_{em} \\) current doesn\u0026rsquo;t have an alternate path, so must be same magnitude across the whole circuit. We can use these kinds of features to deduce the kind of noise being generated.\nMinimizing Noise # Minimization of noise requires careful consideration of grounding and shielding.\nSingle-point \u0026ldquo;star\u0026rdquo; grounding. Connecting all devices to a single ground point (\u0026ldquo;holy ground\u0026rdquo;) can eliminate ground loop and prevent ground currents Minimize loop areas of signal cables and detectors by using twisted-pair wire or coaxial cables.\nMagnetic shielding: Enclose detectors in mu-metal (high-permeability \\( \\mu \\) material), often some sort of \\( FeNi \\) compound. This shields pulsed magnetic fields (and even static fields, if you can spin the detectors). Sometimes necessary for cameras, photomultiplier tubes, CRT, and other sorts of vacuum electronics where loop area can not be reduced.\nElectrostatic shielding: E.g Faraday cage, can reduce capacitive coupling if shield itself is well-grounded. As the plasma creates electric field which charges capacitive coupling, it charges the grounded shield and flows to the star-point ground. Now the capacitive coupling does not drive current through the diagnostic circuit itself.\nData Sampling and Errors # Given a continuous signal \\( y(t) \\) that is sampled at a fixed rate such that we have \\( y_n = y(t_n) \\), what is the sufficient sample rate to guarantee loss-less reconstruction of \\( y(t) \\)? The Nyquist-Shannon sampling theorem states that: If a signal \\( y(t) \\) contains no frequencies higher than \\( B \\) Hz, it is completely determined by giving its coordinates at a series of points spaced \\( 1/2B \\) seconds apart. An equivalent statement to define the bandlimit is \u0026ldquo;any frequency \\( B \\), such that \\( B \u0026lt; f_x ^N /2 \\), where \\( f_s ^N \\) is the sampling rate for perfect construction.\u0026rdquo; The threshold established by \\( f_s^N/2 \\) is called the Nyquist frequency. \\( f_s ^N \\) is confusingly called the Nyquist rate, and is the rate at which you need to sample the signal.\nSignal components higher than the Nyquist frequency produce aliasing errors. Features like delta functions and step functions contain all frequencies. How do we prevent these from contaminating our signal, or how do we set the band limit maximum? It can be set by the detector response rate, or the detector circuit, or by introducing a low-pass filter:\nThe response (Bode plot) of an RC low pass filter looks like the following, with a cutoff frequency given by \\( \\omega_0 = \\frac{1}{RC} \\)\nCounting Errors # When we are counting events (from photomultiplier tubes or spectoscopes, for example), rare events such as photon or neutron events are modeled by a Poisson distribution.\n\\[P(n) = \\frac{m^n e^{-m}}{n!}\\] for an event expected to happen \\( m \\) times on average where \\( n \\) is the observed event count. \\( P(n) \\) is the probability of observing \\( n \\) counts. Earthquakes and flooding are similarly modeled. If you look at the mean value of a Poisson distribution, you get\n\\[\\overline{n} = \\sum_n n P(n) = m\\] The variance is\n\\[V(n) = \\overline{(m - n)^2} = \\sum_n (m - n)^2 P(n) = m\\] so \\( \\sigma = \\sqrt{m} \\)\nWhen you report the number of counts or observations, you need to include the counting error as \\( n = m \\pm \\sqrt m \\), or for large \\( n \\), \\( m \\approx n \\pm \\sqrt n \\).\nCategories of Plasma Diagnostics # One may group diagnostics according to the plasma properties being measured. However, this can make diagnostic reporting difficult because many diagnostics measure multiple parameters, or a combination or convolution of various parameters. So instead, we will categorize them according to the plasma phenomenon that the diagnostic exploits:\nMagnetic field Electrostatic probes Index of refraction Self-emission from free electrons (broadband) Self-emission from bound electrons (line radiation) Laser scattering/interactions Neutron measurements Some basic questions to keep in mind when thinking about how to make a measurement are:\nWhat is being measured? How is the measured quantity related to the desired plasma property? Which assumptions are necessary to close that relationship? And how do those assumptions affect the confidence in the measurement? How accurate is the measurement? What spatial and temporal resolution is needed and provided? For example, index of refraction measurements are chord-integrated, but could be time-resolved or spatially resolved. What are the trade-offs? What is the dynamic range of the measurement? How does the diagnostic perturb the plasma? "},{"id":13,"href":"/r/notes/UWAA543/ch20-2/","title":"Governing Equations","section":"Computational CFD","content":" Governing Equations # Continuity\n\\[\\pdv{\\rho}{t} \u0026#43; v \\cdot \\grad \\rho = - \\rho \\grad \\cdot v\\] Momentum:\n\\[\\rho \\left( \\pdv{v}{t} \u0026#43; v \\cdot \\grad v\\right) \u0026#43; \\grad p - \\frac{1}{\\mu_0} (\\curl \\vec B) \\cross \\vec B = \\div \\vec T \\] \\[\\vec T = \\text{ viscous stress tensor }\\] \\[\\frac{1}{\\mu_0} \\curl \\vec B = \\vec j = \\text{ current density }\\] Magnetic Field\n\\[\\pdv{B}{t} - \\curl ( v \\cross B) = - \\curl \\left[ \\frac{\\eta}{\\mu_0} (\\curl \\vec B) \\right]\\] \\[\\eta = \\text{ electrical resistivity }\\] Energy:\n\\[\\begin{aligned} \\rho \\left( \\pdv{\\mathscr{E}}{t} \u0026#43; v \\cdot \\grad \\mathscr{E} \\right) \u0026#43; p \\div v \u0026amp; = \\div \\left( v \\cdot \\vec T \\right) \u0026amp; \\\\ \u0026amp; \u0026#43; \\div ( k \\div T) \\qquad \\text{ (thermal conduction) }\u0026amp; \\\\ \u0026amp; \u0026#43; \\frac{\\eta}{\\mu_0 ^2} (\\curl B )^2 \\qquad \\text{ (resistive heating) } \u0026amp; \\end{aligned}\\] \\[\\mathscr{E} = \\text{ specific internal energy}\\] \\[\\rho \\mathscr{E} = \\frac{p}{\\gamma - 1}\\] These are the governing equations for magnetohydrodynamics (MHD). It\u0026rsquo;s important to understand where they come from. Starting from the Boltzmann equation, which describes how a distribution evolves over time\n\\[\\pdv{f}{t} \u0026#43; \\vec v \\cdot \\pdv{f}{\\vec x} \u0026#43; \\vec a \\pdv{f}{\\vec v} = \\left. \\pdv{f}{t} \\right|_{\\text{collisions}}\\] We take moments of \\( \\vec v \\) of the Boltzmann equation to get all of the above:\n\\[\\vec u (\\vec x , t) = \\frac{\\int \\vec v f( \\vec x , \\vec v, t) \\dd \\vec v}{\\int f(\\vec x, \\vec v, t) \\dd \\vec v}\\] The 0th moment gives the conservation of mass relation, since . The Knudsen number \\( Kn \\) is the mean free path between collisions\n\\[Kn = \\frac{ \\lambda_{mfp}}{L}\\] When the Knudsen number is sufficiently small, these equations work well. When Kn becomes larger than the simulation volume, then the equations of fluid dynamics break down and we get non-Maxwellian phenomena. Also, if \\( L \\) is very small (for example, a powerful shock), we\u0026rsquo;ll probably miss something.\nWhen taking moments, notice that each moment will introduce the next moment\n\\[\\pdv{}{t} \\int \\vec v ^{n} f \\dd \\vec v \u0026#43; \\int \\vec v ^{n\u0026#43;1} f \\dd \\vec v\\] We get to stop when we define a closure relation that relates moments to higher moments. For example, the ideal gas law is a closure relation, since it relates temperature (\\( \\mathscr{E} \\) to density. Implicit in these equations are the closure relations that we need to keep in mind.\nIf we drop all terms with B and the magnetic field equation, then we get the Navier-Stokes equations. These equations are a system and must be solved self-consistently. The RHS of each equation represents non-ideal contributions due to self collisions. Those include viscosity, resistivity, and conduction.\nUsing vector identities and the fact that \\( \\div B = 0 \\), we can re-write the equations in conservative form.\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho v) = 0\\] \\[\\pdv{(\\rho v)}{t} \u0026#43; \\div \\left[\\rho v v - \\frac{BB}{\\mu_0} \u0026#43; \\vec 1 \\left( \\frac{B^2}{2 \\mu_0} \u0026#43; p \\right) \\right] = \\div \\vec T\\] \\[\\pdv{B}{t} \u0026#43; \\div [v B - B v] = \\div \\left( \\frac{\\eta}{\\mu_0} \\grad B \\right)\\] \\[\\pdv{e}{t} \u0026#43; \\div \\left[ \\left( e \u0026#43; p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) v - \\frac{(B \\cdot v) B}{\\mu_0} \\right] = \\div \\left[ v \\cdot \\vec T \u0026#43; k \\grad T - \\frac{\\eta}{\\mu_0} ( \\curl B) \\cross B \\right]\\] where the total energy \\( e = \\frac{p}{\\gamma - 1} + \\frac{rho v^2}{2} + \\frac{B^2}{2 \\mu_0 } \\).\nAs a quick refresher on dyad notation, we say \\( \\vec A \\vec B \\) is notation for\n\\[\\vec A \\vec B = \\begin{bmatrix} A_x B_x \u0026amp; A_x B_y \u0026amp; A_x B_z \\\\ A_y B_x \u0026amp; A_y B_y \u0026amp; \\ldots \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\end{bmatrix}\\] For that reason, sometimes it\u0026rsquo;ll be written as a matrix multiplication \\( \\vec A \\vec B^T \\)\n\\[\\vec A \\vec B^T = \\begin{bmatrix} A_x \\\\ A_y \\\\ A_z \\end{bmatrix} \\begin{bmatrix} B_x \u0026amp; B_y \u0026amp; B_z \\end{bmatrix}\\] In compact form,\n\\[\\pdv{Q}{t} \u0026#43; \\div \\vec F = \\div \\vec F_p \\quad \\text{(parabolic)}\\] or\n\\[\\pdv{Q}{t} \u0026#43; \\pdv{\\vec F}{x} \u0026#43; \\pdv{\\vec G}{y} \u0026#43; \\pdv{\\vec H}{z} = \\pdv{\\vec F_p}{x} \u0026#43; \\pdv{\\vec G_p}{y} \u0026#43; \\pdv{\\vec H_p}{z}\\] Now, we can re-write the governing equations in a form that\u0026rsquo;s easier to work with, called conservation form. This form involves relating the time derivative of a quantity we are interested in to the divergence of a flux:\n\\[\\pdv{Q}{t} = - \\div \\vec F\\] Our governing relations are not in conservation form, but they can be. Take the conservation of mass relation\n\\[\\pdv{\\rho}{t} = - \\div ( \\rho \\vec v)\\] Writing our expressions in conservation form helps to ensure that numerical codes replicate real physics. If our simulation does not conserve something like mass or momentum due to some numerical errors, we\u0026rsquo;re not likely to get a physical result from the simulation.\nOn to the conservation of momentum, combining with our mass conservation relation\n\\[\\pdv{}{t}(\\rho \\vec v) \u0026#43; \\div \\left[ \\rho \\vec v \\vec v - \\frac{\\vec B \\vec B}{\\mu_0} \u0026#43; (\\frac{B^2}{2 \\mu_0} \u0026#43; p ) \\vec 1 \\right] = \\div \\vec T\\] and the magnetic field equation\n\\[\\pdv{\\vec B}{t} \u0026#43; \\div \\left[ \\vec v \\vec B - \\vec B \\vec v \\right] = \\div \\left[ \\frac{\\eta}{\\mu_0} \\grad \\vec B \\right]\\] and the energy relation\n\\[\\pdv{e}{t} \u0026#43; \\div \\left[ \\left(e \u0026#43; p \u0026#43; \\frac{B^2}{2 \\mu_0}\\right) \\vec v - \\frac{( \\vec B \\cdot \\vec v) \\vec B}{\\mu_0} \\right] = \\div \\left[ \\ldots \\right]\\] \\[\\text{ total energy } \\quad e = \\frac{p}{\\gamma - 1} \u0026#43; \\frac{\\rho v^2}{2} \u0026#43; \\frac{B^2}{2\\mu_0}\\] So all of our governing equations can be written in conservation form. We can get to a more compact form which looks like\n\\[\\pdv{\\overline{Q}}{t} \u0026#43; \\div \\overline{\\overline{F}} = \\div \\overline{\\overline{F_p}}\\] where \\( \\overline{Q} \\) is just the vector containing all of our scalar conserved quantities, and \\( \\overline{\\overline{F} }\\) is a vector of all of our flux vectors.\n\\[\\overline{Q} = \\begin{bmatrix} \\rho \\\\ \\rho \\mu \\\\ \\rho v \\\\ \\rho w \\\\ B_x \\\\ B_y \\\\ B_z \\\\ e \\end{bmatrix}\\] \\[\\overline{\\overline{F}} = \\vec F \u0026#43; \\vec G \u0026#43; \\vec H = \\overline{\\overline{F_x}} \u0026#43; \\overline{\\overline{F_y}} \u0026#43; \\overline{\\overline{F_z}}\\] \\[\\overline{F} = \\begin{bmatrix} \\rho u \\\\ \\rho u^2 - B^2 _x / 2 \\mu \u0026#43; p \\\\ \\rho u v - B_x B_y / \\mu_0 \\\\ \\rho u w - B_x B_z / \\mu \\\\ 0 \\\\ u B_y - B_x v \\\\ u B_z - B_x w \\\\ (e \u0026#43; p \u0026#43; B^2 / 2 \\mu_0 )u - \\frac{\\vec B \\cdot \\vec v}{\\mu_0} B_x \\end{bmatrix} = \\text{ flux vector (x-component) }\\] We can similarly calculate the other factors \\( \\overline{G} \\) and \\( \\overline{H} \\). Now let\u0026rsquo;s move on to \\( \\overline{F_p} \\), the parabolic flux vector (as opposed to the hyperbolic fluxes \\( \\overline{F} \\)). These fluxes typically come from \u0026ldquo;non-ideal\u0026rdquo; effects.\n\\[\\overline{F_p} = \\begin{bmatrix} 0 \\\\ 2 \\mu \\left(\\pdv{u}{x} - \\frac{ \\div \\vec v}{3} \\right) \\\\ \\mu \\left( \\pdv{\\mu}{y} \u0026#43; \\pdv{v}{x} \\right) \\\\ \\mu \\left( \\pdv{u}{z} \u0026#43; \\pdv{w}{z} \\right) \\\\ \\frac{\\eta}{\\mu_0} \\pdv{B_x}{x} \\\\ \\frac{\\eta}{\\mu_0} \\pdv{B_y}{x} \\\\ \\frac{\\eta}{\\mu_0} \\pdv{B_z}{x} \\\\ u \\vec T _{xx} \u0026#43; v T_{yx} \u0026#43; w T_{zx} \u0026#43; k \\pdv{T}{x} - \\frac{\\eta}{\\mu_0} \\left[ ( \\curl B)_y B_z - ( \\curl B)_z B_y \\right] \\end{bmatrix}\\] 20.2.2 The Euler Equations # If we have a fluid which is unmagnetized, inviscid, and thermal conductivity is zero, we are left with the Euler equations\n\\[\\pdv{Q}{t} \u0026#43; \\pdv{F}{x} \u0026#43; \\pdv{G}{y} \u0026#43; \\pdv{H}{z} = 0\\] or, in 2 dimensions\n\\[\\pdv{Q}{t} \u0026#43; \\pdv{F}{x} \u0026#43; \\pdv{G}{y} = 0\\] \\[Q = \\begin{bmatrix} \\rho \\\\ \\rho u \\\\ \\rho v \\\\ e \\end{bmatrix}\\] \\[F = \\begin{bmatrix} \\rho u \\\\ \\rho u ^2 \u0026#43; p \\\\ \\rho u v \\\\ u (e \u0026#43; p) \\end{bmatrix}\\] \\[G = \\begin{bmatrix} \\rho v \\\\ \\rho u v \\\\ \\rho v^2 \u0026#43; p \\\\ v(e \u0026#43; p) \\end{bmatrix}\\] The Euler equations are the inviscid Navier-Stokes equations, and they exhibit many of the nonlinearities that the full N-S and other more complicated flows do, but with a much simpler form. Solution of the Euler equations represents an important advance for any algorithm. Some such important nonlinearities are shockwaves, contact discontinuities, rarefications. It turns out that most of those parabolic fluxes make the situation easier from a CFD standpoint, because they are dissipating effects which actually serve to suppress instabilities.\n"},{"id":14,"href":"/r/notes/UWAA545/02-plasma-models/","title":"Plasma Models","section":"Computational Methods For Plasmas","content":" Plasma Models # \\[\\] First, let\u0026rsquo;s get a working definition of \u0026ldquo;plasma\u0026rdquo;: \u0026ldquo;a quasi-neutral gas of charged and neutral particles that exhibit collective behavior.\u0026rdquo; Many of the approaches we will describe will also apply to non-neutral plasmas. Plasmas are composed of particles (electrons, ions, neutrals) which interact through electric and magnetic fields and through collisions. Therefore, the plasma can be modeled as individual particles.\nPlasmas exhibit collective behavior due to the long-range forces from EM field interactions. As a consequence of this collective behavior, the plasma can alternately be modeled as an electrically conducting fluid. These seemingly conflicting models can be married together by taking a statistical approach to convert from particles to a continuous distribution, or by defining fluid elements to treat as particles.\nWe will see that these statements are partially correct and partially incorrect. The accuracy of a particular sort of model depends on the length scales and time scales of interest, which are themselves determined by the plasma parameters. Consider a typical tokamak plasma which has a lifetime on the order of \\( 10^2 \\) seconds and electron plasma period on the order of \\( 10^{-12} \\) seconds. The physical dimensions are on the order of \\( 1 m \\) with a Debye length of \\( 10^{-5} m \\). We have quite a large range of parameters to handle!\nSometimes the large range of scales allows a separation between \u0026ldquo;fast\u0026rdquo; and \u0026ldquo;slow\u0026rdquo; dynamics, but generally this presents a challenge because the scales interact with each other (challenging multi-scale problem). At very fast time scales, the slower effects can be roughly approximated as equilibria.\nHierarchy of Plasma Time Scales # In broad terms, we can define a hierarchy of plasma time scales.\n\\( \\omega_{pe} \\) \\( (10^{-12} s) \\) If we consider the fastest time scale, we think of the electron plasma frequency. It is associated with high frequency electromagnetic waves (~light speed). Associated with this time scale, but perhaps a little slower, we have electron dynamics, wave propagation (both electrostatic and electromagnetic).\n\\( \\omega_{p, i} \\) \\( (10^{-9} s) \\) The next-fastest time scale is the ion plasma frequency. It is associated with ion oscillations, which become important in magnetized plasmas where electrons can move along magnetic fields in response to charge separation.\n\\(k v_A (10^{-9} - 10^{-6} s) \\) Ion cyclotron waves: electrostatics (Bernstein waves), electromagnetic (Alfvn waves)\n\\(k v_s (10^{-6}s) \\) Ion acoustic waves\n\\( \\omega ^\\star (10^{-4}s) \\) Drift waves (\\( \\omega ^\\star = \\) drift frequency \\( \\approx \\grad p_e / e B n \\))\n\\( (k v_a \\nu)^{1/2} (10^{-3}s) \\) Collisional effects (resistivity): magnetic reconnection, tearing instabilities, plasma disruptions\n\\( (\\sim s) \\) Energy and plasma confinement time scales, including transport phenomena, which depend on effects from shorter time scales, and drift waves\nHierarchy of Plasma Models # We can define a hierarchy of plasma models, which does not correspond directly with the hierarchy of time scales, but is related.\nN-Body Model # In an N-body model, each particle is treated classically. Particles interact through the Lorentz force and collisions. We usually limit collisions to binary collisions, but we don\u0026rsquo;t have to. Electromagnetic fields are generated by particle positions and particle motion. The equation of motion looks like this:\n\\[\\dv{\\vec v_i}{t} = \\frac{q_i}{m_i} (\\vec E \u0026#43; \\vec v_i \\cross \\vec B) \u0026#43; \\sum_{j \\neq i} \\underbrace{\\left. \\dv{\\vec v_{ij}}{t} \\right|_{coll}}_{\\text{binary collisions}} \\underbrace{\\delta (\\vec r_i - \\vec r_j)}_{\\text{point particles}}\\] The collision term only involves collisions between two particles. As we said, we\u0026rsquo;re limiting collisions to binary collisions. The delta function comes from the assumption of infinitesimally small point particles.\nTo solve the model, we just track each particle \\( (\\vec r_i, \\vec v_i) \\) in a Lagrangian frame of reference. In a Lagrangian frame of reference, we track each individual particle in its own frame of reference, as opposed to a lab frame or cell frame.\nThe particles couple to the fields as source terms to the Maxwell equations.\n\\[\\rho(\\vec r) = \\pdv{}{V} \\sum_i q_i \\delta (\\vec r - \\vec r_i)\\] \\[\\vec j (\\vec r) = \\pdv{}{V} \\sum_i q_i \\vec v_i \\delta(\\vec r - \\vec r_i)\\] Maxwell\u0026rsquo;s equations give \\( \\vec E \\) and \\( \\vec B \\) at each particle\u0026rsquo;s position \\( r_i \\)\n\\[\\div \\vec E = \\frac{\\rho}{\\epsilon_0}\\] \\[\\div \\vec B = 0\\] \\[\\frac{1}{c^2} \\pdv{\\vec E}{t} \u0026#43; \\mu_0 \\vec j = \\curl \\vec B\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] More generally we can write a Hamiltonian description\n\\[\\dot p = F \\qquad \\dot q = v\\] where \\( p \\) is the generalized momentum and \\( v \\) is the generalized position. If the system is conservative, we can define \\( \\mathcal{H} \\) such that:\n\\[\\pdv{\\dot q_i}{q} \u0026#43; \\pdv{\\dot p_i}{p} = 0 \\qquad \\text{for each particle}\\] From this, we can write a statement of conservation of phase space, called the Klimontovich equation:\n\\[\\dv{N}{t} = 0 = \\pdv{N}{t} \u0026#43; \\pdv{}{q} \\cdot (\\dot q N) \u0026#43; \\pdv{}{p} \\cdot ( \\dot p N)\\] where the particle phase space is given by\n\\[N(p, q) = \\sum_i \\delta (p - p_i) \\delta (q - q_i)\\] The total number of particles in a typical lab plasma is \\( \\sim 10^{21} \\). Each particle interacts with every other particle, which gives a number of interactions \\( \\sim 10^{42} \\). Implementation of the full N-body model is impractical. If we perform an ensemble average of the very spiky Klimontovich equation, we arrive at a statistical description (the Liouville equation) for the probability distribution function, \\( f(\\vec x, \\vec v, t) \\). This results in an expression that looks like this:\n\\[\\pdv{f}{t} \u0026#43; \\sum_{j=1, 2, 3} \\left[ \\pdv{}{q_j} \\cdot (\\dot q_j f) \u0026#43; \\pdv{}{p_j} \\cdot (\\dot p_j f) \\right] = \\text{(cross terms)}\\] Now we\u0026rsquo;re summing over coordinates, not particles. The RHS \u0026ldquo;cross terms\u0026rdquo; describe the collisional terms, which leads to the BBGKY hierarchy.\nWe can also express the Liouville equation in a general way as the Boltzmann equation:\n\\[\\pdv{f}{t} \u0026#43; \\vec v \\cdot \\pdv{f}{ \\vec x} \u0026#43; \\frac{q}{m} (E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f}{\\vec v} = \\left. \\pdv{f}{t} \\right|_{coll}\\] This describes the evolution of the probability distribution function \\( f \\). We must compute the Boltzmann equation for each species \\( \\alpha \\):\n\\[\\pdv{f}{t} \u0026#43; \\vec v \\cdot \\pdv{f_\\alpha}{ \\vec x} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} (E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} = \\left. \\pdv{f_\\alpha}{t} \\right|_{coll}\\] For small plasma parameters \\( (n \\lambda_D ^3)^{-1} \\) and no collisional effects, the collisional terms vanish and we have the Vlasov equation:\n\\[\\pdv{f_\\alpha}{t} \u0026#43; \\vec v \\cdot \\pdv{f_\\alpha}{\\vec x} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} = 0\\] The advantage of using a continuous distribution \\( f \\) instead of the discrete particle description is that we can directly compute the various derivatives. This is a continuum description, like a fluid but in phase space.\nPhase Space Kinetic Model - Continuum Kinetic Model # We can evolve \\( f_\\alpha (\\vec x, \\vec v) \\) directly in 6D space by coupling Maxwell\u0026rsquo;s equations with the Vlasov equation\n\\[\\rho_c(\\vec x) = \\sum_\\alpha \\int q_\\alpha f_\\alpha \\dd \\vec v \\qquad \\text{charge density}\\] \\[\\vec j (\\vec x) = \\sum _\\alpha \\int q_\\alpha \\vec v f_\\alpha \\dd \\vec v \\qquad \\text{current density}\\] \\( \\rho \\) and \\( j \\) are the sources for the Maxwell equations. With the evolution of \\( \\vec E \\) and \\( \\vec B \\) in hand, we are free to compute all terms in the Vlasov equation (Vlasov-Maxwell Model).\nFor numerical implementations, it is usually advantageous to write the equation to be solved in \u0026ldquo;conservation form\u0026rdquo;\n\\[\\pdv{Q}{t} \u0026#43; \\div ( \\vec F) = 0\\] If collisions are important, then solve Boltzmann-Maxwell model. The conservation law form of the Boltzmann equation is\n\\[\\pdv{f_\\alpha}{t} \u0026#43; \\pdv{}{\\vec x} \\cdot ( \\vec v f_\\alpha) \u0026#43; \\pdv{\\vec v} \\cdot \\left(\\frac{q_\\alpha}{m}(\\vec E \u0026#43; \\vec v \\cross \\vec B) f_\\alpha \\right) = \\left. \\pdv{f_\\alpha}{t} \\right|_{coll}\\] This is called a \u0026ldquo;weakly conservative\u0026rdquo; form, because the right-hand side is not equal to zero. We can also write out Maxwell\u0026rsquo;s equations in conservative form, and combine them with Boltzmann to get the full conservation form of the Boltzmann-Maxwell model.\nIf we assume collisions are small angle scatterings and large angle scattering results as multiple small angle scatterings, then the collision terms can be truncated.\n\\[\\left. \\pdv{f}{t} \\right|_{coll} = \\pdv{}{\\vec v} \\left( \\left\\langle \\frac{\\Delta \\vec v}{\\Delta t} \\right\\rangle f \\right) \u0026#43; \\frac{1}{2} \\pdv{}{\\vec v} \\pdv{}{\\vec v} \\dot \\cdot \\left( \\left\\langle \\frac{ \\Delta \\vec v \\Delta \\vec v}{\\Delta t} \\right\\rangle f \\right) \\] \\[= \\sum_i \\pdv{}{v_i} \\left( \\frac{\\Delta v_i}{\\Delta t} f \\right) \u0026#43; \\frac{1}{2} \\sum_{j, k} \\pdv{}{v_j} \\pdv{}{v_k} \\left( \\langle \\frac{\\Delta v_j \\Delta v_k}{\\Delta t} \\rangle f \\right)\\] \\[\\left\\langle \\frac{\\Delta \\vec v}{\\Delta t} \\right\\rangle \\equiv \\frac{1}{\\Delta t} \\int F(\\vec v \\rightarrow \\vec v \u0026#43; \\Delta \\vec v) \\Delta \\vec v \\dd (\\Delta v)\\] \\[\\left\\langle \\frac{\\Delta \\vec v \\Delta \\vec v}{\\Delta t} \\right\\rangle \\equiv \\frac{1}{\\Delta t} \\int f( \\vec v \\rightarrow \\vec v \u0026#43; \\Delta \\vec v) \\Delta v \\Delta v \\dd (\\Delta v)\\] where \\( F(\\vec v \\rightarrow \\vec v + \\Delta \\vec v) \\) is the probability that a particle\u0026rsquo;s velocity changes from \\( \\vec v \\) to \\( \\vec v + \\Delta \\vec v \\) in a time \\( \\Delta t \\). Since any particle must have a velocity after \\( \\Delta t \\), the \\( F \\) is normalized such that\n\\[\\int F(\\vec v \\rightarrow \\vec v \u0026#43; \\Delta \\vec v) \\dd ( \\Delta \\vec v) = 1\\] The first term of the Fokker-Planck collision operator \\( \\pdv{}{\\vec v} \\left( \\left\\langle \\frac{\\Delta \\vec v}{\\Delta t} \\right\\rangle f \\right) \\) represents the slowing down of particles due to collisions with slower moving particles. For example, consider a beam injected into a stationary plasma. Assuming the beam is dense (i.e. it is not affected by the plasma), the distribution function is composed of a stationary distribution centered about \\( v=0 \\), and a narrow distribution centered at the beam velocity. Over time, the collision operator will act to drive the centroids together. Intuitively, it has a frictional effect:\nMoving on to the second term,\n\\[\\pdv{}{\\vec v} \\pdv{}{\\vec v} \\dot \\cdot \\left( \\left\\langle \\frac{ \\Delta \\vec v \\Delta \\vec v}{\\Delta t} \\right\\rangle f \\right) \\] this represents the heating of particles due to collisions with a hotter population. It will tend to drive the variances (temperature) together, and so acts as a velocity diffusion term:\nThe Fokker-Planck collision model is one of the most complicated forms that can actually be computationally solved, and gives a pretty good description of most laboratory and astrophysics plasmas.\nThe Boltzmann H-theorem tells us that \u0026ldquo;\\( f \\) will tend towards a Maxwellian as a result of collisions.\u0026rdquo; This suggests that simpler forms of the collision operator can be found which maintain the characteristics of the full Fokker-Planck operator, but are easier to solve. One such operator is the BGK operator:\n\\[\\left. \\pdv{f}{t} \\right|_{coll} = \\frac{f_{M} - f} {\\tau}\\] where \\( f_M \\) is a Maxwellian distribution with the same first three velocity moments (\\( n, \\vec u, T \\)) as \\( f \\), and \\( \\tau \\) is a relaxation time.\n\\[f_M (\\vec v) = n \\left( \\frac{m}{2 \\pi k T} \\right) ^{3/2} \\exp \\left( - \\frac{m (\\vec v - \\vec u)^2}{2 k T}\\right)\\] In this sense, the BGK collision operator is a relaxation operator (towards a Maxwellian) of the Fokker-Planck collision model.\n"},{"id":15,"href":"/r/notes/UWAA558/02-plasma-models/","title":"Plasma Models","section":"MHD Theory","content":" Plasma Models # Working towards MHD # Let\u0026rsquo;s start from a full-particle description with the goal of reaching a continuum description (kinetic model). Then, we\u0026rsquo;ll look at the forces on the separate species and form a multi-fluid model, finally simplifying to a single-fluid MHD model.\nThe most important question to ask ourselves is \u0026ldquo;when is this model going to be useful?\u0026rdquo; The MHD model is the mathematical model for magnetized plasmas that are treated as a fluid. This means that we can define a fluid element (some lil\u0026rsquo; box of plasma) and define the physical properties (mass, density, magnetization, etc.) of the element. We need to make some assumptions about scale in order to do this. In terms of spatial scales, we abstract properties below a discrete scale \\( a_0 \\) into the properties of a fluid element\n\\[\\frac{a_0}{L} \\rightarrow 0 \\qquad a_0 = \\text{discrete scale} \\qquad L = \\text{spatial scale of interest}\\] Length scales smaller than the discrete scale will not be properly captured by the model, so scales like the particle radius will be meaningless in our fluid model.\nPlasma Definition # A plasma is a quasi-neutral gas of charged and neutral particles which exhibit collective behavior. The particles (electrons, ions, neutrals) interact through EM fields and collisions.\nMathematically you would think that plasmas could be treated as individual particles. Doing so gives an N-body problem with classical interactions through the Lorentz force (Coulomb interaction removed by assumption of quasi-neutrality) and binary collisions. Each particle \\( i \\) has a well-defined mass \\( m_i \\) and charge \\( q_i \\) which do not change in time. The governing equations are\n\\[\\dv{\\vec v_i}{t} = \\frac{q_i}{m_i} (\\vec E \u0026#43; \\vec v_i \\cross \\vec B) \u0026#43; \\sum_{j \\neq i} \\left[ \\left. \\dv{\\vec v_{ij}}{t} \\right|_{coll} (\\vec r_i - \\vec r_j) \\right]\\] \\[\\dv{\\vec r_i}{t} = \\vec v_i\\] The fields E and B are described by the Maxwell equations\n\\[\\pdv{B}{t} = - \\curl E\\] \\[\\frac{1}{c^2} \\pdv{E}{t} = \\curl B - \\mu_0 \\sum_i q_i v_i \\delta (r - r_i)\\] \\[\\div B = 0\\] \\[\\epsilon_0 \\div E = \\sum_i q_i \\delta (r - r_i)\\] Klimontovich Equation # Re-writing the force relations as a statement of conservation in phase space, we get Klimontovich equation for species \\( \\alpha \\) \\[\\dv{N_\\alpha}{t} = 0 = \\pdv{N_\\alpha}{t} \u0026#43; \\pdv{}{q} \\cdot (\\dot{q} N_\\alpha) \u0026#43; \\pdv{}{p} \\cdot (\\dot{p}N_\\alpha)\\] The particle phase space is defined by \\[N_\\alpha(p, q) = \\sum_i \\delta(p - p_i) \\delta(q - q_i)\\] where \\( p \\) and \\( q \\) are generalized momentum and position coordinates. The resulting \\( N(p) \\) looks very spiky, with nonzero values only at the exact values inhabited by particles. Unfortunately, that means that only the tools of discrete mathematics are applicable to the distribution, and we\u0026rsquo;re forbidden from our favorite tool (calculus). To make the analysis possible, we can smooth over the discreteness by performing an ensemble average of the Klimontovich equation. This gives us a statistical description using smooth distribution functions:\n\\[f(x, v, t) \\qquad \\frac{(\\text{no. of particles})}{(\\text{unit distance})^3(\\text{unit velocity})^3}\\] That is, \\( f(x, v, t) \\) is the number of particles at position \\( x \\) with velocity \\( v \\) at time \\( t \\) . We also work with normalized distributions which give the probability of finding a particle.\nBy our definition, we can integrate to get the total number of particles at time \\( t \\) .\n\\[\\iint \\dd x \\dd v f(x, v, t) = N(t)\\] The ensemble averaging process works well if the the number of particles is very large\n\\[N \\gg 1\\] See a kinetic theory text (e.g. Krall and Trivelpiece) for a full description of the ensemble averaging process.\nNow we can write our Klimontovich equation in terms of continuous quantities\n\\[\\pdv{f}{t} \u0026#43; \\left[ \\pdv{}{q} \\cdot (\\dot{q} f) \u0026#43; \\pdv{}{p} \\cdot (\\dot{p}f) \\right] = \\text{(cross terms)}\\] The collision terms now have an infinite number of cross terms. We call this the BBGKY (BogoliubovBornGreenKirkwoodYvon) hierarchy.\nExpressing as the Boltzmann equation (generally called the Boltzmann-Maxwell equation, since the solution requires solving for the electromagnetic fields of the Maxwell equations)\n\\[\\pdv{f_\\alpha}{t} \u0026#43; \\vec v \\cdot \\pdv{f_\\alpha}{\\vec x} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} = \\left. \\pdv{f_\\alpha}{t} \\right|_{coll}\\] We leave the collision term in. A lot of the work of kinetic theory is coming up with an applicable form of the collision operator which is appropriate but still simple enough to solve. We often write the collision operator as the product of binary collisions\n\\[ \\left. \\pdv{f_\\alpha}{t} \\right|_{coll} = \\sum_\\beta C_{\\alpha \\beta}\\] Notice that the terms \\( \\vec v \\cdot \\pdv{f_\\alpha}{\\vec x} \\) and \\( \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\) are advection equations, advecting in \\( \\vec x \\) and \\( \\vec v \\) respectively. If we ignore the collision term (set the RHS to zero) we have the Vlasov equation.\nNote that the fields E and B at any location are generated from the charges and currents of the entire plasma volume, including externally applied fields. As a result, there\u0026rsquo;s an inherent integrating process taking into account the sources across the whole volume that leads to long-range smoothly varying forces. This is in contrast to the collisional effects, which by their nature lead to very short range abrupt forces. It makes sense to make a distinction between the long-range electromagnetic forces and the short range collisional forces.\nBecause the Boltzmann-Maxwell model is inherently 6-dimensional, it is a very challenging model to implement. The B-M model provides a complete description, but it is often too detailed to solve.\nIf we solve the Vlasov equation for two parallel opposite beams, for example, we see that the\nAs it turns out, the integral, centroid, and variance are all that are required to fully describe a Maxwellian distribution\n\\[f_{M, \\alpha} (\\vec v) = n_0 \\left( \\frac{ m_\\alpha }{2 \\pi T_\\alpha} \\right)^{3/2} \\text{exp}\\left[ - \\frac{ \\frac{1}{2} m_\\alpha (v - v_\\alpha)^2}{T_\\alpha} \\right]\\] In other words, we can write the Maxwellian distribution as \\( f_M(n_0, \\vec v_\\alpha, T_\\alpha ; \\vec v) \\) . We care about Maxwellian distributions so much in plasma physics because it is the solution to the Boltzmann equation for \\( \\left. \\pdv{f_\\alpha}{t} \\right|_{coll} = 0 \\) (Vlasov equation). That\u0026rsquo;s not saying that there are no collisions, it is saying that there are so many collisions that the effect is isotropic and the overall force is zero.\nAnother feature of the Maxwellian distribution is \\( \\vec v \\cdot \\pdv{f_\\alpha}{\\vec x} = 0 \\) . This is a famous result called the Boltzmann H-theorem, and says that any initial distribution will relax (and very quickly) to a Maxwellian distribution.\nBy replacing our velocity distribution with the associated Maxwell distribution, we arrive at a Plasma Fluid Model\n"},{"id":16,"href":"/r/notes/UWAA557/ch10-1/","title":"Statistical Mechanics","section":"Physics of Fusion Plasmas","content":" 10-1 Statistical Mechanics # 10.1.1 Very Large Numbers # Before getting started with real plasma physics concepts, we need to quickly review some statistical mechanics with the goal of deriving the Maxwell-Boltzmann distribution.\nAs we all know, if we have \\( N \\) unique objects, there are \\( N! \\) ways of arranging them. If \\( n_1 \\ldots n_k \\) are identical with N things, the number of combinations is\n\\[\\frac{N!}{n_1 ! \\ldots n_k !}\\] What does probability have to do with a velocity distribution? Consider the one-dimensional random walk, in which we take \\( N \\) steps, and at each step we move in a random direction. We take \\( n_r \\) steps to the right and \\( N - n_r \\) steps to the left. For a random walk we assume the probability of going in each direction is the same\n\\[P_r = p = \\frac{1}{2} \\qquad P_l = q = \\frac{1}{2}\\] After taking \\( N \\) steps, the probability of taking \\( n_r \\) steps to the right and \\( (N - n_r) \\) steps to the left is\n\\[P(n_r) = \\frac{N !}{n_r ! (N - n_r)!} \\cdot p^{n_r} q^{n_r}\\] We are also interested in the final destination, which is the net number of steps to the right\n\\[m_r = n_r - n_l = 2 n_r - N\\] If we have a bias to move in a particular direction, then \\( p \\neq q \\) and the distribution \\( P(n_r) \\) will be shifted towards the bias.\nOf course, if we have a very large \\( N \\) we aren\u0026rsquo;t going to be able to compute \\( P(n_r) \\) directly. We\u0026rsquo;ve got our handy dandy natural logarithm to help us.\n\\[\\ln N! = \\ln N \u0026#43; \\ln (N - 1) \u0026#43; \\ldots \u0026#43; 1\\] \\[\\ln (N \u0026#43; 1) ! = \\ln (N \u0026#43; 1) \u0026#43; \\ln N!\\] \\[\\pdv{\\ln N!}{N} \\approx \\frac{\\ln(N\u0026#43;1)! - \\ln N!}{N - (N-1)} = \\ln(N\u0026#43;1) \\approx \\ln N\\] Recall our expression for the probability distribution\n\\[P(n_r) = \\frac{N!}{n_r !(N - n_r)!} p^{n_r} q^{N- n_r}\\] \\[\\rightarrow \\quad \\ln P(n_r) = \\ln N! - \\ln n_r ! - \\ln ((N-n_r)!) \u0026#43; n_r \\ln p \u0026#43; (N - n_r) \\ln q\\] Now we apply the little log trick \\( \\pdv{\\ln N!}{N} \\approx \\ln N \\)\n\\[\\dv{\\ln P(n_r)}{n_r} = - \\ln r \u0026#43; \\ln (N - n_r) \u0026#43; \\ln p - \\ln q = 0\\] \\[\\qquad \\rightarrow \\qquad \\ln \\left( \\frac{N - n_r}{n_r} \\frac{p}{q} \\right) = 0\\] \\[(N - n_r)p = n_r q\\] \\[Np = n_r(p \u0026#43; q) \\] \\[\\overline{n_r} = Np\\] Surprise surprise, the probability of getting a certain result is just the expectation value of that result. Can we also say anything about the width of the distribution? Taylor expand about \\( \\overline{n_r} = N p \\) and define \\( \\eta \\) by \\( n_r = N p + \\eta \\)\n\\[\\ln (P(n_r)) = \\ln(P(Np)) \u0026#43; B_1 \\eta \u0026#43; \\frac{1}{2} B_2 \\eta^2 \u0026#43; \\frac{1}{6} B_3 \\eta^4\\] \\[B_k = \\frac{d^k \\ln P(n_r)}{d n_r ^k} \\] \\[\\dv{\\ln P(n_r)}{n_r} = - \\ln n_r \u0026#43; \\ln (N - n_r) \u0026#43; \\ln p - \\ln q\\] \\[B_1 = - \\ln(n_r) \u0026#43; \\ln (N - n_r) \u0026#43; \\ln p - \\ln q\\] \\[= - \\ln (Np) \u0026#43; \\ln(N - Np) = 0\\] \\[B_2 = - \\frac{1}{n_r} - \\frac{1}{N - n_r} = - \\frac{1}{Np} - \\frac{1}{N - Np} = -\\frac{1}{Nqp}\\] \\[B_3 = \\frac{1}{n_r ^2} - \\frac{1}{(N - n_r)^2} \\approx \\frac{1}{N^2}\\] \\[B_4 \\approx \\frac{1}{N_3}\\] The expansion converges for \\( N \u0026raquo; \\eta \\)\n\\[\\ln(P(n_r)) \\approx \\ln (P(Np)) - \\frac{1}{2} \\frac{1}{Nqp} \\eta ^2\\] or\n\\[P(n_r) = P(Np) e^{- \\frac{\\eta ^2}{2 Npq}} \\qquad P_0 = P(Np)\\] \\[P(\\eta) = P_0 e^{-\\frac{\\eta ^2}{2Npq}}\\] So what\u0026rsquo;s the width?\n\\[\\langle \\eta ^2 \\rangle = \\frac{ \\int_{-\\infty} ^\\infty \\eta ^2 P(\\eta) \\dd \\eta }{\\int_{-infty}^\\infty P(\\eta) \\dd \\eta} = \\frac{ \\frac{ \\sqrt{\\pi}}{4 \\left( \\frac{1}{2 N pq} \\right) ^{3/2}}}{\\frac{ \\sqrt{\\pi}}{2 \\left( \\frac{1}{2 N pq} \\right)^{1/2}}} = \\frac{2 Npq}{2} = Npq\\] So the uncertainty is \\( \\delta n_r = \\sqrt{ N pq} \\approx \\frac{1}{2} \\sqrt{N} \\). For a very large N, the relative uncertainty \\( \\delta n_r / N \\approx \\frac{1}{2 \\sqrt{N}} \\) diminishes and the distribution (centered at \\( Np \\) ) gets very narrow. The signal-to-noise will go as \\( \\frac{1}{\\sqrt{N}} \\).\nMaxwell-Boltzmann Distribution Function # Suppose we\u0026rsquo;ve got a large number N of particles with total energy \\( W \\). What is the energy distribution among the particles?\nWe call a \u0026ldquo;state\u0026rdquo; a possible distribution (permutation) of energy among the particles which satisfies the constraints. The fundamental principle of statistical mechanics states that all states have an equal chance of being occupied, with the resulting combinatorial factor giving the distribution. Because the number of particles is very large, the distribution will be close to the distribution that has the largest number of states.\nLabel the particles by the energy they have. Let \\( n_i \\) be the number of particles with energy between \\( \\epsilon_i \\) and \\( \\epsilon_{i+1} = \\epsilon_i + \\delta \\epsilon \\). Choose \\( k \\) different energy ranges:\n\\[\\sum_{i = 0} ^k n_i = N \\qquad \\sum_{i=1} ^k \\epsilon_i n_i = W = \\text{ total energy}\\] The number of states with a given distribution is just a partitioning (indistinguishable!) of the energy among \\( k \\) bins, so\n\\[P = \\frac{1}{N} \\frac{N!}{n_1 ! \\ldots n_k ! } = \\frac{N-1!}{n_1 ! \\ldots n_k ! } \\approx \\frac{N!}{n_1 ! \\ldots n_k ! }\\] We know that the distribution will be closely centered about the maximum of P, subject to the conservation constraints of energy and particles\n\\[\\delta N = 0 = \\sum _1 ^ k \\delta n_i\\] \\[\\delta W = 0 = \\sum_1 ^k \\epsilon_i \\delta n_i \\] Going back to our log trick to find the critical point\n\\[\\delta \\ln P = 0\\] \\[\\ln P = \\ln N! - \\sum_1 ^k \\ln n_i !\\] \\[\\delta \\ln P = - \\sum_1 ^k \\ln n_i \\delta n_i = 0\\] Let \\( \\lambda \\) be the Lagrange multiplier for \\( \\delta N \\) and \\( \\beta \\) be the Lagrange multiplier for \\( \\delta W \\), so\n\\[(\\ln n_i \u0026#43; \\lambda \u0026#43; \\beta \\epsilon_i) \\delta n_i = 0\\] \\[\\ln n_i = 0 \\lambda - \\beta \\epsilon_i\\] \\[n_i = e^{-\\lambda - \\beta \\epsilon_i} = n_\\lambda e^{-\\beta \\epsilon_i}\\] Values of \\( n_\\lambda \\) just come from the real constraints \\( N \\) and \\( W \\)\n\\[\\int _0 ^\\infty n_\\lambda e^{- \\beta \\epsilon} \\dd \\epsilon = N\\] \\[\\int_0 ^\\infty \\epsilon n_\\lambda e^{- \\beta \\epsilon} \\dd \\epsilon = W\\] Putting in our constraints, out pops the Maxwell-Boltzmann distribution\n\\[f(\\vec v) \\dd \\vec v = n \\left( \\frac{m}{2 \\pi k T} \\right) ^{3/2} e^{- \\epsilon / kT} \\qquad \\epsilon = \\text{ KE \u0026#43; PE }\\] Example: Distribution under Gravity\nIf we have a bunch of particles under the influence of gravity, the energy is\n\\[\\epsilon = \\frac{1}{2} m v^2 \u0026#43; mgz\\] The distribution is\n\\[f(v) = n_0 \\left( \\frac{m}{2 \\pi k T} \\right) ^{3/2} e^{-( \\frac{1}{2} m v^2 \u0026#43; mgz)/kT}\\] \\[= n(z) \\left( \\frac{m}{2 \\pi k T} \\right) ^{3/2} e^{-\\frac{1}{2} m v^2 / kT}\\] \\[n(z) = n_0 e^{- mgz / kT}\\] In a collisionless picture, the velocity spread is the same at all heights (\\( kT \\) is not a function of z). However lower energy particles do not make it to a higher z. We end up with a density drop with increasing \\( z \\) .\n"},{"id":17,"href":"/r/notes/griffiths/ch1-3/","title":"Integral Calculus","section":"Griffiths Introduction to Electrodynamics","content":" 1.3: Integral Calculus # 1.3.1: Line, Surface, and Volume Integrals # In electrodynamics, we encounter several different kinds of integrals, among which the most important are line (or path) integrals, surface integrals, and volume integrals.\nLine Integrals # A line integral is an expression of the form\n\\[\\int_a ^b \\vec{v} \\cdot \\dd{\\vec{l}} \\tagl{1.48}\\] where v is a vector function, \\( \\dd \\vec{l} \\) is the infinitesimal displacement vector and the integral is to be carried out along a prescribed path P from point a to point b. If the path in question forms a closed loop (that is, if \\( \\vec{b} = \\vec{a} \\), then put a circle on the integral sign:\n\\[\\oint \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.49}\\] At each point on the path, we take the dot product of v (evaluated at that point) with the displacement to the next point on the pat. To a physicist, the most familiar example of a line integral is the work done by a force \\( \\vec{F} \\): \\( W = \\int \\vec{F} \\cdot \\dd \\vec{l} \\)\nOrdinarily, the value of a line integral depends critically on the path taken from a to b, but there is an important special class of vector functions for which the line integral is independent of path and is determined entirely by the end points. It will be our business in due course to characterize this special class of vectors. (A force that has this property is called conservative.)\nExample 1.6 # Q Calculate the line integral of the function \\( \\vec{v} = y^2 \\vu{x} + 2x (y+1) \\vu{y} \\) from the point a = (1, 1, 0) to the point b = (2, 2, 0), along the paths (1) and (2) in Fig 1.21. What is \\( \\oint \\vec{v} \\cdot \\dd \\vec{l} \\) for the loop that goes from a to b along (1) and returns to a along (2)? A As always, \\( \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} \\). Path (1) consists of two parts. Along the \u0026ldquo;horizontal\u0026rdquo; segment, \\( dy = dz = 0 \\) so\n\\[\\dd \\vec{l} = \\dd x \\vu{x} , y = 1, \\vec{v} \\cdot \\dd{\\vec{l}} = y^2 \\dd x = \\dd x, \\text{ so } \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 \\dd x = 1 \\tag{i}\\] On the \u0026ldquo;vertical\u0026rdquo; stretch, dx = dz = 0, so\n\\[\\dd \\vec{l} = \\dd y \\vu{y}, x = 2, \\vec{v} \\cdot \\dd \\vec{l} = 2x(y\u0026#43;1) \\dd y = 4(y\u0026#43;1) \\dd y, \\text{ so } \\tag{ii} \\] \\[\\int \\vec{v} \\dd \\vec{l} = 4 \\int_1 ^2 (y\u0026#43;1) \\dd y = 10\\] By path (1), then\n\\[\\int _{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = 1 \u0026#43; 10 = 11\\] Meanwhile on path (2), \\( x = y, \\dd x = \\dd y, \\) and \\( \\dd z = 0 \\), so\n\\[\\dd \\vec{l} = \\dd x \\vu{x} \u0026#43; \\dd x \\vu{y}, \\vec{v} \\cdot \\dd \\vec{l} = x^2 \\dd x \u0026#43; 2x(x\u0026#43;1) \\dd x = (3x^2 \u0026#43; 2x) \\dd x\\] and\n\\[\\int_{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 (3x^2 \u0026#43; 2x) \\dd x = \\left. (x^3 \u0026#43; x^2)\\right|_{1} ^2 = 10\\] (The strategy here is to get everything in terms of one variable; I could just as well have eliminated x in favor of y.)\nFor the loop that goes out (1) and back (2), then\n\\[\\oint \\vec{v} \\cdot \\dd \\vec{l} = 11 - 10 = 1\\] Surface Integrals # A surface integral is an expression of the form\n\\[\\int_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.50}\\] where v is again some vector function, and the integral is over a specified surface \\( \\mathscr{S} \\). Here \\( \\dd \\vec{a} \\) is an infinitesimal patch of area, with direction perpendicular to the surface (Fig 1.22). There are, of course, two directions perpendicular to any surface, so the sign of a surface integral is intrinsically ambiguous. If the surface is closed (forming a \u0026ldquo;balloon\u0026rdquo;), in which case I again put a circle on the integral sign\n\\[\\oint \\vec{v} \\cdot \\dd \\vec{a}\\] then tradition dictates that \u0026ldquo;outward\u0026rdquo; is positive, but for open surfaces it\u0026rsquo;s arbitrary. If v describes the flow of a fluid (mass per unit area per unit time), then \\( \\int \\vec{v} \\cdot \\dd \\vec{a} \\) represents the total mass per unit time passing through the surface - hence the alternative name, \u0026ldquo;flux.\u0026rdquo;\nOrdinarily, the value of a surface integral depends on the particular surface chosen, but there is a special class of vector functions for which it is independent of the surface and is determined entirely by the boundary line. An important task will be to characterize this special class of functions.\nExample 1.7 # Q Calculate the surface integral of \\( \\vec{v} = 2xz \\vu{x} + (x+2) \\vu{y} + y(z^2 -3) \\vu{z} \\) over five sides (excluding the bottom) of the cubical box (side 2) in Fig 1.23. Let \u0026lsquo;upward and outward\u0026rsquo; be the positive direction, as indicated by the arrows. A Taking the sides one at a time (i) \\( x = 2, \\dd \\vec{a} = \\dd y \\dd z , \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = 2xyz , \\dd y \\dd z = 4z \\dd y \\dd z \\), so\n\\[ \\int \\vec{v} \\cdot \\dd \\vec{a} = 4 \\int_0 ^2 \\dd y \\int_0 ^2 z \\dd z = 16\\] (ii) \\( x = 0, \\dd \\vec{a} = -\\dd y \\dd z , \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = -2xyz , \\dd y \\dd z = 4z \\dd y \\dd z = 0 \\), so\n\\[\\int \\vec{v} \\cdot \\dd \\vec{a} = 0\\] (iii) \\( y = 2, \\dd \\vec{a} = \\dd x \\dd z , \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = (x+2) , \\dd x \\dd z \\), so\n\\[\\int \\vec{v} \\cdot \\dd \\vec{a} = \\int _0 ^2 (x \u0026#43; 2) \\dd x \\int _0 ^2 \\dd z = 12\\] (iv) \\( y = 0, \\dd \\vec{a} = - \\dd x \\dd z , \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = -(x+2) , \\dd x \\dd z \\), so\n\\[\\int \\vec{v} \\cdot \\dd \\vec{a} = -\\int _0 ^2 (x \u0026#43; 2) \\dd x \\int _0 ^2 \\dd z = -12\\] (v) \\( z = 2, \\dd \\vec{a} = \\dd x \\dd y , \\vu{z}, \\vec{v} \\cdot \\dd \\vec{a} = y(z^2 -3) , \\dd x \\dd y = y , \\dd x \\dd y \\), so\n\\[\\int \\vec{v} \\cdot \\dd{a} = \\int_0 ^2 \\dd x \\int_0 ^2 y \\dd y = 4\\] The total flux is\n\\[\\int _{surface} \\vec{v} \\cdot \\dd \\vec{a} = 16 \u0026#43; 0 \u0026#43; 12 - 12 \u0026#43; 4 = 20\\] Volume Integrals # A volume integral is an expression of the form\n\\[\\int_{V} T \\dd \\tau \\tagl{1.51}\\] where T is a scalar function and \\( \\dd \\tau \\) is an infinitesimal volume element. In Cartesian coordinates,\n\\[\\dd \\tau = \\dd x \\, \\dd y \\, \\dd z \\tagl{1.52}\\] For example, if T is the density of a substance (which might vary from point to point), then the volume integral would give the total mass. Occasionally we shall encounter volume integrals of vector functions:\n\\[\\int \\vec{v} \\dd \\tau = \\int (v_x \\vu{x} \u0026#43; v_y \\vu{y} \u0026#43; v_z \\vu{z}) \\dd \\tau = \\vu{x} \\int v_x \\dd \\tau \u0026#43; \\vu{y} \\int v_y \\dd \\tau \u0026#43; \\vu{z} \\int v_z \\dd \\tau \\tagl{1.53}\\] Because the unit vectors are constants, they come outside the integral.\nExample 1.8 # Q Calculate the volume integral of \\( T = xyz^2 \\) over the prism in Fig 1.24.\nA \\[\\begin{aligned} \\int T \\dd \\tau \u0026amp; = \\int _0 ^3 z^2 \\left( \\int _0 ^1 y \\left[ \\int_0 ^{1-y} x \\, \\dd x \\right] \\dd y \\right) \\dd z \\\\ \u0026amp; = \\frac{1}{2} \\int_0 ^3 z^2 \\, \\dd z \\int_0 ^1 (1-y)^2 y \\, \\dd y = \\frac{1}{2} (9) \\left( \\frac{1}{12} \\right) = \\frac{3}{8} \\end{aligned}\\] 1.3.2: The Fundamental Theorem of Calculus # Suppose \\( f(x) \\) is a function in one variable. The fundamental theorem of calculus says\n\\[\\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x = f(b) - f(a) \\tagl{1.54}\\] In case this doesn\u0026rsquo;t look familiar, I\u0026rsquo;ll write it another way:\n\\[\\int_a ^b F(x) \\dd x = f(b) - f(a)\\] where \\( df / dx = F(x) \\). The fundamental theorem tells you how to integrate \\( F(x) \\): you think up a function \\( f(x) \\) whose derivative is equal to F.\nGeometrical interpretation: According to Eq. 1.33, \\( df = (df / dx) dx \\) is the infinitesimal change in f when you go from (x) to (x + dx). The fundamental theorem (Eq. 1.54) says that if you chop the interval from a to b (Fig. 1.25) into many tiny pieces, dx, and add up the increments df from each little piece, the result is (not surprisingly) equal to the total change in f: \\( f(b) - f(a) \\). In other words, there are two ways to determine the total change in the function: either subtract the values at the ends or go step-by-step, adding up all the tiny increments as you go. You\u0026rsquo;ll get the same answer either way.\nNotice the basic format of the fundamental theorem: the integral of a derivative over some region is given by the value of the function at the end points (boundaries). In vector calculus there are three species of derivative (gradient, divergence, and curl), and each has its own \u0026ldquo;fundamental theorem,\u0026rdquo; with essentially the same format. I don\u0026rsquo;t plan to prove these theorems here; rather, I will explain what they mean, and try to make them plausible.\n1.3.3: The Fundamental Theorem for Gradients # Suppose we have a scalar function of three variables T(x, y, z). Starting at point a, we move a small distance \\( \\dd \\vec{l}_1 \\) (Fig 1.26). According to Eq. 1.37, the function T will change by an amount\n\\[\\dd T = (\\grad T) \\cdot \\dd \\vec{l}_1\\] Now we move a little further, by an additional small displacement \\( \\dd \\vec{l}_2 \\); the incremental change in T will be \\( (\\grad T) \\cdot \\dd \\vec{l}_2 \\). In this manner, proceeding by infinitesimal steps, we make the journey to point b. At each step we compute the gradient of T (at that point) and dot it into the displacement dl\u0026hellip; this gives us the change in T. Evidently the total change in Tin going from a to b (along the path selected) is\n\\[\\int_{\\vec{a}} ^{\\vec{b}} (\\grad T) \\cdot \\dd \\vec{l} = T(\\vec{b}) - T(\\vec{a}) \\tagl{1.55}\\] This is the fundamental theorem for gradients; like the \u0026ldquo;ordinary\u0026rdquo; fundamental theorem, it says that the integral (here a line integral) of a derivative (here the gradient) is given by the value of the function at the boundaries (a and b).\nGeometrical Interpretation: Suppose you wanted to determine the height of the Eiffel Tower. You could climb the stairs, using a ruler to measure the rise at each step, and adding them all up (that\u0026rsquo;s the left side of Eq. 1.55), or you could place altimeters at the top and the bottom, and subtract the two readings (that\u0026rsquo;s the right side); you should get the same answer either way (that\u0026rsquo;s the fundamental theorem).\nIncidentally, as we found in Ex. 1.6, line integrals ordinarily depend on the path taken from a to b. But the right side of Eq. 1.55 makes no reference to the path - only to the end points. Evidently, gradients have the special property that their line integrals are path independent:\nCorollary 1: \\( \\int_a ^b (\\grad T) \\cdot \\dd \\vec{l} \\) is independent of the path taken from a to b. Corollary 2: \\( \\oint (\\grad T) \\cdot \\dd \\vec{l} = 0 \\), since the beginning and end points are identical, and hence \\( T(\\vec{b}) - T(\\vec{a}) = 0 \\). Example 1.9 # Q Let \\( T = xy^2 \\), and take point a to be the origin (0, 0, 0) and b to be the point (2, 1, 0). Check the fundamental theorem for gradients. A We know a priori that the integral should be independent of the path, but we must sill pick a specific path in order to evaluate it. Let\u0026rsquo;s go out along the x axis, then up (Fig 1.27). As always, \\( \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z}; \\grad T = y^2 \\vu{x} + 2xy \\vu{y} \\)\n\\[y = 0; \\dd \\vec{l} = \\dd x \\vu{x}; \\grad T \\cdot \\dd \\vec{l} = y^2 \\dd x = 0 \\rightarrow \\int_{i} \\grad T \\cdot \\dd \\vec{l} = 0\\] \\[x = 2; \\dd \\vec{l} = \\dd y \\vu{y}; \\grad T \\cdot \\dd \\vec{l} = 4y \\dd y \\rightarrow \\int_{ii} \\grad T \\cdot \\dd \\vec{l} = \\left. 2y^2 \\right| _0 ^1 = 2 \\] The total line integral is 2. So is this consistent with what we expect from the fundamental theorem? Well, \\\\( T(b) - T(a) = 2 - 0 = 2 \\\\), so yes! 1.3.4: The Fundamental Theorem for Divergences # The fundamental theorem for divergences states that\n\\[\\int _{\\mathscr{V}} (\\div \\vec{v} ) \\dd \\tau = \\oint_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.56}\\] In honor, I suppose, of its great importance, this theorem has at least three special names: Gauss\u0026rsquo;s theorem, Green\u0026rsquo;s theorem, or simply the divergence theorem. Like the other \u0026ldquo;fundamental theorems,\u0026rdquo; it says that the integral of a derivative (in this case the divergence) over a region (in this case a volume, V) is equal to the value of the function at the boundary (in this case the surface S that bounds the volume). Notice that the boundary term is itself an integral (specifically, a surface integral). This is reasonable: the \u0026ldquo;boundary\u0026rdquo; of a line is just two end points, but the boundary of a volume is a (closed) surface.\nGeometrical Interpretation: If v represents the flow of an incompressible fluid, then the flux of v (the right side of Eq. 1.56) is the total amount of fluid passing out through the surface, per unit time. Now, the divergence measures the \u0026ldquo;spreading out\u0026rdquo; of the vectors from a point-a place of high divergence is like a \u0026ldquo;faucet,\u0026rdquo; pouring out liquid. If we have a bunch of faucets in a region filled with incompressible fluid, an equal amount of liquid will be forced out through the boundaries of the region. In fact, there are two ways we could determine how much is being produced: (a) we could count up all the faucets, recording how much each puts out, or (b) we could go around the boundary, measuring the flow at each point, and add it all up. You get the same answer either way:\n\\[\\int (\\text{faucets within the volume}) = \\oint (\\text{flow out through the surface})\\] This, in essence, is what the divergence theorem says.\nExample 1.10 # Q Check the divergence theorem using the function\n\\[\\vec{v} = y^2 \\vu{x} \u0026#43; (2xy \u0026#43; z^2) \\vu{y} \u0026#43; (2yz) \\vu{z} \\] using a unit cube at the origin as the surface boundary (Fig 1.29). A In this case\n\\[\\div \\vec{v} = 2(x \u0026#43; y)\\] and\n\\[\\int_V 2(x\u0026#43;y) \\dd \\tau = 2 \\int_0 ^1 \\dd x \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z (x\u0026#43;y)\\] \\[\\int _0 ^1 \\dd x (x\u0026#43;y) = \\frac{1}{2} \u0026#43; y, \\quad \\int _0 ^1 \\dd y (\\frac{1}{2} \u0026#43; y) \\dd y = 1, \\quad \\int_0 ^1 \\dd z (1) = 1\\] Thus,\n\\[\\int_{V} \\div \\vec{v} \\dd \\tau = 2\\] That takes care of the volume integral part of Gauss\u0026rsquo; Law, now how about the surface integral? We have to it in six parts, for each face of the cube:\n\\[\\tag{i} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z y^2 = \\frac{1}{3} \\] \\[\\tag{ii} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z y^2 = - \\frac{1}{3} \\] \\[\\tag{iii} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd z (2x \u0026#43; z^2) = \\frac{4}{3} \\] \\[\\tag{iv} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd z (z^2) = - \\frac{1}{3} \\] \\[\\tag{v} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd y (2y) = 1\\] \\[\\tag{vi} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd y (0) = 0\\] So the total flux is\n\\[\\oint _S \\vec{v} \\cdot \\dd \\vec{a} = \\frac{1}{3} - \\frac{1}{3} \u0026#43; \\frac{4}{3} - \\frac{1}{3} \u0026#43; 1 \u0026#43; 0 = 2\\] as we should expect.\n1.3.5: The Fundamental Theorem for Curls # The fundamental theorem for curls, which goes by the name Stokes\u0026rsquo; Theorem, states that\n\\[\\int _S(\\curl \\vec{V}) \\cdot \\dd \\vec{a} = \\oint _P \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.57}\\] As always, the integral of a derivative (here, the curl) over a region (here, a patch of surface, S) is equal to the value of the function at the boundary (here, the perimeter of the patch, P). As in the case of the divergence theorem, the boundary term is itself an integral-specifically, a closed line integral.\nGeometrical Interpretation: Recall that the curl measures the \u0026ldquo;twist\u0026rdquo; of the vectors v; a region of high curl is a whirlpool - if you put a tiny paddle wheel there, it will rotate. Now, the integral of the curl over some surface (or, more precisely, the flux of the curl through that surface) represents the \u0026ldquo;total amount of swirl,\u0026rdquo; and we can determine that just as well by going around the edge and finding how much the flow is following the boundary (Fig. 1.31). Indeed, \\( \\oint \\vec{v} \\cdot \\dd \\vec{l} \\) is sometimes called the circulation of v.\nYou may have noticed an apparent ambiguity in Stokes\u0026rsquo; theorem: concerning the boundary line integral, which way are we supposed to go around (clockwise or counterclockwise)? If we go the \u0026ldquo;wrong\u0026rdquo; way, we\u0026rsquo;ll pick up an overall sign error. The answer is that it doesn\u0026rsquo;t matter which way you go as long as you are consistent, for there is a compensating sign ambiguity in the surface integral: Which way does \\( \\dd \\vec{a} \\) point? For a closed surface (as in the divergence theorem), \\( \\dd \\vec{a} \\) points in the direction of the outward normal; but for an open surface, which way is \u0026ldquo;out\u0026rdquo;? Consistency in Stokes\u0026rsquo; theorem (as in all such matters) is given by the right-hand rule: if your fingers point in the direction of the line integral, then your thumb fixes the direction of \\( \\dd \\vec{a} \\) (Fig. 1.32).\nNow, there are plenty of surfaces (infinitely many) that share any given boundary line. Twist a paper clip into a loop, and dip it in soapy water. The soap film constitutes a surface, with the wire loop as its boundary. If you blow on it, the soap film will expand, making a larger surface, with the same boundary. Ordinarily, a flux integral depends critically on what surface you integrate over, but evidently this is not the case with curls. For Stokes\u0026rsquo; theorem says that \\( \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} \\) is equal to the line integral of \\( \\vec{v}\\ \\) around the boundary, and the latter makes no reference to the specific surface you choose.\nCorollary 1: \\( \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} \\) depends only on the boundary line, not on the particular surface used. Corollary 2: \\( \\oint (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = 0 \\) for any closed surface, since the boundary line, like the mouth of a balloon, shrinks down to a point, and hence the right side of \\( \\eqref{1.57} \\) vanishes. Example 1.11 # Q Suppose \\( \\vec{v} = (2xz + 3y^2) \\vu{y} + (4yz^2) \\vu{z} \\). Check Stokes\u0026rsquo; theorem for the square surface shown in Fig 1.33. A Here\n\\[\\curl \\vec{v} = (4z^2 - 2x) \\vu{x} \u0026#43; 2z \\vu{z} \\quad \\text{and} \\quad \\dd \\vec{a} = \\dd y \\, \\dd z \\, \\vu{x}\\] (In saying that \\\\( \\dd \\vec{a} \\\\) points in the x direction, we are committing ourselves to a counterclockwise integral. We could as well write \\\\( \\dd \\vec{a} \\\\) pointing in the other direction (\\\\( \\dd \\vec{a} = - \\dd y \\, \\dd z\\, \\vu{x} \\\\)) and perform the integral in the clockwise direction.) Since x = 0 for this surface, \\[\\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z (4z^2) = \\frac{4}{3} \\] Now for the line integral, which we of course break into 4 pieces:\n\\[\\tag{i} x = 0 \\quad z = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 3y^2 \\dd y = 1\\] \\[\\tag{ii} x = 0 \\quad y = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 4z^2 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 4z^2 \\dd z = \\frac{4}{3} \\] \\[\\tag{iii} x = 0 \\quad z = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 3y^2 \\dd y = -1\\] \\[\\tag{iv} x = 0 \\quad y = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 0 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 0 \\dd z = 0\\] So\n\\[\\oint \\vec{v} \\cdot \\dd \\vec{l} = 1 \u0026#43; \\frac{4}{3} - 1 \u0026#43; 0 = \\frac{4}{3} \\] It all checks out!\n1.3.6: Integration by Parts # The technique known (awkwardly) as integration by parts exploits the product rule for derivatives:\n\\[\\dv{}{x} (fg) = f \\left( \\dv{g}{x} \\right) \u0026#43; g \\left( \\dv{f}{x} \\right)\\] Integrating both sides, and invoking the fundamental theorem,\n\\[\\int_a ^b \\dv{}{x} (fg) \\dd x = \\left. fg \\right| ^b _a = \\int _a ^b f \\left( \\dv{g}{x} \\right) \\dd x \u0026#43; \\int_a ^b g \\left( \\dv{f}{x} \\right) \\dd x\\] or\n\\[\\int_a ^b f \\left( \\dv{g}{x} \\right) \\dd x = - \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x \u0026#43; \\left. fg \\right| ^b _a \\tagl{1.58}\\] That\u0026rsquo;s \u0026ldquo;integration by parts.\u0026rdquo; It applies to the situation in which you are called upon to integrate the product of one function (f) and the derivative of another (g); it says you can transfer the derivative from g to f, at the cost of a minus sign and a boundary term.\nExample 1.12 # Q Evaluate the integral\n\\[\\int _0 ^\\infty x e^{-x} \\dd x\\] A The exponential can be expressed as a derivative:\n\\[e^{-x} = \\dv{}{x} \\left( - e^{-x} \\right)\\] in this case, then, \\( f(x) = x \\), \\( g(x) = - e^{-x} \\), and \\( df /dx = 1 \\), so\n\\[\\int_0 ^\\infty x e^{-x} \\dd x = \\int _0 ^{\\infty} e^{-x} \\dd x - \\left. x e^{-x} \\right| _{0} ^{\\infty} = - \\left. e^{-x} \\right| _0 ^{\\infty} = 1\\] We can exploit the product rules of vector calculus, together with the appropriate fundamental theorems, in exactly the same way. For example, integrating\n\\[\\div (f\\vec{A}) = f(\\div \\vec{A}) \u0026#43; \\vec{A} \\cdot (\\grad f)\\] over a volume, and invoking the divergence theorem, yileds\n\\[\\int \\div (f \\vec{A}) \\dd \\tau = \\int f(\\div \\vec{A}) \\dd \\tau \u0026#43; \\int \\vec{A} \\cdot (\\grad f) \\dd \\tau \\ \\oint f \\vec{A} \\cdot \\dd \\vec{a}\\] or\n\\[\\int _V f(\\div \\vec{A}) \\dd \\tau = - \\int _V \\vec{A} \\cdot (\\grad f) \\dd \\tau \u0026#43; \\oint _S f \\vec{A} \\cdot \\dd \\vec{a} \\tagl{1.59}\\] Here again the integrand is the product of one function (f) and the derivative (in this case the divergence) of another (A), and integration by parts licenses us to transfer the derivative from A to f (where it becomes a gradient), at the cost of a minus sign and a boundary term (in this case a surface integral).\nIn practice, this turns out to be one of the most useful tools at our disposal in vector calculus. Though you might wonder how often you\u0026rsquo;re really likely to encounter an integral involving the product of one function and the derivative of another, the answer is surprisingly often.\n"},{"id":18,"href":"/r/notes/UWAA560/03-magnetic-field-diagnostics/","title":"Magnetic Field Diagnostics","section":"Plasma Diagnostics","content":" B-dot Probe Diagnostics # Faraday\u0026rsquo;s law is the basis for magnetic field coils:\n\\[\\int \\pdv{\\vec B}{t} \\cdot \\dd \\vec S = - \\oint \\vec E \\cdot \\dd \\vec l = - V_{emf}\\] For a rigid, perfectly conducting loop of N turns in the presence of a changing magnetic field,\n\\[V_i = \\oint \\vec E \\cdot \\dd \\vec l = - \\pdv{\\Phi}{t} = - \\pdv{}{t} \\int \\vec B \\cdot \\dd \\vec s = - N A \\frac{\\int \\pdv{\\vec B}{t} \\cdot \\dd \\vec s}{\\int \\dd \\vec s} \\\\ \\rightarrow V_i \\propto \\dot{B_{avg}}\\] where \\( B_{avg} \\) is the area-averaged magnetic field aligned to loop. The signal \\( V_i \\) increases with loop area, but the spatial resolution decreases. Multiple turns of coil increase the signal (effective loop area). The signal must be integrated in time to obtain the local magnetic field strength.\n\\[B_{avg}(t) = - \\frac{1}{NA} \\int_0 ^t V_i \\dd t\u0026#39;\\] Typical probes are small for spatial resolution, and feature windings in 3 orthogonal directions. Mount the probes on insulating material to avoid perturbing local magnetic field.\nB-dot Probe Calibration # Before using a B-dot probe for diagnostics, a calibration must be performed, which involves applying a known time-varying magnetic field and recording the probe response. Consider the following experimental setup for the calibration of a B-dot probe:\nOur coil is mounted in place and connected via RG 58 to an oscilloscope. Nearby the probe, the magnetic field source is a current-carrying wire connected to a switched charging/discharge circuit as shown. When we perform the experiment, we are able to see a very clean source \\( I_s(t) \\)\nbut we measure a very noisy \\( V_0 (t) \\) at the oscilloscope.\nThe problem is that the finite impedance of the various components of our diagnostic circuit (containing the probe itself) gives rise to resonance. We can see this by analyzing the equivalent impedance circuit. In this calibration, the following components were used:\n\\[L_C = 374 \\mu H \\\\ R_C = 3.2 \\Omega \\\\ C_C = 2 pF \\\\ C_{tp} = 12 oF \\\\ L_{coax} = 0.2 \\mu H \\\\ C_{coax} = 70 pF \\\\ C_O = 20 pF \\\\ R_i = 1 M \\Omega\\] With these values, the simplified LRC equivalent circuit is:\nWe analyze the circuit response by the method of Laplace transforms:\nThe equivalent circuit impedance to the generator voltage is then\n\\[Z(s) = s L_C \u0026#43; R_C \u0026#43; \\frac{R_i}{1 \u0026#43; s C_{eq} R_i}\\] which gives a current \\( I(s) = V_i(s) / Z(s) \\). The output voltage is then (with some algebraic manipulation)\n\\[V_O (s) = \\omega_0 ^2 V_i (s) \\left[ s^2 \u0026#43; s \\left( \\frac{R_C}{L_C} \u0026#43; \\frac{1}{C_{eq} R_i} \\right) \u0026#43; \\omega_0 ^2 \\left( \\frac{R_C}{R_i} \u0026#43; 1 \\right) \\right]^{-1} \\\\ \\omega_0 ^2 = (L_C C_{eq} )^{-1}\\] where \\( \\omega_0 ^2 = (L_C C_{eq})^{-1} \\). If we assume a unit step waveform for the generator voltage (that is, \\( \\dot B \\)), then \\( V_i(s) = \\frac{1}{s} \\) and\n\\[V_O(s) = \\frac{\\omega_0 ^2}{s (s^2 \u0026#43; s \\alpha \u0026#43; \\omega_0 ^2 \\beta)}\\] where \\( \\alpha \\) and \\( \\beta \\) are the coefficients above. Taking the inverse Laplace transform (after some algebra), we arrive at\n\\[V_O(t) = \\frac{1}{\\beta} \\left[ 1 - e^{- \\alpha t / 2} \\left( \\cosh (m t) \u0026#43; \\frac{\\alpha}{2m } \\sinh (m t) \\right) \\right]\\] where \\[m^2 = \\frac{\\alpha^2}{4} - \\omega_0 ^2 \\beta\\] There are thus three possible situations:\nCritically damped: \\( m^2 = 0 \\) Under-damped: \\( m^2 \u0026lt; 0 \\) Over-damped: \\( m^2 \u0026gt; 0 \\) If we plug in the values for our experimental setup, we find \\( m^2 \u0026lt; 0 \\) and \\( \\frac{m}{2 \\pi i} = 818 kHz \\). This is the frequency of the noisy oscillations we picked up on our scope. To reduce the noise, we would like to achieve \\( m^2 = 0 \\), which corresponds with an input impedance of \\( 950 \\Omega \\). Typically oscilloscopes have a high-impedance setting of \\( \\sim 1 M\\Omega \\) and a low-impedance setting of \\( \\sim 50\\Omega \\). We can leave the scope at the high-impedance setting and add a shunt resistor in parallel with the oscilloscope to adjust the input impedance to our liking.\nFor a realistic signal, the rise time will be finite (not a perfect step function). To track a more realistic impulse and reduce noise further, we can analyze the circuit assuming an input voltage profile that better matches the expected rise time. By doing so and carefully choosing the impedance to minimize oscillations, the voltage reported by our diagnostic circuit better represents the source, which is the magnetic field we are trying to measure!. In general, you want to set the input impedance as high as possible without generating oscillations.\nIf we go back to our setup and measure the probe response with different input impedances \\( R_i \\), we see the following:\nAn low input impedance results in overd-amping and a smeared-out signal. At \\( R_i = 4.7 k\\Omega \\) we see several oscillations. At \\( R_i = 2.4 k\\Omega \\) we see a signal that closely resembles the \\( \\dot B \\) we expect.\nA similar decrease in oscillations can also be achieved using a series resistance, but this approach is susceptible to noise. Using a lower impedance is generally the preferred approach, but not always! We will see in our analysis of Rogowski coils later that impedance matching in series can be useful.\nFinally, to recover the field strength \\( B \\) we are trying to measure, we need to integrate our signal. This can be achieved numerically from the digitization recorded by our scope. However, a digitizer is only able to record a finite bit depth, often as little as 8-bits, and a lack of precise sampling will result in drift and imprecise measurement. Typically, it is better to electronically integrate the signal by adding a passive integrator:\nNote: the passive integrator here is a low-pass filter. After adding the integrator, we must repeat the circuit analysis with a realistic magnetic field, e.g. a triangular wave (such that \\( \\dot B \\) becomes a square wave):\n\\[\\mathscr{L}(\\text{square}(t/a)) = \\frac{\\tanh(as/2)}{s}\\] and a triangular wave is defined as\n\\[\\text{trw}(t) = \\int _0 ^t \\text{square}(t\u0026#39;) \\dd t\u0026#39;\\] Further B-dot probe considerations # Small coils better localize the measurement. Since you\u0026rsquo;re averaging the magnetic field over the cross-sectional area of the probe, you can better localize the field. Small coils also limit the perturbation of the plasma. However, the downside is that they are more difficult to make. They are typically wound by hand (by some poor graduate student) and can be very tedious to construct (especially 3D coils with many turns). Small coils also produce smaller signals, so they require additional windings to make up for the signal loss. Some labs have used surface-mounted inductors to make miniature B-dot probes. Surface-mounted components have many advantages in size and inductance values, and can be used as very small pickup coils for magnetic field measurements. Additional windings increase the inductance, and this can affect the frequency response of the coil. You can always make up for the frequency response by adjusting the input impedance, but often this will decrease the signal amplitude and may cause the signal-to-noise ratio to decrease. Active integrators can improve the bandwidth response of the probe and automatically eliminate droop and drift. Rogowski Coils # Rogowski coils are also fundamentally magnetic field probes, but instead of measuring the local magnetic field averaged over the coil area as B-dot probes, Rogowski coils measure the line integral of the magnetic field, which is related by Ampere\u0026rsquo;s law to the enclosed current.\nIf we have some plasma current \\( j_z \\) producing a magnetic field \\( B_\\theta \\), then a Rogowski coil is a solenoidal coil that goes around the plasma as shown to produce a voltage \\( V_0 \\).\nThe key geometric property of the Rogowski coil is the constant inner coil radius (area).\nThe total magnetic flux, \\( \\oint \\vec B \\cdot \\dd \\vec s \\), linked by the Rogowski coil is\n\\[\\Phi = N \\int \\dd S \\oint \\vec B \\cdot \\dd \\vec l = \\mu_0 (N \\int \\dd S) I\\] From Faraday\u0026rsquo;s law,\n\\[V_0 = - \\dot \\Phi = \\mu_0 (N \\int \\dd S) \\dot I\\] So the voltage we measure is related to the time rate of change of the current enclosed in the Rogowski coil. The signal \\( V_0 \\) is integrated in time to give the total current enclosed by the Rogowski coil, \\( I(t) \\). Just like the B-dot probes, instead of computing the geometry of the coil we calibrate the probe by driving current from a known source and measuring the response. They are easier to calibrate because the exact geometry does not matter, as long as the Rogowski coil encloses the current being measured.\nConsiderations # The signal is insensitive to the shape of the coil since \\( \\oint \\vec B \\cdot \\dd \\vec l \\) is path independent. We do start to lose signal if we make the loop much larger than the source where the current source is, but as long as the coil is more or less snug about the current source the measurement error is minimal. However, the coil does need to form a complete loop. Signals can be very large, easily \\( \\sim 1 kV \\). So adding an attenuator upstream of your measurement circuit is important. The signal can be contaminated by unwanted \\( \\dot{\\vec B} \\), so the coils are counter-wound. In the lab, we'll often fabricate these coils by stripping the outer sheath and outer conductor from a length of RG58 coaxial cable. Then, we take a wire and solder it to the end of the inner conductor, then wrap the wire about the insulator of the RG58 back towards the un-stripped end. Finally, connect the wire to the outer conductor of the coaxial cable. Hall Probe # B-dot probes are not very useful for steady-state magnetic fields (it\u0026rsquo;s right there in the name). For steady-state fields, we can exploit the Hall effect. Passing a known current through a conductor with an embedded magnetic field will generate a transverse potential:\n\\[\\frac{ \\vec j \\cross \\vec B}{nq} = - \\vec E\\] A simultaneous measurement of the voltage drop and current gives a measurement of the embedded magnetic field. Such a device is usually called a Gaussmeter.\n"},{"id":19,"href":"/r/notes/UWAA545/03-pic-method/","title":"Particle in Cell Model","section":"Computational Methods For Plasmas","content":" \\[\\] Particle-In-Cell Model (Particle model revisited) # The order of magnitude issues with the N-body model prevent a direct application in even simple laboratory plasmas. In a plasma with \\( N = O(10^{21}) \\), we have interactions on the order of \\( O(10^{42}) \\) making direct computation completely impractical.\nWe can instead use representative particles with the same charge to mass ratio, but fewer (\\( N = 10^3 \\sim 10^7 \\)) particles. The governing equations for such a model look very similar to the N-body model:\n\\[\\dv{\\vec v_i}{t} = \\left( \\frac{q}{m} \\right)_i (\\vec E \u0026#43; \\vec v_i \\cross \\vec B) \u0026#43; \\sum_{j \\neq i} \\left. \\dv{\\vec v_{ij}}{t} \\right|_{coll} \\delta (\\vec r_i - \\vec r_j)\\] \\[\\dv{\\vec r_i}{t} = \\vec v_i\\] These hold for each superparticle \\( i \\). We can track the position and velocity in time \\( (\\vec r, \\vec v)_i \\) of each superparticle \\( i \\). The physical properties of superparticle \\( i \\) will be:\n\\[m_i = m_\\alpha \\frac{N_{plasma}}{N_{model}} \\qquad \\alpha = \\text{ion, electron, ...}\\] \\[q_i = q_\\alpha \\frac{N_{plasma}}{N_{model}}\\] \\[n_i = n_\\alpha \\frac{N_{model}}{N_{plasma}}\\] Since \\( q_i / m_i = q_\\alpha / m_\\alpha \\), the collisionless dynamics will be preserved for the same \\( \\vec E \\) and \\( \\vec B \\). Provided a sufficiently large number of superparticles (adequate statistics), this means we can use this heavily reduced model to capture the entire behavior of the plasma.\nLet\u0026rsquo;s compute some plasma parameters for this \u0026ldquo;undersampled\u0026rdquo; system.\n\\[\\omega_p = \\sqrt{\\frac{n_\\alpha q_\\alpha ^2}{\\epsilon_0 m_\\alpha}} = \\sqrt{\\frac{n_i q_i ^2}{\\epsilon_0 m_i}}\\] We find that the plasma frequency remains the same for the sampled system. The cyclotron frequency is also preserved for the physical system.\n\\[\\omega_c = \\frac{q_\\alpha B}{m_\\alpha} = \\frac{q_i B}{m_i}\\] However, if we look at the Debye length, we see that it changes\n\\[\\lambda_D = \\frac{v_{th}}{\\omega_p} = \\sqrt{\\frac{\\epsilon_0 k T_\\alpha}{n_\\alpha q_\\alpha ^2}} = {\\sqrt{\\frac{\\epsilon_0 k T_i}{n_i q_i ^2}}} \\sqrt{\\frac{N_{model}}{N_{plasma}}}\\] This difference will have important implications on the spatial scales and resolution of the model. The coupling to the Maxwell\u0026rsquo;s equations is accomplished as we did in the N-body model.\nThere are two outstanding issues that we need to address before we can move on to computation:\nSingularities. The Coulomb force between two charged particles approaches \\( \\infty \\) as \\( r \\rightarrow 0 \\). Large number of interactions. Even with fewer particles, \\( O(10^7) \\), there are still \\( N^2 \\) particle-particle interactions at all times. \\( O(10^{14}) \\) interactions is still not really practical to compute. To deal with singularities, we can use finite-size particles, with their properties diffusely distributed throughout their volume. When these particle \u0026ldquo;clouds\u0026rdquo; begin to overlap, the overlapping volumes cancel each other, and when the particles overlap entirely (\\( r = 0 \\)), the force is zero\nThe same technique is used in hydrodynamics in a model called SPH (smooth particle hydrodynamics). SPH techniques are used quite often for hypervelocity impacts, like asteroids striking a planet. An interesting application of SPH is the brasil nut problem: why do brasil nuts end up at the top of a can of mixed nuts? As it turns out, if we model simple normal collisions in a can of particles of varying sizes, we don\u0026rsquo;t see the effect at all. However, if we introduce friction between particles then the effect does show up, so we can deduce that the surface area to volume ratio of the brasil nut is important.\nUsing SPH in a plasma is more complicated, because plasma particles interact with every other particle through long range EM forces. We can no longer limit interactions locally in a plasma. Instead, to limit the number of interactions, we use a spatial grid to integrate the EM forces. Electromagnetic sources from each particle \\( i \\) are assigned to a spatial grid node \\( j \\)\nThen, \\( \\vec E \\) and \\( \\vec B \\) are computed on the grid based on \\( \\rho_j \\) and \\( \\vec j _j \\) to give \\( (\\vec E, \\vec B)_j \\). Then, particle positions and velocities are evolved using \\( (\\vec E, \\vec B)_j \\) to compute local forces on each particle.\nThe computational time required to compute the fields depends on the number of grid points, \\( M \\). At each time step, this requires \\( O(M^2) \\) operations. The computational time required to compute assign particles positions and velocities to the grid and compute forces on each particle is \\( O(N) \\). This makes the total compute time \\( O(M^2 + \\alpha N) \\ll N^2 \\)\nRelationship to N-Body and Kinetic Models # PIC is similar to the N-body model in some obvious ways:\nThe plasma is treated as discrete particles that interact through EM fields Same governing equations, only with fewer particles However, the EM fields are computed by currents and charges localized to a spatial grid. This is an important distinction. It is what removes the n-body interactions from the model.\nBy comparison to the kinetic model, kinetic models are derived by performing an ensemble average to transform from discrete space to a smooth, continuous space. The averaging process accounts for smoothly varying long-range forces, which is just what we\u0026rsquo;ve done by mapping our particles to a spatial grid. The short-range abrupt forces are encompassed in the collision operator in the same way. The same idea of separating the smooth long-range forces from the abrupt forces was the key Vlasov insight. As we arbitrarily crank up the spatial resolution, the PIC model converges not to the N-body model, but to the Vlasov kinetic model.\nPIC models are \u0026ldquo;particle ensemble averages,\u0026rdquo; but not in a strict sense. They can be thought of as a sampling or discretization of the continuous phase space.\nGeneral Comments on Kinetic Models # The PIC method is a robust and efficient method for collisionless plasmas. Collisions are usually modeled using Monte Carlo methods. PIC does not work well when the physics is dominated by a small population (small number of particles), because the sampling error becomes very significant. PIC methods work well for handling vacuum boundaries, but physical boundaries are more difficult. This is the opposite of the case for fluid models. PIC methods naturally resolve sharp features in phase space, e.g. a cold beam Continuum kinetic methods can require more computational effort. PIC methods have been around since the 60\u0026rsquo;s, but continuum models have only become computationally practical more recently. There are a couple of reasons for this. Dimensionality is a key component. Continuum distribution methods encompass a six-dimensional space, but particle methods can work in three-dimensional spaces. Additionally, solving the governing equations Boltzmann, Vlasov, Vlasov + Fokker-Planck, etc. are more complicated than the equation of motion for the PIC method. Continuum methods better model tails of distributions, which is often where interesting physics happens. E.g ionization, fusion are driven by the high energy tail of the distribution, while the overall dynamics are determined by the bulk. If tails are not interesting, then computational effort is wasted on large unoccupied regions of phase space. Tails also constrain the numerical solution for continuum methods. High velocity and low density can lead to oscillations where the value of the distribution function is very small, leading to potentially negative values. Continuum methods (more generally fluid methods) can introduce numerical diffusion and dispersion. Diffusion leads to a smoothing of sharp features over time. Dispersion leads to low/high wavenumber features propagating at the wrong speed. Computational Methods for PIC # PIC can easily be extended to include relativistic effects, only requiring minimal changes to the equation of motion. Collisions are often neglected in PIC methods, assuming a low density, collisionless plasma. As we try to apply PIC methods to collisional plasmas, the accuracy gets worse. Let\u0026rsquo;s talk about how we go about solving the governing equations for a plasma. First, we divide the domain into a uniform grid, \\( \\Delta x = \\Delta y = \\Delta z = const. \\). We will have a distribution of particles throughout the domain with independent position and velocity \\( (\\vec x, \\vec v)_i \\).\nThe geometry of the particles themselves depends on dimensionality.\nParticles have an infinite extent in unresolved dimensions. In 3D, this means that particles are spheres. In 2D they are cylindrical, and in 1D they are planar. The general procedure for solving the PIC method goes like this:\nParticles are weighted to the grid. The position of each particle \\( i \\) in phase space \\( (\\vec x, \\vec v)_i \\) is weighted to the grid to compute the electromagnetic source terms \\( (\\rho_c, \\vec j)_j \\) at each grid location \\( j \\). \\[\\underbrace{(\\vec x, \\vec v)_i}_{\\text{particles}} \\rightarrow \\underbrace{(\\rho_c, \\vec j)_j}_{\\text{grid}} \\qquad \\text{(Particle weighting)}\\] Advance the electric and magnetic fields on the grid \\[\\underbrace{(\\rho_c, \\vec j)_j}_{\\text{grid}} \\rightarrow \\underbrace{(\\vec E, \\vec B)_j}_{\\text{grid}} \\qquad \\text{(Field solve)}\\] Electromagnetic fields are weighted to particles \\[\\underbrace{(\\vec E, \\vec B)_j}_{\\text{grid}} \\rightarrow \\underbrace{\\vec F_i}_{\\text{particle}} \\qquad \\text{(Force weighting)}\\] Particles are advanced in time \\[\\vec F_i \\rightarrow \\vec v_i \\rightarrow \\vec x_i\\] Particle \u0026amp; Force Weighting # As mentioned earlier, PIC uses finite size particles rather than point particles to avoid the issue of force singularities. This is particularly important in the 1-dimensional case, since infinitesimal particles would never be able to exchange positions. The exact size and shape of the particles matter. The charge density for a point particle is\n\\[\\rho_c(\\vec r) = q_i \\delta (\\vec r - \\vec r_i)\\] For a finite-size particle,\n\\[\\rho_c (\\vec r) = q_i S(\\vec r - \\vec r_i)\\] where \\( S \\) is a particle shape factor. Conservation of charge requires \\[\\int S(\\vec r - \\vec r_i) \\dd \\vec r = \\int \\delta ( \\vec r - \\vec r _i) \\dd \\vec r = 1\\] The Lorentz force must be weighted to the particles using the same shape factor in order to get consistent dynamics. For \\( \\vec B = 0 \\), \\[\\vec F_i = q_i \\vec E (\\vec r_i) = \\overbrace{q_i \\int \\vec E(\\vec r) \\delta (\\vec r - \\vec r_i) \\dd \\vec r}^{\\text{point particles}} \\\\ = \\underbrace{q_i \\int \\vec E(\\vec r) S(\\vec r - \\vec r_i) \\dd \\vec r}_{\\text{finite-size particles}}\\] Particles must have local support, meaning \\( S(\\vec r - \\vec r_i) \\) is spatially restricted to a small portion of the grid. The simplest weighting/shape is a uniform charge cloud. In 1D, this is\n\\[S(x) = \\begin{cases} 1/a_0 \u0026amp;\\qquad \u0026amp; \\text{if} \\quad |x| \\leq a_0 /2 \\\\ 0 \u0026amp;\\qquad \u0026amp; \\text{otherwise} \\end{cases}\\] This provides zeroth-order weighting to the nearest grid point (NGP). When we go to compute the fields from our source terms, the particle will only contribute to the closest single grid point.\nIf the size of our particle is very small compared to the grid spacing \\( \\Delta x \u0026gt; a_0 \\), what happens to \\( \\rho_c \\) as the particle drifts through the grid? Because we compute the weighted density as \\( \\rho_c (\\vec r) = q_i S(\\vec r - \\vec r_i) \\), there will be some positions for which a particle will not contribute to any grid point. Likewise, if \\( \\Delta x \u0026lt; a_0\u0026gt; \\), then the particle appears on multiple grid points, but only at some times, producing double-counting. Both of these cases violate conservation of charge:\n\\[\\pdv{}{t} \\int q_\\alpha f_\\alpha \\dd \\vec x \\dd \\vec v = 0\\] To conserve charge, \\( \\Delta x \\) must equal \\( a_0 \\). This is a limitation of zeroth-order weighting.\nEven if we have \\( \\Delta x = a_0 \\), nearest grid point still produced particles that appear and disappear on the grid instantaneously, but at least they appear/disappear in a way that conserves total charge. The resulting charge distribution is noisy as a result, hinting at a general issue with PIC methods. The noise introduced by weighting charge distribution to a grid is addressed by using many particles, \\( N \\gg M \\). Typically, we\u0026rsquo;ll use \\( N \\sim M^2 \\).\nThe nearest grid point weighting can be improved to 1st order, using a linear \\( S(x) \\).\nWe maintain the important property \\( \\int S(x) \\dd x = 1 \\). Using \\( a_0 = \\Delta x \\) so that\n\\[\\sum_j S(x_j - x) = 1\\] for any \\( x \\). The resulting charge density after weighting looks like this:\n\\[\\rho_j = q_i \\frac{x_{j\u0026#43;1} - x_i}{\\Delta x} \\qquad \\rho_{j\u0026#43;1} = q_i \\frac{x_i - x_j}{\\Delta x}\\] The same weighting strategy extends to multiple dimensions. In 2D, this is bilinear weighting; in 3D we have trilinear. 1st order weighting requires more computational effort than nearest grid point, but produces smoother fields. The instantaneous appearance/disappearance of particles as they move across the grid is now gone, leading to a smoother electric field. This is the typical PIC weighting method. Higher-order weightings further reduce noise, but with dramatically increased computational expense. For example, a second-order scheme might be\n\\[S_2( x_j - x) = \\frac{3}{4} - \\left( \\frac{x - x_j}{\\Delta x} \\right)^2\\] \\[S_2(x_{j \\pm 1} - x) = \\frac{1}{2} \\left( \\frac{1}{2} \\pm \\frac{x - x_j}{\\Delta x} \\right) ^2\\] All shape functions must satisfy conservation of charge \\( \\sum_j S(x_j - x) = 1 \\) for any \\( x \\). But they must also preserve the first moment\n\\[\\sum_j x_j S(x_j - x) = x\\] to preserve the electric dipole moment. And, of course, we require \\( S(x) \\geq 0 \\) everywhere. This last requirement will trip up higher order methods if we naively use Lagrange interpolation functions. Instead, we can construct higher orders by using functions defined by convolutions of lower order shapes.\n\\[\\begin{aligned} m = 1 \u0026amp; \\qquad \u0026amp; S _{NGP} \\star S_{NGP} = \\int S_0(x\u0026#39;) S_0 (x - x\u0026#39;) \\dd x\u0026#39; = S_1(x) \\\\ m = 2 \u0026amp; \\qquad \u0026amp; S_1 \\star S_{NGP} = S_x(x) \\end{aligned}\\] As you go to higher and higher order, \\( S_m(x) \\) approaches a Gaussian.\nLet us consider, does the particle shape affect the field distribution? What about the force distribution? Consider a uniform, finite-size ion and an infinitesimal electron (sheet), such that \\( q_i + q_e = 0 \\).\n\\[\\int e n_i \\dd x \u0026#43; \\rho _s = 0\\] From Gauss\u0026rsquo;s law,\n\\[\\dv{E_x}{x} = \\frac{\\rho_c}{\\epsilon_0}\\] with \\( E_x = 0 \\) outside. Solving Gauss\u0026rsquo;s law, we find an electric field like this:\nFrom this, we conclude that outside of the particle extent, the field is independent of the particle shape. If we consider a charge sheet with an arbitrary shape \\( \\rho_c(x) \\) in an electric field, bounded between \\( x = a, x = b \\), the force on the sheet is\n\\[F = \\int_a ^b \\rho_c (x) E_x \\dd x = \\int_a ^b \\frac{\\epsilon_0}{2} \\dv{}{x} (E_x ^2) \\dd x \\\\ = \\frac{\\epsilon_0}{2} (E_b ^2 - E_a ^2) = \\epsilon_0 (E_b - E_a) \\left( \\frac{E_b \u0026#43; E_a}{2} \\right) = \\epsilon_0 \\Delta E \\overline{E} = q \\overline{E}\\] The particle shape does not affect the physics outside of the particle shape. The force on the particle is only determined by the average electric field.\nParticle Push # Once the source terms have been weighted to the grid, we wish to advance particles\u0026rsquo; position and velocity based on the Lorentz force.\n\\[m \\dv{\\vec v}{t} = \\vec F \\\\ \\dv{\\vec x}{t} = \\vec v\\] We could solve these using any time integration technique, e.g. Runge-Kutta. But instead, we are going to choose a leap-frog method. It requires the fewest operations for the same order of accuracy (2nd order) and is a symplectic integrator, which means that it is a canonical transformation from one time to another time. Canonical transformations have important conservation properties; symplectic integrators conserve integrators, and are reversible in time.\nTo push the particles, we will use finite difference operators to approximate the ODEs.\n\\[m \\frac{\\vec v^{new} - \\vec v^{old}}{\\Delta t} = \\langle \\vec F \\rangle \\\\ \\frac{\\vec x^{new} - \\vec x^{old}}{\\Delta t} = \\langle \\vec v \\rangle\\] The averaging treatment we use to get the RHS determines the numerical accuracy and stability of the scheme.\n\\[\\vec v^{new} = \\vec v^{old} \u0026#43; \\frac{\\Delta t \\langle F \\rangle}{m} \\\\ \\vec x^{new} = \\vec x^{old} \u0026#43; \\Delta t \\langle \\vec v \\rangle\\] Here is where the leap-frog method comes in. We will center the right-hand side in time to get a 2nd order accuracy. By off-setting the velocity and position calculations, we get the average velocity we need for the centered leap-frog:\nTo properly center the force, \\( \\vec E \\) and \\( \\vec B \\) must be expressed at the same points as position. They are then used to compute the next velocity \\( v(n+1/2) \\), from which we advance the position.\nLeap-frog is second order accurate, but can be dispersive. This will give us incorrect wave speeds/frequencies/phase, but correct magnitude. Consider an electrostatic harmonic oscillator (Langmuir oscillations). For a charged particle oscillating around a fixed particle,\n\\[\\frac{v^{n \u0026#43; 1/2} - v^{n-1/2}}{\\Delta t} = \\frac{q}{m} E(x^n)\\] \\[\\frac{x^{n\u0026#43;1} - x^n}{\\Delta t} = v^{n\u0026#43;1/2} \\qquad \\text{ and } \\qquad \\frac{x^n - x^{n-1}}{\\Delta t} = v^{n-1/2}\\] Gauss\u0026rsquo;s law gives\n\\[\\dv{E}{x} = \\frac{q n }{\\epsilon_0} \\rightarrow E(x^n) = \\frac{q n }{\\epsilon_0} x^n\\] Combine to give\n\\[\\frac{x^{n\u0026#43;1} - 2 x^n \u0026#43; x ^{n-1}}{\\Delta t^2} = \\frac{q}{m} E(x^n) = - \\frac{q^2 n}{m \\epsilon_0} x^n = - \\omega_p ^2 x^n\\] This equation approximates the ODE for Langmuir oscillations\n\\[\\dv{^2 x}{t^2} \u0026#43; \\omega_p ^2 x = 0\\] which has the solution\n\\[x(t) = C e^{\\pm i \\omega_p t}\\] That\u0026rsquo;s the same finite difference we\u0026rsquo;re using for our leap-frog particle push, which indicates that we\u0026rsquo;re solving the same differential equation. We can assume an oscillating solution to our finite difference equation where\n\\[x^n = c e^{i \\omega t^n}\\] where \\( t^n = n \\Delta t \\). Substituting \\( x^n \\) into the finite difference expression, \\[e^{i \\omega \\Delta t} - 2 \u0026#43; e^{- i \\omega \\Delta t} = - \\omega _p ^2 \\Delta t^2\\] Some algebraic manipulation leads to\n\\[2\\left( \\frac{e^{i \\omega \\Delta t} \u0026#43; e^{- i \\omega \\Delta t}}{2} - 1 \\right) = - \\omega_p ^2 \\Delta t^2 \\\\ 2 \\left( \\cos (\\omega \\Delta t) - 1 \\right) = - \\omega _p ^2 \\Delta t^2\\\\ \\sin ^2 \\left( \\frac{\\omega \\Delta t}{2} \\right) = \\left( \\frac{\\omega _p \\Delta t}{2} \\right)^2\\] That\u0026rsquo;s interesting. We should have a solution \\( \\omega = \\pm \\omega_p \\), but instead we have this other solution. Plotting \\( (\\omega \\Delta t / 2) \\) vs \\( (\\omega_p \\Delta t / 2) \\), we get a plot like this:\nThe deviation from \\( \\omega = \\omega_p \\) is what\u0026rsquo;s called a phase error. Nominally, we can set \\( \\Delta t \\) to whatever we want. Setting \\( \\Delta t \\) further out gives an imaginary \\( \\omega = \\pm i \\gamma \\), so\n\\[x = c e^{\\gamma t}\\] which is an unstable solution. So, if \\( \\Delta t \\) is small, the leap frog finite difference gives an accurate solution. Increasing \\( \\Delta t \\) enters a region where phase errors become large. Going all the way up to \\( \\Delta t = 2/\\omega_p \\), we get imaginary solutions to the differential equation and the solution becomes unstable. This is the Leap-frog instability.\nElectromagnetic Field Solver # At the field solver step, we need to advance \\( \\vec E \\) and \\( \\vec B \\) according to Maxwell\u0026rsquo;s equations based on particles\u0026rsquo; positions and velocities.\nIn the electrostatic case, Faraday\u0026rsquo;s law reduces to \\[\\curl \\vec E = 0 \\rightarrow \\vec E = - \\grad \\phi\\] Combined with Gauss\u0026rsquo;s law, yields Poisson\u0026rsquo;s equation\n\\[\\div \\vec E = \\rho_c / \\epsilon_0 \\rightarrow \\grad ^2 \\phi = - \\rho_c / \\epsilon_0\\] Poisson\u0026rsquo;s equation can be solved with finite differences. In 1D,\n\\[\\frac{\\phi _{j\u0026#43;1} - 2 \\phi_j \u0026#43; \\phi_{j-1}}{\\Delta x^2} = - \\frac{\\rho_j}{\\epsilon_0}\\] In matrix form, this yields a tridiagonal matrix equation.\n\\[\\vec A \\vec \\phi = \\vec b \\rightarrow \\vec \\phi = \\vec A ^{-1} \\vec b\\] where\n\\[\\vec \\phi = \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\vdots \\\\ \\phi_J \\end{bmatrix}\\] \\[\\vec A = \\frac{1}{\\Delta x^2} \\begin{bmatrix} \\ldots \u0026amp; 1 \u0026amp; -2 \u0026amp; 1 \u0026amp; \\ldots \\end{bmatrix}\\] \\[\\vec b = \\begin{bmatrix} - \\rho_{j-1} / \\epsilon_0 \\\\ - \\rho_j / \\epsilon_0 \\\\ \\rho_{j\u0026#43;1} / \\epsilon_0 \\\\ \\ldots \\end{bmatrix}\\] We can run into some trouble here with periodic domains. For a periodic boundary condition, \\( \\vec A \\) looks like\n\\[\\vec A \\sim \\begin{bmatrix} -2 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\\\ 1 \u0026amp; -2 \u0026amp; 1 \u0026amp; \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp; 1 \u0026amp; -2 \u0026amp; 1 \\\\ 1 \u0026amp; \\ldots \u0026amp; 1 \u0026amp; -2 \\\\ \\end{bmatrix}\\] which is singular. This problem arises from a lack of appropriate boundary conditions. Even with periodic boundary conditions, we know the solution \\( \\phi \\) is lacking an appropriate gauge, e.g. \\( \\phi_1 = 0 \\) or \\( \\sum_j \\phi_j = 0 \\). Setting \\( \\phi_1 = 0 \\) modifies \\( \\vec A \\):\n\\[\\vec A \\sim \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\\\ 1 \u0026amp; -2 \u0026amp; 1 \u0026amp; \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp; 1 \u0026amp; -2 \u0026amp; 1 \\\\ 1 \u0026amp; \\ldots \u0026amp; 1 \u0026amp; -2 \\\\ \\end{bmatrix}\\] This is no longer singular, and we can invert it.\nWith \\( \\phi_j \\) in hand, we compute the electric field as\n\\[E_{x_{j}} = - \\frac{\\phi_{j\u0026#43;1} - \\phi_{j-1}}{2 \\Delta x}\\] and solve the Vlasov-Poisson system.\nPeriodic Domains - Fourier Series Expansion # For periodic domains, an alternative approach to solving Poisson\u0026rsquo;s equation has some unique advantages compared to the finite difference method. In this approach, we transform from a discrete physical space to Fourier wave space. In wave space, differential operators become multiplications.\n\\[\\rho(x) \\overbrace{\\rightarrow}^{FT} \\rho(k)\\] using the transformation kernel \\( e^{i k x} \\). Poisson\u0026rsquo;s equation becomes\n\\[k ^2 \\phi (k) = \\rho(k) / \\epsilon_0 \\rightarrow \\phi(k) = \\frac{\\rho(k)}{\\epsilon_0 k^2}\\] With the potential, an inverse fourier transform gives \\( \\phi(k) \\rightarrow \\phi(x) \\). We can get the field either by computing the gradient\n\\[- \\grad \\phi(x) = \\vec E(x)\\] or by computing the gradient first in phase space\n\\[i k \\phi(k) = \\vec E(k) \\rightarrow \\vec E(x)\\] We have a finite number of locations on our grid, so likewise our Fourier series expansion is also truncated at a finite \\( k \\).\n\\[g(k) = \\Delta x \\sum_{j=0} ^{J-1} g(x_j) e^{-i k x_j}\\] The continuous inverse transform is\n\\[g(x_j) = \\frac{1}{2\\pi} \\int_{- \\pi / \\Delta x} ^{\\pi / \\Delta x} g(k) e^{i k x_j} \\dd k \\qquad \\text{(infinite k)}\\] but for a discrete number of modes \\( k \\),\n\\[g(x_j) = \\frac{1}{L} \\sum_{n=-J/2} ^{J/2 - 1} g(k) e^{i k x_j}\\] where \\( k = 2 \\pi n / L \\).\nFinite Fourier transformations (FFT) are susceptible to aliasing errors. Higher wave numbers than we can resolve are aliased to lower frequencies.\n\\[E(k) = - i k \\phi(k) \\frac{\\sin(k \\Delta x)}{k \\Delta x} = - i \\kappa \\phi(k)\\] \\[\\phi(k) = \\frac{\\rho(k)}{\\epsilon_0 k^2} \\left[ \\frac{k \\Delta x}{\\sin (k \\Delta x)}\\right]^2 = \\frac{\\rho(k)}{\\epsilon_0 \\kappa ^2}\\] The discrete effects vanish as \\( k \\Delta x \\rightarrow 0 \\) (infinite resolution). Aliasing occurs if \\( |k \\Delta x| \u0026gt; \\pi \\) which appears on the grid as lower values of \\( k \\Delta x \\).\nFull Electrostatic PIC Algorithm # Now we\u0026rsquo;ve got everything we need to implement an electrostatic PIC model.\nParticle weighting to the grid using shape function (\\( \\vec j \\) is not needed for electrostatics) \\[ x_i \\rightarrow \\rho_j \\] Electric field solve from Poisson\u0026rsquo;s equation. Finite difference: \\[ \\grad ^2 \\phi = - \\rho / \\epsilon_0 \\rightarrow \\vec E = - \\grad \\phi \\] or FFT: \\[ \\rho(x_j) \\overbrace{\\rightarrow}^{FFT} \\rho(k) \\overbrace{\\rightarrow}^{k^2} \\phi(k) \\overbrace{\\rightarrow}^{k} \\vec E(k) \\overbrace{\\rightarrow}^{IFFT} E(x_j) \\] Force weighting to particles using shape function \\[ \\vec E_j \\rightarrow \\vec F_i \\] Leap frog advance to push the particles \\[ \\vec F_i \\rightarrow \\vec v_i \\rightarrow \\vec x_i \\] Magnetostatic PIC # In the magnetostatic case, we introduce a constant magnitude magnetic field into the mix. The Lorentz force becomes\n\\[\\vec F_i = q \\vec E \u0026#43; q ( \\vec v_i \\cross \\vec B)\\] The effect of the magnetic field is to rotate the particle\u0026rsquo;s trajectory. To compute the force on the particle, both \\( \\vec E \\) and \\( \\vec B \\) must be known at \\( \\vec x_i \\). This changes how we represent our phase space. The minimum relevant configuration is 1D-2V, e.g. \\( (x, v_x, v_y) \\) with\n\\[\\vec E = E_x \\vu x\\] \\[\\vec B = B_0 \\vu z\\] \\[\\vec v = v_x \\vu x \u0026#43; v_y \\vu y\\] \\( \\vec B \\) does not alter the magnitude \\( | \\vec v | \\). \\( \\vec E \\) can alter the magnitude of \\( | \\vec v | \\).\nBoth \\( \\vec E \\) and \\( \\vec B \\) must be centered in time relative to \\( \\vec v \\). The Leap frog method advances \\( \\vec v ^{n - 1/2} \\) to \\( \\vec v^{n + 1/2} \\) using force and fields at time \\( n \\).\n\\( \\vec v \\) is known at \\( n \\pm \\frac{1}{2} \\), but we need to compute \\( \\vec B \\) at \\( n \\). One solution is to use Strang splitting to split the advance (acceleration \\( q \\vec E \\)) with the rotation \\( q (\\vec v \\cross \\vec B) = \\omega_c (\\vec v \\cross \\vec \\vu z) \\). The way this works is we apply half of the acceleration, then we apply the rotation, then the other half of the acceleration.\nHalf acceleration \\[v\u0026#39; _x = v_x ^{n - 1/2} \u0026#43; \\frac{\\Delta t}{2} \\frac{q}{m} E_x ^n\\] \\[v\u0026#39; _y = v_y ^{n - 1/2}\\] Full Rotation \\[\\begin{bmatrix} v_x \u0026#39;\u0026#39; \\\\ v_y \u0026#39;\u0026#39; \\end{bmatrix} = \\begin{bmatrix} \\cos (\\omega_c \\Delta t) \u0026amp; \\sin ( \\omega_c \\Delta t) \\\\ - \\sin ( \\omega_c \\Delta t) \u0026amp; \\cos ( \\omega_c \\Delta t) \\end{bmatrix} \\begin{bmatrix} v_x \u0026#39; \\\\ v_y \u0026#39; \\end{bmatrix}\\] Half acceleration \\[v_x ^{n \u0026#43; 1/2} = v_x \u0026#39;\u0026#39; \u0026#43; \\frac{\\Delta t}{2} \\frac{q}{m} E_x ^n\\] \\[v_y ^{n \u0026#43; 1/2} = v_y \u0026#39;\u0026#39;\\] This Strang splitting method is similar to e.g. Runge-Kutta methods which calculate an intermediate value, then use that value to step all the way.\nLike all leap frog methods, we have to start magnetostatic PIC at \\( n = 0 \\). The initial conditions for magnetostatic PIC are the initial positions and velocities at \\( t = 0 \\), which give us the fields \\( \\vec E \\) and \\( \\vec B \\) at \\( t = 0 \\). We need \\( \\vec v ^{ n - 1/2} \\), same as we did for the electrostatic case. We can start by applying a similar Strang splitting to integrate backwards in time by \\( \\Delta t / 2 \\). We omit the first half-acceleration, and only perform an half rotation.\nRotate \\[\\begin{bmatrix} v_x \u0026#39; \\\\ v_y \u0026#39; \\end{bmatrix} = \\begin{bmatrix} \\cos ( - \\omega _c \\Delta t / 2) \u0026amp; \\sin ( - \\omega _c \\Delta t / 2) \\\\ - \\sin ( - \\omega _c \\Delta t / 2) \u0026amp; \\cos ( - \\omega_c \\Delta t / 2) \\end{bmatrix} \\begin{bmatrix} v_x ^{n=0} \\\\ v_y ^{n=0} \\end{bmatrix}\\] Accelerate \\[v_x ^{n = - 1/2} = v_x \u0026#39; - \\frac{\\Delta t}{2} \\frac{q}{m} E_x ^{n=0}\\] \\[v_y ^{n = - 1/2} = v_y \u0026#39;\\] From there, we can proceed with the leap frog algorithm.\nElectrodynamic PIC # Finally, we allow the magnetic field to change in time. We still limit our model to 1D-2V (\\( x, v_x, v_y \\)), but now \\( B_z \\) is not constant and electric field \\( E_y \\) and current density \\( j_y \\) in the \\( \\vu y \\) direction are allowed.\nPhysically, this means that waves can propagate, but only in the \\( \\vu x \\) direction. To compute the fields from grid sources, we need to solve the full Maxwell\u0026rsquo;s equations. We\u0026rsquo;ll do so in Heaviside-Lorentz units:\n\\[\\pdv{\\vec E}{t} = c \\curl \\vec B - \\vec j\\] \\[\\pdv{\\vec B}{t} = - c \\curl \\vec E\\] In the 1D-2V system, these become\n\\[\\pdv{E_y}{t} = - c \\pdv{B_z}{x} - j_y\\] \\[\\pdv{B_z}{t} = - c \\pdv{E_y}{x}\\] Let\u0026rsquo;s define some new variables:\n\\[F^\u0026#43; \\equiv \\frac{E_y \u0026#43; B_z}{2}\\] \\[F^{-} \\equiv \\frac{E_y - B_z}{2}\\] These mean that\n\\[E_y = F^\u0026#43; \u0026#43; F^-\\] \\[B_z = F^\u0026#43; - F^-\\] So Maxwell\u0026rsquo;s equations become (in matrix form)\n\\[\\pdv{}{t} \\begin{bmatrix} F^\u0026#43; \\\\ F^- \\end{bmatrix} \u0026#43; c \\pdv{}{x} \\begin{bmatrix} F^\u0026#43; \\\\ - F^- \\end{bmatrix} = - \\frac{1}{2} \\vec j_y \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\] We notice the form of a conservation law. Notice on the left-hand side, we have the total derivative in Lagrangian frame moving at velocity \\( \\pm c \\). We can define some other relevant terms. The Poynting flux is then\n\\[P = c \\left[ \\overbrace{(F^\u0026#43;)^2}^{\\text{\u0026#43; direction}} - \\overbrace{(F^-)^2}^{\\text{- direction}} \\right]\\] The energy density is \\( (F^+)^2 + (F^-)^2 \\). We can update the fields in the Lagrangian frame of reference\n\\[\\frac{F^\u0026#43;( t \u0026#43; \\Delta t, x \u0026#43; c \\Delta t) - F^\u0026#43;(t, x)}{\\Delta t} = - \\frac{1}{2} j_y (t \u0026#43; \\frac{\\Delta t}{2}, x \u0026#43; c \\frac{\\Delta t}{2})\\] \\[\\frac{F^-( t \u0026#43; \\Delta t, x - c \\Delta t) - F^-(t, x)}{\\Delta t} = - \\frac{1}{2} j_y (t \u0026#43; \\frac{\\Delta t}{2}, x - c \\frac{\\Delta t}{2})\\] To properly center the current density relative to \\( \\vec E \\), \\( \\vec B \\) it must be centered at time step \\( n + 1/2 \\). Velocity is already centered properly to give \\( j_y ^{n + 1/2} \\). The location is given at \\( x_i ^n \\) and \\( x_{i} ^{n+1} \\), and are used to center \\( j_y \\) to the grid.\nConsidering the special case \\( \\Delta x = c \\Delta t \\), the method simplifies (though this is not a limitation of the method, just a simplified case):\n\\[{F_j^\u0026#43;}^{n\u0026#43;1} = {F_{j-1} ^\u0026#43;}^n - \\frac{\\Delta t}{4} \\underbrace{\\left({j_y}_j ^{n \u0026#43; 1/2} \u0026#43; {j_y}_{j - 1} ^{n \u0026#43; 1/2} \\right)}_{2{j_y}_{j - 1/2}}\\] \\[{F_j^-}^{n\u0026#43;1} = {F_{j\u0026#43;1} ^-}^n - \\frac{\\Delta t}{4} \\underbrace{\\left({j_y}_j ^{n \u0026#43; 1/2} \u0026#43; {j_y}_{j \u0026#43; 1} ^{n \u0026#43; 1/2} \\right)}_{2{j_y}_{j \u0026#43; 1/2}}\\] Current Weighting # For arbitrary values of \\( \\Delta x \\), we interpolate the value of \\( j_y \\) using the shape functions. Current weighting is always required for electrodynamic PIC. We still need charge weighting to compute \\( E_x \\).\nGiven the particle\u0026rsquo;s position at \\( x_i ^n \\) and \\( x_i ^{n+1} \\), we use the shape function to distribute \\( v_i ^{n + 1/2} \\) across \\( j_{j - 1/2} ^{n + 1/2} \\) and \\( j_{j + 1/2} ^{n + 1/2} \\)\n\\[{j_y}_{j \\pm 1/2} ^{n \u0026#43; 1/2} = \\sum_i q_i v_i ^{n \u0026#43; 1/2} \\left[ \\frac{S(x_{j \\pm 1} - x_i ^{n \u0026#43; 1}) \u0026#43; S(x_j - x_i ^n)}{2} \\right]\\] Longitudinal Electric Field # In electrodynamic PIC, the longitudinal electric field \\( E_x \\) is computed as we did in the electrostatic case\n\\[\\div \\vec E = \\div ( E_x \\vu x \u0026#43; E_y \\vu y) = \\pdv{}{x} E_x = \\rho / \\epsilon_0\\] This implies that the wave vector \\( \\vec k \\) is only in the \\( \\vu x \\) direction.\nElectrodynamic PIC algorithm # To advance electrodynamic PIC by a single time step.\nAt time \\( n-1 \\), we already know:\n\\( x^{n-1} \\) \\( v^{n-3/2} \\) \\( E_x ^{n-1} \\) \\( E_y ^{n-1} \\) \\( B_z ^{n-1} \\) At position \\( x^{n-1} \\), we compute the particle position \\[x^{n-1} \\rightarrow \\rho_c ^{n-1}\\] We solve Poisson\u0026rsquo;s equation using the source terms at \\( n-1 \\) \\[\\rho_c ^{n-1} \\rightarrow E_x ^{n-1}\\] Weight the fields to the grid to get forces \\[E_x ^{n-1}, E_y ^{n-1}, B_z ^{n-1} \\rightarrow F_x ^{n-1}, F_y ^{n-1}\\] Advance the velocities using the weighted forces \\[v^{n-3/2}, F_x ^{n-1}, F_y ^{n-1} \\rightarrow v^{n-1/2}, j_y ^{n-1/2}\\] Advance the particle\u0026rsquo;s position using the accelerated velocity \\[v^{n-1/2} \\rightarrow x^n\\] Solve Maxwell\u0026rsquo;s equation to advance the transverse fields \\[E_y ^{n-1}, B_z ^{n-1}, j_y ^{n-1/2} \\rightarrow E_y ^n, B_z ^n\\] Application of PIC - Two Stream Instability # An early use of the PIC method was investigation of beam-beam fusion. The idea is that you could quite easily achieve fusion by colliding two cold, counter-streaming beams. Two ion beams at 100keV can fuse to produce 17.6 MeV, at a gain of about 200. Easy peasy, right? It turns out the two-stream instability prevents us from achieving this exact scenario.\nInitially, we can start with two cold beams. Uniform beams will exert no force on each other. But very quickly, we will see out of phase bunching. Any non-uniformity in the density will cause a perturbation in the velocity, along with a matching perturbation in the other beam. These perturbations travel in opposite directions, producing forces that mix the beams and thermalize the plasma. This converts kinetic into thermal energy, spreading the energy across a broad range of velocity. This means that the only ions that still have enough energy to fuse are those out at the tails of the distribution, so the gain goes to pot.\nDispersion Relation # The dispersion relation for a cold plasma is:\n\\[\\frac{\\omega_p ^2}{\\omega ^2} = 1\\] If the plasma has a uniform velocity, such that the plasma becomes a cold beam, then the frequency is Doppler shifted\n\\[\\frac{\\omega_p ^2}{(\\omega - k v_0) ^2} = 1\\] And if we have two cold beams, then\n\\[\\frac{{\\omega_p}_1 ^2}{(\\omega - k v_1)^2} \u0026#43;\\frac{{\\omega_p}_2 ^2}{(\\omega - k v_2)^2} = 1\\] For identical, counter-streaming beams, \\( v_1 = - v_2 = v_0 \\):\n\\[\\frac{\\omega_p ^2}{(\\omega - k v_0)^2} \u0026#43; \\frac{\\omega_p ^2}{(\\omega \u0026#43; k v_0)^2} = 1\\] We can solve for \\( \\omega \\) and plot \\( |\\omega| / \\omega_p \\) vs \\( k v_0 / \\omega_p \\):\nThe unstable range where we have imaginary solutions for \\( \\omega \\) is the two-stream instability. We got here from a linear analysis, so we assume \\( v_1 \\propto e^{-i \\omega t} \\). That\u0026rsquo;s why the unstable solution goes like \\( e^{\\gamma t} \\). The dispersion relation is a linear result. As the instability grows over time, nonlinear effects (like trapped particles) and saturation determine the long-term solution.\nBoundary Conditions for Finite Systems # As with all PDEs, boundary conditions can critically affect the solution and require special numerical treatment. Let\u0026rsquo;s consider some specific boundary conditions.\nConducting Wall. The wall will reflect EM fields: \\[\\vu n \\cross \\vec E = 0 \\qquad \\rightarrow \\qquad \\vec E_{\\parallel} = 0\\] Gauss\u0026rsquo;s law tells us this leads to to the accumulation of a surface charge density\n\\[\\vu n \\cdot \\vec E = \\rho / \\epsilon_0\\] \\[\\rightarrow \\vec E_n = \\rho_s / \\epsilon_0\\] where \\( \\rho_s \\) is a surface charge density.\n\\[\\vu n \\cdot \\vec B = 0 \\qquad \\rightarrow \\qquad \\vec B_n = 0\\] \\[\\vu n \\cdot \\pdv{\\vec B}{t} = 0 \\rightarrow \\vec B_n = \\text{const.}\\] \\[\\vu n \\cross \\vec B = \\mu_0 \\vec j_s \\rightarrow \\vec B_t = \\mu_0 \\vec j_s \\cross \\vu n\\] where \\( \\vec j_x \\) is the surface current density.\nInsulating Wall. The boundary conditions for an insulating wall will depend on the dielectric properties of the insulating wall. For a perfect insulator, \\[\\pdv{\\vec E}{\\vu n} = 0 \\quad (\\rho_s = 0)\\] and \\[\\pdv{\\vec B}{\\vu n} = 0 \\quad (\\vec j_s \\cdot \\vu n = 0)\\] So that\u0026rsquo;s the fields taken care of. What about the particles? At a boundary, they can be reflected, absorbed, or emitted, depending on the material properties and the properties of the impacting/emitted particles. What happens when a particle hits a material boundary depends on:\nThe work function of the material Secondary electron and ion emission coefficient Applied or induced field/potential Dielectric strength of material Sputtering properties It is important to implement numerical boundary conditions that are consistent with the physics we\u0026rsquo;re trying to model.\nImplementation - Reflected Particles # For the reflection boundary condition, we want to generate a mirror image of the particle\u0026rsquo;s trajectory. To view the scenario, let\u0026rsquo;s look at a 1-dimensional particle traveling towards a reflecting boundary at \\( x=0 \\). We\u0026rsquo;ll plot position along the horizontal axis and time along the vertical.\nWe reflect the velocity when the particle crosses the boundary, such that \\( {v^{n + 3/2}}\u0026rsquo; \\) now takes us to the reflected position \\( {x^{n + 2}}\u0026rsquo; \\) instead of \\( x ^{n+2} \\). This eliminates the normal current, \\( \\vec j _n = 0 \\)\nImplementation - Absorbed/Emitted particles # For an absorption boundary, when charges and currents reach the boundary, they are deposited on the boundary and build up, unless it is an open boundary.\nThe inverse situation is emitting particles at the boundary. Charge and current from the boundary.\nLet\u0026rsquo;s consider a configuration in which we have electrodes at both side of our boundary, with a driving external current \\( I(t) \\) flowing between them.\nThe total current \\( I \\) is the sum of hte convective plasma current and the displacement current. The total current density is\n\\[\\vec j_{total} = (\\rho \\vec v)_{plasma} \u0026#43; \\pdv{}{t} \\vec E _{bias}\\] where \\( \\vec E_{bias} \\) is the electric field that will develop across the domain.\nTo fit such a boundary within our leapfrog framework, all of the values must be properly centered for computing Maxwell\u0026rsquo;s equations.\n\\[\\vec j _{total} ^{n \u0026#43; 1/2} = \\vec j _{plasma} ^{n \u0026#43; 1/2} \u0026#43; \\frac{\\vec E _{bias} ^{n\u0026#43;1} - \\vec E _{bias} ^n}{\\Delta t}\\] where\n\\[\\vec j_{plasma} ^{n \u0026#43; 1/2} = \\frac{1}{L} \\sum _i q_i \\vec v_i ^{n \u0026#43; 1/2}\\] and\n\\[\\vec E_{bias} ^n = - \\frac{\\phi _b ^n - \\phi _a ^n}{L} \\vu x\\] The current density is then used to solve Ampere\u0026rsquo;s law for the local electric field at the boundary \\( \\vec E_{local} \\).\nParticles are emitted from the electrode when the total electric field exceeds the threshold to pull particles from the surface (the work function). The process of pulling particles from the electrode when the field exceeds the work function is called field emission.\n\\[\\vec E_{total} = \\vec E_{bias} \u0026#43; \\vec E_{local}\\] We typically emit particles at \\( x_j = \\pm \\Delta x / 2 \\) with a Maxwellian distribution at the wall temperature using Monte Carlo. We emit particles as long as the fied exceeds the work function.\nAbsorption leads to material interactions. Absorbed particles deposit their energy into the surface. This can lead to effects like:\nsecondary electron and ion emission sputtering surface heating, which can lead to thermionic emission Implementation - Open Boundaries # In general, open boundaries can be difficult for PIC and Poisson solvers in general. This can be thought of as an impedance mismatch between the domain and the boundary. This scenario might occur when we\u0026rsquo;re modeling plasmas in space, where we would like the boundary to be a perfect vacuum, and particles have no interaction with the boundary whatsoever. But with any impedance mismatch, we will get artificial reflections in the EM fields.\nA common treatment for PIC codes is to append a dissipative layer to damp outgoing EM waves that enter the layer. Schematically, we draw this as a ramp-up of the impedance at the boundary over a width \\( \\delta \\).\nA damping layer like this which dissipates all EM fields is called a Perfectly Matched Layer (PML). To get a PML, \\( \\delta \\) must be greater than the longest wavelength of fields in the system \\( \\lambda_{longest} \\), or allow non-physical configurations \\( \\div \\vec B = 0 \\).\n"},{"id":20,"href":"/r/notes/UWAA558/03-plasma-fluid-model/","title":"Plasma Fluid Model","section":"MHD Theory","content":" Plasma Fluid Model # We take velocity moments of each of the pieces of the kinetic model:\ndistribution function, \\( f_\\alpha \\) \\( \\rightarrow \\) fluid variables Boltzmann Equation \\( \\rightarrow \\) governing equations describing the evolution of the fluid variables. Starting with the zeroth moment (integral) of the distribution function:\n\\[\\int f_\\alpha (\\vec x, \\vec v, t) \\dd \\vec v = n_\\alpha(\\vec x, t)\\] 1st Moment (momentum):\n\\[m_\\alpha \\int \\vec v \\cdot f_\\alpha(\\vec x, \\vec v, t) \\dd \\vec v = \\vec p_\\alpha (\\vec x, t) = m_\\alpha n_\\alpha \\vec v_\\alpha\\] which is to say that the velocity is the 1st moment divided by the zeroth moment\n\\[v_\\alpha = \\frac{\\int \\vec v f_\\alpha}{\\int f_\\alpha}\\] 2nd Moment:\n\\[\\int \\vec v \\vec v f_\\alpha (\\vec x, \\vec v, t) \\dd \\vec v = \\vec E_\\alpha(\\vec x, t) (\\text{energy tensor})\\] We can simplify the 2nd moment by taking a reduced 2nd moment. This means that we\u0026rsquo;re going to insert a dot product\n\\[\\int \\vec v \\cdot \\vec v f_\\alpha (\\vec x, \\vec v, t) \\dd \\vec v\\] Before moving forward, we want to define a \u0026ldquo;random\u0026rdquo; velocity. Note that\n\\[\\int \\vec v f_\\alpha - \\vec v_\\alpha \\int f_\\alpha = 0 \\rightarrow \\int (\\vec v - \\vec v_\\alpha) f_\\alpha \\dd \\vec v = 0\\] We can define a random velocity \\( \\vec w = \\vec v - \\vec v_\\alpha \\) . It is random in the sense that it is a fluctuation about the mean velocity, and when we integrate it we get zero. We can use this to define the energy tensor using the mean velocity to get a meaningful result. The pressure tensor is the second moment, using the random velocity\n\\[\\vec P_\\alpha = m_\\alpha \\int \\vec w \\vec w f_\\alpha \\dd \\vec v = P_\\alpha \\overline{I} \u0026#43; \\overline{\\Pi}_\\alpha\\] where we\u0026rsquo;ve decomposed the pressure into an isotropic value \\( P_\\alpha \\) and what\u0026rsquo;s called the Braginskii stress tensor \\( \\overline{\\Pi}_\\alpha \\) . The average isotropic pressure is given by the reduced 2nd moment:\n\\[P_\\alpha = n_\\alpha T_\\alpha = \\frac{1}{3} \\int m_\\alpha \\vec w \\cdot \\vec w f_\\alpha\\] where the factor of \\( 1/3 \\) comes from the number of degrees of freedom in our system. It is related to the thermodynamic factor \\( \\gamma \\) where \\( \\gamma = \\frac{DOF \u0026#43; 2}{DOF} \\) . Now we can define the temperature \\( T_\\alpha \\) as\n\\[T_\\alpha (\\vec x, t) = \\frac{1}{DOF} \\frac{\\int m_\\alpha \\vec w \\cdot \\vec w f_\\alpha \\dd \\vec v}{\\int f_\\alpha \\dd \\vec v}\\] Now that we\u0026rsquo;ve got \\( n_\\alpha \\) , \\( v_\\alpha \\) , and \\( T_\\alpha \\) we have what we need to define a Maxwellian distribution. Higher moments would be required to describe non-Maxwellian distribution functions. For example, the 3rd moment is called the skewness of the distribution. The 4th moment is the kurtosis. So on and so forth. These give a measure of degree of departure from a Maxwellian distribution, in which case it is often more useful to talk about the excess kurtosis, where the excess kurtosis of a Maxwellian is defined to be zero. You can continue to calculate the moment expansion, and in general it requires an infinite number of moments to describe an arbitrary distribution function. Because of the Boltzmann H-theorem and the tendency of plasmas to quickly relax to Maxwellian, we can usually get away with using just the first three moments.\nNow, what are the governing equations? We get these by taking moments of the Boltzmann equation. Let\u0026rsquo;s proceed carefully in sections, so we\u0026rsquo;ll integrate each piece of the BE in terms.\n0th Moment of Boltzmann Equation (Conservation) # \\[\\int \\pdv{f_\\alpha}{t} \\dd \\vec v \u0026#43; \\int \\vec v \\cdot \\pdv{f_\\alpha}{x} \\dd v \u0026#43; \\int \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\dd \\vec v = \\int \\left. \\pdv{f_\\alpha}{t} \\right|_{coll} \\dd \\vec v\\] First, we take \\( \\int \\pdv{f_\\alpha}{t} \\dd \\vec v \\) . Because t and v are both independent variables, with an argument of sufficient smoothness we can reverse the order of integration and differentiation\n\\[\\int \\pdv{f_\\alpha}{t} \\dd \\vec v = \\pdv{}{t}\\int f_\\alpha \\dd v = \\pdv{n_\\alpha}{t}\\] For \\( \\int \\vec v \\cdot \\pdv{f_\\alpha}{x} \\dd v \\) we can perform an integration by parts\n\\[ \\int \\vec v \\cdot \\pdv{f_\\alpha}{x} \\dd v = \\int \\pdv{}{\\vec x} \\cdot ( \\vec v f_\\alpha) \\dd \\vec v - \\int f_\\alpha \\pdv{}{\\vec x} \\cdot \\vec v \\dd \\vec v \\\\ = \\int \\pdv{}{\\vec x} \\cdot ( \\vec v f_\\alpha) \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot \\int \\vec v f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot (n_\\alpha \\vec v_\\alpha) = \\div (n_\\alpha \\vec v_\\alpha)\\] Once again, we\u0026rsquo;ve switched the order of integration of \\( x \\) and \\( v \\) , which we can only do because we have specified that \\( f \\) is a distribution function, and as such meets the criterion of sufficient smoothness.\nFor the last part, we can write it as a surface integral\n\\[\\int \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\dd \\vec v = \\oint f_\\alpha \\frac{q_\\alpha}{m_\\alpha} ( \\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\dd S_v - \\int \\frac{q_\\alpha}{m_\\alpha} f_\\alpha \\pdv{}{\\vec v} \\cdot (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\dd \\vec v\\] For \\( f_\\alpha \\) to be well-defined, we require \\( \\lim_{v \\rightarrow \\infty} v^3 f_\\alpha = 0 \\) so the surface term vanishes and we\u0026rsquo;re left with\n\\[\\int \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\dd \\vec v = - \\int \\frac{q_\\alpha}{m_\\alpha} f_\\alpha \\pdv{}{\\vec v} \\cdot (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\dd \\vec v \\] Distributing the divergence through,\n\\[\\pdv{}{\\vec v} \\cdot \\vec E = 0\\] because \\( \\vec E \\) is independent of \\( \\vec v \\) .\n\\[\\pdv{}{\\vec v} \\cdot ( \\vec v \\cross \\vec B) = 0\\] because \\( \\vec v \\cross \\vec B \\) is always orthogonal to \\( \\vec v \\) .\nWe\u0026rsquo;ve been working through this in normal vector notation for familiarity, but there is another notation known as Einstein Tensor Notation using the lovely Levi-Civita symbol \\( \\epsilon_{ijk} \\) .\n\\[\\epsilon_{ijk} = 1 \\qquad \\text{even permutations of i j k} \\\\ = -1 \\qquad \\text{odd permutations of i j k} \\\\ = 0 \\qquad \\text{any repeated indexes}\\] We can write out vector products as products of indices and operators, and any repeated indices are implicitly summed:\n\\[\\vec v \\cross \\vec B = \\epsilon_{ijk} v_j B_k = \\sum_{jk} \\epsilon_{ijk} v_j B_k\\] where \\( \\epsilon_{ijk} \\) is defined to be 1 for even permutations of ijk, -1 for negative permutations if ijk, and 0 for any repeated indices.\nWe can now write the derivative with respect to \\( \\vec v \\) as\n\\[\\pdv{}{\\vec v} \\cdot \\vec v \\cross \\vec B = \\partial _{v_i} \\epsilon_{ijk} v_j B_k \\\\ = \\epsilon_{ijk} (\\partial_{v_i} v_j) B_k\\] We see that we\u0026rsquo;re taking the derivative of the j-th component of velocity with respect to the i-th component of velocity, and that\u0026rsquo;s how we can most easily point out that the quantity is zero without relying on properties of 3-vector products.\nWe\u0026rsquo;ll also want to use the divergence in Einstein tensor notation\n\\[\\div \\vec A = \\partial_i A_j \\delta_{ij} = \\partial_i A_i\\] Finally, we come back to the collision term in the zeroth moment of the B-M equation\n\\[\\sum_\\beta \\int C_{\\alpha \\beta} \\dd \\vec v = 0\\] We can say the collision term is zero by making a physical argument, rather than a mathematical one. We assert that collisions cannot create or destroy particles. This assumption is now baked into our equations going forward, but is not always true! Ionization, recombination, fusion reactions all create/destroy species.\nFinally, we\u0026rsquo;ve got\n\\[\\pdv{n_\\alpha}{t} \u0026#43; \\div ( n_\\alpha \\vec v_\\alpha) = 0 \\qquad \\text{continuity equation}\\] So by taking the 0th moment of the Boltzmann Equation, we\u0026rsquo;ve arrived at the continuity equation for \\( n_\\alpha \\) by introducing \\( \\vec v_\\alpha \\) .\n1st Moment of Boltzmann Equation # \\[\\int \\vec v \\pdv{f_\\alpha}{t} \\dd \\vec v \u0026#43; \\int \\vec v \\vec v \\pdv{f_\\alpha}{\\vec x} \\dd \\vec v \u0026#43; \\int \\vec v \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\dd \\vec v = \\int \\left. \\vec v \\pdv{f_\\alpha}{t} \\right|_{coll}\\] Term 1:\n\\[\\int \\vec v \\pdv{f_\\alpha}{t} \\dd \\vec v = \\pdv{}{t} \\int \\vec v f_\\alpha \\dd \\vec v - \\int \\pdv{\\vec v}{t} f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{t} \\left( n_\\alpha \\vec v_\\alpha \\right)\\] Before we get to the second term, let\u0026rsquo;s have an aside about dyad math. An outer product \\( \\vec A \\vec B \\) gives a second-rank tensor. In Cartesian coordinates, it looks like\n\\[\\vec A \\vec B = \\begin{bmatrix} A_x B_x \u0026amp; A_x B_y \u0026amp; A_x B_z \\\\ A_y B_x \u0026amp; A_y B_y \u0026amp; A_y B_z \\\\ A_z B_x \u0026amp; A_z B_y \u0026amp; A_z B_z \\\\ \\end{bmatrix}\\] A useful property to do with dot products:\n\\[\\vec A \\vec B \\cdot \\vec C = \\vec A ( \\vec B \\cdot \\vec C ) = (\\vec A \\vec B) \\cdot \\vec C\\] \\[( \\vec V \\cdot \\grad) \\vec B = \\vec V \\cdot ( \\grad \\vec B)\\] Back to term 2:\n\\[\\int \\vec v \\vec v \\cdot \\pdv{f_\\alpha}{\\vec x} \\dd \\vec v = \\int \\vec v \\cdot \\pdv{}{\\vec x} ( \\vec v f_\\alpha) \\dd \\vec v - \\int \\vec v f_\\alpha \\pdv{}{\\vec x} \\cdot \\vec v \\dd \\vec v \\\\ = \\int \\pdv{}{\\vec x} \\cdot ( \\vec v \\vec v f_\\alpha) \\dd \\vec v - \\int \\vec v f_\\alpha \\cdot \\pdv{\\vec v}{\\vec x} \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot \\int \\vec v \\vec v f_\\alpha \\dd \\vec v\\] Re-expanding in terms of random velocity \\( \\vec v = \\vec v_\\alpha \u0026#43; \\vec w \\) \\[= \\pdv{}{\\vec x} \\cdot \\int (\\vec v_\\alpha \u0026#43; \\vec w) (\\vec v_\\alpha \u0026#43; \\vec w) f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot \\int (\\vec v_\\alpha \\vec v_\\alpha \u0026#43; \\vec v_\\alpha \\vec w \u0026#43; \\vec w \\vec v_\\alpha \u0026#43; \\vec w \\vec w) f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot \\left[ \\vec v_\\alpha \\vec v_\\alpha \\int f_\\alpha \\dd \\vec v \u0026#43; \\vec v_\\alpha \\int \\vec w f_\\alpha \\dd \\vec v \u0026#43; \\left( \\int \\vec w f_\\alpha \\dd \\vec v \\right) \\vec v_\\alpha \u0026#43; \\int \\vec w \\vec w f_\\alpha f_\\alpha \\dd \\vec v \\right]\\] The middle two terms are zero by the very definition of \\( \\vec v_\\alpha \\) . The random velocity \\( \\vec w \\) is defined such that the integral of the random velocity over all phase space is zero.\n\\[= \\pdv{}{\\vec x} \\cdot \\left( \\vec v_\\alpha \\vec v_\\alpha n_\\alpha \\right) \u0026#43; \\pdv{}{\\vec x} \\cdot \\frac{ \\vec P_\\alpha}{m_\\alpha} \\\\ = \\div (n_\\alpha \\vec v_\\alpha \\vec v_\\alpha) \u0026#43; \\frac{1}{m_\\alpha} \\div \\vec P_\\alpha\\] Substituting in the pressure tensor,\n\\[= \\div ( n_\\alpha \\vec v_\\alpha \\vec v_\\alpha ) \u0026#43; \\frac{1}{m_\\alpha} \\left( \\grad P_\\alpha \u0026#43; \\div \\vec \\Pi _\\alpha \\right)\\] Moving on to term 3:\n\\[\\int \\vec v \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\dd \\vec v = \\int \\frac{q_\\alpha}{m_\\alpha} \\pdv{}{\\vec v} \\cdot \\left[( \\vec E \u0026#43; \\vec v \\cross \\vec B) f_\\alpha \\right] \\vec v \\dd \\vec v - \\int \\frac{q_\\alpha}{m_\\alpha} f_\\alpha \\pdv{}{\\vec v} \\cdot \\left[ \\vec E \u0026#43; \\vec v \\cross \\vec B \\right] \\vec v \\dd \\vec v \\\\ = \\int \\frac{q_\\alpha}{m_\\alpha} \\pdv{}{\\vec v} \\cdot \\left[ f_\\alpha (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\vec v \\right] \\dd \\vec v - \\int \\frac{q\\alpha}{m\\alpha} f ( \\vec E \u0026#43; \\vec v\\alpha \\cross \\vec B) \\cdot \\pdv{\\vec v}{\\vec v} \\dd \\vec v \\\\ = \\frac{q\\alpha}{m\\alpha} \\oint f\\alpha ( \\vec E \u0026#43; \\vec v\\alpha \\cross \\vec B) \\vec v \\cdot \\dd \\vec S_v - \\frac{q\\alpha}{m\\alpha} \\left[ \\vec E \\int f \\dd \\vec v\\alpha - \\vec B \\cross \\int \\vec v f\\alpha \\dd \\vec v \\right] \\\\ = - \\frac{q\\alpha}{m\\alpha} n\\alpha (\\vec E \u0026#43; v\\alpha \\cross \\vec B)\\] Finally, the 4th term gives\n\\[ \\int \\left. \\vec v \\pdv{f_\\alpha}{t} \\right|_{coll} = \\sum_\\beta \\int \\vec v C_{\\alpha \\beta} \\dd \\vec v \\\\ = \\int \\vec v C_{\\alpha \\alpha} \\dd \\vec v \u0026#43; \\sum_{\\beta \\neq \\alpha} \\int \\vec v C_{\\alpha \\beta} \\dd \\vec v\\] Collisions of like particles \\( \\alpha \\) do not result in a net change of momentum of species \\( \\alpha \\) , so all we have left is the change in momentum due to collisions of unlike particles\n\\[= \\sum_{\\beta \\neq \\alpha} \\left[ \\vec v_{\\alpha} \\int C_{\\alpha \\beta} \\dd \\vec v \u0026#43; \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v \\right]\\] For the same reason as before, we assert that collisions between particles \\( \\alpha \\) and \\( \\beta \\) do not lead to the creation or destruction of any species, so the 0th moment \\( \\int C_{\\alpha \\beta} \\dd \\vec v = 0\\) . This leads to the conclusion that only random motion contributes to momentum transfer, not \\( \\vec v_\\alpha \\) . Viscosity and friction are good examples of similar physical processes where bulk velocity does not transfer momentum, but random motion does.\nThe momentum transfer from \\( \\alpha \\) to \\( \\beta \\) must equal transfer from \\( \\beta \\) to \\( \\alpha \\) . Momentum is globally conserved.\n\\[\\int \\vec w C_{\\alpha \\beta} \\dd \\vec v = - \\int \\vec w C_{\\beta \\alpha} \\dd \\vec v\\] Now we can finally write out the full momentum equation\n\\[\\pdv{}{t} (n_\\alpha \\vec v_\\alpha ) \u0026#43; \\div (n_\\alpha \\vec v_\\alpha \\vec v_\\alpha) \u0026#43; \\frac{1}{m_\\alpha} \\div \\vec P_\\alpha - \\frac{q_\\alpha}{m_\\alpha} n_\\alpha ( \\vec E \u0026#43; \\vec v _\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v\\] The collision term is often represented as the momentum transfer vector \\( \\vec R_{\\alpha \\beta} \\) to \\( \\alpha \\) from \\( \\beta \\) .\n\\[\\vec R_{\\alpha \\beta} \\equiv m_\\alpha \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v\\] \\[\\pdv{}{t} (m_\\alpha n_\\alpha v_\\alpha) \u0026#43; \\div (m_\\alpha n_\\alpha \\vec v_\\alpha \\vec v_\\alpha) \u0026#43; \\grad \\vec P_{\\alpha} \u0026#43; \\div \\vec \\Pi_{\\alpha} - q_\\alpha n_\\alpha (\\vec E \u0026#43; \\vec v _\\alpha \\cross \\vec B) = \\sum_{\\beta} R_{\\alpha \\beta}\\] Once again, we\u0026rsquo;ve written the above in a conservation law form. The terms that aren\u0026rsquo;t strictly conservation terms are the source term \\( q_\\alpha n_\\alpha (\\vec E \u0026#43; \\vec v _\\alpha \\cross \\vec B) \\) and sink term \\( \\sum_{\\beta} R_{\\alpha \\beta} \\) .\nIntroducing the mass density\n\\[\\rho_\\alpha = m_\\alpha n_\\alpha \\qquad \\text{mass density}\\] \\[\\pdv{}{t} (\\rho_\\alpha \\vec v _\\alpha) \u0026#43; \\div (\\rho_\\alpha \\vec v_\\alpha v_\\alpha \u0026#43; \\ldots \\\\ = \\rho_\\alpha \\pdv{\\vec v_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\left( \\pdv{\\rho_\\alpha}{t} \u0026#43; \\div \\rho_\\alpha \\vec v_\\alpha \\right) \u0026#43; \\rho_\\alpha \\vec v_\\alpha \\cdot \\grad \\vec v_\\alpha \u0026#43; \\ldots\\] The term in parentheses is just the continuity equation, which is zero, and what\u0026rsquo;s left is a more usual form of the momentum equation\n\\[\\text{Momentum Equation:}\\\\ \\rho_\\alpha \\left(\\pdv{\\vec v_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad \\vec v_\\alpha \\right) \u0026#43; \\grad \\vec P_\\alpha \u0026#43; \\div \\vec \\Pi_\\alpha - q_\\alpha n_\\alpha (\\vec E \u0026#43; \\vec v_\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\vec R_{\\alpha \\beta}\\] So by taking the 1st moment of the BE, we arrive at the momentum equation for \\( \\vec v_\\alpha \\) and we have introduced \\( \\vec P_\\alpha \\) .\nMomentum transfer (collisions) # Looking back at \\( \\vec R_{\\alpha \\beta} \\) , it\u0026rsquo;s worth noting that the actual situation is complicated by the potential for multi-body collisions. While it is true that the probability of, e.g. three-body collisions is small, but there are many three-body collisional processes which are extremely important in plasma physics. Three-body recombination is the primary loss term in the ionization balance, for example. For now the simplest model is to only include binary collisions that result in small angel deflections. If we suppose we only have two species with distributions that look like the following:\nthen we expect collisions between species to tend to drag the distributions towards each other. We should expect the collision-based momentum transfer to be proportional to the distance between the distributions, i.e. the total current density \\( \\vec j \\) .\nFor an ion-electron plasma, the collision momentum transfer vector (to ions from electrons) is\n\\[\\vec R_{ie} = - n_e e \\eta \\vec j\\] where\n\\[\\eta = \\frac{m_e \\nu_{ei}}{n e^2} \\qquad \\text{resistivity}\\] \\[\\vec j = n e (\\vec v_i - \\vec v_e) \\qquad \\text {current density for Z = 1}\\] 2nd Moment of Boltzmann Equation # Finally we\u0026rsquo;re on to our last expansion term. In general, we would say that the second moment would be\n\\[\\int \\vec v \\vec v \\pdv{f_\\alpha}{t} \\dd \\vec v = \\pdv{}{t} \\int \\vec v \\vec v f_\\alpha \\dd \\vec v = \\pdv{}{t} \\vec E_\\alpha / m_\\alpha \\rightarrow \\pdv{}{t} \\vec P_\\alpha\\] Usually we think of the second moment as giving us energy, a scalar, rather than a tensor equation that we have here. Since the pressure tensor is symmetric, the energy equation will contain 6 independent quantities. Together with the continuity and velocity equations, we have 10 quantities in total, so we call this expansion a 10-moment expansion.\nInstead of the full tensor equation, we\u0026rsquo;re actually going to compute the reduced 2nd moment by contracting \\( \\vec v \\cdot \\vec v \\) \\[\\int \\vec v \\cdot \\vec v \\pdv{f_\\alpha}{t} \\dd v\\] So, let\u0026rsquo;s get after it:\n\\[\\int v^2 \\pdv{f_\\alpha}{t} \\dd \\vec v \u0026#43; \\int v^2 \\vec v \\cdot \\pdv{f_\\alpha}{\\vec x} \\dd \\vec v \u0026#43; \\int v^2 \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\dd \\vec v = \\left. \\int v^2 \\pdv{f_\\alpha}{t} \\right|_{coll} \\dd \\vec v\\] Simplifying\u0026hellip; Term 1:\n\\[\\int v^2 \\pdv{f_\\alpha}{t} \\dd \\vec v = \\int \\pdv{}{t} (v^2 f_\\alpha) \\dd \\vec v - \\int \\pdv{v^2}{t} f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{t} \\int (\\vec v_\\alpha \u0026#43; \\vec w) \\cdot (\\vec v_\\alpha \u0026#43; \\vec w) f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{t} \\left[ \\int v_\\alpha ^2 f_\\alpha \\dd \\vec v \u0026#43; 2 \\int \\vec v_\\alpha \\cdot \\vec w f_\\alpha \\dd \\vec v \u0026#43; \\int w^2 f_\\alpha \\dd \\vec v \\right] \\\\ = \\pdv{}{t} \\left[ v_\\alpha ^2 \\int f_\\alpha \\dd \\vec v \u0026#43; 2 \\vec v_\\alpha \\cdot \\int \\vec w f_\\alpha \\dd \\vec v \u0026#43; 3 \\frac{n_\\alpha T_\\alpha}{m_\\alpha} \\right] \\\\ = \\pdv{}{t} \\left[ n_\\alpha v_\\alpha ^2 \u0026#43; 3 \\frac{n_\\alpha T_\\alpha}{m_\\alpha} \\right]\\] Term 2:\n\\[\\int v^2 \\vec v \\cdot \\pdv{f_\\alpha}{\\vec x} \\dd \\vec v = \\int \\pdv{}{\\vec x} \\cdot (v^2 \\vec v f_\\alpha) \\dd \\vec v - \\int f_\\alpha \\pdv{}{\\vec x} \\cdot (v^2 \\vec v) \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot \\int ( \\vec v_\\alpha \u0026#43; \\vec w) \\cdot (\\vec v_\\alpha \u0026#43; \\vec w) \\vec w f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot \\int (v_\\alpha ^2 \u0026#43; 2 \\vec v_\\alpha \\cdot \\vec w \u0026#43; w^2) \\vec v f_\\alpha \\dd \\vec v \\\\ = \\pdv{}{\\vec x} \\cdot \\left[ \\int v_\\alpha ^2 (\\vec v_\\alpha \u0026#43; \\vec w) f_\\alpha \\dd \\vec v \u0026#43; \\int 2 \\vec v_\\alpha \\cdot \\vec w (\\vec v_\\alpha \u0026#43; \\vec w) f_\\alpha \\dd \\vec v \u0026#43; \\int w^2 (\\vec v_\\alpha \u0026#43; \\vec w) f_\\alpha \\dd \\vec v \\right] \\\\ = \\pdv{}{\\vec x} \\cdot \\left[ v_\\alpha ^2 \\vec v_\\alpha \\int f_\\alpha \\dd \\vec v \u0026#43; v_\\alpha ^2 \\int \\vec w f_\\alpha \\dd \\vec v \u0026#43; 2 \\vec v_\\alpha \\cdot \\left(\\int \\vec w f_\\alpha \\dd \\vec v \\right) \\vec v_\\alpha \\\\ \\qquad \u0026#43; 2 \\vec v_\\alpha \\cdot \\int \\vec w \\vec w f_\\alpha \\dd \\vec v \u0026#43; \\vec v_\\alpha \\int w^2 f_\\alpha \\dd \\vec v \u0026#43; \\int \\vec w w^2 f_\\alpha \\dd \\vec v \\right] \\\\ = \\pdv{}{\\vec x} \\cdot (n_\\alpha v_\\alpha ^2 \\vec v_\\alpha) \u0026#43; \\pdv{}{\\vec x} \\cdot 2 \\vec v_\\alpha \\cdot \\frac{\\vec P_\\alpha}{m_\\alpha} \u0026#43; \\pdv{}{\\vec x} \\cdot \\left(\\frac{3 n_\\alpha T_\\alpha \\vec v_\\alpha}{m_\\alpha} \\right) \u0026#43; \\pdv{}{\\vec x} \\cdot \\left( \\frac{2 \\vec h_\\alpha}{m_\\alpha} \\right)\\] In keeping with the progression so far, the very last term would be something like a full 3rd moment \\( \\int \\vec w \\vec w \\vec w f_\\alpha \\dd \\vec v \\) . Instead, we have a \u0026ldquo;contracted 3rd moment\u0026rdquo; of the distribution \\( \\vec h_\\alpha \\) which is actually the heat flux, or the random energy flux of random energy.\nTerm 3:\n\\[\\int v^2 \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} \\dd \\vec v \\\\ = \\oint f_\\alpha \\frac{q_\\alpha}{m_\\alpha} v^2 ( \\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\dd \\vec S_v - \\int f_\\alpha \\frac{q_\\alpha}{m_\\alpha} \\pdv{}{\\vec v} \\cdot \\left[ v^2 ( \\vec E \u0026#43; \\vec v \\cross \\vec B) \\right] \\dd \\vec v \\\\ = - \\frac{q_\\alpha}{m_\\alpha} \\int f_\\alpha \\left[ v^2 \\left(\\pdv{}{\\vec v} \\cdot \\vec E \\right) \u0026#43; \\left(\\vec E \\cdot \\pdv{}{\\vec v} v^2 \\right) \\right] \\dd \\vec v \\\\ \\qquad - \\frac{q_\\alpha}{m_\\alpha}\\int f_\\alpha \\left[v^2 \\pdv{}{\\vec v} \\cdot (\\vec v \\cross \\vec B) \u0026#43; \\vec v \\cross \\vec B \\cdot \\pdv{}{\\vec v} v^2 \\right] \\dd \\vec v \\\\ = - \\frac{q_\\alpha}{m_\\alpha} \\int f_\\alpha (2 \\vec E \\cdot \\vec v) \\dd \\vec v \\\\ = - \\frac{2 q_\\alpha}{m_\\alpha} n_\\alpha \\vec v_\\alpha \\cdot \\vec E\\] We\u0026rsquo;ve ended up with the work done on a fluid by the electric field. As expected, there is no work term associated with the bulk magnetic field.\nTerm 4:\n\\[\\left. \\int v^2 \\pdv{f_\\alpha}{t} \\right|_{coll} \\dd \\vec v = \\int v^2 C_{\\alpha \\alpha} \\dd \\vec v \u0026#43; \\sum _{\\beta \\neq \\alpha} \\int v^2 C_{\\alpha \\beta} \\dd \\vec v \\] We can make an energy conservation argument to get rid of the first term. The first term goes to 0 since collisions of like particles results in no net energy exchange.\n\\[= \\sum_{\\beta \\neq \\alpha} \\int ( \\vec v_\\alpha \u0026#43; \\vec w) \\cdot(\\vec v_\\alpha \u0026#43; \\vec w) C_{\\alpha \\beta} \\dd \\vec v \\\\ = \\sum_{\\beta \\neq \\alpha}\\left( v_\\alpha ^2 \\int C_{\\alpha \\beta} \\dd \\vec v \u0026#43; 2 \\vec v_\\alpha \\cdot \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v \u0026#43; \\int w^2 C_{\\alpha \\beta} \\dd \\vec v \\right)\\] Since collisions (we assume) don\u0026rsquo;t create or destroy particles, the first term \\( v_\\alpha ^2 \\int C_{\\alpha \\beta} \\dd \\vec v \\) goes away. The second term \\( \\vec v_\\alpha \\cdot \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v \\) we interpret as a frictional force due to the relative motion between \\( \\alpha \\) and \\( \\beta \\) \\[= \\sum_{\\beta \\neq \\alpha} 2 \\frac{\\vec v_\\alpha \\cdot \\vec R_{\\alpha \\beta}}{m_\\alpha} \u0026#43; 2 \\frac{Q_{\\alpha \\beta}}{m_\\alpha}\\] where \\( Q_{\\alpha \\beta} \\) is the heat exchange term (heat generation term in Braginskii). It describes the heat exchange by random collisions of unlike particles, analogous to viscous heating.\n\\[Q_{\\alpha \\beta} = \\frac{1}{2} \\int m_\\alpha w^2 C_{\\alpha \\beta} \\dd \\vec v\\] If we were to draw distribution functions for species \\( \\alpha \\) and \\( \\beta \\) (as shown), even though they have the same centroid they will interact by viscous heating. Species \\( \\beta \\) will heat up and \\( \\alpha \\) will cool\nIf we multiply by \\( \\frac{1}{2} m_\\alpha \\) then the energy equation becomes\n\\[\\text{Energy Equation: } \\\\ \\qquad \\pdv{}{t} \\left( \\frac{3}{2} n_\\alpha T_\\alpha \u0026#43; \\frac{1}{2} \\rho_\\alpha v_\\alpha ^2 \\right) \u0026#43; \\\\ \\div \\left(\\frac{3}{2} n_\\alpha T_\\alpha \u0026#43; \\frac{1}{2} \\rho_\\alpha v_\\alpha ^2 \\right) \\vec v_\\alpha \u0026#43; \\div (\\vec v_\\alpha \\cdot \\vec P_\\alpha) \u0026#43; \\div \\vec h_\\alpha - q_\\alpha n_\\alpha \\vec v_\\alpha \\cdot \\vec E \\\\ = \\sum_{\\beta \\neq \\alpha} (v_\\alpha \\cdot \\vec R_{\\alpha \\beta} \u0026#43; Q_{\\alpha \\beta})\\] Notice that the total energy appears\n\\[E_\\alpha = \\frac{3}{2} n_\\alpha T_\\alpha \u0026#43; \\frac{1}{2} \\rho_\\alpha v_\\alpha ^2\\] The factor of \\( \\frac{3}{2} \\) as usual comes from \\( \\gamma \\) \\[\\frac{3}{2} = \\frac{1}{\\gamma - 1} = \\frac{1}{5/3 - 1}\\] where\n\\[\\gamma = \\frac{DOF \u0026#43; 2}{DOF} = \\frac{5}{3}\\] We can remove the kinetic portion of the energy equation by subtracting the product of the momentum equation with \\( \\vec v_\\alpha \\) .\n\\[\\rho_\\alpha \\vec v_\\alpha \\cdot \\pdv{v_\\alpha}{t} = \\frac{1}{2} \\pdv{}{t} (\\rho_\\alpha v_\\alpha ^2)\\] \\[\\pdv{}{t} \\left( \\frac{3}{2} n_\\alpha T_\\alpha \\right) \u0026#43; \\div \\left(\\frac{3}{2} n_\\alpha T_\\alpha \\vec v_\\alpha \\right) \u0026#43; \\div \\left( \\vec v_\\alpha \\cdot \\vec P_\\alpha \\right) - \\vec v_\\alpha \\cdot (\\div \\vec P_\\alpha) \u0026#43; \\div \\vec h_\\alpha = \\sum_{\\beta \\neq \\alpha} Q_{\\alpha \\beta}\\] \\[\\frac{3}{2} n_\\alpha \\left( \\pdv{T_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\div T_\\alpha \\right) \u0026#43; \\frac{3}{2} T_\\alpha \\left[ \\pdv{n_\\alpha}{t} \u0026#43; \\div (n_\\alpha \\vec v_\\alpha) \\right] \u0026#43; \\vec P_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha \u0026#43; \\div \\vec h_\\alpha = \\sum_{\\beta \\neq \\alpha} Q_{\\alpha \\beta}\\] \\[\\frac{3}{2} n_\\alpha \\left( \\pdv{T_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\div T_\\alpha \\right) \u0026#43; \\vec P_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha \u0026#43; \\div \\vec h_\\alpha = \\sum_{\\beta \\neq \\alpha} Q_{\\alpha \\beta}\\] Where \\( \\vec P_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha \\) is a tensor contraction in two indices. It is a generalization of the dot product, i.e. \\[\\vec P_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha = \\delta_{ik} \\delta_{jl} P_{ij} \\partial_k v_l \\\\ = P_{xx} \\partial_x v_x \u0026#43; P_{xy} \\partial_y v_x \u0026#43; \\ldots\\] \\[\\frac{3}{2} n_\\alpha \\left( \\pdv{T_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad T_\\alpha \\right) \u0026#43; P_\\alpha \\div \\vec v_\\alpha \u0026#43; \\vec \\Pi_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha \u0026#43; \\div \\vec h_\\alpha = \\sum_{\\beta \\neq \\alpha} Q_{\\alpha \\beta}\\] Now the continuity, momentum, and energy equations describe the evolution of each fluid \\( \\alpha \\) . The only velocity information we have retained in taking moments is the centroid \\( \\vec v_\\alpha \\) and the width \\( T_\\alpha \\) . This is the only information we will include in the 5-moment model (5N-moment plasma fluid model). The higher moments (describing skewness, kurtosis) are not evolved with the 5-moment model. There are fluid models that do evolve those moments, but we won\u0026rsquo;t touch on those here.\nClosure Problem # We still need to \u0026ldquo;close\u0026rdquo; the fluid model (i.e. solving the Closure Problem). As we have been finding out, each additional moment of the Boltzmann Equation introduces the next-higher moment. In calculating the 0th moment, we introduce the 1st moment, etc. We need to address the closure problem by relating higher moment variables to lower moment variables. Applying a closure scheme (relating variables that are not evolved directly) is usually equivalent to making a statement on heat flow.\nIn our model, we are not evolving the heat flux \\( \\vec h_\\alpha \\) , so we need to relate it to one of the other variables that we do evolve directly. We usually do that by writing down a conductivity relation\n\\[\\vec h_\\alpha = - \\kappa \\grad T_\\alpha\\] for some conductivity \\( \\kappa \\) .\nWe\u0026rsquo;re also only evolving the scalar pressure, so we need to relate the stress tensor \\( \\vec \\Pi_\\alpha \\) to the other variables. We can do that with a viscosity relation\n\\[\\vec \\Pi_\\alpha = \\nu \\grad \\vec v_\\alpha\\] for some viscosity \\( \\nu \\) . We call the introduced quantities \\( \\kappa \\) and \\( \\nu \\) the transport coefficients. They are usually derived by performing an expansion of the equations we\u0026rsquo;ve already discussed. They are what lead to \u0026ldquo;nondimensional\u0026rdquo; numbers that characterize the flow in our system.\nWe can also achieve closure by constraining portions of the system. In an isothermal system\n\\[\\text{isothermal: } \\quad T = \\text{const.}, p \\propto n\\] \\[\\text{adiabatic: } \\quad p \\propto n^\\gamma\\] \\[\\text{force-free, cold: } \\quad p = 0\\] The expressions that lead us to a closure scheme are the equations of state. They are not time-dependent, they simply relate fluid parameters to other properties of the system.\nMaxwell\u0026rsquo;s equations to couple the electromagnetic terms to the fluid variables. \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\epsilon_0 \\pdv{\\vec E}{t} = \\frac{1}{\\mu_0} \\curl \\vec B - \\sum_{alpha} q_\\alpha n_\\alpha \\vec v_\\alpha \\] \\[\\epsilon_0 \\div \\vec E = \\sum_\\alpha q_\\alpha n_\\alpha \\] \\[\\div \\vec B = 0\\] The fluid variables provide the source terms for Ampere\u0026rsquo;s law and Gauss\u0026rsquo; law, and we\u0026rsquo;ve already seen how the electromagnetic fields appear in the momentum equations as source terms for the fluid forces.\nStopping after the 2nd moment (5N-Moment Plasma Fluid Model) # What assumptions have we made by stopping here? It\u0026rsquo;s important to know what they are and to recognize what they mean for the fluid model.\nEach species \\( \\alpha \\) is well-represented by a Maxwellian with a small perturbation. The pressure tensor is not strictly diagonal and the heat flux is not zero, so the small perturbations are what lead to the transport coefficients. This is due to the process by which we obtain the transport coefficients, called the Chapman-Enskog expansion. Kinetic effects (stream instabilities, counter-flow instabilities, etc.) are not captured. The variables of the 5-moment model that we do evolve are \\[n, v_x, v_y, v_z, T\\] We can take additional moments. The 10N-moment equation evolves, in addition to the quantities in the 5N-moment model, we evolve all of the independent terms of the pressure tensor \\[n, v_x, v_y, v_z, P_{xx}, P_{xy}, P_{xz} , P_{yy} , P_{yz} , P_{zz} \\] The 13-N moment model has everything from the 10N-moment model plus the heat flux tensor \\( h_x, h_y, h_z \\) "},{"id":21,"href":"/r/notes/UWAA543/ch20-3/","title":"Reduced Models","section":"Computational CFD","content":" 20.3 Reduced Models # Further simplifications to the fluid models provide equations that are easier to solve. But we have to make sure that the reduced models (anything that\u0026rsquo;s not the full equation system) still capture the relevant physics of the problem.\nLet\u0026rsquo;s assume the (2D) flow is\nirrotational \\( (\\curl v = 0) \\) isentropic steady-state This lets us apply potential flow theory. Since the flow is irrotational, the curl of the velocity field is zero, so we can define a potential\n\\[\\vec v = - \\grad \\phi\\] That lets us define the flow direction in each direction in terms of the potential\n\\[u = - \\pdv{\\phi}{x} \\qquad v = - \\pdv{\\phi}{y}\\] similarly, continuity (conservation of mass) equation:\n\\[(\\rho \\phi_x )_x \u0026#43; (\\rho \\phi_y)_y = 0\\] and conservation of momentum\n\\[\\rho \\pdv{\\vec v}{t} \u0026#43; \\rho \\vec v \\cdot \\grad \\vec v \u0026#43; \\grad p = 0\\] \\[\\rightarrow \\rho \\left[ \\grad \\left( \\frac{v^2}{2} \\right) - \\vec v \\cross (\\curl \\vec v) \\right] \u0026#43; \\grad p = 0\\] \\[\\rightarrow \\frac{\\rho}{2} \\dd (\\phi_x ^2 \u0026#43; \\phi_y ^2) = - \\dd p\\] We can define the speed of sound \\( a \\) for an isentropic flow as\n\\[a = \\sqrt{\\dv{p}{\\rho}} = \\sqrt{\\frac{\\gamma p}{\\rho}}\\] so\n\\[\\dd \\rho = - \\frac{\\rho}{2 a^2} \\dd ( \\phi_x ^2 \u0026#43; \\phi _y ^2)\\] \\[\\rightarrow \\rho_x = - \\frac{\\rho}{2 a^2} (\\phi_x ^2 \u0026#43; \\phi_y ^2) _x = 0 \\frac{\\rho}{2a^2} ( \\phi_x \\phi_{xx} \u0026#43; \\phi_y \\phi_{xy})\\] similarly for \\( \\rho_y \\)\n\\[\\rho_y = - \\frac{\\rho}{2 a^2} (\\phi_x ^2 \u0026#43; \\phi_y ^2) _y = - \\frac{\\rho}{2a^2} ( \\phi_x \\phi_{yx} \u0026#43; \\phi_y \\phi_{yy})\\] Substituting \\( \\rho_x \\) and \\( \\rho_y \\) back into the continuity equation\n\\[\\left( 1 - \\frac{\\phi_x ^2}{a^2} \\right) \\phi_{xx} \u0026#43; \\left( 1 - \\frac{\\phi_y ^2}{a^2} \\right) \\phi_{yy} - \\frac{2}{a^2} \\phi_x \\phi_y \\phi_{xy} = 0\\] This is the full potential equation that is equivalent to the Euler equations for irrotational, isentropic, steady-state flows. Why would we want to solve this instead of the Euler equations? We\u0026rsquo;ve taken a 5-vector equation and turned it into a scalar equation! Note, we are no longer in conservation law form, which means we do not get the benefits that come with conservation form, but it is still easier to solve.\nNote, for incompressible flow, \\( a \\rightarrow \\infty \\) and\n\\[\\phi_{xx} \u0026#43; \\phi_{yy} = 0 \\qquad (\\text{Laplace eq.})\\] 20.3.2 Transonic Small Disturbance # If a feature of interest (e.g. wing) does not have a large impact on the bulk flow \\( u_{\\infty} \\) , we can define a perturbed potential to linearize.\n\\[u = u_{\\infty} \u0026#43; u\u0026#39;\\] \\[= u_{\\infty} - \\phi_x\\] \\[v = v\u0026#39; = - \\phi_y\\] We assume that \\( u\u0026rsquo;, v\u0026rsquo; \u0026laquo; \\infty \\), which is the basis of our perturbation theory.\nThe compressible Bernoulli\u0026rsquo;s equation tells us\n\\[\\frac{|\\vec v|^2}{2} \u0026#43; \\frac{\\gamma}{\\gamma \u0026#43; 1} \\frac{p}{\\rho} = \\text{const.}\\] \\[\\frac{\\gamma - 1}{2} |v|^2 \u0026#43; a^2 = \\frac{\\gamma - 1}{2} u_{\\infty}^2 \u0026#43; a_{\\infty}^2\\] \\( a_{\\infty} \\) is the bulk speed of the flow\n\\[a_{\\infty}^2 = \\frac{\\gamma p_{\\infty}}{\\rho}\\] Expanding the velocity modulus in our linearization\n\\[| \\vec v | ^2 = u_{\\infty}^2 \u0026#43; 2 u\u0026#39; u_{\\infty} \u0026#43; (u\u0026#39;) ^2 \u0026#43; (v \u0026#39;) ^2\\] Substituting into the full potential gives\n\\[\\left[ \\frac{1 - M_{\\infty}^2}{M_{\\infty}^2} - (\\gamma \u0026#43; 1) \\frac{\\phi_x}{u_{\\infty}}\\right] M_{\\infty} ^2 \\phi_{xx} \u0026#43; \\phi_{yy} = 0\\] We call this the transonic small disturbance equation. It is useful when we want to know some property like the lift of a wing, in which we can treat the whole feature as a sort of black box and we want to know what small change in the flow results from its presence.\nNow we can compare with the full potential equation, and see that we\u0026rsquo;ve gotten rid of the \\( \\phi_y \\) and \\( \\phi_{xy} \\) terms. For linear flows (no shocks, completely subsonic or supersonic) we get\n\\[(1 - M_{\\infty}^2 ) \\phi_{xx} \u0026#43; \\phi_{yy} = 0\\] And for sub-sonic flow where \\( M_{\\infty} \\rightarrow 0 \\) we\u0026rsquo;re right back to the equation for incompressible flow.\n"},{"id":22,"href":"/r/notes/UWAA557/ch10-2/","title":"Review of E\u0026M","section":"Physics of Fusion Plasmas","content":" 10.2 Review of E\u0026amp;M # Homework group: just me n leek42@uw.edu\nRealistically, all of the E\u0026amp;M review comes from https://peppyhare.github.io/griffiths-em/\n10.1.1 Maxwell Stress Tensor # The Maxwell stress tensor encompasses the way that electromagnetic fields an exert forces/stresses in 3D space:\n\\[\\vec p = \\text{ momentum }\\] The basic force laws of E\u0026amp;M are\n\\[\\dv{\\vec p}{t} = q \\left( \\vec E \u0026#43; \\vec v \\cross \\vec B \\right)\\] On an element of volume, the change in momentum is\n\\[\\dv{P}{t} = \\int_V (\\rho E \u0026#43; j \\cross B) \\dd V\\] The maxwell equations give the source terms as\n\\[\\rho = \\epsilon _0 \\div \\vec E \\qquad \\vec j = \\frac{1}{\\mu_0} \\left( \\curl \\vec B - \\mu_0 \\epsilon_0 \\pdv{E}{t} \\right)\\] \\[\\rho \\vec E \u0026#43; \\vec j \\cross \\vec B = \\vec E \\epsilon_0 (\\div E) - \\frac{B}{\\mu_0} \\cross ( \\curl B - \\mu_0 \\epsilon_0 \\pdv{E}{t} )\\] Using the chain rule \\( \\pdv{}{t} (E \\cross B) = \\pdv{E}{t} \\cross B + E \\cross \\pdv{B}{t} \\) and \\( B( \\div B) = 0 \\)\n\\[\\rho E \u0026#43; j \\cross B = \\epsilon_0 E( \\div E) - \\frac{B \\cross (\\curl B)}{\\mu_0} \u0026#43; \\epsilon_0 \\left[ E \\cross \\pdv{B}{t} - \\pdv{}{t} (E \\cross B) \\right]\\] From Faraday, \\( \\pdv{B}{t} \\rightarrow \\curl E \\)\n\\[= \\epsilon_0 E(\\div E) \u0026#43; \\frac{B(\\div B)}{\\mu_0} - \\frac{B \\cross (\\curl B)}{\\mu_0} - \\epsilon_0 E \\cross (\\curl E) - \\epsilon_0 \\pdv{}{t} (E \\cross B)\\] Finally we\u0026rsquo;ve got everything in place to use the identity\n\\[\\frac{1}{2} \\grad (B^2) = \\frac{1}{2} \\grad (\\vec B \\cdot \\vec B) = (\\vec B \\cdot \\grad) \\vec B \u0026#43; \\vec B \\cross (\\curl \\vec B)\\] All together,\n\\[\\rho E \u0026#43; j \\cross B = \\epsilon_0 \\left[ E ( \\div E ) \u0026#43; (\\vec E \\cdot \\grad) E - \\frac{1}{2} \\grad (E^2) \\right] \u0026#43; \\frac{1}{\\mu_0} \\left[B (\\div B) \u0026#43; (\\vec B \\cdot \\grad) B - \\frac{1}{2} \\grad (B^2) \\right] - \\epsilon_0 \\pdv{}{t} ( \\vec E \\cross \\vec B)\\] Integrating over a volume,\n\\[\\dv{P}{t} \u0026#43; \\epsilon_0 \\pdv{}{t} \\int \\vec E \\cross \\vec B \\dd V\\] \\[\\qquad = \\int \\epsilon_0 \\left[ E(\\div E) \u0026#43; (E \\cdot \\grad) E - \\frac{1}{2} \\grad(E^2) \\right] \u0026#43; \\frac{1}{\\mu_0} \\left[ B(\\div B) \u0026#43; (B \\cdot \\grad) B - \\frac{1}{2} \\grad (B^2) \\right] \\dd V\\] To simplify, we use another identity\n\\[E (\\div E) \u0026#43; (E \\cdot \\grad) E - \\frac{1}{2} \\grad (E^2) = \\div (\\vec E \\vec E - \\frac{1}{2} \\vec 1 E^2)\\] And the same is true for \\( \\vec B \\). The momentum change expression is simplified if we define the Maxwell stress tensor as\n\\[\\vec T = \\epsilon_0 \\vec E \\vec E \u0026#43; \\frac{\\vec B \\vec B}{\\mu_0} - \\frac{1}{2} \\vec 1 \\left( \\epsilon_0 E^2 \u0026#43; \\frac{B^2}{\\mu_0} \\right)\\] That lets us re-cast the momentum change in a volume as a stress tensor on a surface\n\\[\\dv{\\vec p}{t} \u0026#43; \\epsilon_0 \\dv{}{t} \\int ( \\vec E \\cross \\vec B) \\dd V = \\int \\div \\vec T \\dd V = \\int \\vec T \\cdot \\vu{n} \\dd \\vec a\\] Example: Application to FRC (Field-Reversed Configuration) # In a FRC confinement experiment, a bias field \\( B_{\\infty} \\) is applied and quickly reversed, resulting in poloidal fields opposite the bias current.\n\\[\\vec T = \\frac{\\vec B \\vec B}{\\mu_0} - \\frac{1}{2} \\vec I \\left( \\frac{B^2}{\\mu_0} \\right)\\] If we want to confine a plasma in the center of the FRC, we want to calculate the force on our plasma (electron fluid).\n\\[\\vec F = \\int \\vec T \\cdot \\vu n \\dd a\\] At the left end of the configuration where we have \\( \\vec B = B_z \\vu z \\) and we define \\( B_1 = B_r, B_2 = B_z, B_3 = B_\\theta \\)\n\\[\\vec T = \\begin{bmatrix} - \\frac{1}{2} \\frac{B_0 ^2}{\\mu_0} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{B_0 ^2}{\\mu_0} - \\frac{1}{2} \\frac{B_0 ^2}{\\mu_0} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; - \\frac{1}{2} \\frac{B_0 ^2}{\\mu_0} \\end{bmatrix}\\] \\[\\vec T \\cdot \\vu n = \\begin{bmatrix} 0 \\\\ -\\frac{1}{2} \\frac{B_0 ^2}{\\mu_0} \\vu z \\\\ 0 \\end{bmatrix}\\] Now what about at the sides? There, \\( B_r \\neq 0 \\) and\n\\[\\vec T \\cdot \\vu n = \\begin{bmatrix} - \\frac{1}{2} \\frac{B^2}{\\mu_0} \u0026#43; \\frac{B_r ^2}{\\mu_0} \u0026amp; \\frac{B_r B_z}{\\mu_0} \u0026amp; 0 \\\\ \\frac{B_r B_z}{\\mu_0} \u0026amp; - \\frac{1}{2} \\frac{B^2}{\\mu_0} \u0026#43; \\frac{B_z ^2}{\\mu_0} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; - \\frac{1}{2} \\frac{B^2}{\\mu_0} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} - \\frac{1}{2} \\frac{B^2}{\\mu_0} \u0026#43; \\frac{B_r ^2}{\\mu_0} \\\\ \\frac{B_r B_z}{\\mu_0} \\\\ 0 \\end{bmatrix}\\] The total force is the sum\n\\[\\vec F = \\int \\vec T \\cdot \\vu n \\dd a = \\left( - \\frac{1}{2} \\frac{B_0 ^2}{\\mu_0} \u0026#43; \\frac{1}{2} \\frac{B_{0} ^2}{\\mu_0} \\right) \\pi a ^2 \\vu z \u0026#43; \\text{ contrib. from sides}\\] By symmetry, the \\( r \\) components add to zero\n\\[F_z = \\frac{\\pi a^2}{2 \\mu_0} (B_{\\infty} ^2 - B_0 ^2) \u0026#43; \\frac{2 \\pi a}{\\mu_0} \\int B_r B_z \\dd z\\] The magnetic flux at the open ends must go somewhere\n\\[\\Phi = B_z \\pi a^2 (1 - x_s ^2) \\] \\[2 \\pi a B_r \\dd z = - \\dd \\Phi\\] \\[F_z = \\frac{1}{2 \\mu_0 \\pi a^2} ( \\Phi_{\\infty} ^2 - \\Phi_0 ^2 ) - \\frac{1}{\\mu_0 } \\int_{\\Phi_0} ^{\\Phi_{\\infty}} \\frac{\\Phi}{\\pi a^2 ( 1 - x_s ^2)} \\dd \\Phi\\] Assume \\( x_s \\) is constant (or that it is an appropriate average value)\n\\[F_z = \\frac{1}{2 \\mu_0 \\pi a^2} ( \\Phi_{\\infty} ^2 - \\Phi_0 ^2 ) - \\frac{ \\Phi_{\\infty} ^2 - \\Phi_0 ^2}{2 \\mu_0 \\pi a ^2 (1 - x_s ^2)}\\] \\[= \\frac{\\pi a^2}{2 \\mu_0} ( B_\\infty ^2 - B_0 ^2) \\left( 1 - \\frac{1}{1 - x_s ^2} \\right)\\] \\[= \\frac{\\pi a^2}{2 \\mu_0 } ( B_0 ^2 - B_{\\infty} ^2 ) \\frac{x_s ^2}{1 - x_s ^2}\\] \\[= \\frac{A_{frc}}{2 \\mu_0} ( B_0 ^2 - B_{\\infty} ^2 ) \\left( \\frac{1}{1 - x_s ^2} \\right)\\] Where \\( A_{frc} = \\pi a^2 x_s^2 \\) is the cross-sectional area of the FRC. So, in the simple picture the force is the same as the initial flux, reduced by an equivalent area of the FRC torus.\n"},{"id":23,"href":"/r/notes/griffiths/ch1-4/","title":"Curvilinear Coordinates","section":"Griffiths Introduction to Electrodynamics","content":" 1.4: Curvilinear Coordinates # 1.4.1: Spherical Coordinates # You can label a point P by its Cartesian coordinates (x, y, z), but sometimes it is more convenient to use spherical coordinates \\( (r, \\theta, \\phi) \\); \\( r \\) is the distance from the origin (the magnitude of the position vector r), \\( \\theta \\) (the angle down from the z axis) is called the polar angle, and \\( \\phi \\) (the angle around from the x axis) is the azimuthal angle. Their relation to Cartesian coordinates can be read trigonometrically from Fig 1.36:\n\\[x = r \\sin \\theta \\cos \\phi, \\qquad y = r \\sin \\theta \\sin \\phi, \\qquad z = r \\cos \\theta \\tagl{1.62}\\] Figure 1.36 also shows three unit vectors, \\( \\vu{r}, \\vu{\\theta}, \\vu{\\phi} \\), pointing in the direction of increase of the corresponding coordinates. They constitute an orthogonal (mutually perpendicular) basis set (just like \\( \\vu{x}, \\vu{y}, \\vu{z} \\)), and any vector A can be expressed in terms of them, in the usual way:\n\\[\\vec{A} = A_r \\vu{r} \u0026#43; A_\\theta \\vu{\\theta} \u0026#43; A_\\phi \\vu{\\phi} \\tagl{1.63}\\] \\( A_r, A_{\\theta}, A_{\\phi} \\) are the radial, polar, and azimuthal components of A. In terms of the Cartesian unit vectors,\n\\[\\begin{aligned} \\vu{r} \u0026amp; = \\sin \\theta \\cos \\phi \\vu{x} \u0026#43; \\sin \\theta \\sin \\phi \\vu{y} \u0026#43; \\cos \\theta \\vu{z} \\\\ \\vu{\\theta} \u0026amp; = \\cos \\theta \\cos \\phi \\vu{x} \u0026#43; \\cos \\theta \\sin \\phi \\vu{y} - \\sin \\theta \\vu{z} \\\\ \\vu{\\phi} \u0026amp; = - \\sin \\phi \\vu{x} \u0026#43; \\cos \\phi \\vu{y} \\end{aligned} \\tagl{1.64}\\] as you can check for yourself (Prob 1.38).\nThere is a poisonous snake lurking here that I\u0026rsquo;d better warn you about: \\( \\vu{r} \\) , \\( \\vu{\\theta} \\), and \\( \\vu{\\phi} \\) are associated with a particular point P, and they change direction as P moves around. For example, \\( \\vu{r} \\) always points radially outward, but \u0026ldquo;radially outward\u0026rdquo; can be in the x direction, the y direction, or any other direction, depending on where you are. In Fig. 1.37, \\( \\vec{A} = \\vu{y} \\) and \\( \\vec{B} = - \\vu{y} \\), and yet both of them would be written as \\( \\vu{r} \\) in spherical coordinates. One could take account of this by explicitly indicating the point of reference: \\( \\vu{r}(\\theta, \\phi), \\vu{\\theta}(\\theta, \\phi), \\vu{\\phi}(\\theta, \\phi) \\), but this would be cumbersome, and as long as you are alert to the problem, I don\u0026rsquo;t think it will cause difficulties. In particular, do not naively combine the spherical components of vectors associated with different points (in Fig. 1.37, \\( \\vec{A} + \\vec{B} = 0 \\), not \\( 2 \\vu{r} \\), and \\( \\vec{A} \\cdot \\vec{B} = -1 \\) , not \\( +1 \\)). Beware of differentiating a vector that is expressed in spherical coordinates, since the unit vectors themselves are functions of position (\\( \\partial \\vu{r} / \\partial \\theta = \\vu{\\theta} \\), for example). And do not take \\( \\vu{r}, \\vu{\\theta}, \\vu{\\phi} \\) outside an integral, as I did with the Cartesian unit vectors. In general, if you\u0026rsquo;re uncertain about the validity of an operation, rewrite the problem using Cartesian coordinates, for which this difficulty does not arise.\nAn infinitesimal displacement in the \\( \\vu{r} \\) direction is simply dr (Fig. 1.38a), just as an infinitesimal element of length in the x direction is dx:\n\\[\\dd l_r = dr \\tagl{1.65}\\] On the other hand, an infinitesimal element of length in the \\( \\vu{\\theta} \\) direction (Fig 1.38b) is not just \\( \\dd \\theta \\) - that doesn\u0026rsquo;t even have the right units for a length! Rather,\n\\[\\dd l_{\\theta} = r \\dd \\theta \\tagl{1.66}\\] Similarly, an infinitesimal element of length in the \\( \\vu{\\phi} \\) direction (Fig 1.38c) is\n\\[\\dd l_{\\phi} = r \\sin \\theta \\dd \\phi \\tagl{1.67}\\] so that we can write the general infinitesimal displacement as\n\\[\\dd \\vec{l} = \\dd r \\vu{r} \u0026#43; r \\dd \\theta \\vu{\\theta} \u0026#43; r \\sin \\theta \\dd \\phi \\vu{\\phi} \\tagl{1.68}\\] This plays the role that \\( \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} \\) played in Cartesian coordinates.\nThe infinitesimal volume element \\( \\dd \\tau \\) in spherical coordinates, is the product of the three infinitesimal displacements:\n\\[\\dd \\tau = \\dd l_r \\dd l_{\\theta} \\dd l_{\\phi} = r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\tagl{1.69}\\] I cannot give you a general expression for surface elements \\( \\dd \\vec{a} \\) , since these depend on the orientation of the surface. You simply have to analyze the geometry for any given case (this goes for Cartesian and curvilinear coordinates alike). If you are integrating over the surface of a sphere, for instance, then \\( r \\) is constant, whereas \\( \\theta \\) and \\( \\phi \\) change (Fig. 1.39), so\n\\[\\dd \\vec{a}_1 = \\dd l_\\theta \\dd l_\\phi \\vu{r} = r^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r}\\] On the other hand, if the surface lies in the xy plane, say, so that \\( \\theta \\) is constant (\\( \\pi / 2 \\)) while \\( r \\) and \\( \\phi \\) may vary, then\n\\[\\dd \\vec{a}_2 = \\dd l_r \\dd l_{\\phi} \\vu{\\theta} = r \\dd r \\dd \\phi \\vu{\\theta}\\] Notice, finally, that \\( r \\) ranges from \\( 0 \\) to \\( \\infty \\), \\( \\phi \\) from \\( 0 \\) to \\( 2 \\pi \\), and \\( \\theta \\) from \\( 0 \\) to \\( \\pi \\).\nExample 1.13 # Q Find the volume of a sphere of radius R A Well, we know that we should get \\( \\frac{4}{3} \\pi R^3 \\). Let\u0026rsquo;s see what happens\u0026hellip;\n\\[\\begin{aligned} V \u0026amp; = \\int \\dd \\tau = \\int_{r = 0} ^R \\dd r \\int_{\\theta = 0} ^{\\pi} r \\dd \\theta \\int_{\\phi = 0} ^{2 \\pi} r \\sin \\theta \\dd \\phi \\\\ \u0026amp; = \\left( \\int_{0} ^R r^2 \\dd r \\right) \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) \\\\ \u0026amp; = \\left( \\frac{R^3}{3} \\right)(2)(2 \\pi) = \\frac{4}{3} \\pi R^3 \\end{aligned}\\] Great!\nSo far we have talked only about the geometry of spherical coordinates. Now I would like to \u0026ldquo;translate\u0026rdquo; the vector derivatives (gradient, divergence, curl, and Laplacian) into \\( r, \\theta, \\phi \\) notation. In principle, this is entirely straightforward: in the case of the gradient,\n\\[\\grad T = \\pdv{T}{x} \\vu{x} \u0026#43; \\pdv{T}{y} \\vu{y} \u0026#43; \\pdv{T}{z} \\vu{z}\\] for instance, we would first use the chain rule to expand the partials\n\\[\\pdv{T}{x} = \\pdv{T}{r} \\left( \\pdv{r}{x} \\right) \u0026#43; \\pdv{T}{\\theta} \\left( \\pdv{\\theta}{x} \\right) \u0026#43; \\pdv{T}{\\phi} \\left( \\pdv{\\phi}{x} \\right)\\] The terms in parentheses could be worked out from \\( \\eqref{1.62} \\) - or rather, their inverse. Then we\u0026rsquo;d do the same for y and z, and then substitute in the formulas for \\( \\vu{x}, \\vu{y}, \\vu{z} \\) in terms of \\( \\vu{r}, \\vu{\\theta}, \\vu{\\phi} \\). It would take an hour to carry out this very brute-force approach, and I suppose this is how it was originally done, but there is a much more efficient indirect approach, which has the extra advantage of treating all coordinate systems at once. I describe the \u0026ldquo;straightforward\u0026rdquo; method only to show you that there is nothing subtle or mysterious about transforming to spherical coordinates: you\u0026rsquo;re expressing the same quantity in different notation, that\u0026rsquo;s all. The indirect method is relegated to one of the appendices, which I may add later.\nHere, then, are the vector derivatives in spherical coordinates:\nGradient:\n\\[\\grad T = \\pdv{T}{r} \\vu{r} \u0026#43; \\frac{1}{r} \\pdv{T}{\\theta} \\vu{\\theta} \u0026#43; \\frac{1}{r \\sin \\theta} \\pdv{T}{\\phi} \\vu{\\phi} \\tagl{1.70}\\] Divergence:\n\\[\\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} (r^2 v_r) \u0026#43; \\frac{1}{r\\sin \\theta} \\pdv{}{\\theta} (\\sin \\theta v_{\\theta}) \u0026#43; \\frac{1}{r \\sin \\theta} \\pdv{v_{\\phi}}{\\phi} \\tagl{1.71} \\] Curl:\n\\[\\begin{aligned} \\curl \\vec{v} = \u0026amp; \\frac{1}{r \\sin \\theta} \\left[ \\pdv{}{\\theta} (\\sin \\theta V_{\\phi}) - \\pdv{v_{\\theta}}{\\phi} \\right] \\vu{r} \\\\ \u0026amp; \\quad \u0026#43; \\frac{1}{r} \\left[ \\frac{1}{\\sin \\theta} \\pdv{v_r}{\\phi} - \\pdv{}{r} (r v_{\\phi}) \\right] \\vu{\\theta} \\\\ \u0026amp; \\quad \u0026#43; \\frac{1}{r} \\left[ \\pdv{}{r} (r v_{\\theta}) - \\pdv{v_r}{\\theta} \\right] \\vu{\\phi} \\end{aligned} \\tagl{1.72}\\] Laplacian:\n\\[\\laplacian T = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{T}{r} \\right) \u0026#43; \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{T}{\\theta} \\right) \u0026#43; \\frac{1}{r^2 \\sin ^2 \\theta} \\frac{\\partial ^2 T}{\\partial \\phi ^2} \\tagl{1.73} \\] 1.4.2: Cylindrical Coordinates # The cylindrical coordinates \\( (s, \\phi, z) \\) of a point P are defined in Fig 1.42. Notice that \\( \\phi \\) has the same meaning as in spherical coordinates, and z is the same as Cartesian. \\( s \\) is the distance to P from the z axis, whereas the spherical coordinate \\( r \\) is the distance from the origin. The relation to Cartesian coordinates is somewhat cleaner than the spherical sort\n\\[x = s \\cos \\phi, \\qquad y = s \\sin \\phi, \\qquad z = z \\tagl{1.74}\\] The unit vectors (Prob 1.42) are\n\\[\\begin{aligned} \\vu{s} \u0026amp; = \\cos \\phi \\vu{x} \u0026#43; \\sin \\phi \\vu{y} \\\\ \\vu{\\phi} \u0026amp; = - \\sin \\phi \\vu{x} \u0026#43; \\cos \\phi \\vu{y} \\\\ \\vu{z} \u0026amp; = \\vu{z} \\end{aligned} \\tagl{1.75}\\] The infinitesimal displacements are\n\\[dl_s = ds, \\qquad dl_{\\phi} = s \\dd \\phi, \\qquad dl_z = dz \\tagl{1.76} \\] so\n\\[\\dd \\vec{l} = ds \\vu{s} \u0026#43; s \\dd \\phi \\vu{\\phi} \u0026#43; dz \\vu{z} \\tagl{1.77}\\] and the volume element is\n\\[\\dd \\tau = s \\, \\dd s \\, \\dd \\phi \\, \\dd z \\tagl{1.78}\\] The range of s is \\( 0 \\rightarrow \\infty \\), \\( \\phi \\) goes from \\( 0 \\rightarrow 2\\pi \\), and \\( z \\) from \\( -\\infty \\rightarrow \\infty \\).\nThe vector derivatives in cylindrical coordinates are:\nGradient:\n\\[\\grad T = \\pdv{T}{s} \\vu{s} \u0026#43; \\frac{1}{s} \\pdv{T}{\\phi} \\vu{\\phi} \u0026#43; \\pdv{T}{z} \\vu{z} \\tagl{1.79}\\] Divergence:\n\\[\\div \\vec{v} = \\frac{1}{s} \\pdv{}{s} (s v_s) \u0026#43; \\frac{1}{s} \\pdv{v_\\phi}{\\phi} \u0026#43; \\pdv{v_z}{z} \\tagl{1.80} \\] Curl:\n\\[\\begin{aligned} \\curl \\vec{v} = \u0026amp; \\left( \\frac{1}{s} \\pdv{v_z}{\\phi} - \\pdv{v_{\\phi}}{z} \\right) \\vu{s} \\\\ \u0026amp; \u0026#43; \\left( \\pdv{v_s}{z} - \\pdv{v_z}{s} \\right) \\vu{\\phi} \\\\ \u0026amp; \u0026#43; \\frac{1}{s} \\left[ \\pdv{}{s} (s v_{\\phi}) - \\pdv{v_s}{\\phi} \\right] \\vu{z} \\end{aligned} \\tagl{1.81}\\] Laplacian:\n\\[\\laplacian T = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{T}{s} \\right) \u0026#43; \\frac{1}{s^2} \\frac{\\partial ^2 T}{\\partial \\phi ^2} \u0026#43; \\frac{\\partial ^2 T}{\\partial z^2} \\tagl{1.82} \\] "},{"id":24,"href":"/r/notes/UWAA560/04-electrostatic-diagnostics/","title":"Electrostatic Diagnostics","section":"Plasma Diagnostics","content":" \\[\\] Langmuir Probes # Langmuir probes are the oldest and simplest plasma diagnostic: You simply insert an electrode into the plasma and measure the \\( I \\)-\\( V \\) curve. Doing so is fairly simple, but the probe measurements are often difficult to interpret, since extracting accurate plasma properties from measurements can be challenging.\nConsider a probe inserted into a steady-state plasma. We assume that there is an electrode (e.g. the wall of the vacuum chamber) which is essentially in electrical contact with the plasma, and we will treat that as ground.\nAssuming a thermal plasma (Maxwellian) with \\( T_i \\approx T_e \\), ions and electrons strike the probe in all directions. Assume the bulk plasma is static (the bulk velocity is zero). Then the particle flux for each species \\( s \\) is\n\\[\\Gamma_s=\\frac{1}{4} n_s \\overline{v}_s\\] where \\( \\overline{v}_s \\) is the mean particle speed. A probe of area \\( A \\) initially at zero potential inserted in a singly charged, quasi-neutral plasma will emit a current\n\\[I = - e A \\frac{1}{4} (n_i \\overline v _i - n_e \\overline v _e) \\\\ \\approx \\frac{1}{4} e A n_e \\overline v_e \u0026gt; 0\\] A positive current here means the probe absorbs electrons. The probe potential, which started at ground, drops and repels incoming electrons until \\( \\Gamma _e = \\Gamma _i \\) such that \\( I = 0 \\). When this happens, we can measure the probe voltage to give what we call the \u0026ldquo;floating potential\u0026rdquo; \\( V_f \\). If the probe potential can be externally controlled, then we can find the floating potential without depleting the plasma of electrons. We can start out with a negative probe \\( V \\) and adjust it until the probe current vanishes.\nA thermal plasma by definition has a probability distribution of speed of the form\n\\[f(v) = \\left( \\frac{m}{2 \\pi k T} \\right) ^{3/2} 4 \\pi v^2 e^{- \\frac{m v^2}{2T}}\\] Since the electron mass is small, the speed distribution is higher than that of the ions for \\( T_i \\approx T_e \\):\nAs the probe potential is more positive, eventually no electrons are repelled. This potential is the plasma potential (or space potential) \\( V_p \\). Increasing \\( V \u0026gt; V_p \\) further produces no increase in current, because at \\( V_p \\) the probe is already absorbing all of the incoming electrons.\nAs a point of reference, the plasma potential is approximately 5 times the electron temperature\n\\[V_p \\approx 5 k T_e\\] An idealized I-V characteristic looks like this:\nSince \\( m_i \\gg m_e \\), \\( I_{is} \\ll I_{es} \\). If \\( T_i = T_e \\) then\n\\[\\frac{I_{is}}{I_{es}} = \\left( \\frac{m_e}{m_i} \\right)^{1/2}\\] The transition region is where we make most of our measurements of plasma parameters. As \\( V \\) is adjusted between \\( (V_f, V_p) \\), the electron current varies exponentially as\n\\[I_e = I_{es} e^{\\frac{ V - V_p}{k T_e}}\\] where \\[I_{es} = \\frac{1}{4} e A n_e \\overline{v}_e = e n_e A \\left( \\frac{k T_e}{2 \\pi m_e} \\right)^{1/2}\\] If we measure the electron current \\( I_e \\) vs. \\( V \\), this allows us a measurement to get the electron temperature \\( T_e \\). Note that \\( I_e \\) is not the total current; we need to subtract the ion current \\( I_e = I - I_i \\). So to recover the electron temperature, we plot \\( \\ln (I) \\) vs. \\( V \\) and check the slope in the transition region:\nSince \\( I_{is} \\) is fairly constant at low \\( V \\), it is often easier to adjust \\( I_i \\) until the linear portion is maximized. With \\( T_e \\) determined, the density could be extracted from the electron saturation current \\( I_{es} \\). Looking at the form of \\( I_{es} \\), you can see that it depends only on the probe geometry, electron temperature, and density. So we could measure the density as\n\\[n_e = \\frac{I_{es}}{e A} \\left( \\frac{k T_e}{2 \\pi m_e} \\right) ^{1/2}\\] but besides being difficult to measure, \\( I_{es} \\) can be very large, leading to Bad Things like altering the plasma properties, damaging the probe, or destroying the probe circuitry.\nInstead, we take a look at the other end of the \\( I \\)-\\( V \\) curve and determine the ion density \\( n_i \\) from the ion saturation current, which is generally less than \\( 10 \\% \\) of the electron saturation current.\nIf the electrons have an energetic population (e.g. a beam or other population of high-temperature electrons) in addition to the thermal population, then the distribution is not Maxwellian. Useful information can still be obtained from the \\( I \\)-\\( V \\) characteristic, but it will contain additional inflection points\nFloating Potential Measurement # As the probe becomes more positive, it attracts more electrons, and since the probe is of finite size it reaches a point where it can\u0026rsquo;t absorb any more electrons. As the voltage increases, a sheath forms around the probe, and increases the effective area of the probe.\nIf we use the Bohm sheath criterion which determines a balance between the sheath thickness and ion velocities entering. Ions will be accelerated to the sound speed as they enter the sheath, and any ions less than the sound speed will be repelled away.\n\\[I_i = I_B = \\alpha n_i e A c_{r, i}\\] where \\( A \\) is the area of the probe, \\( c_{s, i} \\) the ion sound speed, and \\( \\alpha \\) a parameter to account for the probe geometry. It is \\( 1/2 \\) for a planar probe, and generally falls in the range \\( (0.5, 3.0) \\). The ion sound speed is given by\n\\[c_{s, i} = \\left( \\frac{k T_e}{m_i} \\right)^{1/2}\\] Note that it is the electron temperature which determines the sound speed. So if \\( T_e \\) is known, the density can be computed from \\( I_{i, s} \\). To compute the floating potential, we equate the currents for charge neutrality\n\\[I_i = I_e = \\alpha n_i e A \\left( \\frac{ k T_e }{m_i} \\right)^{1/2} = e n_e A \\left( \\frac{ k T_e}{2 \\pi m_e} \\right) ^{1/2} \\exp \\left( \\frac{ V_f - V_p}{k T_e} \\right)\\] where \\( V_f \\) is the floating potential and \\( V_p \\) the plasma potential. Solving gives\n\\[V_f = V_p - \\frac{k T_e}{2} \\ln \\left( \\frac{m_i}{2 \\pi \\alpha ^2 m_e} \\right)\\] Plasma Potential Measurement # Ideally, we would get \\( V_p \\) from the point where the I-V curve departs from the linear segment. But in practice, the I-V curve is not as clean as shown and doing so can\u0026rsquo;t be done by inspection\nInstead, we can plot a curve of \\( \\dv{I}{V} \\) vs \\( V \\) and choose the peak:\nNote that whenever you differentiate experimental data, you won\u0026rsquo;t get very smooth results, especially at low \\( I \\) where the signal-to-noise ratio can be quite low. This is not a problem at large \\( I \\) where we find \\( V_p \\), so you can generally get a good measurement by this method.\nDouble and Triple Langmuir Probes # \\( V_p \\) can fluctuate significantly in time, which complicates the shape of the signal and can push into the electron saturation region (and kill our probe). We can solve this problem by using a double probe, such that the entire probe floats:\nSince the entire probe floats as the plasma potential, the current is based on the bias potential and not the plasma potential with respect to ground.\nIn the lab we usually make Langmuir probes by taking a small piece of tungsten wire (1/16\u0026quot; diameter, 1\u0026quot; long) and insert into the tip of a hypodermic needle and solder in place. Insert the whole thing into a boron nitride cylinder. As the probe tip wears down with use, we can expose additional tungsten wire from the needle.\nThe way we trace I-V with a single or double Langmuir probe is by sweeping the bias potential, so that the probe potential sweeps, and measure the current. We assume that the plasma properties are quasi-constant over a period of the sweep voltage, and this assumption can limit the time resolution or applicability of the measurement. If plasma parameters are changing rapidly, instead of sweeping the voltage we can use a triple probe to measure multiple points on the I-V characteristic simultaneously.\nIf probe 3 floats to \\( V_f \\), then probe 2 will be at a lower potential and probe 1 will be at a higher potential. Probe 1 will be at the measured \\( V \\) plus the floating potential, and the difference between probes 1 and 2 is the bias potential. Always operate such a probe such that \\( V + V_f \u0026lt; V_p \\) to avoid saturation. If \\( V_{bias} \\) is sufficiently large, the current through the double probe (probes 1 and 2) will be \\( I_{i, s} \\). The simultaneous measurement of the slope of the I-V curve gives \\( T_e \\) and \\( I_{i, s} \\) gives \\( n_i \\). To accomplish this, we solve for the current through each probe:\n\\[(1) \\qquad I_1 = I_{i} (V_1) \u0026#43; I_{e, s} \\exp \\left( \\frac{V_1 - V_p}{k T_e} \\right)\\] \\[(2) \\qquad I_2 = - I_1 = I_i (V_2) \u0026#43; I_{e, s} \\exp \\left( \\frac{ V_2 - V_p}{k T_e} \\right)\\] \\[(3) \\qquad I_3 = 0 = I_i (V_3) \u0026#43; I_{e, s} \\exp \\left( \\frac{V_3 - V_p}{Pk T_e} \\right)\\] Solving these simultaneously gives\n\\[2 \\exp \\left( - \\frac{V}{k T_e} \\right) = 1 \u0026#43; \\exp \\left( - \\frac{V_{bias}}{k T_e} \\right)\\] which we can solve for \\( T_e \\) given \\( V_{bias} \\) and \\( V \\). Another solution gives \\( n_i \\).\nSome useful references on Langmuir probes are Chen, JAP (1965) and Qayyum, RSI (2013).\nConsiderations and Assumptions # Electrostatic probes are affected by sheath conditions and by plasma parameters. Errors can be large, \\( \\sim 40 \\% \\) for \\( T_e \\) and \\( \\sim 300 \\% \\) for \\( n_i \\) Probes can not be used in high-temperature or high-density plasmas (\\( T_e \u0026gt; 20 keV\\) or \\( n_e \u0026gt; 10^{13} cm^{-3} \\)). In this regime, probe effects modify the behavior and give less reliable data. Assume that the probe radius is much smaller than the plasma mean free path (which is to say, assume the plasma is collisionless) Assume that the sheath is collisionless: the Debye length is much less than the probe radius. This is what allows for the application of the Bohm criterion For double/triple probes, assume all probes have the same collection area and geometry Assume that probes are insulated to prevent arcing. High voltage difference between probe tips can cause arcing, which destroys the probe and ruins data collection. It is possible to use probes in magnetized plasmas, sort of. Corrections exist that account for finite Larmor radius (FLR) effects, but the analysis is complicated and quite limited. Generally, we say that electrostatic probes are only valid in weakly magnetic plasmas, where the electron gyroradius \\( r_{L, e} \\) is much greater than the probe size.\nThe plasma flow velocity can be measured by adding an additional probe, forming a quadruple Langmuir probe (also called Mach probe). The probe faces the incoming plasma, and we modify the insulator to create a barrier around three of the probes. The leading probe is the Mach probe, and the shadowed portion is a triple-probe to make \\( T_e \\) and \\( n_e \\) measurements as we just described.\nIn the diagram, we see the probe tips head-on and the shaded portion is a raised insulator. The plasma flow enters from the bottom side. The Mach probe measures a current \\( I_{i, s} \\) that is enhanced by the plasma flow velocity \\( u_{\\infty} \\)\n\\[I_{is, M} = - \\frac{ e A n_i}{4} \\left( \\overline{v}_i \u0026#43; u_{\\infty} \\right)\\] Some good references for Mach probes are Hutchinson, Physics of Plasmas (1987) and Peterson, RSI (1994). There is still active debate about how best to interpret measurements. Langmuir probes are among the most simple diagnostic measurements, but assumptions and analyses are much more complicated.\nGridded Energy Analyzer (GDA) # Also known as Retarding Potential Analyzer (RPA). Langmuir probes assume features of the distribution function and extract plasma parameters from that. In contrast, the GEA measures the distribution function \\( f(v) \\) more or less directly. They are typically used to measure the ion distribution function \\( f_i(v) \\), more specifically the energy distribution function \\( f_i(\\phi) = f_i (\\frac{1}{2} m_i v^2) \\).\nThe grid voltage \\( V_g \\) is at a positive potential, so it is an ion repeller (also called the discriminator or gate). The way the GEA works is that only ions with an energy above \\( V_g \\) reach the collector. Any with energy less than \\( V_g \\) are repelled. The collector current (for collector area \\( A_c \\) is\n\\[I(V_g) = \\frac{q^2 A_c}{m_i} \\int_{V_g }^\\infty f_i(\\phi) \\dd \\phi\\] If we think back to the distribution function for ions, anything to the right of the gate voltage is collected and everything to the left is repelled, so we are collecting the high-energy tail above \\( V_g \\). As a function of \\I( V_g \\), we get the ion cumulative distribution function, and if \\( f_i(\\phi) \\) is Maxwellian, then \\( I(V_g) \\) is an error function.\nIn a real setup, we want to collect the ions at a specific bias voltage. First, we place a density filter to decrease the plasma density. This is necessary because the Debye length needs to be large compared to the grid spacing or the grid potential will be shielded. We need to repel the electrons, so a negatively charged grid is placed to repel them. The ions are gated at a high-voltage ion repeller, then collected at the collector. When ions strike the collector, there is a chance to liberate electrons which would increase the measured current. We place a secondary electron deflector to prevent this.\nConsiderations for GEA/RPA # We need very low densities for GEA analysis. Debye shielding must not filter the grid potential, so the grid spacing must be less than the Debye length. This usually means a density filter is applied upstream. All electrons must be removed: this is the electron repeller grid. Ions will strike the collector and liberate electrons from the surface, artificially increasing the measured current (secondary electrons). As far as our ammeter is concerned, electron leaving the collector has the same effect as an ion reaching the collector. "},{"id":25,"href":"/r/notes/UWAA543/ch20-4/","title":"Equation Types","section":"Computational CFD","content":" Equation Types # PDE\u0026rsquo;s can be classified as elliptic, parabolic, or hyperbolic. The type of equation will suggest the choice of an appropriate algorithm.\nThe equation type can change for different flow parameters, or even for different regions within the same domain.\n20.4.1 Scalar Equations # All of the scalar equations in fluid dynamics can be written in canonical form as the following 2nd order PDE.\n\\[a \\phi_{xx} \u0026#43; b \\phi_{xy} \u0026#43; c \\phi_{yy} \u0026#43; d \\phi_x \u0026#43; e \\phi_y \u0026#43; f \\phi = g\\] where \\( x, y \\) are any two independent variables. I.e. we could let \\( x \\rightarrow t \\). In this form, we can identify the equation type by transforming into a quadratic form (derivation not shown)\n\\[a \\left( \\dv{y}{x} \\right) ^2 \u0026#43; b \\left( \\dv{y}{x} \\right) \u0026#43; c = 0\\] The equation types are\nElliptic: \\( b^2 \u0026lt; 4ac \\) Parabolic: \\( b^2 = 4ac \\) Hyperbolic: \\( b^2 \u0026gt; 4ac \\) Apparently, the equation type is going to have something to do with the types of roots of the scalar PDE.\nSome examples:\nLaplace\u0026rsquo;s Eq. \\[\\phi_{xx} \u0026#43; \\phi_{yy} = 0 \\qquad (0) ^2 \u0026lt; 4(1)(1) \\rightarrow \\text{elliptic}\\] We know that typical solutions to Laplace\u0026rsquo;s equation are entirely determined by the value at the boundary. The solutions are the smoothest possible functions that satisfy the boundary condition. There are no waves.\nHeat conduction equation \\[\\theta _t = \\alpha \\theta_{xx} \\qquad (0)^2 = 4 (\\alpha)(0) \\rightarrow \\text{parabolic}\\] We know that the typical solutions have diffusion-like behavior. 3. Linear advection equation\n\\[u_t \u0026#43; a u_x = 0\\] We\u0026rsquo;re not already in canonical form, but we can get there with some algebraic manipulation\n\\[0 = u_{tt} \u0026#43; a u_{xy} = u_{tt} \u0026#43; a (u_t)_x = u_{tt} - a^2 u_{xx}\\] \\[\\rightarrow 0 \u0026gt; 4(1)(-a^2) \\rightarrow \\text{hyperbolic}\\] Typical solutions to the linear advection equation are waves which propagate at a set speed.\nLet\u0026rsquo;s look back at the linear potential equation\n\\[(1 - M_{\\infty}^2) \\phi_{xx} \u0026#43; \\phi_{yy} = 0\\] The equation type depends on the flow speed (Mach number) of the bulk \\( M_{\\infty} \\). For \\( M_{\\infty}^2 \u0026lt; 1 \\) it is elliptic. For \\( M_{\\infty}^2 \u0026gt; 1 \\) it is hyperbolic. We don\u0026rsquo;t get parabolic solutions because near \\( M_\\infty = 1 \\) we are going to get shocks and we are obviously not in linear flow.\nIt is also possible for the equation type to change within the domain. In particular, look at the TSD (transonic disturbance) equation. The selection parameter is\n\\[\\frac{1 - M_{\\infty}^2}{M_{\\infty}^2} \u0026gt; (\\gamma \u0026#43; 1) \\frac{u\u0026#39;}{u_{\\infty}} \\] So for \\( u_{\\infty} + u\u0026rsquo; \u0026lt; a \\) we have an elliptic equation, but for \\( u_{\\infty} + u\u0026rsquo; \u0026gt; a \\) we have a hyperbolic equation. These are locally evaluated, so we can change types within the same domain. We can imagine the flow around a wing for which \\( u_{\\infty} \u0026gt; a_{\\infty} \\), then in most of the domain \\( u \u0026gt; a \\) and the problem is hyperbolic, but near the front of the wing where \\( u \u0026lt; a \\) the problem becomes elliptic.\n20.4.2 Systems of Equations # Moving away from scalar equations to the world of our Euler equations,\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{x} = 0\\] \\[Q = \\begin{bmatrix} \\rho \\\\ \\rho u \\\\ e \\end{bmatrix} = \\begin{bmatrix} \\rho \\\\ m \\\\ e \\end{bmatrix}\\] Where we\u0026rsquo;ve written \\( \\rho u \\) as a separate independent variable \\( m \\) (we can do that because \\( \\rho \\) and \\( u \\) are independent).\n\\[\\pdv{\\vec{Q}}{t} \u0026#43; \\vec A \\cdot \\pdv{\\vec F}{x} = 0 \\qquad \\vec A \\equiv \\pdv{\\vec F}{\\vec Q} \\quad \\text{(Flux Jacobian)}\\] The equation type is given by the characteristics of the flux Jacobian:\nElliptic: The eigenvalues of \\( \\vec A \\) are imaginary. Parabolic: The eigenvalues are repeated and the eigenvectors are not unique. Hyperbolic: The eigenvalues are real and the eigenvectors are unique. 20.4.3 Flux Jacobian for Euler Equations # Recall the definition\n\\[\\overline{F}= \\begin{bmatrix} m \\\\ \\frac{m^2}{\\rho} \u0026#43; p \\\\ \\frac{m}{\\rho} (e \u0026#43; p) \\end{bmatrix}\\] \\[\\pdv{\\overline{F}}{\\overline{Q}} = \\begin{bmatrix} \\left. \\pdv{F_1}{Q_1} \\right|_{Q_2 Q_3} \u0026amp; \\left. \\pdv{F_1}{Q_2} \\right|_{Q_1 Q_3} \u0026amp; \\left. \\pdv{F_1}{Q_1} \\right|_{Q_2 Q_3} \\\\ \\left. \\pdv{F_2}{Q_1} \\right|_{Q_2 Q_3} \u0026amp; \\ldots \u0026amp; \\ldots \\end{bmatrix}\\] Starting at the first element, and recalling that we take the partial derivative holding \\( Q_2 \\) and \\( Q_3 \\) constant,\n\\[\\pdv{F_1}{Q_1} = \\pdv{m}{\\rho} = 0\\] Skipping to the end result,\n\\[\\overline{A} = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ - \\frac{m^2}{\\rho^2} \u0026#43; p_{\\rho} \u0026amp; \\frac{2 m}{\\rho} \u0026#43; p_m \u0026amp; p_e \\\\ - \\frac{m}{\\rho^2} (e \u0026#43; p) \u0026#43; \\frac{m}{\\rho} p_{\\rho} \u0026amp; \\frac{e \u0026#43; p}{\\rho} \u0026#43; \\frac{m}{\\rho} p_m \u0026amp; \\frac{m}{\\rho} (1 \u0026#43; p_e) \\end{bmatrix}\\] Let\u0026rsquo;s talk about how we deal with these partial derivatives of pressure with respect to our conserved quantities\n\\[p_{\\rho} = \\left. \\pdv{p}{\\rho} \\right| _{m e}\\] From the definition,\n\\[e = \\frac{p}{\\gamma - 1} \u0026#43; \\frac{\\rho v^2}{2}\\] Solve for pressure,\n\\[p = ( \\gamma - 1) \\left( e - \\frac{\\rho v^2}{2} \\right)\\] \\[= ( \\gamma - 1 ) \\left( e - \\frac{m ^2}{2 \\rho} \\right)\\] Now we can get our partial derivatives easily\n\\[p_{\\rho} = \\left. \\pdv{p}{\\rho} \\right| _{m e} = (\\gamma - 1) \\frac{m^2}{\\rho ^2} = \\frac{1}{2} ( \\gamma - 1) u^2\\] We also define the sound speed\n\\[c_S ^2 = \\frac{\\gamma p}{\\rho}\\] Writing our the flux Jacobian in these terms,\n\\[\\overline{A} = \\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 0 \\\\ \\frac{\\gamma - 3}{2} u^2 \u0026amp; - (\\gamma - 3) u \u0026amp; \\gamma - 1 \\\\ - \\frac{u c_s ^2}{\\gamma - 1} \u0026#43; \\frac{\\gamma - 2}{2} u^3 \u0026amp; \\frac{c_s ^2}{\\gamma - 1} - ( \\gamma - 3/2) u^2 \u0026amp; \\gamma u \\end{bmatrix} \\] If we perform an eigen decomposition of \\( \\overline{A} \\) we find that it has eigenvalues\n\\[\\Lambda = \\begin{pmatrix} u \u0026amp; u-c_s \u0026amp; u \u0026#43; c_s \\end{pmatrix}\\] And a complete set of eigenvectors are\n\\[\\overline{X} = \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\\\ u \u0026amp; u - c_s \u0026amp; u \u0026#43; c_s \\\\ \\frac{u^2}{2} \u0026amp; \\frac{u^2}{2} - c_s u \u0026#43; \\frac{c_s ^2}{\\gamma - 1} \u0026amp; \\frac{u^2}{2} \u0026#43; c_s u \u0026#43; \\frac{c_s ^2}{\\gamma - 1} \\end{bmatrix}\\] This set of eigenvectors are linearly independent and the eigenvalues are all real, which means the Euler equations are a hyperbolic system of equations. In the same way that the scalar advection equation gives wave solutions, the Euler equations will give solutions in which information propagates along characteristics (the eigenvalues) with a wave-like behavior. The eigenvalues we got here are the same characteristics you would get via the method of characteristics in linear fluid dynamics:\n\\( u + c_s \\): Shock wave \\( u \\): Contact discontinuity \\( u - c_s \\): Rarefaction (expansion) waves Mathematically, by doing the eigen decomposition we have solved \\( \\overline{A} \\overline{X} = \\overline{X} \\overline{\\Lambda} \\). This solves the complete Euler equations\n\\[\\pdv{Q}{t} \u0026#43; A \\pdv{Q}{x} = 0\\] We can right-multiply by the inverse of \\( \\vec X \\)\n\\[\\vec A \\vec X \\vec X^{-1} = \\vec X \\vec{\\Lambda} \\vec X ^{-1}\\] \\[\\rightarrow A = \\vec X \\vec{\\Lambda} \\vec X ^{-1}\\] \\[\\rightarrow \\pdv{Q}{t} \u0026#43; \\vec X \\vec \\Lambda \\vec X^{-1} = 0\\] Now we make an approximation: Assume that the eigenvector \\( \\vec X \\) is locally constant (in both space in time) such that\n\\[\\pdv{\\vec X}{t} = 0 \\qquad \\pdv{\\vec X}{x} = 0\\] We can re-arrange our \\( \\vec X \\)\u0026rsquo;s again by pre-multiplying by \\( \\vec X ^{-1} \\)\n\\[\\vec X^{-1} \\pdv{Q}{t} \u0026#43; ( \\vec X^{-1} \\vec X) \\vec \\Lambda \\vec X ^{-1} \\pdv{Q}{x} = 0\\] \\[\\rightarrow \\pdv{}{t} (\\vec X ^{-1} Q) \u0026#43; \\vec \\Lambda \\pdv{}{x} ( \\vec X^{-1} \\vec Q) = 0\\] We can define a new variable\n\\[\\vec W \\equiv \\vec X^{-1} \\vec Q\\] So we have\n\\[\\pdv{\\vec W}{t} \u0026#43; \\vec \\Lambda \\pdv{\\vec W}{x} = 0\\] Since \\( \\Lambda \\) is simply a diagonal 3x3 matrix, we have now de-coupled our system into three independent equations\n\\[\\pdv{W_i}{t} \u0026#43; \\lambda_i \\pdv{W_i}{x} = 0 \\qquad i = 1, 2, 3\\] While this is not equivalent to the Euler equations, it is a pretty close approximation and it maintains all of the non-linear properties of the Euler equations. Indeed, because of the values \\( \\lambda_i \\) the system has the same characteristics \\( (u, u+c_s, u-c_s) \\) as the Euler equations. Now we have a set of 3 advection equations. Therefore, a convenient model for the Euler equations is the linear wave equation\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = 0\\] because it will capture the same wave-like behavior of the Euler equations, but most importantly because we have an analytic solution of the linear advection equation which allows us to check both the stability and the accuracy of our solution.\n\\[u ( x - at) = \\text{const.}\\] With periodic boundary conditions, we expect the solution to wrap back around to its initial conditions, allowing us to check for errors we may have introduced in our algorithm.\nAnother model equation we can use for the Euler equations is Burger\u0026rsquo;s equation, which is a nonlinear advection equation\n\\[\\pdv{u}{t} \u0026#43; u \\pdv{u}{x} = 0\\] Other model equations we can use to model the Navier-Stokes equations is an advection-diffusion equation, which is simply an addition of a diffusion term to the linear wave equation\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = b \\pdv{^2 u }{x^2}\\] A viscous version of Burger\u0026rsquo;s equation also works as a good model for Navier-Stokes\n\\[\\pdv{u}{t} \u0026#43; u \\pdv{u}{x} = b \\pdv{^2 u}{x^2}\\] This presents a development path for CFD algorithms. We take an algorithm, apply it to the advection equations and check stability/accuracy, then apply Burger\u0026rsquo;s equation, then add viscous effects, then we may be able to solve the full Euler\u0026rsquo;s equations.\n"},{"id":26,"href":"/r/notes/UWAA557/ch10-3/","title":"Lagrange Multipliers","section":"Physics of Fusion Plasmas","content":" Lagrange Multipliers # When we have a problem of variation where one function is maximized or minimized subject to a constraint imposed by another function\n\\[f(x_1, \\ldots x_n) \\rightarrow \\text{ function }\\] \\[g(x_1, \\ldots, x_n) = 0 \\rightarrow \\text { constraint }\\] Without the constraint we would have the problem\n\\[\\delta f = \\pdv{f}{x_1} \\delta x_1 \u0026#43; \\ldots \\pdv{f}{x_n} \\delta x_n\\] \\[\\pdv{f}{x_i} = 0 \\qquad i = 1 \\ldots n\\] and apply any of our multivariate optimization strategies to solve. This can be hard. Luckily, at any stationary point of the function that also satisfies the constraint, the gradient of the function at that point can be expressed as a linear combination of the gradients of the constraints at that point.\n\\[\\pdv{f}{x_i} - \\lambda \\pdv{g}{x_i} = 0\\] With a constraint \\( g = 0 \\), the change in \\( f \\) becomes a change in the functional of \\( f \\) and \\( g \\).\n\\[f, g \\rightarrow h(f(x_1, \\ldots, x_{n-1}), g(x_1, \\ldots, x_{n-1})) = h(x_1, \\ldots, x_{n-1}) = 0\\] \\[\\pdv{h}{x_i} \\rightarrow i = 1 \\rightarrow n-1\\] At the stationary points we\u0026rsquo;re looking for,\nSimplest example: Find the maximum area of a rectangle with perimeter \\( 4a \\)\n\\[f = A = x_1 x_2\\] \\[g = 0 = 2x_1 \u0026#43; 2x_2 - 4a\\] \\[\\delta f = x_2 \\delta x_1 \u0026#43; x_1 \\delta x_2\\] \\[\\lambda \\delta g = \\lambda 2 \\delta x_1 \u0026#43; \\lambda 2 \\delta x_2\\] \\[x_2 \u0026#43; 2\\lambda = 0 \\qquad x_1 \u0026#43; 2\\lambda = 0\\] \\[x_2 = - 2 \\lambda \\qquad x_1 = - 2 \\lambda\\] \\[\\rightarrow - 4 \\lambda - 4 \\lambda - 4 a = 0 \\rightarrow \\lambda = - \\frac{a}{2}\\] "},{"id":27,"href":"/r/notes/UWAA545/04-pic-example/","title":"PIC - Example Implementation","section":"Computational Methods For Plasmas","content":" \\[\\] Notes and details for an electrostatic PIC implementation.\nDimensions and Normalizations # To make the relevant physics as easy to model and visualize as possible, let\u0026rsquo;s choose units such that time is normalized by the plasma frequency and length scales of interest are of order unity. This will make things a lot easier to plot and interpret in the long run, even though normalizations like \\( m = q /(q / m) \\) seem arcane.\nThe fundamental quantities of interest are:\nVariable Physical Expression Computer Value Number of Particles n \\( N \\) Variable (default 128) System Length L \\( L \\) 4 Grid Cells M \\( M \\) 32 Grid Spacing dx \\( L / M \\) 1/M Plasma Frequency \\( (\\omega_p) \\) wp \\( \\sqrt{\\frac{n q^2}{\\epsilon_0 m}} = \\sqrt{\\frac{N}{\\hat{L}} \\frac{1}{\\epsilon_0}q \\frac{q}{m}} \\) 1 Position x[i] \\( x_{min} + \\frac{j L}{m} \\) (x[i] - x_min)/(x_max - x_min) or j/m. This means it is bounded \\( [0, 1] \\) Electric Constant eps0 \\( \\epsilon_0 \\) 1 Charge to Mass Ratio qm \\( q/m \\) -1 Particle Charge q \\( \\frac{\\omega_p ^2 (L/N) \\epsilon_0}{(q/m)} \\) wp**2 * eps0 / (n * qm) (simplifies to -1/n) Particle Mass m \\( q / (q/m) \\) wp**2 * eps0 / (n * qm**2) (simplifies to 1/n) In SI units, we have:\n\\( x_i \\): Position (meters) of particle \\( i \\) within the range \\( [-\\frac{L}{2}, \\frac{L}{2}] \\), relative to the center of the periodic domain. \\( L \\): Length (meters) of periodic spatial domain \\( x_{min} \\): Equal to \\( - L / 2 \\) \\( x_{max} \\): Equal to \\( L / 2 \\) \\( v_i \\): Velocity (meters/s) of particle \\( i \\). Comparing results to theory # Finding the distance between peaks in the field energy is a good way of measuring the frequency.\nNotes for write-up # When you read a paper, you typically read the title, abstract, introduction, conclusions, and look at the figures and captions. Then, if you want more details on something in particular, you dive into the larger sections. The figures are key to this. They should be clearly labelled and captions, and they should clearly convey a specific idea.\n"},{"id":28,"href":"/r/notes/UWAA558/04-two-fluid-plasma-model/","title":"Two-Fluid Plasma Model","section":"MHD Theory","content":" Two-Fluid Plasma Model (ions-electrons) # Restricting our multi-species fluid model to ions and electrons, what can we say about wave behavior in a magnetized 2-fluid plasma? Let\u0026rsquo;s start with a cold plasma approximation ( \\( p = 0 \\) ) and neglect collisions. The momentum equation reduces to\n\\[m_\\alpha \\left( \\pdv{\\vec v_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad \\vec v_\\alpha \\right) - q_\\alpha (\\vec E \u0026#43; \\vec v_\\alpha \\cross \\vec B) = 0\\] From here on out we can avoid some clutter (and wrist strain) by dropping the \\( \\alpha \\) subscripts and acknowledging that we have sets of equations for ions and electrons. Apply a perturbation to an equilibrium \\( g = g_0 \u0026#43; g_1 \\) \\[m \\left(\\pdv{\\vec v_0}{t} \u0026#43; \\vec v_0 \\cdot \\grad \\vec v_0 \u0026#43; \\pdv{\\vec v_0}{t} \u0026#43; \\vec v_1 \\cdot \\grad \\vec v_0 \u0026#43; \\vec v_0 \\cdot \\grad \\vec v_1 \u0026#43; \\vec v_1 \\cdot \\grad \\vec v_1 \\right) \\\\ \\qquad - q (E_0 \u0026#43; E_1) - q (\\vec v_0 \\cross \\vec B_0 \u0026#43; \\vec v_1 \\cross \\vec B_0 \u0026#43; \\vec v_0 \\cross \\vec B_1 \u0026#43; \\vec v_1 \\cross \\vec B_1) = 0\\] We can drop some terms because equilibrium has to satisfy the original equation. We can balance all of the subscript-0 terms and sum them to get zero.\n\\[m \\left( \\pdv{\\vec v_1}{t} \u0026#43; \\vec v_1 \\cdot \\grad \\vec v_0 \u0026#43; \\vec v_0 \\cdot \\grad \\vec v_1 \u0026#43; \\vec v_1 \\cdot \\grad \\vec v_1 \\right) \u0026#43; q (- E_1 - \\vec v_1 \\cross \\vec B_0 - \\vec v_0 \\cross \\vec B_1 - \\vec v_1 \\cross \\vec B_1)\\] Let\u0026rsquo;s now make the assumption that the perturbation is small, that is \\( g_1 \\ll g_0 \\) . That means that nonlinear products of perturbation terms are negligible (linearization process).\n\\[m \\left( \\pdv{\\vec v_1}{t} \u0026#43; \\vec v_0 \\cdot \\grad \\vec v_1 \u0026#43; \\vec v_1 \\cdot \\grad \\vec v_0 \\right) - q \\vec E_1 - q (\\vec v_1 \\cross \\vec B_0 \u0026#43; \\vec v_0 \\cross \\vec B_1) = 0\\] Now, assume that the equilibrium is a static equilibrium, that is \\( \\vec v_0 = 0 \\) . If we decompose into components that are parallel and perpendicular to the equilibrium magnetic field \\( \\vec B_0 \\) , then\n\\[\\pdv{v_{1, \\parallel}}{t} - \\frac{q}{m} E_{1, \\parallel} = 0\\] \\[\\pdv{\\vec v_{1, \\perp}}{t} - \\frac{q}{m} \\left(\\vec E_{1, \\perp} \u0026#43; B_0 \\vec v_{1, \\perp} \\cross \\vu z \\right) = 0\\] The parallel component \\( E_{1, \\parallel} \\) will lead us to the ordinary wave (O-wave). Consideration of the more general case with perpendicular components will lead to the X-wave.\nThe plasma velocity is related to the fields through the current density (Maxwell equations). Faraday\u0026rsquo;s law gives\n\\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\rightarrow \\curl \\pdv{\\vec B}{t} = - \\curl \\curl \\vec E = - \\grad (\\div \\vec E) \u0026#43; \\nabla ^2 \\vec E \\] Ampere\u0026rsquo;s law gives\n\\[\\epsilon_0 \\pdv{\\vec E}{t} = \\frac{1}{\\mu_0} \\curl \\vec B - \\sum_{\\alpha} q_\\alpha n_\\alpha \\vec v_\\alpha\\] \\[\\epsilon_0 \\pdv{^2 \\vec E }{t ^2} = \\frac{1}{\\mu_0} \\curl \\pdv{\\vec B}{t} \\\\ = \\frac{1}{\\mu_0} \\left[ \\nabla ^2 \\vec E - \\grad (\\div \\vec E) \\right] - \\sum_\\alpha q_\\alpha \\pdv{}{t} (n_\\alpha \\vec v_\\alpha) \\] Since this is a linear system, assume that the perturbed quantities have a wave-like structure. That is, the perturbed quantities \\( g_1 \\) are proportional to \\( e^{i(\\omega t \u0026#43; \\vec k \\cdot \\vec r)} \\) . This lets us transform the spatial and temporal derivatives into factors of \\( \\omega \\) and \\( \\vec k \\) \\[- \\epsilon_0 \\omega ^2 \\vec E_1 = - \\frac{1}{\\mu_0} \\left[k^2 \\vec E_1 - \\vec k (\\vec k \\cdot \\vec E_1) \\right] \u0026#43; i \\omega e n_0 \\vec v_1\\] Let\u0026rsquo;s now consider only high frequency oscillations, assuming that only the electrons respond and the ions remain stationary. There\u0026rsquo;s nothing particularly complicated about including the ion response, this just lets us drop the \\( \\alpha \\) subscripts and focus on a single set of equations.\n\\[- i \\omega \\frac{e n_0}{\\epsilon_0} \\vec v_1 = (\\omega ^2 - c^2 k^2) \\vec E_1 \u0026#43; c^2 \\vec k (\\vec k \\cdot \\vec E_1)\\] Now let\u0026rsquo;s apply the perturbed form to the linearized momentum equation\n\\[\\pdv{v_{1, \\parallel}}{t} - \\frac{q}{m} E_{1, \\parallel} = 0 \\\\ \\rightarrow i \\omega v_{1, \\parallel} = - \\frac{e}{m} E_{1, \\parallel} \\\\ \\rightarrow i \\omega \\frac{e n_0}{\\epsilon_0} v_{1, \\parallel} = \\frac{e^2 n_0}{\\epsilon_0 m} E_{1, \\parallel}\\] Combine the momentum equation and the Maxwell equations to eliminate \\( \\vec E_1 \\) and \\( \\vec v_1 \\) \\[\\frac{e^2 n_0}{\\epsilon_0 m} E_{1, \\parallel} = ( \\omega ^2 - c^2 k^2 ) E_{1, \\parallel} \u0026#43; c^2 k_{\\parallel} \\vec (k \\cdot \\vec E_1)\\] Consider different possibilities for the \\( \\vec k \\) vector. If it is along the magnetic field \\( \\vec k = k_{\\parallel} \\vu{e}_\\parallel \\) (longitudinal wave) then\n\\[\\frac{e^2 n_0}{\\epsilon_0 m} = \\omega ^2 - c^2 k^2 \u0026#43; c^2 k^2 = \\omega_{pe}^2\\] For \\( \\vec k = k_{\\perp} \\vu e_\\perp \\) (transverse wave) then we get the dispersion relation for the O-wave\n\\[\\text{dispersion relation for O-waves:} \\qquad \\omega^2 - c^2 k^2 = \\omega_p ^2\\] The electric field is in the same direction as the magnetic field \\( (\\vec E_1 = \\vec E_{1, \\parallel}) \\) , which means the O-wave is linearly polarized. At large \\( k \\) we just have regular light waves, but as we turn the frequency downwards we see a cut-off at the plasma frequency:\nIt turns out that the dispersion relation for the X-wave has the same cut-off, but also has another branch with a resonance\nThe two-fluid plasma model is highly reduced from the full kinetic model, but it is still too complete to be useful when studying gross plasma behavior. Further reductions of the model are possible by making asymptotic assumptions:\nLow-frequency Asymptotic Assumption # Eliminate high frequency, short wavelength phenomena by using pre-Maxwell field equations. Formally, this is \\( \\epsilon_0 \\rightarrow 0 \\) . The direct consequences of the low-frequency approximation are\n\\[c^2 = \\frac{1}{\\epsilon_0 \\mu_0} \\qquad c \\rightarrow \\infty\\] \\[\\omega_p ^2 = \\frac{n e^2}{\\epsilon_0 m} \\qquad \\omega_p \\rightarrow \\infty\\] \\[\\lambda_D = \\frac{v_T}{\\omega_p} \\rightarrow 0\\] This means that all phenomena will have \\( \\omega \\ll \\omega_p \\) , limiting the frequencies we can resolve to the ion plasma frequency. The characteristic speeds will be limited by the speed of light\n\\[\\frac{\\omega}{k} \\ll c\\] and all characteristic lengths will be much greater than the Debye length\n\\[x_0 \\gg \\lambda_D\\] Looking at Gauss\u0026rsquo; law, \\[\\epsilon_0 \\div \\vec E = \\sum_\\alpha q_\\alpha n_\\alpha \\rightarrow \\sum_\\alpha q_\\alpha n_\\alpha = 0\\] so we now have charge neutrality everywhere in the domain. For H plasma, locally we have \\( n_e = n_i \\) everywhere.\nLooking at Ampere\u0026rsquo;s law,\n\\[\\epsilon_0 \\pdv{\\vec E}{t} = \\frac{1}{\\mu_0} \\curl \\vec B - \\sum_\\alpha q_\\alpha n_\\alpha \\vec v_\\alpha = 0 \\\\ \\rightarrow \\vec j = \\frac{1}{\\mu_0} \\curl \\vec B\\] Things we do not get from this approximation are \\( \\vec E = 0 \\) or \\( \\pdv{\\vec E}{t} = 0 \\) . It does mean that plasma dynamics occur on a sufficiently large spatial scale that charge separation is small, and they occur on a sufficiently long temporal scale that electrons respond quickly.\nTiny electron asymptotic assumption # 2nd approximation: neglect electron inertia in the momentum equation. Formally, we let the electron mass \\( m_e \\rightarrow 0 \\) \\[\\omega_{pe}^2 = \\frac{n e^2}{\\epsilon_0 m_e} \\rightarrow \\infty\\] \\[\\omega_{c, e} = \\frac{e B}{m_e} \\rightarrow \\infty\\] \\[v_{T, e} \\rightarrow \\infty\\] The Larmor radius goes to zero \\[r_{l, e} = \\frac{v_{T, e}}{\\omega_{c, e}} \\rightarrow 0\\] Importantly, as the gyroradius \\( r_{l, e} \\) goes to 0 (because the thermal velocity goes as \\( \\sqrt{m_e} \\) and the cyclotron frequency goes as \\( m_e \\) ), this means that the electrons are tied to the magnetic field. The skin depth is also small. \\[\\delta_e = \\frac{c}{\\omega_{p, e}} \\rightarrow 0\\] So all phenomena that we capture must have \\( \\omega \\ll \\omega_{p, e} \\) , \\( \\omega \\ll \\omega_{c, e} \\) , and \\( x_0 \\gg r_{L, e} \\) , \\( x_0 \\gg \\delta_e \\) .\nThe electron momentum equation becomes\n\\[\\grad P_e \u0026#43; \\div \\vec \\Pi _e \u0026#43; e n_e (\\vec E \u0026#43; \\vec v \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\vec R_{\\alpha \\beta}\\] The momentum equation is now a state equation, not an evolution equation. It now simply relates the dynamical variables to each other at any point in time.\nNow, note that along magnetic field lines electrons can travel long distances at very fast (finite) speeds which can produce low frequency, long wavelength phenomena. Neglecting electron inertia implies that electrons respond instantaneously, meaning we cannot capture these modes. An example of such a phenomena is drift waves.\nThe characteristic speeds \\( c \\) and \\( v_{T, e} \\) have disappeared from the model. Remaining is \\( v_{T, i} \\) . This means that the ion dynamics dictate the plasma evolution.\n"},{"id":29,"href":"/r/notes/UWAA560/05-index-of-refraction-measurements/","title":"Index of Refraction Measurements","section":"Plasma Diagnostics","content":" \\[\\] Index of Refraction Measurements # The refractive index is defined as the speed of light (in vacuum) over the phase velocity \\( n = \\frac{c}{v_{\\phi}} \\), \\( v_\\phi = \\omega / k \\). Recall that in vacuum the phase velocity is just the speed of light (\\( n = 1 \\) in vacuum). In a plasma, we know that the dispersion relation will be substantially different.\nIn most materials, \\( n \u0026gt; 1 \\), but from our crude diagram we can expect plasmas to have \\( n \u0026lt; 1 \\). \\( n \\) is related to the plasma density, so laser interferometry can be a technique to measure \\( n \\).\nConsider the setup of a Michelson interferometer. A laser source is split by a beam splitter. A portion passes through following path \\( L_1 \\), reflects from a mirror, and reflects to a detector. Another portion following path \\( L_2 \\) is reflected to a different mirror, is reflected by a mirror, and passes through to the detector. The co-aligned, recombined beams at the detector is the signal of interest to us. The laser source is convenient because it gives us a coherent light source. If there is a difference in travel time between the two split beams, then there will be a phase difference measured at the detector.\nSpecifically, the phase shift of each leg is\n\\[\\phi_1 = \\int _{L_1} k \\dd l \\quad (\\text{mod}\\, 2 \\pi) \\\\ \\phi_2 = \\int _{L_2} k \\dd l \\quad (\\text{mod}\\, 2 \\pi) \\\\ \\Delta \\phi = \\phi_2 - \\phi_1\\] If we introduce a plasma in the path of path \\( L_1 \\), now the phase difference will be different than what it was previously. We call \\( L_1 \\) the \u0026ldquo;scene\u0026rdquo; beam, as it is the path through the material we want to measure the refractive index of. The split beam \\( L_2 \\) is the \u0026ldquo;reference\u0026rdquo; beam. With no plasma, the interference between scene and reference beams at the detector will have a fixed phase \\( \\phi_0 \\). If plasma is present along the scene beam, the phase will change according to \\( n \\) integrated along the scene beam.\nWith regular layout of the Michelson interferometer, note that a portion of the source beam will pass back through the beam splitter the way it came, so we would effectively be shining the laser back into itself! That\u0026rsquo;s generally not good for the laser. Instead, we can actually use a variant called a Mach-Zedner interferometer by introducing an additional beam splitter:\nThe scene beam and reference beam each reflect off their own mirror at a 45 degree angle, so they recombine into a forward beam splitter. Note that in the Mach-Zedner setup, the scene beam now only passes through the plasma once, as opposed to the Michelson interferometer where it passes through twice. So you get half the \u0026ldquo;signal\u0026rdquo; response from a Mach-Zedner.\nThe co-aligned, recombined beam reaches the detector, which may be a PMT or photo-optic array, but is essentially just a glorified photodiode, so what we get out is just the total incident power. How does that depend on the phase shift \\( \\phi \\)?\n\\[E_{r} E^{i \\omega t} \\quad (\\text{reference}) \\\\ E_{s} E^{i \\omega t \u0026#43; \\phi} \\quad (\\text{scene}) \\\\ E_t ^2 = E_r ^2 \u0026#43; E_s ^2 \u0026#43; 2 E_r E_s \\cos \\phi\\] So the detector will measure a modulated signal oscillating about a baseline value. The temporal resolution of the detector must be good enough to detect the frequency of this modulated signal. Before we get into the measurement technique, let\u0026rsquo;s take a quick digression into the behavior of electromagnetic waves in a plasma:\nElectromagnetic Waves in a Plasma # If we consider an unmagnetized plasma. Analysis with the two-fluid plasma model provides a dispersion relation\n\\[\\omega ^2 = c^2 k^2 \u0026#43; \\omega_{p, e} ^2\\] where \\[\\omega_{p, e} ^2 = \\frac{e^2 n_e }{\\epsilon_0 m_e}\\] This is the O-wave from the two-fluid plasma model notes. For a magnetized plasma, we arrive at the same dispersion relation for a linearly polarized electromagnetic wave with \\( \\vec E _\\nu \\parallel \\vec B _0 \\).\nWaves with \\( \\omega \u0026lt; \\omega_{p, e} \\) do not propagate, so the frequency of the laser must be above the cutoff. Re-writing the dispersion relation as \\( n^2 = 1 - \\frac{\\omega_{p, e}^2}{\\omega ^2} = 1 - \\frac{n_e}{n_c} \\) where \\( n_e \\) is the electron density, and \\( n_c \\) is called the \u0026ldquo;cutoff density\u0026rdquo; and is equal to \\( n_c = \\omega^2 \\frac{\\epsilon_0 m_e}{e^2} \\).\nWe can use the dispersion relation to compute the phase shift\n\\[\\phi = \\int k \\dd l = \\frac{\\omega}{c} \\int n \\dd l\\] A vacuum wave number \\( k_0 = \\omega / c \\) of a laser beam and a plasma wave number \\( k_p \\) give\n\\[\\Delta \\phi = \\int (k_p - k_0) \\dd l = \\int ( n - 1) \\frac{\\omega}{c} \\dd l \\\\ = \\frac{\\omega}{c} \\int \\left[\\left(1 - \\frac{n_e}{n_c} \\right)^{1/2} - 1 \\right] \\dd l\\] If we apply an approximation \\( n_e \\ll n_c \\), then \\( \\left( 1 - \\frac{n_e}{n_c} \\right)^{1/2} \\approx 1 - \\frac{1}{2} \\frac{n_e}{n_c} \\) and\n\\[\\Delta \\phi = \\frac{1}{2} \\frac{\\omega}{c} \\int \\frac{n_e}{n_c} \\dd l\\] where \\( \\omega \\) and \\( n_c \\) are defined entirely by the laser source. Once we have selected a laser source, the phase change is proportional to the line integral of the electron density. If we know the plasma size \\( L \\), we can determine the average density\n\\[\\langle n_e \\rangle = \\frac{\\int n_e \\dd l}{L} = \\frac{2 c}{\\omega L} n_c \\Delta \\phi\\] Density Profile - Multichord Interferometry # Now, say we are interested in obtaining a density profile, not just a line-integrated average density measurement. If we add another beam splitter and direct an additional beam through a different chord in the plasma, we can obtain multiple simultaneous chord measurements. We can use these to determine the density profile. Assuming an axisymmetric \\( n_e(r) \\) and finite extent \\( n_e = 0 \\) for \\( r \u0026gt; a \\), we can define impact parameter \\( y \\) as the closest point of approach of the beam to the axis of symmetry:\n\\[F(y) = \\int n_e \\dd l = \\int _{- \\sqrt{a^2 - y^2}} ^ {\\sqrt{a^2 - y^2}} n_e (x) \\dd x \\\\ = 2 \\int_y ^a n_e (r) \\frac{r \\dd r}{\\sqrt{r^2 - y^2}}\\] So we can measure \\( F(y) \\), but we really want \\( n_e(r) \\). We invert \\( F(y) \\) to give the density profile by making multiple chord measurements by the process of Abel inversion\n\\[n_e (r) = - \\frac{1}{\\pi} \\int _r ^a \\frac{\\dd F}{\\dd y} \\frac{\\dd y}{\\sqrt{y^2 - r^2}}\\] With a sufficient number of chordal measurements, we can arrive at an accurate profile \\( n_e(r) \\), usually. There are some pathological cases where it is difficult to determine the profile, even when it is axisymmetric. The upcoming homework is mostly about this process and these odd cases.\nPhase Ambiguity # Photodetectors, e.g. photodiodes, measure incident optical power, not just the phase shift\n\\[E_t ^2 = E _r ^2 \u0026#43; E_s ^2 \u0026#43; 2 E_r E_s \\cos \\phi\\] At the points \\( \\phi =0, \\pi, 2\\pi, \\ldots \\) there is a phase ambiguity, because we cannot distinguish between \\( \\phi + \\Delta \\phi \\) and \\( \\phi - \\Delta \\phi \\). The incident optical power changes identically whether the phase increases or decreases. Adding a second interferometer with a different phase can resolve the ambiguity, since the points of ambiguity will be separated, but adding an additional interferometer is a lot of trouble. However, we can actually resolve the ambiguity with a single interferometer by modulating the phase (or frequency) of the reference beam. The scene beam of frequency \\( \\omega_1 ^0 \\) will now interfere with the reference beam of frequency \\( \\omega_2 ^0 \\) to create a beat frequency \\( \\Delta \\omega^0 = \\omega_2 ^0 - \\omega _1 ^0 \\).\nEven with no plasma present, the phase will change as \\( \\phi(t) = \\Delta \\omega_0 t \\). The phase will always be propagating in time. A modulated interferometer like this is a Hetrodyne interferometer. With a plasma present, the phase is further altered from the scene beam.\nModulation techniques # Rotating wheel (for interferometers that use wavelengths in the far infrared). Physical wheel rotating at edge speed \\( v \\). Diffraction grating etched onto wheel to ensure reflection. The Doppler effect gives a frequency shift according to: \\[\\frac{\\omega_2}{\\omega_1} = \\frac{1 \u0026#43; v_i / c}{1 - v_r / c}\\] Acousto-optic oscillators (Bragg cells). Used for lasers in the visible range (like HeNe). Composed of a crystal (quartz) attached to a vibrator/transducer, usually a piezoelectric crystal. The transducer sends compressive sound waves through the crystal to an absorber to reduce reflections. An incident laser at incident angle \\( \\omega_1 \\) will pass through at \\( \\omega_1 \\) (the zeroth-order beam), but there will also be a first-order Bragg reflection component which reflects off of the sound waves at frequency \\( \\omega_2 = \\omega_1 \\pm \\omega_{ao} \\) where \\( \\omega_{ao} \\) is the frequency of the acousto-optic oscillator. The detector signal \\( f(t) \\) is combined with the local oscillator \\( \\omega_{ao} \\) to output \\( \\sin (\\Delta \\phi) \\) and \\( \\cos (\\Delta \\phi) \\) in a mixer-splitter quadrature detector. Whenever setting up these interferometers, it is very useful to plot \\( \\sin(\\Delta \\phi) \\) and \\( \\cos (\\Delta \\phi) \\) to produce a circle called a Lissajous figure. A very well-aligned interferometer will have a very circular Lissajous figure with large radius, so it is a useful reference when aligning the interferometer.\nDensity Imaging # As the plasma has an index of refraction change, the scene beam can refract (and change direction) as is passes through the plasma. This can cause the scene beam to become misaligned from the reference beam. This particularly happens when you go through a density gradient. When the wavefronts of the scene and reference beams are no longer co-planar, a new interference pattern appears at the detector which complicates the signal, and can even eliminate any modulation. We can actually take advantage of this problem if we have a sufficiently large beam:\nIntentional misalignment of an expanded beam produces an interference pattern that we can image. The signal measured at the detector now corresponds with the second derivative of the density gradient.\nThe detector will measure an interference pattern of horizontal fringes, spaced depending on the laser wavelength and \\( \\theta \\). The fringe spacing can be small, depending on the wavelength and misalignment. In the before-times, you would generally use holographic glass plates at the detector, but nowadays consumer-grade SLR cameras have sufficient resolution to detect.\nWith plasma present, the fringes are distorted. Encoded in the fringe pattern is the phase difference between the scene and reference beams. The image with plasma will contain deflections in the fringes. The deflection away from original horizontal fringe pattern gives \\( \\Delta \\phi \\).\nFaraday Rotation # The polarization angle of the laser changes as a result of the Faraday effect. The effect can be used to measure the magnetic field.\nTODO: Fill in section on March 4 during lecture on pulsed polarimetry.\n"},{"id":30,"href":"/r/notes/UWAA558/05-mhd-model/","title":"Magnetohydrodynamic (MHD) Model","section":"MHD Theory","content":" Magnetohydrodynamic (MHD) Model # Applying approximations to the two-fluid plasma model will allow us to arrive at a single-fluid (center-of-mass) description. The result is the ideal magnetohydrodynamic model (MHD).\nFirst, define the MHD variables:\n\\[\\text{mass density:} \\qquad \\rho = n_i m_i \u0026#43; n_i m_e\\] \\[\\text{fluid velocity:} \\qquad \\vec v = \\frac{n_i m_i \\vec v_i \u0026#43; n_e m_e \\vec v_e}{n_i m_i \u0026#43; n_e m_e}\\] \\[\\text{current density:} \\qquad \\vec j = q_i n_i \\vec v_i - e n_e \\vec v_e \\rightarrow \\vec v_e = \\frac{q_i n_i \\vec v_i}{e n_e} - \\vec j / e n_e\\] \\[\\text{total pressure:} \\qquad p = p_i \u0026#43; p_e = n_e T_e \u0026#43; n_i T_i\\] \\[\\text{total temperature:} \\qquad T = \\frac{n_i T_i \u0026#43; n_e T_e}{(n_i \u0026#43; n_e)/2}\\] Now let\u0026rsquo;s begin applying asymptotic approximations. For the mass density, applying the first approx (charge neutrality) we have\n\\[\\rho \\approx n (m_i \u0026#43; m_e)\\] Using approx 2 (vanishing electron mass)\n\\[\\rho \\approx n m_i\\] where \\[n = n_i \\qquad n_e = Z n\\] The center-of-mass velocity (charge neutrality) gives\n\\[\\vec v \\approx \\frac{m_i \\vec v_i \u0026#43; m_e Z \\vec v_e}{m_i \u0026#43; Z m_e}\\] with small electron mass approximation:\n\\[\\vec v \\approx \\vec v_i\\] The current density is (charge neutrality approx)\n\\[\\vec j \\approx Z e n (\\vec v_i - \\vec v_e) \\qquad \\vec v_e = \\vec v_i - \\vec j / Z e n\\] The pressure and total temperature are (with charge neutrality)\n\\[P \\approx n (T_i \u0026#43; Z T_e)\\] \\[T \\approx \\frac{T_i \u0026#43; Z T_e}{(1 \u0026#43; Z)/2}\\] MHD Momentum Equation # Now we combine the two-fluid equations with these asymptotic approximations to obtain the governing equations for the MHD variables. Multiplying the ion continuity equation by the ion mass gives\n\\[\\pdv{}{t} (m_i n_i) \u0026#43; \\div (m_i n_i \\vec v_i) = 0 \\\\ \\rightarrow \\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0 \\quad \\text{(MHD continuity eq.)}\\] If we multiply the two-fluid continuity equations by the charge and sum them, we get\n\\[\\pdv{}{t}(q_i n_i - e n_e) \u0026#43; \\div (q_i n_i \\vec v_i - e n_e \\vec v_e) = 0 \\\\ \\rightarrow \\div \\vec j = 0 \\quad \\text{(no accumulation of charge)}\\] If we add the electron and ion momentum equations and apply the small electron mass approximation, and recognizing that \\( \\vec R_{ei} = - \\vec R_{ie} = n e \\eta \\vec j \\) \\[\\rho_i \\left(\\pdv{\\vec i}{t} \u0026#43; \\vec v_i \\cdot \\grad \\vec v_i \\right) \u0026#43; \\grad (P_i \u0026#43; P_e) \u0026#43; \\div (\\vec \\Pi _i \u0026#43; \\vec \\Pi_e) \\\\ \\qquad - (q_i n_i - e n_e) \\vec E - (q_i n_i \\vec v_i - e n_e \\vec v_e) \\cross \\vec B = 0\\] \\[\\text{MHD Momentum Equation} \\\\ \\rightarrow \\rho \\left(\\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad p - \\vec j \\cross \\vec B = - \\div (\\vec \\Pi _i \u0026#43; \\vec \\Pi _e)\\] \u0026ldquo;MHD Momentum Equation\u0026rdquo;\n\\[\\rho \\left(\\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad p - \\vec j \\cross \\vec B = - \\div (\\vec \\Pi _i \u0026#43; \\vec \\Pi _e)\\] If we take the electron momentum equation, apply the small electron mass asymptotic approximation and introduce the current density, then we have\nMHD Ohm\u0026rsquo;s Law # \\[e n_e (\\vec E \u0026#43; \\vec v_i \\cross \\vec B - \\vec j \\cross \\vec B / Z e n) = - \\grad P_e - \\div \\vec \\Pi_e \u0026#43; \\vec R_{ei}\\] Substituting \\( \\vec R_{ei} = e n_e \\eta \\vec j \\) \\[\\text{MHD Generalized Ohms Law}\\\\ \\vec E \u0026#43; \\vec v \\cross \\vec B = \\eta \\vec j \u0026#43; \\frac{1}{Z e n} \\left(\\vec j \\cross \\vec B - \\grad p_e - \\div \\vec \\Pi_e \\right)\\] \u0026ldquo;MHD Generalized Ohms Law\u0026rdquo;\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = \\eta \\vec j \u0026#43; \\frac{1}{Z e n} \\left(\\vec j \\cross \\vec B - \\grad p_e - \\div \\vec \\Pi_e \\right)\\] MHD Energy Equation # The energy equation is found by adding the ion and electron energies, but first we need to manipulate them into a common form. The ion energy equation is\n\\[\\frac{1}{\\gamma - 1} \\left[ \\pdv{}{t} (n_i T_i) \u0026#43; \\div (n_i T_i \\vec v_i) \\right] \u0026#43; n_i T_i \\div \\vec v_i = RHS_i\\] where \\( RHS_i \\equiv - \\vec \\Pi_i \\cdot \\cdot \\grad \\vec v_i - \\div \\vec h_i \u0026#43; Q_{ie} \\) \\[\\frac{1}{\\gamma - 1} \\left( \\pdv{p_i}{t} \u0026#43; \\vec v_i \\cdot \\grad p_i \\right) \u0026#43; \\frac{\\gamma}{\\gamma - 1} p_i (\\div \\vec v_i) = RHS_i\\] Multiply by \\( \\frac{\\gamma - 1}{n_i ^\\gamma} \\) and define the total derivative with respect to the ion velocity as\n\\[\\dv{}{t} \\equiv \\pdv{}{t} \u0026#43; \\vec v_i \\cdot \\grad\\] \\[\\rightarrow \\dv{}{t} \\left( \\frac{p_i}{n_i ^\\gamma} \\right) - p_i \\dv{}{t} \\left( \\frac{1}{n_i ^\\gamma} \\right) \u0026#43; \\frac{\\gamma p_i}{n_i ^\\gamma} \\div \\vec v_i = \\frac{\\gamma - 1}{n_i ^\\gamma} RHS_i\\] Simplify further by recognizing that carrying out the total derivative gives\n\\[\\dv{}{t} \\left( \\frac{p_i}{n_i ^\\gamma} \\right) - \\frac{\\gamma p_i}{n_i ^{\\gamma \u0026#43; 1}} \\dv{}{t} n_i \u0026#43; \\frac{\\gamma p_i}{n_i ^\\gamma} \\div \\vec v_i = \\frac{\\gamma - 1}{n_i ^\\gamma} RHS_i\\] The continuity equation says \\[\\dv{}{t} n_i = - n_i \\div \\vec v_i\\] Multiply by \\( m_i ^{- \\gamma} \\) and use \\( \\rho = n m_i \\) and the resulting ion energy equation is\n\\[\\dv{}{t} \\left( \\frac{p_i}{\\rho ^\\gamma} \\right) = \\frac{\\gamma - 1}{\\rho ^\\gamma} RHS_i\\] We want to get the electron energy equation in the same form. The steps are very similar:\n\\[\\dv{}{t} \\left( \\frac{p_e}{n_e ^\\gamma} \\right) \u0026#43; \\vec v_e \\cdot \\grad \\left( \\frac{p_e}{n_e ^\\gamma} \\right) = \\frac{\\gamma - 1}{n_e ^\\gamma} RHS_e\\] Since we\u0026rsquo;ve defined our center-of-mass reference frame to be that of the ions, we can not use the same total derivative\n\\[\\pdv{}{t} \u0026#43; \\vec v_e \\cdot \\grad \\neq \\dv{}{t}\\] We now apply the charge neutrality approximation\n\\[n_e = Z n \\qquad \\vec v_e = \\vec v_i - \\frac{1}{Z e n} \\vec j\\] \\[\\rightarrow \\dv{}{t} \\left( \\frac{p_e}{Z^\\gamma n^\\gamma} \\right) \u0026#43; \\vec v_i \\cdot \\grad \\left( \\frac{p_e}{Z^\\gamma n^\\gamma} \\right) = \\frac{1}{Z e n} \\vec j \\cdot \\grad \\frac{p_e}{Z^\\gamma n^\\gamma} \u0026#43; \\frac{\\gamma - 1}{Z^\\gamma n^\\gamma} RHS_e\\] Multiply by \\( (Z / m_i)^\\gamma \\) and the result is\n\\[\\dv{}{t} \\frac{p_e}{\\rho ^\\gamma} = \\frac{1}{Z e n} \\vec j \\cdot \\grad \\frac{p_e}{\\rho^\\gamma} \u0026#43; \\frac{\\gamma - 1}{\\rho^\\gamma} RHS_e\\] Finally we can add the ion energy equation to the electron energy equation to get\n\\[\\dv{}{t} \\left( \\frac{p}{\\rho^\\gamma} \\right) = \\\\ \\frac{\\gamma - 1}{\\rho^\\gamma} \\left[ Q_{ie} \u0026#43; Q_{ei} - \\div (\\vec h_i \u0026#43; \\vec h_e) - \\vec \\pi_i \\cdot \\cdot \\grad \\vec v_i - \\vec \\Pi_e \\cdot \\cdot \\grad \\vec v_e \\right] \u0026#43; \\frac{\\vec j}{Z e n} \\cdot \\grad \\frac{p_e}{\\rho^\\gamma}\\] \u0026ldquo;MHD Energy Equation\u0026rdquo;\n\\[\\dv{}{t} \\left( \\frac{p}{\\rho^\\gamma} \\right) = \\frac{\\gamma - 1}{\\rho^\\gamma} \\left[ Q_{ie} \u0026#43; Q_{ei} - \\div (\\vec h_i \u0026#43; \\vec h_e) - \\vec \\pi_i \\cdot \\cdot \\grad \\vec v_i - \\vec \\Pi_e \\cdot \\cdot \\grad \\vec v_e \\right] \u0026#43; \\frac{\\vec j}{Z e n} \\cdot \\grad \\frac{p_e}{\\rho^\\gamma}\\] Obviously, we\u0026rsquo;ve retained a number of terms that are specific to the behavior of the electrons. It is possible to incorporate the electron behavior by using a single-fluid MHD model with two temperatures \\( T_i \\neq T_e \\) . One can imagine a hierarchy of models, in which the most simplified is the single-fluid MHD model in which you evolve \\( \\rho \\) , \\( \\vec v \\) , and \\( T \\) . Moving up a level, you have a MHD model with two temperatures in which you evolve \\( \\rho \\) , \\( \\vec v \\) , \\( T_i \\) , and \\( T_e \\) . Upwards from there you move back into the realm of multi-fluid models.\nNow to relate the fields back to source terms. The low-frequency Maxwell\u0026rsquo;s equations are\n\\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\vec j = \\frac{1}{\\mu_0} \\curl \\vec B\\] \\[\\div \\vec B = \\div \\vec E = 0\\] Like in the multi-fluid plasma model, we still need to close the system by expressing some of our variables using equations of state ( \\( \\vec h \\) , \\( \\vec \\Pi \\) ).\nTo simplify further, we can make some assumptions about heat flow\n\\[\\text{isothermal} \\qquad \\rightarrow p \\propto n \\qquad T = \\text{const.} \\qquad \\gamma = 1\\] \\[\\text{adiabatic} \\qquad \\rightarrow p \\propto n^\\gamma\\] \\[\\text{cols plasma / force-free} \\qquad \\rightarrow p = \\text{const.} \\qquad \\gamma = 0\\] Ideal MHD Model # The extended MHD equations are simpler than the two-fluid model, but they can still be quite complicated. We can often still get useful analysis from further reductions. The ideal MHD model is such a reduction that we can get by dropping (with justification) several terms from the extended model. We justify the simplifications by comparing the magnitude of the neglected terms to the terms that are retained.\nRecall the characteristic speed is \\( v_{T, i} \\) . If we say that the characteristic length plasma length is \\( L \\) , then we can define characteristic time \\( \\tau = L / v_{T, i} \\) .\nThe derivation of the two-fluid plasma model assumed a Maxwellian velocity distribution. We need the velocity distribution to thermalize, reach local thermodynamic equilibrium, and become Maxwellian. This means that we need many collisions, in fact so many collisions occurring frequently enough that we can ignore collisional effects. There must then be many collisions during the characteristic time \\( \\tau \\) .\nFor ions to be thermalized,\n\\[\\frac{\\tau_{ii}}{\\tau} \\ll 1\\] And similarly for electrons\n\\[\\frac{\\tau_{e}}{\\tau} \\ll 1\\] The continuity equation remains unchanged from the extended MHD model\n\u0026ldquo;Ideal MHD Continuity Equation\u0026rdquo;\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0\\] The momentum equation is\n\\[\\rho \\left( \\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad p - \\vec j \\cross \\vec B = - \\div (\\vec \\Pi_e \u0026#43; \\vec \\Pi_e)\\] Drop the anisotropic pressure\n\u0026ldquo;Ideal MHD Momentum Equation\u0026rdquo;\n\\[\\rho \\left( \\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad p - \\vec j \\cross \\vec B = 0\\] The generalized Ohm\u0026rsquo;s law is\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = \\eta \\vec j \u0026#43; \\frac{1}{Z e n} \\left( \\vec j \\cross B - \\grad p_e - \\div \\vec \\Pi_e \\right)\\] We\u0026rsquo;re going to drop the entire right hand side\n\u0026ldquo;Ideal MHD Generalized Ohm\u0026rsquo;s Law\u0026rdquo;\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = 0\\] For the energy equation we have\n\\[\\dv{}{t} \\left( \\frac{p}{\\rho^\\gamma} \\right) = \\frac{\\gamma - 1}{\\rho^\\gamma} [Q_{ie} \u0026#43; Q_{ei} - \\div (\\vec h_i \u0026#43; \\vec h_e) - \\vec \\Pi_i \\cdot \\cdot \\grad \\vec v_i - \\vec \\Pi_e \\cdot \\cdot \\grad \\vec v_e] \u0026#43; \\frac{\\vec j}{Z e n} \\cdot \\grad \\frac{p_e}{\\rho^\\gamma}\\] We neglect the entire right-hand side\n\u0026ldquo;Ideal MHD Energy Equation\u0026rdquo;\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (p \\vec v) = (1 - \\gamma) p \\div \\vec v \\] Collision/Pressure terms # If we assume that the ions and electrons are in thermal equilibrium \\( T_i = T_e \\) , we can relate the collision times\n\\[\\tau_{ee} : \\tau_{ii} : \\tau_{ei} = 1 : \\left( \\frac{m_i}{m_e} \\right) ^{1/2} : \\frac{m_i}{m_e}\\] The collision times are specifically collisional relaxation times of the Boltzmann equation\n\\[\\left. \\pdv{f}{t} \\right|_{coll} = \\frac{f - f_{\\text{Maxwellian}}}{\\tau_{\\alpha \\beta}}\\] For electrons, the thermalization condition is much stricter for the ions\n\\[\\frac{\\tau_{ee}}{\\tau} = \\left( \\frac{m_e}{m_i} \\right)^{1/2} \\frac{\\tau_{ii}}{\\tau} \\ll 1\\] Neglect the anisotropic pressure tensor in the momentum and generalized Ohm\u0026rsquo;s law, \\( \\div \\vec \\Pi \\) . \\( \\vec \\Pi \\) is primarily the shear stress tensor. The ion thermal speed gives us a characteristic velocity for the plasma, so we use it to characterize the shear stress\n\\[\\vec \\Pi_{i, max} \\sim 2 \\mu \\left( \\pdv{u}{x} - \\frac{1}{3} \\div \\vec v \\right) \\sim \\mu \\frac{v_{T, i}}{L}\\] Standard treatments of the viscosity (Braginskii, etc.) show that viscosity scales with the number density, temperature, and collision time\n\\[\\mu \\sim n T_i \\tau_{ii} \\sim p_i \\tau_{ii}\\] \\[\\Pi_{i, max} \\sim p_i \\frac{\\tau_{ii} v_{T, i}}{L}\\] The specific term we want to get rid of is \\( \\div \\vec \\Pi \\) , so let\u0026rsquo;s compare it to a term we want to keep \\( \\grad P \\) \\[\\frac{\\div \\vec \\Pi}{\\grad p} \\sim \\frac{p_i \\tau_{ii} v_{T, i} / L^2}{p_i / L} \\sim \\frac{\\tau_{ii}}{\\tau}\\] So, to neglect the anisotropic pressure term in the momentum equation, once again we require\n\\[\\frac{\\tau_{ii}}{\\tau} \\ll 1\\] In other words, as long as the plasma is collision-dominated, we can drop the ion anisotropic pressure term. What about associated the electron term? If you can assume \\( T_i \\approx T_e \\) . Then \\( p_i \\approx p_e \\) for a neutral plasma, and\n\\[\\frac{\\div \\vec \\Pi_e}{\\grad p} \\sim \\frac{p_i \\tau_{ee} v_{T, i} / L^2}{p_i / L} \\sim \\frac{\\tau_{ee}}{\\tau} \\ll 1\\] Magnetic terms # In the generalized Ohm\u0026rsquo;s law, the diamagnetic drift term is\n\\[\\frac{\\grad p_e}{Z e n} \\sim \\frac{n T_e / L}{Z en} \\sim \\frac{T_i / L}{Ze} \\sim \\frac{m_i v_{T, i}^2}{L Z e}\\] Compare \\( \\grad p_e / Z e n \\) to a term that we\u0026rsquo;re going to keep, which is the dynamo term \\( \\vec v \\cross \\vec B \\) \\[\\frac{ \\grad p_e / Z en}{|\\vec v \\cross \\vec B|} \\sim \\frac{m_i v_{T, i}^2}{LZe}{v_{T, i} B} \\sim \\frac{m_i}{ZeB}\\frac{v_{T, i}}{L} = \\frac{v_{T, i}}{\\omega_{c, i}} \\frac{1}{L} \\sim \\frac{r_{L, i}}{L} \\ll 1\\] So to neglect the diamagnetic drift term, we need the plasma to be well-magnetized. This means the Larmor radius must be much less than the plasma characteristic length \\( r_{L, i} \\ll L \\) Now what can we do with the Hall term \\( \\frac{\\vec j \\cross \\vec B}{Zen} \\) . For a static plasma (or one with subsonic flows):\n\\[\\rho (\\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v) - \\grad p - \\vec j \\cross \\vec B = 0\\] so by \u0026ldquo;subsonic\u0026rdquo; we mean that the static pressure is much larger than the dynamic pressure and we can discard the \\( \\vec v \\) terms.\n\\[\\vec j \\cross \\vec B \\approx \\grad p\\] Comparing the Hall term to the dynamo term \\( \\vec v \\cross \\vec B \\) also gives the same requirement\n\\[\\frac{r_{L, i}}{L} \\ll 1\\] How well do some real plasmas hold up to the requirements of ideal MHD? Consider field nulls in a Z-pinch or an FRC, and weakly magnetized plasmas (such as those in a Hall thruster). Clearly, the magnetization requirement does not hold up at all points in space, and ideal MHD does not necessarily apply across the whole domain.\nResistivity # We neglect resistivity and the resistive electric field \\( \\eta \\vec j \\) in the generalized Ohm\u0026rsquo;s law\n\\[\\eta \\vec j \\sim \\eta \\frac{\\grad p}{B} \\sim \\frac{ \\eta n T}{LB}\\] \\[\\eta = \\frac{m_e \\nu_{ei}}{n e^2} = \\frac{m_e}{n e^2 \\tau_{ei}} = \\frac{m_e ^2/ m_i}{n e^2 \\tau_{ee}}\\] \\[\\rightarrow \\eta \\vec j \\sim \\frac{m_e ^2}{e^2 L B} \\frac{v_{T, i} ^2}{\\tau_{ee}}\\] How does it compare to \\( \\vec v \\cross \\vec B \\) \\[\\frac{\\eta \\vec j}{|\\vec v \\cross \\vec B|} \\sim \\left(\\frac{m_e}{m_i} \\right)^2 \\frac{m_i}{e ^2 B^2} \\frac{v_{T, i} ^2}{L^2} \\frac{L}{v_{T, i} \\tau_{ee}} \\sim \\left( \\frac{m_e}{m_i}\\right)^2 \\frac{v_{T, i} ^2}{\\omega_{c, i} ^2} \\frac{1}{L^2} \\sim \\left( \\frac{m_e}{m_i} \\right) ^{3/2} \\left( \\frac{r_{L, i}}{L} \\right) ^{2} \\frac{\\tau}{\\tau_{ii}} \\ll 1\\] This now places a lower limit on \\( \\tau_{ii} \\) ; to neglect resistivity \\( \\tau_{ii} \\) cannot be too low\n\\[\\left( \\frac{m_e}{m_i} \\right) ^{3/2} \\left( \\frac{r_{L, i}}{L} \\right) ^{2} \\ll \\frac{\\tau_{ii}}{\\tau} \\ll 1\\] Heating sources # Neglect the collisional heating sources \\( Q_{ei} \\) , \\( Q_{ie} \\) in the energy equation. We do that by assuming that they are equal and opposite, which only happens when the temperatures are equal. In other words, we are again assuming local thermodynamic equilibrium between electrons and ions.\n\\[1 \\gg \\frac{\\tau_{ei}}{\\tau} = \\left( \\frac{m_i}{m_e} \\right)^{1/2} \\frac{\\tau_{ii}}{\\tau} \\rightarrow \\frac{\\tau_{ii}}{\\tau} \\ll \\left( \\frac{m_e}{m_i} \\right) ^{1/2}\\] This is a much more restrictive condition than ion collisionality. Alternatively, we could track two temperature independently.\nWe also neglect the heat flux terms \\( \\div (\\vec h_i \u0026#43; \\vec h_e) \\) in the energy equation. Consider parallel (to the magnetic field) heat conduction which dominates since \\( \\kappa_\\perp \\ll \\kappa_\\parallel \\) , so\n\\[\\div \\vec h \\approx \\grad_\\parallel \\cdot (\\kappa_\\parallel \\grad_\\parallel T)\\] \\[\\grad_\\parallel \\cdot \\left[ ( \\kappa_{\\parallel, i} \u0026#43; \\kappa_{\\parallel, e}) \\grad_\\parallel T \\right]\\] \\[\\kappa_{\\parallel, e} \\sim \\frac{n T_e}{m_e} \\tau_{ee} \\qquad \\text{ and } \\qquad \\kappa_{\\parallel, i} \\sim \\frac{n T_i}{m_i} \\tau_{ii}\\] \\[\\frac{\\kappa_{\\parallel, e}}{\\kappa_{\\parallel, i}} \\sim \\frac{\\tau_{ee}}{\\tau_{ii}} \\frac{m_i}{m_e} \\sim \\left( \\frac{m_i}{m_e} \\right) ^{1/2}\\] Compare the thermal conductivity to the rate of pressure change \\( \\pdv{p}{t} \\) \\[\\frac{\\grad_\\parallel \\cdot (\\kappa_{\\parallel, e} \\grad_\\parallel T)}{\\pdv{p}{t}} \\sim \\frac{n T^2 \\tau_{ee}/m_e L^2}{nT/\\tau} \\sim \\left( \\frac{m_i}{m_e} \\right)^{1/2} \\frac{\\tau_{ii}}{\\tau} \\ll 1\\] We\u0026rsquo;re back to the same requirement that we have ion-electron local thermodynamic equilibrium.\nAnisotropic pressure terms in energy equation # Neglect the anisotropic pressure terms in the energy equation. That is, \\( \\vec \\Pi_i \\cdot \\cdot \\grad \\vec v_i \\) and \\( \\vec \\Pi_e \\cdot \\cdot \\grad \\vec v_e \\) . Skipping ahead, the result is\n\\[\\frac{\\tau_{ii}}{\\tau} \\ll 1 \\] \\[\\left( \\frac{m_e}{m_i} \\right)^{1/2} \\frac{\\tau_{ii}}{\\tau} \\frac{r_{L, i}}{L} \\ll 1\\] Electron convection term # Lastly, we neglect the electron convection term in the energy equation \\( \\frac{\\vec j}{Zen} \\cdot \\grad \\frac{P_e}{\\rho^\\gamma} \\) . The result is\n\\[\\frac{r_{L, i}}{L} \\ll 1\\] Conservation Law Form of MHD # \\[\\pdv{}{t} q \u0026#43; \\div \\vec f = 0\\] We can express momentum in conservation law form:\n\\[\\pdv{}{t} (\\rho \\vec v) \u0026#43; \\div \\left[ \\rho \\vec v \\vec v - \\frac{ \\vec B \\vec B}{\\mu_0} \u0026#43; (p \u0026#43; \\frac{B^2}{2 \\mu_0})\\vec 1 \\right] = 0\\] The conservation law form for the magnetic field looks like\n\\[\\pdv{ \\vec B}{t} \u0026#43; \\div \\left[ \\vec v \\vec B - \\vec B \\vec v \\right ] = 0\\] And of course the energy equation for\n\\[E = \\frac{1}{\\gamma - 1} p \u0026#43; \\frac{1}{2} \\rho v^2 \u0026#43; \\frac{ B^2}{2 \\mu_0}\\] \\[\\pdv{E}{t} \u0026#43; \\div \\left[ \\left( E \u0026#43; p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) - \\left( \\frac{ \\vec B \\cdot \\vec v}{\\mu_0} \\right) \\vec B \\right] = 0\\] Conservation law forms are particularly useful when considering equilibrium steady-state force balance. This means that in steady-state equilibrium we have\n\\[\\div \\left[ \\rho \\vec v \\vec v - \\frac{ \\vec B \\vec B}{\\mu_0} \u0026#43; \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\vec 1 \\right] = 0\\] We can use this relationship and integrate over various volumes to determine the relationship between the various force balance terms\nExamples of equilibrium plasma confinement # For a plasma in static equilibrium \\( \\vec v = 0 \\rightarrow \\vec j \\cross \\vec B = \\grad p \\) \\[ \\div \\vec T = 0 = \\div \\left( \\right) \\] \\[\\vec j \\cross \\vec B = \\frac{1}{\\mu_0} (\\curl \\vec B) \\cross \\vec B = \\frac{1}{\\mu_0} (\\vec B \\cdot \\grad \\vec B - \\frac{1}{2} \\grad B^2) \\\\ = \\frac{1}{\\mu_0} ( B^2 \\vu e_B \\cdot \\grad \\vu e_B \u0026#43; \\frac{1}{2} \\vu e_B \\vu e_B \\cdot \\grad B^2 - \\frac{\\grad B^2}{2} )\\] where \\( \\vu e_B = \\vec B / B \\) \\( \\vu e_B \\cdot \\grad \\vu e_B \\) is the curvature of \\( \\vec B \\) . Write it like a curvature\n\\[\\vec K = - \\vu r / R_c\\] \\[\\vu e_B \\vu e_B \\cdot \\grad B^2\\] is gradient of \\( B^2 \\) that is parallel to B. Multiply by \\( e_B \\) gives the component of gradient along \\( e_B \\) . The difference between that and \\( \\grad B^2 / 2 \\) gives you the perpendicular gradient\n\\[\\vec j \\cross \\vec B = \\frac{1}{\\mu_0} (B^2 \\vec \\kappa - \\frac{1}{2} \\grad _\\perp B^2) = \\grad p = \\grad_\\perp p\\] identify \\( B^2 \\vec \\kappa \\) is magnetic tension resulting from having a bent magnetic field line. \\( \\frac{1}{2} \\grad_\\perp B^2 \\) is magnetic pressure. They have to balance the plasma pressure at equilibrium.\nFor example, consider a cylindrical plasma that\u0026rsquo;s in equilibrium with a helical magnetic field \\[\\vec B = B_\\theta (r) \\vu \\theta \u0026#43; B_z (r) \\vu z\\] How is plasma pressure profile determined by the different components of the magnetic field? If we want to maximize the amount of pressure we confine, what should be maximized/minimized?\n\\[\\frac{B^2}{\\mu_0} \\vu \\kappa = \\grad _\\perp ( p \u0026#43; \\frac{B^2}{2 \\mu_0} ) \\] \\( B_z \\) is straight and has no curvature, so the only magnetic tension comes from \\( B_\\theta \\) , so the magnetic tension from \\( B_\\theta \\) must balance the total pressure.\nThe role of \\( B_z \\) is displacing plasma pressure. The utility in defining \\( \\beta \\) as\n\\[\\beta = \\frac{\\text{plasma pressure}}{\\text{magnetic pressure}}\\] Conditions of Ideal MHD Validity # The conditions for ideal MHD to be valid are\nHigh Ion Collisionality: \\( \\frac{\\tau_{ii}}{\\tau} \\ll 1 \\) Small ion Larmor radius: \\( \\frac{r_{L, i}}{L} \\ll 1 \\) Low resistivity: \\( \\left(\\frac{m_e}{m_i} \\right)^{3/2} \\left( \\frac{r_{L, i}}{L} \\right)^2 \\ll 1 \\) For a given plasma in force balance, we can relate the plasma pressure to the magnetic pressure\n\\[\\beta = \\frac{n T}{B^2 / 2 \\mu_0} = 4 \\times 10^{-16} \\frac{n_{cm^{-3}} T_{keV}}{B^2 _{T}}\\] Ion collision time is (Spitzer collisionality)\n\\[\\tau_{ii} = 2.09 \\times 10^{7} \\frac{T_{eV} ^{3/2} \\mu ^{1/2}}{\\ln \\Lambda n_{cm ^{-3}}} \\left[\\text{s}\\right]\\] where \\( \\mu \\equiv m_i / m_p \\) .\nPutting the conditions for ideal MHD in terms of \\( \\beta \\) and \\( \\tau_{ii} \\) ,\nHigh collisionality: \\[\\frac{\\tau_{ii}}{\\tau} = 2.14 \\times 10^{12} \\frac{T^2}{n L} \\ll 1 \\] \\[\\rightarrow T_{eV} \\ll 6.8 \\times 10^{-7} L_{cm} ^{1/2} n_{cm^{-3}} ^{1/2}\\] Small gyroradius: \\[\\frac{r_{L, i}}{L} = 2.3 \\times 10^7 \\frac{1}{Z L} \\sqrt{ \\frac{\\mu B}{n}} \\ll 1\\] \\[\\rightarrow n_{cm^{-3}} \\gg 5.3 \\times 10^{14} \\frac{\\mu B_{T}}{Z^2 L_{cm} ^2}\\] Low resistivity: \\[\\left( \\frac{m_e}{m_i} \\right) ^{3/2} \\left( \\frac{r_{L, i}}{L} \\right) ^2 \\frac{\\tau}{\\tau_{ii}} = 5.65 \\frac{\\mu B}{Z^2 L T^2}\\] \\[\\rightarrow T_{eV} \\gg 2.4 \\frac{\\mu ^{3/2} \\beta^{1/2}}{Z L _{cm} ^{1.2}}\\] Perpendicular MHD # For most configurations for magnetic fusion confinement, we are able to satisfy 2. and 3. but often have densities much too low to meet the high collisionality constraint. However, in practice ideal MHD does accurately model macroscopic behavior of many plasmas. At the same time, magnetized / fusion plasmas are often largely collisionless. We can understand why by re-writing the collisionality requirement as\n\\[1 \\gg \\frac{\\tau_{ii}}{\\tau} = \\frac{\\tau_{ii} v_{T, i}}{\\tau v_{T, i}} \\sim \\frac{\\lambda}{L}\\] where \\( \\lambda \\) is the mean free path. The ratio \\( \\lambda / L \\) is also known as the Knudsen number. In magnetized plasmas, the mean free path is often very long, but the path between collisions can cover a great distance only by following magnetic field lines. Motion \\( \\perp \\) to the magnetic field is constrained by \\( r_{L, i} \\) . This suggests an approach wherein we divide the plasma model into a 1-D kinetic model to be solved along the magnetic field and a 2-D MHD model \\( \\perp \\) to the magnetic field.\nWe consider diffusivity (terms like viscosity, conductivity) parallel and perpendicular to \\( \\vec B \\) \\[k_\\parallel \\sim \\frac{\\lambda ^2}{\\tau_{ii}}\\] \\[k_\\perp \\sim \\frac{r_{L, i} ^2}{\\tau_{ii}}\\] \\[\\frac{k_\\parallel}{k_\\perp} \\sim \\left( \\frac{\\lambda}{r_{L, i}} \\right)^2 \\sim (\\omega_{c, i} \\tau_{ii} )^2\\] This changes one of our conditions for validity. Specifically, we can write\n\\[\\frac{r_{L, i}}{L} \\frac{1}{ \\omega_{c, i} \\tau_{i i}} = 254 \\frac{\\mu \\beta}{Z^2 L_{cm} T_{eV} ^2}\\] or\n\\[T_{eV} \\gg 16 \\frac{\\mu ^{1/2} \\beta^{1/2}}{Z L^{1/2}}\\] This is now only slightly more restrictive than the low-resistivity condition. In fact, most of the plasmas that we looked at before (large tokamaks \u0026amp; toruses, propulsion systems) which had temperatures above the validity range now fall comfortably within the region of validity. The perpendicular MHD model is equivalent to a collisionless model, giving a much wider applicability than the collisional MHD model.\nWhat about the parallel component? Our collisionality condition still isn\u0026rsquo;t valid parallel to the field. Basically, we ignore the parallel component, which is the same as assuming that \\( \\rho \\) , \\( T \\) , \\( p \\) are constant along magnetic field lines, with \\( \\div \\vec v = 0 \\) . Let\u0026rsquo;s write out the expressions for collision-less MHD and for ideal MHD and compare:\nCollisionless MHD Ideal MHD Continuity \\( \\pdv{\\rho}{t} = 0 \\) \\( \\pdv{\\rho}{t} = - \\rho \\div \\vec v \\) Momentum \\( \\rho \\dv{\\vec v_\\perp}{t} = \\vec j \\cross \\vec B - \\grad_\\perp p \\) \\( \\rho \\dv{\\vec v}{t} = \\vec j \\cross \\vec B - \\grad p \\) Parallel constraint \\( \\vec B \\cdot \\grad \\frac{v_\\parallel}{B} = - \\div \\vec v _\\perp \\) Energy \\( \\dv{p}{t} = 0 \\) \\( \\dv{p}{t} = - \\gamma p \\div \\vec v \\) Collisionless MHD reproduces many of the effects of ideal MHD but has a wider region of validity. Corollary: ideal MHD is accurate beyond its region of validity, unless results lead to parallel gradients. For example, we know that MHD is not valid when representing confinement of a plasma confined in a magnetic mirror, which is an inherently kinetic phenomenon. But ideal MHD can generate parallel gradients within its region of validity, and we need to be careful. Ideal MHD does not require different models \\( \\parallel \\) and \\( \\perp \\) to the magnetic field, and is therefore preferred. We will continue to use ideal MHD outside of its region of validity.\n"},{"id":31,"href":"/r/notes/UWAA545/05-electrodynamic-pic/","title":"Multidimensional Electrodynamic PIC","section":"Computational Methods For Plasmas","content":" \\[\\] Multidimensional Electrodynamic PIC # Now that we\u0026rsquo;ve touched on each of the components of a 1-dimensional electrodynamic PIC method, let\u0026rsquo;s try to move our model into a full multidimensional one. The main difficulty in moving from 1D to 2D/3D is the increased variety of waves present in the solution.\nField Equations # The EM fields must be solved for the full Maxwell\u0026rsquo;s equations, not just Poisson\u0026rsquo;s equation, in order to capture the full dynamics.\n\\[\\pdv{\\vec E}{t} = c \\curl \\vec B - \\vec j\\] \\[\\pdv{B}{t} = - c \\curl \\vec E\\] Again, we\u0026rsquo;ll use our leapfrog algorithm to advance in time, with \\( \\vec j \\) determined by the particle velocities, \\( \\vec v_i \\) at \\( n + 1/2 \\).\n\\[\\frac{\\vec E ^{n\u0026#43;1} - \\vec E^n}{\\Delta t} = c \\curl \\vec B ^{n \u0026#43; 1/2} - \\vec j ^{n\u0026#43;1/2}\\] We need to know the magnetic field at a different time step than the particle positions / electric field, so we\u0026rsquo;ll have to keep that in mind.\n\\[\\frac{\\vec B^{n \u0026#43; 1/2} - \\vec B^{n - 1/2}}{\\Delta t} = - c \\curl \\vec E^n\\] When we compute spatial derivatives like \\( \\curl \\vec A \\), they must also be centered. In 2D the curl is\n\\[\\text{2D} \\quad \\curl \\vec A = \\vu x \\pdv{A_z}{y} \u0026#43; \\vu y \\left( - \\pdv{A_z}{x} \\right) \u0026#43; \\vu z \\left( \\pdv{A_y}{x} - \\pdv{A_x}{y} \\right)\\] In a naive implementation, we could simply calculate these spatial derivatives using finite differences placed at the cell vertices. This would give us estimates of the derivatives at the midpoints between the cells. But we need those derivatives at the cell vertices to determine \\( \\vec E \\) and \\( \\vec B \\), so we would have to perform some averaging. This means that splitting the curl operator to different locations reduces overall accuracy.\nA much better approach is to not collocate the field components. Instead we use a \u0026ldquo;staggered grid\u0026rdquo; called a Yee grid.\nStore \\( E_z \\) at the grid points Store \\( B_z \\) at the center of the grid cell Store \\( E_x \\) and \\( B_y \\) along the horizontal Store \\( E_y \\) and \\( B_x \\) along the vertical cell edges Differences on this staggered grid provide 2nd order accuracy for spatial derivatives, in a very similar way to the 2nd order accuracy we get from our leapfrog integration.\nLet\u0026rsquo;s start with a differencing of Faraday\u0026rsquo;s law. We\u0026rsquo;re taking the difference \\( E_x \\) in the \\( y \\) direction, and the difference \\( E_y \\) in the \\( x \\) direction\n\\[{B_z ^{n \u0026#43; 1/2}}_{j,k} = {B_z ^{n - 1/2}}_{j,k} - \\Delta t c \\left(\\frac{{E_y ^n}_{j\u0026#43;1, k} - {E_y ^n}_{j, k}}{\\Delta x} - \\frac{{E_x ^n}_{j, k\u0026#43;1} - {E_x ^n}_{j, k}}{\\Delta y} \\right)\\] Now let\u0026rsquo;s try Ampere\u0026rsquo;s law:\n\\[{E_x ^{n\u0026#43;1}}_{j, k} = {E_x ^n}_{j, k} \u0026#43; \\Delta t c \\left(\\frac{{B_z ^{n\u0026#43;1/2}} - {B_z ^{n \u0026#43; 1/2}}_{j, k-1} }{\\Delta y}\\right) - \\Delta t {j_x ^{n \u0026#43; 1/2}}_{j, k}\\] Here, we need the \\( \\vec j \\) components to be collocated with the electric field \\( \\vec E \\).\nWe only get second order accuracy from the Yee grid if \\( \\Delta x \\) and \\( \\Delta y \\) are constant, that is, if we have a uniform grid. Computing the spatial derivatives with mismatched cell sizes does not give second order accuracy. As we\u0026rsquo;ve written this, we don\u0026rsquo;t necessarily require \\( \\Delta x = \\Delta y \\), but we will require \\( \\Delta x = \\Delta y \\) later as we move particles through the grid.\nThe requirement \\( \\Delta x = \\Delta y = const. \\) means curved boundaries are difficult. Curved sections of boundaries must be stair-stepped:\nResolving geometric details and features requires decreasing \\( \\Delta x, \\Delta y, \\Delta z \\) globally. This is still an outstanding issue for PIC models. There has been some success with \u0026ldquo;cut cell\u0026rdquo; methods, in which the cells at the boundary are cut into triangles and trapezoids to match the curve of the boundary (with straight lines). This does solve the problem of resolving geometric detail at the boundary, but it means that those cells at the edge need to be treated differently from all of the other cells in the model. It\u0026rsquo;s also not entirely clear that cut cell methods preserve the second-order accuracy of Yee grids.\nParticle Push # For the particle advance, we want to advance velocities \\( v ^{n - 1/2} \\) to \\( v^{n + 1/2} \\), so the force is needed at time \\( n \\). This is fine for the electric field, which is known at \\( n \\). However, \\( \\vec B \\) is known at \\( n + 1/2 \\). Again, we use Strang splitting to split the magnetic field advance into two stages. The initial advance gives us \\( \\vec B^n \\):\n\\[\\vec B^{n} = \\vec B^{n - 1/2} - \\frac{c \\Delta t}{2} \\curl \\vec E^n\\] After the particle push, advance \\( \\vec B \\) again\n\\[\\vec B^{n \u0026#43; 1/2} = \\vec B^n - \\frac{c \\Delta t}{2} \\curl \\vec E^n\\] This is a pretty trivial Strang splitting, just equivalent to an average \\( \\vec B^n = \\frac{B^{n + 1/2} + B^{n - 1/2}}{2} \\). That\u0026rsquo;s because Strang splitting is only really important when the quantity used to advance changes between the split advances.\nCurrent Weighting # Let\u0026rsquo;s talk about how to perform current weighting. We need to weight the particles\u0026rsquo; trajectories at the grid points. We\u0026rsquo;ll do this in the same manner as in 1D. Here are a couple of options.\nMethod 1: \\[\\vec j_{j, k} ^{n \u0026#43; 1/2} = \\sum _i q_i \\vec v_i ^{n \u0026#43; 1/2} \\left[ \\frac{ S(\\vec x_{j, k} - \\vec x _i ^{n \u0026#43; 1}) \u0026#43; S(\\vec x _{j, k} - \\vec x ^n)}{2} \\right]\\] Method 2: Use the midpoint position \\( \\vec x_i ^{n + 1/2} = \\frac{1}{2} (\\vec x_i ^{n + 1} + \\vec x_i ^n) \\), so we use the shape function at \\[S(\\vec x_{j, k} - \\vec x_i ^{n \u0026#43; 1/2})\\] Depending on the location we use for the shape function, we will get different results. Consider an example case of particles moving from \\( \\vec x_i ^n \\) to \\( \\vec x_i ^{n+1} \\):\nLet\u0026rsquo;s try out Method 1, using nearest grid point weighting with the above particle positions:\n\\[\\begin{aligned} \u0026amp; j_x: \\quad \u0026amp; S({x_x}_{j, k} - x_i ^n) = 1 \\\\ \u0026amp; \u0026amp; S({x_x}_{j, k} - x_i ^{n\u0026#43;1}) = 0 \\\\ \u0026amp; \u0026amp; S({x_x}_{j, k\u0026#43;1} - x_i ^{n}) = 0 \\\\ \u0026amp; \u0026amp; S({x_x}_{j, k\u0026#43;1} - x_i ^{n\u0026#43;1}) = 1 \\\\ \u0026amp; \u0026amp; S({x_x}_{j\u0026#43;1, k\u0026#43;1} - x_i ^{n\u0026#43;1}) = 0 \\\\ \\end{aligned}\\] For nearest grid point, we just check which grid element is inside the particle extent for particle \\( \\vec x_i \\).\nFor method 2, let \\( b \\) be the vertical distance from cell \\( i, j \\) to the midpoint \\( \\vec x_i ^{n + 1/2} \\), and \\( a \\) be the horizontal distance from cell \\( i, j \\) to the midpoint \\( \\vec x_i ^{n + 1/2} \\).\n\\[{j^{n \u0026#43; 1/2}_x}_{j, k} = \\frac{1 - b}{\\Delta x} q_i {v_x ^{n \u0026#43; 1/2}}_i\\] \\[{j^{n \u0026#43; 1/2}_x}_{j, k\u0026#43;1} = \\frac{b}{\\Delta x} q_i {v_x ^{n \u0026#43; 1/2}}_i\\] \\[{j^{n \u0026#43; 1/2}_y}_{j, k} = \\frac{1 - a}{\\Delta y} q_i {v_y ^{n \u0026#43; 1/2}}_i\\] \\[{j^{n \u0026#43; 1/2}_y}_{j\u0026#43;1, k} = \\frac{a}{\\Delta y} q_i {v_y ^{n \u0026#43; 1/2}}_i\\] We see that the different weighting methods give different current distributions. As it turns out, method 1 is better at conserving charge. Methods exist that exactly conserve charge by satisfying Poisson\u0026rsquo;s equation. Villasenor \u0026amp; Bunemen (Comp Phys. Comm. 69 306, 1992) is such a method, and is the standard method used for PIC methods.\nWe can see the connection between charge conservation and current weighting by looking at Poisson\u0026rsquo;s equation\n\\[q_\\alpha \\left( \\pdv{n_\\alpha}{t} \u0026#43; \\div (n_\\alpha \\vec v_\\alpha) \\right) = 0\\] \\[\\pdv{}{t} \\left( q_i n_i - e n_e \\right) \u0026#43; \\div (q_i n_i \\vec v_i - e n_e \\vec v_e) = 0\\] \\[\\pdv{\\rho_c}{t} \u0026#43; \\div \\vec j = 0\\] \\[\\div \\vec j = - \\pdv{}{t} (\\rho) = \\pdv{}{t} (\\epsilon_0 \\grad ^2 \\phi)\\] The reason our two methods don\u0026rsquo;t do a very good job at charge conservation is that they do not preserve the correct value of \\( \\div \\vec j \\).\nIn multi-dimensional EM PIC, we no longer have to weight charges to the grid, because we no longer need to solve Poisson\u0026rsquo;s equation. The only equations we need are Faraday and Ampere.\nParticle Advance # Advancing particles requires weighting the fields at the particle locations\n\\[\\frac{\\vec v_i ^{n \u0026#43; 1/2} - \\vec v_i ^{n - 1/2}}{\\Delta t} = \\frac{ \\vec F_i ^n}{m_i}\\] \\[\\frac{\\vec x_i ^{n \u0026#43; 1} - \\vec x_i ^n}{\\Delta t} = \\vec v _i ^{n \u0026#43; 1/2}\\] Forces are weighted to particle positions using 1st-order particle shape, which amounts to using inverse areas for each sub-grid for the various field components. For example, the electric field is weighted to the nearest grid points by the inverse areas as shown:\nThe magnetic field will be weighted to the nearest half-grid points (cell centers)\nOnly the time-dependent Maxwell\u0026rsquo;s equations are evolved to advance \\( \\vec E \\) and \\( \\vec B \\). We still the divergence constraints. Gauss\u0026rsquo;s law has to be initially satisfied; afterwards, mathematically they are preserved.\n\\[\\div \\vec B = 0\\] \\[\\div \\vec E = \\rho / \\epsilon_0\\] If we take the divergence of Faraday\u0026rsquo;s law, \\[\\pdv{}{t} \\div \\vec B \u0026#43; \\div ( \\curl \\vec E) = 0 \\rightarrow \\pdv{}{t}(\\div \\vec B ) = 0\\] We can do the same with Ampere\u0026rsquo;s law \\[\\pdv{\\vec E}{t} = \\frac{1}{\\epsilon_0} \\vec j = c^2 \\curl \\vec B\\] \\[\\pdv{}{t}( \\div \\vec E) \u0026#43; \\frac{1}{\\epsilon_0} \\div \\vec j = c^2 \\div ( \\curl \\vec B) \\\\ \\rightarrow \\pdv{}{t} \\left(\\div \\vec E - \\frac{1}{\\epsilon_0} \\pdv{\\rho}{t} \\right) = 0\\] So if Gauss\u0026rsquo;s law is initially satisfied, then it will remain so over time. Divergence cleaning techniques are sometimes used to help maintain.\nIn practice, Poisson\u0026rsquo;s equation is solved for the initial particle locations and applied fields, throughout the domain. This gives \\( \\vec E(t = 0) \\). The Biot-Savart law is solved for the initial internal currents and applied magnetic fields, giving \\( \\vec B (t = 0) \\). With these initial fields, we then only advance using Faraday and Ampere.\nAs an example, consider a Penning trap, consisting of a solenoidal coil (which applies external magnetic field \\( B_ext \\)) and a circumferential electrode held at potential \\( \\phi_3 \\) with respect to end electrodes \\( \\phi_1 = \\phi_2 \\).\nWith \\( \\phi_1 \u0026gt; \\phi_3 \\), positively charged particles will be trapped in the center of the configuration, gyrating about the magnetic field. When modeling a Penning trap, we initially need to solve for the fields due to both the initial particle distribution and the electrodes, and the magnetic field due to both the particles and the externally applied field.\n"},{"id":32,"href":"/r/notes/UWAA543/ch20-5/","title":"Panel Method","section":"Computational CFD","content":" 20.5 Panel Method # Panel methods are a classical approach to solving flow problems that is still very useful today when finding solutions to even very complicated geometries. You represent a body as a series of panels.\nA few definitions first: we say a source panel of strength \\( \\lambda \\) (units m/s) is defined by:\n\\[\\phi_p = \\frac{1}{2 \\pi} \\int \\lambda \\ln (r) \\dd s\\] \\[= \\frac{1}{2 \\pi} \\int \\lambda \\ln \\left( \\sqrt{ (x_p - x)^2 \u0026#43; (y_p - y)^2}\\right) \\dd s\\] \\[\\lambda = 2 u_\\lambda\\] Approximate a body as a polygon of N sides. Each panel has a uniform source \\\\( \\lambda_i \\\\). Define angle of flow relative to the panel normal \\\\( \\beta_i \\\\). To define boundary conditions we say the boundary must be a streamline, which means that the normal velocity at each panel is zero \\\\( u_{n_i} = 0 \\\\). So we need to balance the free stream flow and the sources from all other panels. So, at each panel \\( i \\) the normal velocity comes from the contribution from the panel, the contribution from all other panels, and this must equal the free flow normal \\( - u_{\\infty} \\cos \\beta_i \\)\n\\[\\frac{\\lambda_i}{2} \u0026#43; \\sum_{j \\neq i} \\frac{\\lambda_j}{2 \\pi} \\int _j \\pdv{}{n_i} \\ln ( r_{i j}) \\dd s_j = - u_{\\infty} \\cos \\beta_i\\] This gives N equations and N unknowns (\\( \\lambda_i \\)). We can solve them to compute\n\\[u_{t_{i}} = \\frac{\\lambda_i}{2} \\tan \\beta _i\\] \\[\\phi(x, y) = \\sum_i \\phi_i (x, y) \\rightarrow \\vec v = - \\grad \\phi\\] This method is applicable for non-lifting bodies. Without circulation (vertices) there can\u0026rsquo;t be any lift. We can modify the model by including a term that corresponds with circulation, we can allow for lift.\nDefine a distributed vortex panel of strength \\( \\gamma \\) (units m/s):\n\\[\\gamma = \\lim_{\\Delta s \\rightarrow 0} \\frac{1}{\\Delta s} \\oint \\vec v \\cdot \\dd \\vec l = 2 u_{\\gamma}\\] \\[\\dd v = \\frac{\\gamma \\dd s}{2 \\pi r}\\] \\[\\phi _p = \\frac{1}{2 \\pi} \\int \\gamma \\dd s \\theta = \\frac{1}{2 \\pi} \\tan ^{-1} \\left( \\frac{y_p - y}{y_p - x} \\right) \\gamma \\dd s\\] Apply the boundary condition to the vortex panels \\\\( u_{n, i} = 0 \\\\) \\[\\sum_{j \\neq i} ^N \\frac{\\gamma_j}{2 \\pi } \\int \\pdv{}{n_i} \\left[ \\tan ^{-1} \\left( \\frac{y_i - y_j}{x_i - x_j} \\right) \\right] \\dd s_j = 0\\] This gives N unknowns \\( \\gamma_i \\) and only N-1 equations. The last equation comes from the Kutta condition, which says that the tangential velocity at the trailing edge has to be the same from the top as from the bottom of the airfoil. That is to say, the flow must detach at the trailing edge:\n\\[u_{t_1} = - u_{t_N}\\] Once we\u0026rsquo;ve solved for \\( \\gamma_i \\) and \\( \\lambda_i \\) we can compute what the flow velocity is\n\\[u_{t_i} = \\frac{\\lambda_i}{2} \\tan \\beta_i \u0026#43; \\frac{\\gamma_i}{2} \u0026#43; \\sum_{j \\neq i} \\frac{\\gamma_j}{2 \\pi} \\int \\pdv{}{t_i} \\left[ \\tan ^{-1} \\left( \\frac{y_i - y_j}{x_i - x_j} \\right) \\right] \\dd s _j\\] \\[\\phi(x, y) = \\sum_i \\phi_i (x, y) \\rightarrow \\vec v(x, y) = - \\grad \\phi\\] The coefficient of pressure is\n\\[C_p = \\frac{p - p_{\\infty}}{\\frac{1}{2} \\rho u_{\\infty}^2} = 1 - \\frac{u_t ^2}{u_\\infty ^2}\\] "},{"id":33,"href":"/r/notes/griffiths/ch1-5/","title":"The Dirac Delta Function","section":"Griffiths Introduction to Electrodynamics","content":" 1.5: The Dirac Delta Function # 1.5.1: The Divergence of \\( \\vu{r} / r^2 \\) # Consider the vector function\n\\[\\vec{v} = \\frac{1}{r^2} \\vu{r} \\tagl{1.83}\\] At every location, v is directed radially outward (Fig. 1.44); if ever there was a function that ought to have a large positive divergence, this is it. And yet, when you actually calculate the divergence (using Eq. 1.71), you get precisely zero:\n\\[\\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\frac{1}{r^2} \\right) = \\frac{1}{r^2} \\pdv{}{r} (1) = 0 \\tagl{1.84}\\] (You will have encountered this paradox already, if you worked Prob. 1.16.) The plot thickens when we apply the divergence theorem to this function. Suppose we integrate over a sphere of radius R, centered at the origin (Prob. 1.38b); the surface integral is\n\\[\\begin{aligned} \\oint \\vec{v} \\cdot \\dd \\vec{a} \u0026amp; = \\int \\left( \\frac{1}{R^2} \\vu{r} \\right) \\cdot (R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r}) \\\\ \u0026amp; = \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) = 4 \\pi \\end{aligned} \\tagl{1.85}\\] But if we really believe \\( \\eqref{1.84} \\), then the volume integral \\( \\int \\div \\vec{v} \\dd \\tau \\) must be zero. What the heck is going on here??\nThe source of the problem is obviously the point \\( r = 0 \\) , where v blows up (and where, in Eq. 1.84, we have unwittingly divided by zero). It is quite true that \\( \\div \\vec{v} = 0 \\) everywhere except the origin, but right at the origin the situation is more complicated. Notice that the surface integral (Eq. 1.85) is independent of R; if the divergence theorem is right (and it is), we should get \\( \\int (\\div \\vec{v}) \\dd \\tau = 4 \\pi \\) for any sphere centered at the origin, no matter how small. Evidently the entire contribution must be coming from the point \\( r = 0 \\) ! Thus, \\( \\div \\vec{v} \\) has the bizarre property that it vanishes everywhere except at one point, and yet its integral (over any volume containing that point) is \\( 4 \\pi \\) . No ordinary function behaves like that. (On the other hand, a physical example does come to mind: the density (mass per unit volume) of a point particle. It\u0026rsquo;s zero except at the exact location of the particle, and yet its integral is finite-namely, the mass of the particle.) What we have stumbled on is a mathematical object known to physicists as the Dirac delta function. It arises in many branches of theoretical physics. Moreover, the specific problem at hand (the divergence of the function \\( \\vu{r} / r^2 \\) ) is not just some arcane curiosity - it is, in fact, central to the whole theory of electrodynamics. So it is worthwhile to pause here and study the Dirac delta function with some care.\n1.5.2: The One-Dimensional Dirac Delta Function # The one-dimensional Dirac delta function, \\( \\delta(x) \\), can be pictured as an infinitely high, infinitesimally narrow \u0026ldquo;spike,\u0026rdquo; with area 1 (Fig 1.45). That is to say:\n\\[\\delta(x) = \\begin{cases} 0, \u0026amp; \\qquad \\text{ if } x \\neq 0 \\\\ \\infty , \u0026amp; \\qquad \\text{ if } x = 0 \\end{cases} \\tagl{1.86}\\] and\n\\[\\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = 1 \\tagl{1.87}\\] Technically, \\( \\delta(x) \\) is not a function at all, since its value is not finite at x = 0; in the mathematical literature it is known as a generalized function, or distribution. It is, if you like, the limit of a sequence of functions, such as rectangles \\( R_n(x) \\) of height \\( n \\) and width \\( 1/n \\), or isosceles triangles \\( T_n(x) \\) of height \\( n \\) and base \\( 2/n \\) (Fig 1.46)\nIf \\( f(x) \\) is some \u0026ldquo;ordinary\u0026rdquo; function (let\u0026rsquo;s just say continuous, just to be safe), then the product \\( f(x) \\delta(x) \\) is zero everywhere except at x = 0. It follows that\n\\[f(x) \\delta(x) = f(0) \\delta(x) \\tagl{1.88}\\] This is probably the most important fact about the delta function! Since the product is zero anyway except at x = 0, we may as well replace f(x) by the value it assumes at the origin. In particular,\n\\[\\int_{-\\infty} ^{\\infty} f(x) \\delta(x) \\dd x = f(0) \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = f(0) \\tagl{1.89}\\] Under an integral, then, the delta function \u0026ldquo;picks out\u0026rdquo; the value of f(x) at a particular point. Of course, the limits of integration need not be all space, as long as the origin is included. We can also shift the spike from x = 0 to some other point, x = a, as well (Fig 1.47)\n\\[\\delta(x - a) = \\begin{cases} 0, \u0026amp; \\text{ if } x \\neq a \\\\ \\infty , \u0026amp; \\text{ if } x = a \\end{cases} \\quad \\text{ with } \\quad \\int_{-\\infty} ^{\\infty} \\delta(x - a) \\dd x = 1 \\tagl{1.90} \\] Equation \\( \\eqref{1.88} \\) becomes\n\\[f(x) \\delta(x - a) = f(a) \\delta(x - a)\\] and \\( \\eqref{1.89} \\) generalizes to\n\\[\\int_{-\\infty} ^{\\infty} f(x) \\delta(x - a) \\dd x = f(a) \\tagl{1.92}\\] Example 1.14 # Q Evaluate the integral\n\\[\\int_0 ^3 x^3 \\delta(x-2) \\dd x \\] A Easy peasy. The delta function picks out the value of \\( x^3 \\) at the point x = 2, so the integral is \\( 2^3 = 8 \\). Notice that if the limits of integration had not included x = 2, then the answer would be 0. Although \\( \\delta(x) \\) itself is not a legitimate function, integrals over \\( \\delta \\) are perfectly acceptable. In fact, it\u0026rsquo;s best to think of the delta function as something that is always intended for use under an integral sign. In particular, two expressions involving delta functions are considered equal if\n\\[\\int_{-\\infty} ^{\\infty} f(x) D_1 (x) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) D_2 (x) \\dd x \\tagl{1.93}\\] for all (\u0026ldquo;ordinary\u0026rdquo;) functions f(x).\nExample 1.15 # Q Show that\n\\[\\delta(kx) = \\frac{1}{|k||}\\delta(x)\\] where k is any (nonzero) constant. (In particular, \\( \\delta(-x) = \\delta(x). \\))\nA For an arbitrary test function f(x), consider the integral\n\\[\\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x\\] Changing variables, we let \\( y \\equiv kx \\) so that \\( x = y/k \\) and \\( \\dd x = 1 / k \\dd y \\). If k is positive, the integration still runs from \\( -\\infty \\) to \\( \\infty \\), but if k is negative, then \\( x = \\infty \\) implies \\( y = -\\infty \\), and vice versa, so the order of the limits is reversed. Restoring the \u0026ldquo;proper\u0026rdquo; order costs a minus sign. Thus\n\\[\\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x \u0026#43; \\pm \\int_{-\\infty} ^{\\infty} f(y/k) \\delta(y) \\frac{dy}{k} = \\pm \\frac{1}{k} f(0) = \\frac{1}{|k|} f(0) \\] (where here the lower signs apply when k is negative, and we account for this neatly by putting absolute value bars around the final k.) Under the integral sign, then, \\( \\delta(kx) \\) serves the same purpose as \\( (1/|k|)\\delta(x) \\):\n\\[\\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) \\left[ \\frac{1}{|k|} \\delta(x) \\right] \\dd x\\] According to our criterion \\( \\eqref{1.93} \\), therefore, \\( \\delta(kx) \\) and \\( (1/|k|)\\delta(x) \\) are equal.\nThe Three-Dimensional Delta Function # It is easy to generalize the delta function to three dimensions:\n\\[\\delta ^3 (\\vec{r}) = \\delta(x) \\delta(y) \\delta(z) \\tagl{1.96}\\] This three-dimensional delta function is zero everywhere except at (0, 0, 0), where it blows up. Its volume integral is 1.\n\\[\\int_{\\text{all space}} \\delta ^3 (\\vec{r}) \\dd \\tau = \\int_{-\\infty} ^{\\infty} \\dd x \\int_{-\\infty} ^{\\infty} \\dd y \\int_{-\\infty} ^{\\infty} \\dd z \\delta(x) \\delta(y) \\delta(z) = 1 \\tagl{1.97}\\] And, generalizing \\( \\eqref{1.92} \\)\n\\[\\int_{\\text{all space}} f(\\vec{r}) \\delta^3(\\vec{r} - \\vec{a}) \\dd \\tau = f(\\vec{a}) \\tagl{1.98}\\] As in the one-dimensional case, integration with \\( \\delta \\) picks out the value of the function f at the location of the spike.\nWe are now in a position to resolve the paradox introduced in Sect. 1.5.1. As you will recall, we found that the divergence of \\( \\vu{r}/r^2 \\) is zero everywhere except at the origin, and yet its integral over any volume containing the origin is a constant (to wit: \\( 4\\pi \\) ). These are precisely the defining conditions for the Dirac delta function; evidently\n\\[\\div \\left( \\frac{\\vu{r}}{r^2} \\right) = 4 \\pi \\delta^3(\\vec{r}) \\tagl{1.99}\\] More generally,\n\\[\\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta^3(\\gr) \\tagl{1.100}\\] where, as always, \\( \\vec{\\gr} \\) is the separation vector \\( \\vec{\\gr} \\equiv \\vec{r} - \\vec{r\u0026rsquo;} \\). Note that differentiation here is with respect to \\( \\vec{r} \\), while \\( \\vec{r\u0026rsquo;} \\) is held constant. Incidentally, since\n\\[\\grad \\left( \\frac{1}{\\gr} \\right) = - \\frac{\\vu{\\gr}}{\\gr ^2} \\] (from Problem 1.13), it follows that\n\\[\\laplacian \\frac{1}{\\gr} = - 4 \\pi \\delta^3 (\\vec{\\gr}) \\tagl{1.102}\\] Example 1.16 # Q Evaluate the integral\n\\[J = \\int_V (r^2 \u0026#43; 2) \\div \\left( \\frac{\\vu{r}}{r^2} \\right) \\dd \\tau\\] where \\( V \\) is a sphere of radius R centered at the origin.\nA Solution 1 Use \\( \\eqref{1.99} \\) to rewrite the divergence, and \\( \\eqref{1.98} \\) to do the integral:\n\\[J = \\int_V (r^2 \u0026#43; 2) 4 \\pi \\delta ^3(\\vec{r}) \\dd \\tau = 4 \\pi (0 \u0026#43; 2) = 8 \\pi\\] This one-line solution demonstrates something of the power and beauty of the delta function, but I would like to show you a second method, which is much more cumbersome but serves to illustrate the method of integration by parts (Sect. 1.3.6).\nSolution 2\nUsing Eq 1.59, we transfer the derivative from \\( \\vu{r}/r^2 \\) to \\( (r^2 + 2) \\)\n\\[J = - \\int_V \\frac{\\vu{r}}{r^2} \\cdot [\\grad(r^2 \u0026#43; 2)] \\dd \\tau \u0026#43; \\oint_S (r^2 \u0026#43; 2) \\frac{\\vu{r}}{r^2} \\cdot \\dd \\vec{a}\\] The gradient is\n\\[\\grad(r^2 \u0026#43; 2) = 2 r \\vu{r}\\] so the volume integral becomes\n\\[\\int \\frac{2}{r} \\dd \\tau = \\int \\frac{2}{r} r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi = 8 \\pi \\int_0 ^R r \\dd r = 4 \\pi R^2\\] Meanwhile on the boundary of the sphere (where r = R)\n\\[\\dd \\vec{a} = R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r}\\] so the surface integral is\n\\[\\int(R^2 \u0026#43; 2) \\sin \\theta \\dd \\theta \\dd \\phi = 4 \\pi (R^2 \u0026#43; 2)\\] which, all together makes\n\\[J = -4 \\pi R^2 \u0026#43; r \\pi (R^2 \u0026#43; 2) = 8 \\pi\\] In proper mathematical jargon, \u0026ldquo;sphere\u0026rdquo; denotes the surface, and \u0026ldquo;ball\u0026rdquo; the volume it encloses. But physicists are (as usual) sloppy about this sort of thing, and I use the word \u0026ldquo;sphere\u0026rdquo; for both the surface and the volume. Where the meaning is not clear from the context, I will write \u0026ldquo;spherical surface\u0026rdquo; or \u0026ldquo;spherical volume.\u0026rdquo; The language police tell me that the former is redundant and the latter an oxymoron, but a poll of my physics colleagues reveals that this is (for us) the standard usage.\n"},{"id":34,"href":"/r/notes/UWAA557/ch11-1/","title":"Wall-supported Plasma","section":"Physics of Fusion Plasmas","content":" Wall-supported Plasma # Consider a \u0026ldquo;hot\u0026rdquo; plasma surrounded by a \u0026ldquo;cold\u0026rdquo; wall. What are the properties of the plasma (both electrons and ions) at the wall? The speed of a particle headed towards the wall is\n\\[\\frac{1}{2} m v_{th} ^2 \\approx \\frac{1}{2} kT \\rightarrow v_{th} \\approx \\sqrt{ \\frac{kT}{m}}\\] Since electrons are much lighter than ions (\\( m_e = m_i / 1800 \\) for hydrogen), they are moving much faster and will leave the plasma at a much higher rate than the ions. The outflow of negative charge causes a positive buildup in the plasma, slowing the electron loss until the electrons and ions leave at the same rate\n\\[n_{e, w} v_e = n_{i, w} v_i\\] \\( n_{i,w} \\) is the ion density at the wall \\( n_{e, w} \\) is the electron density at the wall \\( v_e \\) is the electron thermal speed at the wall \\( v_i \\) is the ion speed into the wall Now, what would be the thickness of electron-free plasma needed to stop a thermal electron? Near the wall itself, since the electron mass is so much smaller than the ion mass, we can estimate \\( n_i \\approx n_0 \\). Gauss\u0026rsquo;s law gives the electric field generated by a volume \\( A \\cdot x \\) of electron-free plasma\n\\[E A = \\frac{A \\rho x}{\\epsilon_0} \\quad \\rightarrow \\quad E = \\frac{\\rho x}{\\epsilon_0}\\] \\[\\rho \\approx \u0026#43; n_0 e\\] \\[V = - \\int E \\cdot \\dd l = - \\frac{n_0 e}{2 \\epsilon_0} x^2\\] The potential energy of electrons reaching the wall is\n\\[PE = V(-e)= \\frac{n_0 e^2 x^2}{2 \\epsilon_0}\\] The total energy of the electron is\n\\[KE \u0026#43; PE = \\frac{1}{2} k T_e\\] where \\( T_e \\) is the electron temperature. The electron stops when \\( KE = 0 \\) or\n\\[\\frac{1}{2} k T_e = \\frac{n e^2 \\lambda _D ^2 }{2 \\epsilon_0}\\] \\[\\rightarrow \\lambda_D = \\sqrt{ \\frac{ \\epsilon_0 k T_e}{n_0 e^2}}\\] Cool, so the sheath will be somewhere on the order of the Debye length.\nWhat voltage drop between the plasma bulk and the wall is necessary to maintain ambipolar \\( (n_i v_i = n_e v_e) \\) flow to the wall? Let\u0026rsquo;s assume the electrons satisfy a Boltzmann distribution\n\\[n_e \\propto e^{- \\varepsilon/kT}\\] \\[\\varepsilon = PE \u0026#43; KE = \\frac{1}{2} m_e v_e ^2 \u0026#43; V(-e)\\] \\[\\rightarrow n_e \\propto e^{\\frac{1}{2} \\frac{ m v^2}{kT}} e^{\\frac{-V(-e)}{kT}}\\] \\[\\rightarrow n_e = n_0 e^{\\frac{V e}{kT}}\\] Plugging this into the ambipolar flow condition\n\\[n_0 v_0 = n_0 e^{\\frac{Ve}{kT}} v_e\\] In equilibrium the energy is distributed evenly across species. Once again, because the ions are so much heavier, \\( v_i \\approx v_0 \\)\n\\[kT = m_e v_e ^2 = m_i v_0 ^2\\] \\[\\rightarrow \\frac{v_0}{v_e} = \\sqrt{\\frac{m_e}{m_i}} = e^{\\frac{Ve}{kT}}\\] \\[\\rightarrow \\frac{Ve}{kT} = - \\ln \\sqrt{\\frac{m_i}{m_e}}\\] If we measure temperature in electron volts (because of course we do), then Boltzmann\u0026rsquo;s constant is equal to the electric charge, so the ratio of the sheath voltage to the wall temperature is given by\n\\[\\frac{V}{T_e} = - \\ln \\sqrt{\\frac{m_i}{m_e}}\\] For a specific species (ion-to-electron mass ratio) we can calculate this ratio numerically. For D it is about -4.1, for H it is -3.7, so for a D-H plasma it will be about 4.\n\\[V_{sheath} \\approx 4 T_e \\qquad (\\text{D or H})\\] Since the sheath voltage is negative, the ions which are able to overcome the positive charge buildup will be accelerated towards the wall. The final energy of the ions will be:\n\\[W_{impact} \\approx 4 Z_i k T_e \u0026#43; \\frac{1}{2} k T_i \\qquad (\\text{D or H})\\] Now that we roughly know the total sheath voltage required to maintain ambipolar flow, what do \\( n_e \\) and \\( V \\) look like as we move through the sheath towards the wall?\nThe voltage will be the gradient of the electric field, which will come from a difference in number density between the ions and electrons. Gauss\u0026rsquo;s law says that\n\\[\\div \\vec E = \\frac{e(n_i - n_e)}{\\epsilon_0}\\] \\[E = - \\grad V\\] \\[\\rightarrow \\nabla ^2 V = - \\frac{e(n_i - n_e)}{\\epsilon_0}\\] Recall that we can relate \\( n_e \\) to temperature by\n\\[n_e = n_0 e^{\\frac{ Ve}{kT}}\\] For the ion density, Mass conservation in the direction of the wall gives\n\\[n_0 v_0 = n_i v_i\\] Conservation of energy of the ions gives\n\\[\\frac{1}{2} m v_i ^2 = - Ve \u0026#43; \\frac{1}{2} m_i v_0 ^2\\] where \\( v_0 \\) is the thermal speed of the ions in the plasma bulk. With that, all we need are boundary conditions at the wall and in the plasma bulk.\n"},{"id":35,"href":"/r/notes/UWAA558/06-boundary-conditions/","title":"Boundary Conditions","section":"MHD Theory","content":" Boundary Conditions # Mathematically, a well-posed problem requires both governing equations and a complete set of boundary conditions (the Cauchy data for the problem). The most common boundary conditions we use are perfectly conducting walls (flux surfaces) or a vacuum region.\nPerfectly Conducting Wall # For the case where the plasma extends out to a perfectly conducting (impermeable) wall. Perfectly conducting walls do not support tangential electric field:\n\\[\\left. \\vec E_t \\right|_{wall} = 0 \\quad \\rightarrow \\quad \\left. \\vu n \\cross \\vec E \\right| _{wall} = 0\\] Applying Faraday\u0026rsquo;s law at the wall,\n\\[\\left. \\vu n \\cdot \\pdv{\\vec B}{t} \\right|_{wall} = \\left. - \\vu n \\cdot \\curl \\vec E \\right|_{wall} = \\left. \\div (\\vu n \\cross \\vec E) \\right| _{wall} = 0\\] \\[\\pdv{}{t} \\vu n \\cdot \\vec B |_{wall} = 0\\] If initially there is no normal magnetic field, then \\[\\vu n \\cdot \\vec B|_{wall} = 0 \\quad \\text{if initially true}\\] And of course, for an impermeable wall,\n\\[\\vu n \\cdot \\vec v |_{wall} = 0\\] Is this a sufficient set of boundary conditions? Think back to the governing equations in conservation form \\[\\pdv{}{t} \\vec Q \u0026#43; \\div \\vec F = 0\\] The boundary conditions come into play when defining \\( \\vec F \\) at the boundary. In particular, we need to know what \\( \\dd \\vec S \\cdot \\vec F |_{wall} \\) is. In our governing equations, this will involve \\( \\vec E \\) , \\( \\vec B \\) , and \\( \\vec v \\) .\nInsulating Boundary # As a slight modification, an insulating boundary can have a tangential electric field. Consider a simple geometry of parallel electrodes with an insulating wall between them.\nFrom Ohm\u0026rsquo;s law \\[\\vec E \u0026#43; \\vec v \\cross \\vec B = 0\\] so the only way an electric field tangential to the wall can exist is if \\( \\vu n \\cdot \\vec v \\neq 0 \\) .\nFor either a perfectly conducting or an insulating boundary, the other variables are arbitrary: \\( \\rho \\) , \\( p \\) , \\( \\vec v_t \\) , \\( \\vec B_t \\) .\nVacuum Region # The plasma (radius \\( R_p \\) ) is supported by a region of vacuum out to a perfectly conducting wall at some radius \\( R_w \\) . We assume that there is no plasma in the vacuum region. The governing equations in vacuum are just Maxwell\u0026rsquo;s equations\n\\[\\curl \\vec B_{vac} = 0 \\qquad \\text{and} \\qquad \\div \\vec B_{vac} = 0\\] At the wall, \\[\\vu n \\cross \\vec E |_{wall} = 0\\] \\[\\left. \\vu n \\cdot \\pdv{\\vec B}{t} \\right|_{wall} = 0\\] What happens at the plasma-vacuum interface? We need to specify jump conditions and continuity conditions. Let\u0026rsquo;s use square brackets to signify a jump:\n\\[\\left[ X \\right] = \\left. X \\right|_{R_p \u0026#43; dr} - \\left. X \\right|_{R_p - dr}\\] The normal magnetic field has to be continuous.\n\\[[\\vu n \\cdot \\vec B]_{R_p} = 0\\] The tangential magnetic field jump is given by the surface current density at the jump\n\\[\\left[ \\vu n \\cross \\vec B \\right] _{R_p} = \\mu_0 \\vec K \\] Integrating \\( \\grad_\\perp (p \u0026#43; \\frac{B^2}{2 \\mu_0}) = \\frac{B^2}{\\mu_0} \\vec \\kappa \\) over a differential volume across the surface gives\n\\[\\left[ p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right] _{R_p} = 0\\] The plasma shape is determined self-consistently by the wall shape and surface current. This is a free-boundary problem. Another option is to specify the plasma shape, and then determine the required wall shape. This is a fixed-boundary problem.\nThe most realistic case includes externally applied magnetic fields coming from source coils, perhaps computed by Biot-Savart law. The vacuum magnetic field is then \\( \\vec B_{vac} = \\vec B_{ext} \u0026#43; \\vec B_{plasma} \\) . The crazy coil shapes in the stellarator design come from the 3D geometry computations solving this problem.\nConservation of Magnetic Flux (\u0026ldquo;Frozen-In\u0026rdquo; Flux) # Locally, \\( \\vec E \u0026#43; \\vec v \\cross \\vec B = 0 \\) with Faraday\u0026rsquo;s law \\[\\pdv{B}{t} = - \\curl \\vec E = - B \\div \\vec v \u0026#43; \\vec B \\cdot \\grad \\vec v - \\vec v \\cdot \\grad B\\] From the continuity equation, \\[\\pdv{\\rho}{t} \u0026#43; \\vec v \\cdot \\grad \\rho = - \\rho \\div \\vec v\\] Combining we find that \\[\\dv{\\vec B}{t} = \\frac{\\vec B}{\\rho} \\dv{\\rho}{t} \u0026#43; \\vec B \\cdot \\grad \\vec v\\] \\[\\rightarrow \\dv{}{t} \\left( \\frac{\\vec B}{\\rho} \\right) = \\frac{\\vec B}{\\rho} \\cdot \\grad \\vec v\\] This says that the field and plasma density move together. Locally, if the magnetic field increases then mass density increases, such that the ratio \\( \\vec B / \\rho \\) remains constant. In the direction parallel to the magnetic field we have a term that involves field line twisting, which is a bit more complicated, but in the perpendicular direction \\[\\dv{}{t} \\left( \\frac{\\vec B}{\\rho} \\right) _\\perp = 0\\] If we consider globally the magnetic flux through a moving surface S at velocity \\( \\vec u \\) . The magnetic flux penetrating the surface is \\[\\Psi = \\int \\vec B \\cdot \\dd \\vec S\\] or \\[\\dv{\\Psi}{t} = \\int \\dv{\\vec B}{t} \\cdot \\vu n \\dd S\\] \\[= \\int \\pdv{\\vec B}{t}\\cdot \\vu n \\dd S \u0026#43; \\oint \\vec B \\cross \\vec u \\dd \\vec l\\] Using Faraday\u0026rsquo;s law\n\\[\\dv{\\Psi}{t} = \\int - \\curl \\vec E \\cdot \\vu n \\dd S \u0026#43; \\oint \\vec B \\cross \\vec u \\cdot \\dd \\vec l\\] \\[= \\oint (- \\vec E \u0026#43; \\vec B \\cross \\vec u) \\cdot \\dd \\vec l\\] Using the electric field from Ohm\u0026rsquo;s law \\[\\dv{\\Psi}{t} = \\oint(\\vec v - \\vec u) \\cross \\vec B \\cdot \\dd \\vec l\\] This tells us that if the surface moves with the plasma \\( \\vec u = \\vec v \\) then \\[\\dv{\\Psi}{t} = 0\\] the flux through the surface is constant, and the flux is a constant of the topology. This is a direct consequence of ideal MHD. If we add even a small amount of resistivity, we dramatically alter the results in a process called \u0026ldquo;tearing\u0026rdquo; where the magnetic field \u0026ldquo;tears\u0026rdquo; and reconnects with itself.\n"},{"id":36,"href":"/r/notes/UWAA557/ch11-2/","title":"Collisions","section":"Physics of Fusion Plasmas","content":" Collisions # If \\( \\Lambda \\) is large, then small-angle scattering dominates.\n\\[\\nu = \\frac{n}{2 \\pi \\epsilon_0 ^2} \\frac{ e^4}{m^2 v_0 ^3} \\ln \\Alpha \\propto \\frac{n}{T^{3/2} m^{1/2}}\\] Paschen curve for breakdown # Say we have two large parallel plates\nYou can stay on the g=1 curve by putting a large resistor on your power supply, so it\u0026rsquo;s always running like it\u0026rsquo;s about to break down. Neutral-dominated plasma (neutral resistance).\nTo the left of the Paschen minimum, the spacing is less than the mean free path. There are surface losses. Electron loss to wall.\nTo the right of the Paschen minimum, there\u0026rsquo;s a fairly linear region with\nLocal losses There is a certain \\( \\Delta V \\) required between collisions that must be large enough to generate another electron before the electron is lost (\\( l \\) = spacing between the plates, \\( l_{mfp} \\) = mean free path \\[\\Delta V = E l_{mfp} = \\frac{V}{l} l_{mfp} \\qquad l_{mfp} \\propto \\frac{1}{n} \\propto \\frac{1}{p}\\] If you solve,\n\\[\\Delta V = \\frac{V}{l p} \\cdot \\text{const.}\\] \\[V = p l \\left( \\frac{\\Delta V}{\\text{const.}} \\right) = p l \\cdot \\text{const.}\\] \\( g = 1 \\) is used for voltage regulation.\nDefinition of Plasma # Some basic criteria for plasma are\n\\[L \\gg \\lambda_D \\qquad \\text{ (neutral plasma) }\\] \\[\\Lambda \\gg 1 \\qquad (\\omega_{pe} \\gg \\nu_{ei}\\] \\[\\nu_{en} \u0026lt; \\omega_{pe} \\quad \\text{(if neutrals)}\\] The collision frequencies have the following meanings:\n\\( \\nu_{ei} \\) - Electron momentum loss rate on ions. Used in resistivity. \\( \\nu_{ee} \\) Electron energy exchange rate with electrons. In other words, if you do something to the electrons this is how long it will take to get back to Maxwellian. Same order as \\( \\nu_{ei} \\) \\( \\nu_{ii} \\) Ion energy exchange rate with ions \\( \\nu_{ie} \\) Electron energy exchange rate between electrons and ions. It\u0026rsquo;s about the same as ions slowing down in electrons: \\( \\approx \\frac{m_e}{m_i} \\nu_{ee} \\). For fusion to work, need confinement times longer than this time. Electrical Resistivity # Place a plasma of density $n$ in an electric field (generated by voltage difference V). Electrons accelerate in one direction and ions in the other\n\\[\\Delta p = F \\cdot \\Delta t\\] Electrons and ions both get accelerated then collide and both stop since they had equal and opposite momentum\n\\[\\Delta w = F \\cdot \\text{distance}\\] The energy transfer will be much higher for the electrons because of their lower mass. So the electrons carry the current and receive ohmic heating for the resistive part of impedance\n\\[\\langle m_i v_i \\rangle = - \\langle m_e v_e \\rangle \\rightarrow v_e \\gg v_i\\] The approx current is given by the drift velocity \\( v_d \\) by disregarding the velocity of the ions:\n\\[j = - n e \\langle v_e \\rangle \u0026#43; n e \\langle v_i \\rangle \\approx - n e \\langle v_e \\rangle = |n e v_d|\\] Identifying the resistivity with Ohm\u0026rsquo;s law\n\\[E = \\eta j\\] The force on the electrons is the rate at which momentum is lost by the electrons, which is the drift velocity times the electron-ion collision rate:\n\\[F_{elec} = E e = m v_d \\nu_{ei} = \\text{momentum loss rate}\\] \\[\\rightarrow \\eta j = E = \\frac{m_e \\nu_{ei} n e v_d}{n e^2}\\] \\[\\rightarrow \\eta = \\frac{m_e \\nu_{ei}}{n e^2}\\] Recall the collision frequency\n\\[\\nu_{ei} = \\frac{n}{2 \\pi} \\frac{e^4}{\\epsilon_0 ^2 m^2 v_0 ^2} \\ln \\Lambda\\] \\[\\eta = \\frac{m_e}{n e^2} \\frac{n}{2 \\pi \\epsilon_0 ^2} \\frac{Z_{eff}}{m_e ^2} \\frac{e^4}{v^3 _0} \\ln \\Lambda\\] The velocity is given by the electron thermal speed\n\\[v_0 ^3 \\propto \\left( \\frac{ T_e}{m_e} \\right) ^{3/2}\\] The densities cancel and we can plug in some values\n\\[\\eta = 5 \\cdot 10^{-5} \\frac{\\ln \\Lambda}{T_e ^{3/2}} \\qquad \\text{(Hydrogen)}\\] Magnetic Decay Time # The magnetic decay time for parallel current. For current parallel to the magnetic field, the curl of B is just some multiple \\( \\lambda \\) of B:\n\\[\\curl B = \\lambda B = \\mu_0 j = \\mu_0 \\frac{E}{\\eta} \\rightarrow E = \\frac{\\lambda \\eta B}{\\mu_0}\\] \\[\\curl E = - \\pdv{B}{t}\\] \\[\\rightarrow \\curl \\frac{\\eta \\lambda}{\\mu_0 } B = - \\pdv{B}{t}\\] This tells you the rate of decay of the magnetic field when you have helicity. The relevant timescale of the decay is\n\\[\\frac{\\eta \\lambda^2 B}{\\mu_0} = - \\dv{B}{t} \\rightarrow \\tau = \\frac{\\mu_0}{\\eta \\lambda^2}\\] Thermal Conductivity # Consider a region of space where we have a hot side and a cold side. There is a heat flux \\( Q \\) flowing from the hot to the cold side\n\\[Q = \\frac{\\text{power}}{\\text{area}}\\] The thermal conductivity \\( \\kappa \\) is defined by\n\\[Q \\equiv - \\kappa \\pdv{T}{z}\\] If the energy/particle going up / going down is \\( \\varepsilon \\)\n\\[Q = \\varepsilon _0 n_{down} v_{down} - \\varepsilon _\u0026#43; n_{up} v_{up}\\] For mass conservation we must have\n\\[n_{down} v_{down} = n_{up} v_{up} = nv\\] \\[Q = \\varepsilon _0 n v - \\varepsilon_\u0026#43; nv\\] If we\u0026rsquo;re calculating the heat flux at some position \\( z \\) and the mean free path is \\( l \\) then particles come from about a mean free path distance. The energy dependence on \\( z \\) is given as \\( \\varepsilon(z) \\)\n\\[Q \\approx n v \\left[ \\varepsilon (z - l) - \\varepsilon(z \u0026#43; l) \\right]\\] \\[\\rightarrow Q \\approx n v \\left[ \\left(\\varepsilon (z) - l \\pdv{\\varepsilon}{z}\\right) - \\left(\\varepsilon(z) \u0026#43; l \\pdv{\\varepsilon}{z}\\right) \\right]\\] \\[\\approx - n v l \\pdv{\\varepsilon}{z}\\] The Maxwell-Boltzmann energy of the particles is\n\\[\\varepsilon (z) = k T(z)\\] \\[Q \\approx - n v l k \\pdv{T}{z}\\] \\[\\kappa = knv l_{mfp}\\] Now we plug in the mean free path, assuming \\( l_{mfp} \\ll z_0 \\) (where \\( z_0 = \\) plasma size.\n\\[l_{mfp} = v t_c = \\frac{v}{\\nu} \\approx \\frac{(k T)^{1/2} T^{2/3} m^{1/2}}{m^{1/2} n \\ln \\Lambda}\\] \\[\\rightarrow \\kappa \\sim \\frac{T^{5/2}}{m^{1/2} \\ln \\Lambda} \\sim \\frac{T^{5/2}}{m^{1/2}} \\] If there is no magnetic field (or we\u0026rsquo;re looking parallel to the field) then\n\\[\\kappa \\propto \\frac{ T^{5/2}}{m^{1/2}}\\] There\u0026rsquo;s a thing called the conductivity of the Lorentz gas (ions are infinitely massive)\n\\[\\kappa_{lorentz} \\approx 4.67 \\cdot 10^{-12} \\frac{T^{5/2}}{Z \\ln \\Lambda}\\] \\[\\kappa_{Z=1} \\approx 4.4 \\cdot 10^{-13} \\frac{T^{5/2}}{Z \\ln \\Lambda}\\] What happens when we add a magnetic field? \\( B = B_0 \\)\n\\[l_{mfp} \\rightarrow r_g\\] \\( r_g \\) = radius of gyration = \\( \\frac{v}{\\omega} \\)\n\\[r_g = \\frac{v}{\\omega} \\qquad l_{mfp} = v \\tau_c \\qquad \\tau_c = \\frac{1}{\\nu}\\] \\[\\frac{r_g}{l_{mfp}} = \\frac{v}{\\omega} \\frac{1}{v \\tau_c} = \\frac{1}{\\omega \\tau_c}\\rightarrow \\kappa\\] where \\( \\kappa \\) is reduced by a factor \\( \\frac{1}{\\omega \\tau_c} \\) compared to the non-magnetized plasma. The energy is now transmitted over a shorter distance, but it also has a lower fluence. The particles move a distance \\( r_g \\) over every time \\( \\tau_c \\), so the transport velocity is reduced\n\\[v \\rightarrow \\frac{r_g}{\\tau _c} \\rightarrow \\text{ reduced by } \\frac{r_g}{v \\tau_c} = \\frac{1}{\\omega \\tau} \\quad (v = \\omega r_g)\\] That\u0026rsquo;s an additional reduction by \\( \\frac{1}{\\omega \\tau} \\), so\n\\[\\kappa_B \\approx \\kappa_0 \\left( \\frac{1}{\\omega \\tau}\\right) ^2\\] What is the mass dependence\n\\[\\kappa_B \\rightarrow \\frac{1}{m^{1/2}} \\left( \\frac{m}{m^{1/2}} \\right) ^2 = m^{1/2}\\] In the cross-field direction, ions dominate the cross field thermal conduction\n\\[\\kappa _\\perp \\approx \\kappa_{\\parallel, e} \\left( \\frac{1}{\\omega_{ce} \\tau_{ee}} \\right)^2 \\left( \\frac{m_i }{m_e} \\right) ^{1/2} \\] or\n\\[\\approx \\kappa_{\\parallel, i} \\left( \\frac{1}{\\omega_{ci} \\tau_{ii}} \\right) \\propto m^{1/2} T^{5/2} \\left( \\frac{n}{B T^{3/2}} \\right) ^2 \\propto \\frac{m^{1/2} n^2}{B^2 T^{1/2}}\\] To get real numbers, use Spitzer\n\\[\\kappa_\\perp = 1.6 \\cdot 10^{-40} \\frac{A_i ^{1/2} n^2 \\ln \\Lambda}{T^{1/2} B^2}\\] where \\( T \\) is in \\( eV \\), \\( B \\) is in Tesla, \\( n \\) is in \\( m^{-3} \\), \\( Z \\) is the atomic charge, and \\( A_i \\) is the atomic mass number\n\\[\\eta = \\frac{b}{T^{3/2}} \\qquad b = 5.2 \\cdot 10^{-5} Z \\ln \\Lambda\\] Ohmic heating balancing cross field thermal conduction # Let\u0026rsquo;s consider a region of plasma (radius \\( a \\) and length \\( L \\) and estimate the parameters required to achieve a balance between ohmic heating and the cross-field thermal conduction.\nThe thermal conduction power is\n\\[Q 2 \\pi a L\\] The ohmic heating is\n\\[\\eta j^2 \\pi a^2 L\\] \\[\\kappa \\frac{T}{a/2} = \\eta \\frac{\\lambda^2 B^2 a}{\\mu_0 ^2 2} \\qquad \\curl B = \\lambda B = \\mu_0 j\\] \\[\\frac{C n^2 T}{T^{1/2} B^2} = \\frac{b B^2}{T^{3/2} 2^2 \\mu_0 ^2} \\lambda ^2 a ^2 \\qquad \\kappa_\\perp = C \\frac{n^2}{T^{1/2}B^2} \\qquad \\eta = \\frac{b}{T^{3/2}}\\] \\[\\beta ^2 \\equiv \\frac{(2 k n T)^2}{(B^2/2 \\mu_0)^2} = 4 k^2 \\frac{b}{C} \\lambda ^2 a^2 \\] where \\( \\beta \\) is the ratio of the plasma pressure \\( p = 2 n k T \\) (the factor of 2 accounts for both species in the plasma) to the magnetic pressure \\( p_{mag} = B^2 / 2 \\mu_0 \\).\nFor \\( Z = 1 \\) and \\( A_i = 2 \\),\n\\[\\rightarrow \\beta = 0.15 \\lambda a\\] \\( \\lambda a \\approx 2.4 \\) for spheromak, but only \\( \\approx 0.5 \\) for tokamak.\nAxial thermal conduction cools ohmic heating # Consider a voltage applied parallel to the magnetic field\nAssume that the length is short enough that cross field transport is small compared to axial loss\n\\[Q = - \\kappa \\dv{T}{X} \\qquad \\kappa \\approx A T^{5/2}\\] Let\u0026rsquo;s just ignore the temperature dependence of \\( \\Lambda \\) (since it is slowly varying). Also assume that a current density \\( j \\) is being driven and the system is in a steady state.\n\\[\\dv{Q}{X} = \\eta j^2 \\qquad \\eta = \\frac{b}{T^{3/2}}\\] \\[b = 5.2 \\cdot 10^{-5} Z \\ln \\Lambda\\] Make things dimensionless by normalizing \\( X \\) by \\( \\sqrt{b/A} j/T_0 ^{5/2} \\), \\( q \\) by \\( -Q/\\sqrt{Ab}T_0 j \\) and \\( T \\) by \\( T_0 \\).\n\\[x = X \\sqrt{\\frac{b}{A}} \\frac{j}{T_0 ^{5/2}}\\] \\[q = \\frac{-Q}{T_0 j \\sqrt{Ab}}\\] \\[T = \\frac{T}{T_0}\\] \\[\\rightarrow Q = - A T^{5/2} \\dv{T}{X}\\] \\[\\dv{Q}{X} = \\frac{b}{T^{3/2}} j^2\\] \\[- \\frac{Q}{A T^{5/2}} = \\dv{T}{X} \\] Replacing the dimensionless quantities,\n\\[\\rightarrow \\frac{q}{T^{5/2}} = \\dv{T}{x}\\] \\[\\dv{Q}{X} = \\eta j^2 = \\frac{b}{T^{3/2}} j^2\\] \\[\\rightarrow - \\dv{q}{x} = \\frac{1}{T^{3/2}}\\] Write equation in terms of \\( T_0 \\)\n\\[\\sqrt{Ab} T_0 j = - Q\\] This is \\( Q \\) into one electrode\nPower per unit area going into the wall is equal to power per unit area in terms of the voltage\n\\[\\sqrt{Ab} T_0 j = \\frac{V j}{2}\\] \\[\\sqrt{4AB} T_0 = V\\] \\[T_0 = \\frac{V}{\\sqrt{4Ab}} \\approx \\frac{V}{\\sqrt{6}}\\] True for anything without losses across the electrodes.\nViscosity # The shear force is written as\n\\[P_{yx} = - \\mu \\pdv{u_x}{y}\\] Viscosity is a force per unit area with the force parallel to the surface. It tends to make things want to go the same velocity. It comes about by particles exchanging across the surface\n\\[\\mu_{B=0} \\propto \\frac{T^{5/2} A_i ^{1/2}}{4Z\\ln \\Lambda}\\] \\[\\mu_B \\propto \\frac{n_i ^2 Z^2}{T^{1/2} B^2} A_i ^{3/2} \\ln \\Lambda\\] "},{"id":37,"href":"/r/notes/UWAA543/ch21-1/","title":"Finite Difference Algorithms","section":"Computational CFD","content":" 21.1 Finite Difference Algorithms # Definitions # By definition, Finite Differencing is a method to approximate partial differential equations which we cannot solve, into a system of algebraic equations which we can.\nNotation to simplify our representations:\nSuperscripts: We use superscripts to denote steps in the time domain \\( t^n = n \\Delta t \\). Here \\( n = [0, N] \\) is the step index and \\( \\Delta t \\) is the time-step \\( T/N \\) Subscripts: \\( x_j = j \\Delta x \\). Here \\( j = [0, J] \\) is the step index and \\( \\Delta x = L/J \\) is the spatial step. Together: \\( u_j ^n = u(t^n, x_j) \\) Explicit algorithms: Use data that is already known at the present time to advance the solution to the next time step. They are easier and faster to implement, but they introduce stability constraints. \\[ u_j ^{n\u0026#43;1} = f(u_j ^n, u_{j\u0026#43;1} ^n , u_{j\u0026#43;2} ^n \\ldots) \\] Implicit algorithms: Use data from the next time step when advancing the solution. Leads to a system of equations that must be solved simultaneously \\[ u_{j} ^{n\u0026#43;1} = f(u_{j} ^n , u_{j\u0026#43;1} ^{n\u0026#43;1}, u_{j-1} ^{n\u0026#43;1}, \\ldots) \\] One look at the typical definition of the derivative suggests an algebraic approximation\n\\[\\dv{f}{x} = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x \u0026#43; \\Delta x) - f(x)}{\\Delta x} \\quad \\rightarrow \\quad \\dv{f}{x} \\approx \\frac{f(x \u0026#43; \\Delta x) - f(x) }{\\Delta x}\\] Applying to our model equation, the linear advection equation\n\\[\\pdv{u}{t} \u0026#43; c \\pdv{u}{x} = 0\\] The first-order forward difference approximation for the temporal derivative is\n\\[\\pdv{u}{t} \\approx \\frac{u_j ^{n\u0026#43;1} - u_j ^n}{\\Delta t} \\equiv \\Delta _t u\\] And the first-order backward approximation for \\( \\pdv{u}{x} \\) is\n\\[\\pdv{u}{x} \\approx \\frac{u_{j} ^n - u_{j-1} ^n}{\\Delta x} \\equiv \\nabla _x u\\] These are sometimes called Euler differencing, since they are first-order. Plugging in our approximations, we arrive at the \u0026ldquo;Forward Euler Algorithm\u0026rdquo;\n\\[\\frac{u_{j} ^{n\u0026#43;1} u_j ^n}{\\Delta t} \u0026#43; c \\frac{u_{j} ^n - u_{j-1} ^n}{\\Delta x} = 0\\] This is an explicit scheme: solve for \\( u^{n+1} _j \\)\n\\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{c \\Delta t}{\\Delta x} (u_j ^n - u_{j-1} ^n)\\] The multiplier out front \\( \\frac{c \\Delta t}{\\Delta x} \\) is very important for stability, so we call it\n\\[\\frac{c \\Delta t}{\\Delta x} = \\text{ Courant number (or CFL number)}\\] For the problem to be mathematically well-posed, we must know initial conditions and boundary conditions\n\\[u(t = 0, j) \\qquad u(t, x = 0)\\] Accuracy # We can be more precise with the error in our difference approximation: the error will be on the order of \\( \\Delta x \\):\n\\[\\dv{f}{x} \\approx \\frac{f(x \u0026#43; \\Delta x) - f(x) }{\\Delta x}\\] \\[\\rightarrow \\dv{f}{x} = \\frac{f(x \u0026#43; \\Delta x) - f(x) }{\\Delta x} \u0026#43; O(\\Delta x)\\] As it turns out, we can improve the accuracy of the finite difference operators by using centered differences:\n\\[\\left. \\pdv{u}{x} \\right|_{j} = \\frac{u_{j\u0026#43;1} ^n - u_{j-1} ^n}{2 \\Delta x} \u0026#43; O(\\Delta x ^2)\\] So the forward-time, centered-space (FTCS) PDE becomes\n\\[\\frac{u_j ^{n\u0026#43;1} - u_j ^n}{\\Delta t} \u0026#43; c \\frac{u_{j\u0026#43;1} ^n - u_{j-1} ^n }{2 \\Delta x} = 0\\] This is more accurate than the Euler method in space. We can write the accuracy as \\( O(\\Delta t, \\Delta x^2) \\), or \u0026ldquo;it is first-order accurate in time and second-order accurate in space.\u0026rdquo;\nHow do we get accuracy estimates? The accuracy is defined by using a Taylor series expansion for the finite-difference operators:\n\\[u_j ^{n\u0026#43;1} = u_j ^n \u0026#43; \\pdv{u}{t} \\Delta t \u0026#43; \\frac{1}{2} \\pdv{ ^2 u}{t^2} \\Delta t^2 \u0026#43; \\frac{1}{6} \\pdv{ ^3 u}{t ^3} \\Delta t^3 \u0026#43; \\ldots\\] \\[u_{j\u0026#43;1} ^n = u_j ^n \u0026#43; \\pdv{u}{x} \\Delta x \u0026#43; \\frac{1}{2} \\pdv{ ^2 u}{x^2} \\Delta x^2 \u0026#43; \\frac{1}{6} \\pdv{ ^3 u }{x ^3} \\Delta x ^3 \u0026#43; \\ldots\\] \\[u_{j-1} ^n = u_j ^n - \\pdv{u}{x} \\Delta x \u0026#43; \\frac{1}{2} \\pdv{ ^2 u }{x^2} \\Delta x^2 - \\frac{1}{6} \\pdv{^3 u }{x^3} \\Delta x ^3 \u0026#43; \\ldots \\] Substituting into the Forward Euler algorithm:\n\\[u_j ^n \u0026#43; \\pdv{u}{t} \\Delta t \u0026#43; \\frac{1}{2} \\pdv{^2 u }{t^2} \\Delta t^2 \u0026#43; \\frac{1}{6} \\pdv{ ^3 u}{t ^3} \\Delta t^3 \u0026#43; \\ldots\\] \\[= u_j ^n - \\frac{c \\Delta t}{\\Delta x} \\left( u_j ^n - \\left[ u_j ^n - \\pdv{u}{x} \\Delta x \u0026#43; \\pdv{^2 u }{x^2} \\frac{\\Delta x ^2}{2} \\pm \\ldots \\right] \\right)\\] After simplifying and re-arranging/canceling like terms, we get\n\\[\\pdv{u}{t} \u0026#43; c \\pdv{u}{x} = - \\frac{1}{2} \\pdv{^2 u}{t^2} \\Delta t - \\frac{1}{6} \\Delta t^2 \u0026#43; \\ldots \u0026#43; \\frac{c}{2} \\pdv{^2 u}{x^2} \\Delta x - \\frac{c}{6} \\pdv{ ^3 u }{x ^3} \\Delta x ^2 \u0026#43; \\ldots\\] The left-hand side is the PDE we\u0026rsquo;re trying to solve, so everything on the right-hand side is the error term introduced by our approximation. The solution we\u0026rsquo;re going to get is actually the solution to the modified PDE with all of the error terms. Reading off the lowest-order terms of \\( \\Delta x \\) and \\( \\Delta t \\) we see that the algorithm is first-order accurate in space and time.\nAlgorithm Requirements # For an algorithm to work, it requires the following properties:\nConsistency: Whatever finite difference operator \\( \\delta_x u \\) we use, applied to our solution, has to approximate the derivative as the spacing goes to zero \\[ \\delta _x u \\rightarrow \\pdv{u}{x} \\quad \\text{ as } \\quad \\Delta x \\rightarrow 0 \\] Stability: The solution must be bounded, so for some initial condition \\( u \\) the norm goes to zero as the number of points goes to infinity \\[ |u_j - u| \\rightarrow 0 \\quad \\text{ as } \\quad I \\rightarrow \\infty \\] Accuracy: The accuracy is bounded by the finite difference operator \\[ \\pdv{u}{x} - \\delta _x u = O(\\Delta x ^m) \\] Convergence: The numerical solution must approach the exact solution as the grid spacing goes to zero \\[ u_j \\rightarrow u \\quad \\text{ as } \\quad \\Delta x \\rightarrow 0 \\] A useful theorem is Lax\u0026rsquo;s Theorem: An algorithm that is consistent and stable will converge. This means that we only have to prove consistency, stability, and accuracy.\nConsistency # Let\u0026rsquo;s take our Forward Euler algorithm as an example: From analyzing the accuracy, we derived the modified PDE and noted the leading-order error terms. Forward Euler gave \\( O(\\Delta x, \\Delta t) \\). As \\( \\Delta t, \\Delta x \\rightarrow 0 \\), we recover the original PDE. Therefore, FE is consistent.\nLikewise the FTCS is also consistent, since \\( \\Delta t, \\Delta x \\rightarrow 0 \\) also recovers the original PDE. In fact, because the accuracy of FTCS was \\( O(\\Delta x ^2, \\Delta t) \\) it is also consistent.\nStability # The way to perform the stability analysis is to use Fourier transforms to convert from a discrete spatial domain to a continuous frequency domain. Broadly speaking, the mathematical tools we have at our disposal to analyze stability only apply to continuous functions - you can\u0026rsquo;t take a derivative of a set of discrete points. By transforming to a continuous domain, we can use properties of our basis functions to examine the growth of errors over time. This process is called Von Neumann Stability Analysis.\nExample: Stability analysis of FTCS\n\\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{a \\Delta t}{2 \\Delta x} \\left( u_{j\u0026#43;1} ^n - u_{j-1} ^n \\right)\\] We define an error norm by subtracting the exact solution\n\\[\\varepsilon_j ^n = u_j ^n - \\overline{u}\\] \\[\\varepsilon_j ^{n\u0026#43;1} = \\varepsilon_j ^n - \\frac{a \\Delta t}{2 \\Delta x} \\left( \\varepsilon_{j\u0026#43;1} ^n - \\varepsilon_{j-1} ^n \\right)\\] To study the evolution of the errors, apply a Fourier transform to get to frequency space:\n\\[\\varepsilon_j ^n = V^n e^{ik_x x} = V^n e^{ik_x (j \\Delta x)}\\] \\[V^n = \\text{ wave amplitude}\\] \\[k = \\text{ wavenumber } = \\frac{2 \\pi}{\\lambda_x}\\] Substituting,\n\\[V^{n\u0026#43;1} e^{i k_x j \\Delta x} = V^n e^{i k_x j \\Delta x} - \\frac{a \\Delta t}{2 \\Delta x} \\left(V^n e^{i k_x (j\u0026#43;1) \\Delta x } - V^n e^{i k_x (j-1) \\Delta x} \\right)\\] \\[\\rightarrow \\frac{V^{n\u0026#43;1}}{V_n} = 1 - \\frac{a \\Delta t}{2 \\Delta x} \\left( e^{i k_x \\Delta x} - e^{ik_x \\Delta x} \\right)\\] The ratio \\( V^{n+1}/V^n \\) tells us how the amplitudes of the errors will evolve in time. We define the amplification factor \\( G \\)\n\\[G = \\left| \\frac{V^{n\u0026#43;1}}{V^n} \\right|\\] For stability, we need the errors not to grow over time, so the Von Neumann stability criterion is:\n\\[G^m \\leq 1 \\quad \\forall n\\] Continuing with our example\n\\[\\frac{V^{n\u0026#43;1}}{V_n} = 1 - \\frac{a \\Delta t}{2 \\Delta x} \\left[ \\cos (k_x \\Delta x) \u0026#43; i \\sin(k \\Delta x) - \\cos (-k_x \\Delta x) - i \\sin (- k_x \\Delta x) \\right]\\] \\[= 1 - i \\frac{ a \\Delta t}{\\Delta x} \\sin (k_x \\Delta x)\\] To get the amplification factor, we need the norm\n\\[G = \\left[ \\left(1 - i \\frac{ a \\Delta t}{\\Delta x} \\sin (k_x \\Delta x) \\right) \\left( 1 \u0026#43; i \\frac{ a \\Delta t}{\\Delta x} \\sin (k_x \\Delta x)\\right) \\right] ^{1/2}\\] \\[= \\sqrt{ 1 \u0026#43; \\left( \\frac{a \\Delta t}{\\Delta x} ^2 \\sin ^2 (k_x \\Delta x)\\right)}\\] But that\u0026rsquo;s always greater than 1! So FTCS is unconditionally unstable.\nPerforming the same analysis for Forward Euler, we get a stability condition:\n\\[0 \\leq 1 \u0026#43; 2\\frac{a \\Delta t}{\\Delta x} \\left( \\frac{a \\Delta t}{ \\Delta x} -1 \\right) (1 - \\cos (k_x \\Delta x)) \\leq 1\\] We can see that the CFL number must be\n\\[0 \\leq \\frac{a \\Delta t}{\\Delta x} \\leq 1\\] Possible values of \\( k_x \\Delta x = \\frac{2 \\pi}{\\lambda} \\Delta x \\) . The minimum of \\( k_x \\Delta x \\) occurs at \\( \\lambda = \\infty \\), uniform error throughout the domain. This only occurs if the error is everywhere zero, since the boundary conditions are presumed accurate. In practice, the maximum wavelength is equal to twice the length of the domain\n\\[\\lambda_{max} = 2L\\] \\[(k_x \\Delta x)_{max} = \\frac{\\pi}{J-1}\\] The maximum of \\( k_x \\Delta x \\) is for \\( \\lambda = 0 \\), but this is not possible on a grid with finite grid points. The Nyquist limit tells us that \\( \\lambda_{min} = 2 \\Delta x \\). That means that\n\\[(k_x \\Delta x)_{max} = \\pi\\] When you violate a stability condition, what do we expect to see? The error growth is largest for the largest value of \\( k_x \\Delta x \\), which corresponds to a wavelength equal to the grid spacing. In practice, that looks like errors that blow up from point to point.\nConvergence # We want to show that the numerical solution approaches the exact solution to the original PDE.\nLax\u0026rsquo;s equivalence theorem states:\nIf an algorithm is consistent and stability requirements are satisfied, the numerical solution will converge to the solution of the original PDE.\nThis is why consistency and stability is so important; if you have both, then you have convergence, which tells you that the problem you\u0026rsquo;re solving is the problem you are actually trying to solve.\nAccuracy # The solution exists at discrete locations for multiple variables, e.g \\( \\rho, p, u, v, \\ldots \\), so we want a convenient measure of solution accuracy.\nThe general p-norm is defined as\n\\[\\text{p-norm} = L_p = \\left[\\sum_{j=1} ^J |z|^p \\right]^{1/p}\\] In practice, the most common norms that we use are the 1, 2, or \\( \\infty \\)-norms.\n\\[L_1 = \\sum |z| \\quad \\text{(average)}\\] \\[L_2 = \\left[ \\sum |z|^2 \\right]^{1/2} \\quad \\text{(variance)}\\] \\[L_{\\infty} = \\text{max}|z| \\quad \\text{(max-norm)}\\] In practice, the 2-norm is the most rigorous definition of the error. Specifically, we define the error norm in the form\n\\[\\text{Error norm} = \\left[ \\sum_{j=1} ^{J} \\Delta x ( \\varepsilon) ^2 \\right] ^{1/2} = \\sqrt{\\Delta x} || \\varepsilon ||_2\\] \\[\\varepsilon_j = u_j - \\overline{u}\\] Sources of Errors? # Where do errors come from? In order to reduce our model\u0026rsquo;s error, we must first understand where the errors come from.\nTruncation errors - Result from the terms in the Taylor series difference approximation that are neglected. They can be reduced by using higher accuracy difference operators.\nRound-off errors - Result of the limited machine accuracy (accuracy of floating point representation). It can be reduced by using higher precision to store values, or by performing fewer calculations.\nBugs - Don\u0026rsquo;t have bugs. ha. ha. ha. But really, we can minimize programming errors through practices like good planning, good comments, etc.\n"},{"id":38,"href":"/r/notes/UWAA545/06-fluid-models/","title":"Fluid Models for Plasmas","section":"Computational Methods For Plasmas","content":" \\[\\] Motivation for Fluid Models # Up to this point, we\u0026rsquo;ve been discussing particle models, and in particular, kinetic descriptions in which we\u0026rsquo;ve sampled the distribution function at particular locations to get our particles. We retained detailed information about the distribution function by tracking the positions and velocities of these representative super-particles, which should continue to \u0026ldquo;represent\u0026rdquo; the sampled distribution function at all points in time. Tracking this detailed information is computationally expensive, so PIC is limited to small spatial scale or short time scale phenomena. Non-linear waves and wave-plasma interactions are good examples of PIC use cases.\nApplying 3D kinetic models to a confined plasma, for example, stretches beyond the current state of the art. This desire motivates the use of reduced models. Fluid models are examples of reduced models.\nOther courses (like the course under the \u0026ldquo;MHD Theory\u0026rdquo; section of these notes) go into much more detail on fluid models, so this section will not be as complete as the PIC description.\nMoment Reductions # Fluid models reduce the information about the distribution functions, retaining only the minimum required information to characterize the distribution. This is done by retaining velocity moments of \\( f(\\vec v) \\).\n0th moment \\( \\int f(v) \\dd v \\) is the number density 1st moment \\( \\int v f(v) \\dd v \\) gives the mean particle velocity, also called the drift or fluid velocity. 2nd moment \\( \\int (v - v_d)(v - v_d) f(v) \\dd v \\) gives the root mean square of the particle velocity, related to the fluid temperature. These three moments are the only ones required to describe a Maxwellian distribution. A fluid system which tracks only these three moments necessarily assumes a local thermodynamic equilibrium (LTE). We don\u0026rsquo;t have to assume LTE; we can allow for deviations away from a Maxwellian distribution, but we must include additional moments to do so.\n3rd moment (skewness) is related to the energy flux. 4th moment (kurtosis) doesn\u0026rsquo;t have a precise physical meaning, referred to as the \u0026ldquo;weight of the tail of the distribution.\u0026rdquo; The higher moments are really just statistical concepts, without real physical meaning.\nFluid models evolve only the moments of \\( f( \\vec v) \\) to reduce the dimensionality of a full 6D (3P-3D) kinetic description down to a three-dimensional description\n\\[f(\\vec x, \\vec v) \\rightarrow n(\\vec x), v(\\vec x), T(\\vec x)\\] We\u0026rsquo;ve gone back down from 6D to 3D, but we have more variables at each point (moment values are scalars, vectors, and even tensors).\nMoments for species \\( \\alpha \\) are defined as\n\\[{M_{\\alpha}}_{n} = \\int_{- \\infty} ^{\\infty} \\vec v ^n f_\\alpha (\\vec x, \\vec v) \\dd \\vec v\\] where \\( \\vec v^{n} \\) is the vector dyad product of \\( \\vec v \\) with itself. This means that in general, \\( {M_{\\alpha}}_{n} \\) will be a tensor, for each species \\( \\alpha \\) in our system. These are the primary dependent variables of the fluid model.\n\\[n = 0: \\qquad \\int f_{\\alpha} \\dd \\vec v = n_{\\alpha}\\] \\[n = 1: \\qquad \\frac{\\int \\vec v f_{\\alpha} \\dd \\vec v}{n_{\\alpha}} = \\vec v_{\\alpha}\\] \\[n = 2: \\qquad \\int m_{\\alpha} ( \\vec v - \\vec v_{\\alpha})( \\vec v - \\vec v_{\\alpha}) f_{\\alpha} \\dd \\vec v = \\overline{\\vec P_{\\alpha}}\\] \\[n = 3: \\qquad \\frac{1}{n_{\\alpha}} \\int m_{\\alpha} (\\vec v - \\vec v_{\\alpha})(\\vec v - \\vec v_{\\alpha}) ( \\vec v - \\vec v_{\\alpha}) f_{\\alpha} \\dd \\vec v = \\overline{\\vec H_{\\alpha}}\\] where \\( n_{\\alpha} \\) is a rank 0 tensor, \\( \\vec v_{\\alpha} \\) is a rank 1 tensor, \\( \\overline{ \\vec P}{\\alpha} \\) is a rank 2 tensor (with only 6 unique components), \\( \\overline{\\vec H}{\\alpha} \\) is a rank 3 tensor, and so on.\nA common reduction is to define a heat flux vector (tensor contraction) \\[\\vec h_{\\alpha} = \\frac{m_{\\alpha}}{n_{\\alpha}} \\int ( \\vec v - \\vec v_{\\alpha}) \\cdot ( \\vec v - \\vec v_{\\alpha})(\\vec v - \\vec v_{\\alpha}) f_{\\alpha} \\dd \\vec v\\] by contracting a dimension, we get a vector, which is the standard heat flux vector we usually think about. The full system is the 13N-moment plasma model (where N is the number of species). The moments are: \\( n \\), \\( v_x \\), \\( v_y \\), \\( v_z \\), \\( P_{xx} \\), \\( P_{xy} \\), \\( P_{xz} \\), \\( P_{yy} \\), \\( P_{yz} \\), \\( P_{zz} \\), \\( h_z \\), \\( h_y \\), \\( h_z \\) for each species \\( \\alpha \\).\nTo cap off the moments, we need to \u0026ldquo;close\u0026rdquo; the model with a closure expression. Typically, we define a closure for the heat flux by defining a Fourier law for the conductivity:\n\\[\\vec h = - \\kappa \\grad T\\] This is the 10N-moment plasma model.\n5N-Moment Model # Another reduction is to define a closure for the anisotropic pressure tensor. This leaves us with the isotropic pressure (and temperature), and the shear stress is related to lower moment variables by a viscosity, like \\( \\nu \\grad \\vec v \\). We then end up with the 5N-moment plasma model, which is one of the most common models for capturing many fluids. It\u0026rsquo;s typically called the multi-fluid plasma model.\nThe governing equations are derived by taking moments of the Boltzmann equation\n\\[n = 0: \\quad \\pdv{n_{\\alpha}}{t} \u0026#43; \\div ( n_{\\alpha} \\vec v_{\\alpha}) = 0\\] \\[n =1: \\quad n_{\\alpha} m_{\\alpha} \\left( \\pdv{\\vec v_{\\alpha}}{t} \u0026#43; \\vec v_{\\alpha} \\cdot \\grad \\vec v _{\\alpha} \\right) \u0026#43; \\div \\overline{\\vec P}_{\\alpha} - n_{\\alpha} q_{\\alpha} ( \\vec E \u0026#43; \\vec v_{\\alpha} \\cross \\vec B) \\\\ = m_{\\alpha} \\int \\vec v \\left. \\pdv{f_{\\alpha}}{t} \\right| _{coll} \\dd \\vec v \\\\ = - \\sum_ \\beta n_{\\alpha} m_{\\alpha} ( \\vec v_{\\alpha} - \\vec v_\\beta) \\nu_{\\alpha \\beta} = \\sum_\\beta - \\vec R_{\\alpha \\beta}\\] Note that this is the full Boltzmann equation, not the Vlasov equation, so we include the collision operator \\( \\vec R_{\\alpha \\beta} \\) between unlike species \\( \\alpha \\) and \\( \\beta \\).\n\\[\\overline{\\vec P}_{\\alpha} = P_{\\alpha} \\overline{\\vec I} \u0026#43; \\overline{\\vec \\Pi}_{\\alpha}\\] where the scalar pressure is \\( P_{\\alpha} = n_{\\alpha} k T_{\\alpha} \\)\n\\[n = 2: \\quad \\frac{1}{\\gamma - q} n_{\\alpha} \\left( \\pdv{T_{\\alpha}}{t} \u0026#43; \\vec v_{\\alpha} \\cdot \\grad T_{\\alpha} \\right) \u0026#43; P_{\\alpha} \\div \\vec v_{\\alpha} = \\dot{Q}_{\\alpha}\\] where \\( \\dot{Q} _ {\\alpha} \\) includes the thermal conduction (heat flux vector), collisional heating (Ohmic heating), radiation losses, external heating, and so on. In this way, we\u0026rsquo;ve used our equations of state (closures) to express the full equations of motion in terms of our 5N moments \\( (n, \\vec v, T) _ {\\alpha} \\). The closure relations we\u0026rsquo;ve used for higher moment variables are \\[\\overline{\\vec \\Pi}_{\\alpha} = \\nu _{\\alpha} \\grad \\vec v_{\\alpha}\\] \\[\\vec h_{\\alpha} = - \\kappa _{\\alpha} \\grad T_{\\alpha}\\] where the constants \\( \\nu_{\\alpha} \\) and \\( \\kappa_{\\alpha} \\) are called \u0026ldquo;transport coefficients.\u0026rdquo; They represent any physics which are not accurately captured by the LTE system, and so they represent approximations.\nEquations of State # Equations of state are used in defining closure relations. For example, for isothermal conditions, \\( T_{\\alpha} \\) is constant, so \\( P_{\\alpha} \\propto n_{\\alpha} \\).\nAnother assumption might be a force-free, cold assumption, so \\( T_{\\alpha} = 0 \\) and therefore \\( P_{\\alpha} = 0 \\).\nAdiabatic assumption gives \\( P_{\\alpha} \\propto n_{\\alpha} ^\\gamma \\) for adiabatic index \\( \\gamma \\) (ratio of specific heats).\nEquations of Motion # The fields are governed by Maxwell\u0026rsquo;s equations\n\\[\\epsilon_0 \\div \\vec E = \\sum _\\alpha q_\\alpha n_\\alpha\\] \\[\\div \\vec B = 0\\] \\[\\epsilon_0 \\pdv{\\vec E}{t} = \\frac{1}{\\mu_0} \\curl \\vec B - \\sum_\\alpha q_\\alpha n_\\alpha \\vec v _\\alpha\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] The thermal conductivity \\( \\vec \\kappa \\) and electrical resistivity \\( \\vec \\eta \\) are often functions of the magnetic field \\( \\vec B \\).\nSingle-Fluid MHD Model # To reduce the 5N fluid model even further, we work to combine the ions and electrons into a single fluid by applying some asymptotic approximations.\n\\[\\epsilon_0 \\rightarrow 0 (c \\rightarrow \\infty) \\quad \\text{and} \\quad m_e \\rightarrow 0\\] \\[n_e = Z n_i = n \\quad \\text{Quasineutrality}\\] \\[Z = 1, q = e \\quad \\text{Singly-charged}\\] We define our MHD variables as the center-of-mass values:\nThe mass density \\( \\rho \\) is approximately the ion density\n\\[\\rho = n_i m_i \u0026#43; n_e m_e = n m_i \\left( 1 \u0026#43; \\frac{m_e}{m_i} \\right) \\approx n m_i\\] The center-of-mass velocity is approximately the ion velocity\n\\[\\vec v = \\frac{n_i m_i \\vec v_i \u0026#43; n_e m_e \\vec v_e}{n_i m_i \u0026#43; n_e m_e} = \\frac{m_i \\vec v_i \u0026#43; m_e \\vec v_e}{m_i \u0026#43; m_e} \\approx \\vec v_i\\] The pressure is the sum of the species pressures\n\\[P = P_i \u0026#43; P_e\\] And the current density is approximately the electron current density\n\\[\\vec j = n e (\\vec v_i - \\vec v_e) \\approx - n e \\vec v_e\\] We now write the MHD equations in terms of the center of mass quantities\nContinuity:\n\\[\\pdv{\\rho}{t} \u0026#43; \\vec v \\cdot \\grad \\rho = - \\rho \\div \\vec v\\] Momentum:\n\\[\\rho \\left[ \\pdv{\\vec v}{t} \u0026#43; \\frac{m_e \\vec v_e \\cdot \\div \\vec v_e}{m_i \u0026#43; m_e} \u0026#43; \\frac{m_i \\vec v_i \\cdot \\div \\vec v_i}{m_i \u0026#43; m_e} \\right] \u0026#43; \\div P - \\vec j \\cross \\vec B = 0\\] We\u0026rsquo;ll want to revisit the negligible electron mass approximation. If we neglect the electron mass, then the \\( m_e \\vec v_e \\cdot \\grad \\vec v_e \\) term can not necessarily be dropped. However, if we neglect the electron inertia, then we can drop it. Doing so gives\n\\[\\rho \\left( \\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad P - \\vec j \\cross \\vec B = 0\\] The generalized Ohm\u0026rsquo;s law relates current density to the electric field\n\\[\\begin{aligned} \\pdv{\\vec j}{t} \u0026#43; \\div ( \\vec v \\vec j \u0026#43; \\vec j \\vec v) \u0026amp; = \u0026amp; n \\left( \\frac{e^2}{m_e} \u0026#43; \\frac{e^2}{m_i} \\right) ( \\vec E \u0026#43; \\vec v \\cross \\vec B) - \\left( \\frac{e m_i}{m_e} - \\frac{e m_e}{m_i} \\right) \\\\ \u0026amp; \u0026amp; \u0026#43; \\frac{\\vec j \\cross \\vec B}{m_i \u0026#43; m_c}- \\frac{e}{m_i} \\grad P_i \u0026#43; \\frac{e}{m_e} \\grad P_e \u0026#43; \\frac{e}{m_e} \\vec R_{ei} - \\frac{e}{m_i} \\vec R_{ie} \\end{aligned}\\] For \\( m_e \\ll m_i \\) and \\( m_e v_e \\ll m_i v_i \\), we get the single-fluid equation of motion:\n\\[\\frac{1}{\\epsilon_0 \\omega_{p, e} ^2} \\pdv{\\vec j}{t} = \\vec E \u0026#43; \\vec v \\cross \\vec B - \\underbrace{\\frac{1}{en} \\vec j \\cross \\vec B}_{\\text{Hall}} \u0026#43; \\underbrace{\\frac{1}{en} \\div P_e} _ {\\text{diamagnetic}} \u0026#43; \\underbrace{\\frac{1}{en} \\vec R_{ei}}_{\\text{collisions}}\\] Assumptions / Asymptotic Approximations # Let\u0026rsquo;s be specific about our assumptions, and provide justifications.\nThe negligible electron mass \\( m_e \\ll m_i \\) is okay since \\[\\frac{m_e}{m_i} \u0026lt; \\frac{1}{1836} \\approx 0.05 \\%\\] Quasineutrality \\( n_i = n_e \\) is accurate down to some scale. If we have a globally neutral plasma \\( N_i = N_e \\), then scale is the Debye length \\( L \\gg \\lambda_D \\).\nThe negligible electron inertia \\( m_e v_e \\ll m_i v_i \\) is not always justified\n\\[j = n e (v_i - v_e) = n e v_i \\left( 1 - \\frac{v_e}{v_i} \\right)\\] \\[\\frac{m_e v_e}{m_i v_i} = \\frac{m_e}{m_i} \\left( 1 - \\frac{1}{M} \\frac{j}{n e v_{th}} \\right)\\] where the Mach number \\( M \\) is \\( v_i / v_{th} \\).\nLet\u0026rsquo;s see how well this approximation checks out in a laboratory plasma. In the ZaP experiment,\n\\[I = 10^5A \\qquad n = 10^{22} m^{-3} \\qquad v_{th} = 10^5 m/s\\] \\[M = 1 \\qquad a = 0.01m\\] Plugging everything in, we get \\[\\frac{m_e v_e}{m_i v_i} = 0.3\\%\\] So, not a bad assumption. In contrast, in the HIT-SI3 experiment, we have\n\\[I = 10^4 \\qquad n = 10^{19} m^{-3} \\qquad v_{th} = 10^4 m/s \\\\ M = 0.1 \\qquad a = 0.1 m\\] In this case, \\[\\frac{m_e v_e}{m_i v_i} = 10\\%\\] so the single-fluid approximation does not necessarily apply well there. We might think that moving to larger and larger scale lengths inevitably breaks this assumption, but in a really big tokamak (ITER) we do a pretty good job\n\\[I = 10^6 A \\qquad n = 10^{20} m^{-3} \\qquad v_{th} = 10^6 m/s \\\\ M =0.1 \\qquad a = 1 m\\] \\[\\frac{m_e v_e}{m_i v_i} = 0.2 \\%\\] Ultimately, most of our assumptions boil down to a low frequency assumption, since large spatial scales and slow evolution, such that \\( c \\rightarrow \\infty \\) imply low frequency evolution, and the quasineutral condition \\( L \\gg \\lambda_D \\) leads to a large frequency \\( \\omega_p = v_{th} / \\lambda_D \\). For \\( \\omega \\ll \\omega_p \\), we can drop the left-hand side of the generalized Ohm\u0026rsquo;s law\n\\[\\cancel{\\pdv{\\vec j}{t} \u0026#43; \\div ( \\vec v \\vec j \u0026#43; \\vec j \\vec v)} = n \\left( \\frac{e^2}{m_e} \u0026#43; \\frac{e^2}{m_i} \\right) ( \\vec E \u0026#43; \\vec v \\cross \\vec B) - \\left( \\frac{e m_i}{m_e} - \\frac{e m_e}{m_i} \\right) \\\\ \u0026#43; \\frac{\\vec j \\cross \\vec B}{m_i \u0026#43; m_c}- \\frac{e}{m_i} \\grad P_i \u0026#43; \\frac{e}{m_e} \\grad P_e \u0026#43; \\frac{e}{m_e} \\vec R_{ei} - \\frac{e}{m_i} \\vec R_{ie} = 0\\] For quasineutrality, we can also drop the displacement current from Ampere\u0026rsquo;s law\n\\[\\mu_0 \\epsilon_0 \\pdv{\\vec E}{t} \\approx 0 = \\curl \\vec B - \\mu_0 \\vec j\\] \\[c^2 \\rightarrow \\infty, \\quad \\mu_0 \\epsilon_0 = 1/c^2 \\rightarrow 0\\] \\[\\epsilon_0 \\div \\vec E = \\rho_c \\rightarrow 0\\] Taken together, this is the Hall-MHD plasma model, which is the most complete single-fluid MHD model available.\nIf the Hall and diamagnetic terms are negligible, we have well magnetized ions with small Larmor radius \\( r_{L, i} \\):\n\\[\\frac{r_{L, i}}{L} \\ll 1\\] then the generalized Ohm\u0026rsquo;s law becomes\n\\[0 = \\vec E \u0026#43; \\vec v \\cross \\vec B - \\eta \\vec j\\] This is the Resistive MHD model. If we remove collisions entirely (\\( \\eta \\rightarrow 0 \\)), then we have the Ideal MHD Model.\nThe energy equation is still:\n\\[\\pdv{P}{t} \u0026#43; \\vec v \\cdot \\grad P = - \\gamma P \\div \\vec v\\] If we use the definition of energy\n\\[e = \\frac{p}{\\gamma - 1} \u0026#43; \\frac{1}{2} \\rho v^2 \u0026#43; \\frac{B^2}{2 \\mu_0}\\] If we take the time derivative of this,\n\\[\\pdv{e}{t} = \\frac{1}{\\gamma - 1} \\pdv{p}{t} \u0026#43; \\pdv{}{t} \\left( \\frac{1}{2} \\rho v^2 \\right) \u0026#43; \\pdv{}{t} \\left( \\frac{B^2}{2 \\mu_0} \\right)\\] To get the second term, it helps to dot \\( \\vec v \\) with \\( v \\):\n\\[\\vec v \\cdot \\pdv{(\\rho \\vec v)}{t} \u0026#43; \\ldots \\rightarrow \\pdv{}{t} \\left( \\frac{1}{2} \\rho v^2 \\right)\\] and similarly, taking the dot product of \\( \\vec B \\) with the induction equation helps to get the third term\n\\[\\vec B \\cdot \\pdv{\\vec B}{t} \u0026#43; \\ldots \\rightarrow \\pdv{}{t} \\left( \\frac{B^2}{2 \\mu_0} \\right)\\] For all of these assumptions, the single-fluid MHD model is widely applicable to many plasmas.\nNumerical Solution of Fluid Models # The equations of the plasma fluid models result in different characteristics of the dynamics, depending on the model and the physics that are captured by that model. We can get an idea of what those characteristics are by analyzing the governing equation type, which dictates the appropriate numerical algorithm.\nEquation Types # In general, PDEs and systems of PDEs can be classified as elliptic, parabolic, or hyperbolic.\nConsider a general second-order ordinary scalar PDE given by\n\\[a \\pdv{^2 \\psi}{x^2} \u0026#43; b \\frac{\\partial ^2 \\psi}{\\partial x \\partial y} \u0026#43; c \\pdv{^2 \\psi}{y^2} \u0026#43; d \\pdv{\\psi}{x} \u0026#43; e \\pdv{\\psi}{y} \u0026#43; f \\psi = g\\] The canonical equation for this PDE in canonical form is\n\\[a \\left( \\dv{y}{x} \\right)^2 - b \\left( \\dv{y}{x} \\right) \u0026#43; c = 0\\] The PDE is\nelliptic if \\( b ^2 \u0026lt; 4 a c \\) paraboolic if \\( b^2 = 4 a c \\) hyperbolic if \\( b^2 \u0026gt; 4 a c \\) For example, Poisson\u0026rsquo;s equation is an elliptic equation\n\\[\\pdv{^2 \\phi}{x^2} \u0026#43; \\pdv{^2 \\phi}{y^2} = - \\rho / \\epsilon_0\\] \\[b = 0 \\quad 4ac = 4 \\rightarrow b^2 - 4 a c \u0026lt; 0\\] The equation for magnetic diffusion is a parabolic equation\n\\[\\pdv{B_z}{t} = \\frac{\\eta}{\\mu_0} \\pdv{^2 B_z}{x^2}\\] \\[b = 0 \\quad 4 a c = 0 \\rightarrow b^2 - 4 ac = 0\\] Maxwell\u0026rsquo;s equations in a vacuum are hyperbolic:\n\\[\\pdv{\\vec E}{t} = c \\curl \\vec B\\] \\[\\pdv{\\vec B}{t} = - c \\curl \\vec E\\] \\[\\pdv{^2 \\vec B}{t^2} = - c \\curl \\left( \\pdv{\\vec E}{t} \\right) = c^2 \\nabla ^2 \\vec B\\] In 1D, \\[\\pdv{^2 B_z}{t^2} = c^2 \\pdv{^2 B_z}{x^2}\\] \\[b = 0 \\quad 4 a c = - 4 c^2 \\rightarrow b^2 - 4 a c \u0026gt; 0\\] Hyperbolic Equations # For hyperbolic PDEs, the solution at any point depends on a subset of the domain (domain of dependence), and it influences a subset of the domain (domain of influence). This is the same concept as a light cone, where only portions of the domain less than \\( \\pm c t \\) away from the current time can affect the current state.\nThe slope of the curves that define the domains of influence/dependence are the characteristics of the equation. They are velocities; typically hyperbolic PDEs describe wave-like behavior, where the characteristic is the speed of the wave. The solution at a point does not necessarily depend on the boundary conditions; it will only do so when the domain of dependence intersects the boundary.\nParabolic Equations # Parabolic PDEs have solutions that depend on the entire domain, but only at previous times. The domain of dependence includes the boundary conditions, and the initial condition.\nParabolic PDEs are generally associated with diffusion-like behavior. The heat equation, viscous flow are parabolic equations.\nElliptic Equations # For elliptic PDEs, the solution depends on and influences every other point in the domain. The boundary conditions and initial conditions determine the solution at every point in the domain.\nThese equations are associated with steady-state and eigenvalue problems.\nWe\u0026rsquo;ve done this analysis assuming that the constants \\( a \\), \\( b \\), \\( c \\), \u0026hellip; are just that, constant. If they depend on \\( x \\), \\( y \\), or the solution \\( \\psi \\) itself, then the equation type can change.\nSystems of Equations # As we\u0026rsquo;ve written it, the MHD model is not a scalar equation, but rather a system of equations. To deal with systems of PDEs, we typically write the system as a vector equation in conservation form, then examine the eigenvalues:\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{\\vec x} = 0\\] where \\( \\vec Q \\) is the vector of variables, and \\( \\vec F \\) is the vector of fluxes. We can define the flux Jacobian \\( \\overline \\vec A \\equiv \\pdv{\\vec F}{\\vec Q} \\) and rewrite the governing system as an eigenvalue problem\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\overline \\vec A \\pdv{\\vec Q}{\\vec x} = 0\\] The equation type is given by the characteristics of the flux Jacobian:\nElliptic if the eigenvalues of \\( \\overline \\vec A \\) are imaginary. Parabolic if eigenvalues of \\( \\overline \\vec A \\) are repeated and the eigenvectors are not unique. Hyperbolic if the eigenvalues of \\( \\overline \\vec A \\) are real and the eigenvectors are unique. Equation type of Ideal MHD Model # Let\u0026rsquo;s apply this analysis to our ideal MHD model.\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\div \\overline \\vec T = 0\\] where the vector of variables is\n\\[\\vec Q = \\begin{bmatrix} \\rho \\\\ \\rho \\vec v \\\\ \\vec B \\\\ e\\end{bmatrix} \\qquad e = \\frac{P}{\\gamma - 1} \u0026#43; \\frac{\\rho v^2}{2} \u0026#43; \\frac{B^2}{2 \\mu_0}\\] and the flux tensor is\n\\[\\overline \\vec T = \\begin{bmatrix} \\rho \\vec v \\\\ \\rho \\vec v \\vec v - \\frac{ \\vec B \\vec B}{\\mu_0} \u0026#43; \\left( P \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\overline \\vec 1 \\\\ \\vec v \\vec B - \\vec B \\vec v \\\\ \\left( e \u0026#43; P \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\vec v - \\frac{\\vec B \\cdot \\vec v}{\\mu_0} \\vec B \\end{bmatrix}\\] Note that the vector of variables expands out to all of the various components of the vectors we\u0026rsquo;ve written:\n\\[\\vec Q = [ \\rho, \\rho v_x, \\rho v_y, \\rho v_z, B_x, B_y, B_z, e]\\] We call a system of equations \u0026ldquo;conservative\u0026rdquo; if the time derivative of a conserved quantity can be written as a divergence of a flux. The divergence theorem makes this clear\n\\[\\pdv{}{t} \\int Q \\dd V \u0026#43; \\oint \\dd \\vec S \\cdot \\overline{\\vec T} = 0\\] Splitting \\( \\overline{\\vec T} \\) into components along the \\( x \\), \\( y \\), \\( z \\) directions,\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{x} \u0026#43; \\pdv{\\vec G}{y} \u0026#43; \\pdv{\\vec H}{z} = 0\\] where \\[\\vec F = \\begin{bmatrix} \\rho u \\\\ \\rho u^2 - B_x ^2 / \\mu_0 \u0026#43; p \u0026#43; B^2 / 2 \\mu_0 \\\\ \\rho u v - B_x B_y / \\mu_0 \\\\ \\rho u w - B_x B_z / \\mu_0 \\\\ 0 \\\\ u B_y - B_x v \\\\ u B_z - B_x w \\\\ \\left( e \u0026#43; p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) u - \\frac{\\vec B \\cdot \\vec v}{\\mu_0} B_x \\end{bmatrix}\\] and similar for \\( \\vec G \\), \\( \\vec H \\). To determine the equation type of our system in 1 dimension, we want to calculate the eigenvalues of the flux Jacobian of \\( \\vec F \\)\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{x} = 0\\] \\[\\pdv{\\vec Q}{t} \u0026#43; \\underbrace{\\pdv{\\vec F}{\\vec Q}}_{\\vec A} \\pdv{\\vec Q}{x} = 0\\] where \\[\\vec A = \\begin{bmatrix} \\left. \\pdv{F_1}{Q_1} \\right|_{Q_i} \u0026amp; \\left. \\pdv{F_1}{Q_2} \\right|_{Q_i} \u0026amp; \\ldots \\\\ \\left. \\pdv{F_2}{Q_1} \\right|_{Q_i} \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\end{bmatrix}\\] When we compute these derivatives, we need to be careful to keep our other conserved quantities constant. For example,\n\\[\\pdv{F_1}{\\rho} = \\pdv{}{\\rho} (\\rho u) = 0\\] because \\( (\\rho u) \\) is one of the conserved quantities. For \\( F_2 \\),\n\\[F_2 = \\frac{(\\rho u)^2}{\\rho} - \\frac{B_x ^2}{2 \\mu_0} \u0026#43; \\frac{B_y ^2}{2 \\mu_0} \u0026#43; \\frac{B_z ^2}{2 \\mu_0} \u0026#43; p\\] \\[\\pdv{F_2}{Q_1} = - \\frac{(\\rho u )^2}{\\rho ^2} \u0026#43; \\pdv{p}{Q_1} = - u^2 \u0026#43; \\pdv{p}{\\rho}\\] where we can write the pressure in terms of the conserved quantities as\n\\[p = (\\gamma - 1) \\left[ e - \\frac{(\\rho u )^2 \u0026#43; (\\rho v)^2 \u0026#43; (\\rho w)^2}{2 \\rho} - \\frac{B_x ^2 \u0026#43; B_y ^2 \u0026#43; B_z ^2}{2 \\mu_0} \\right]\\] \\[\\pdv{p}{Q_1} = (\\gamma - 1) \\left[ \\frac{(\\rho u )^2 \u0026#43; (\\rho v )^2 \u0026#43; (\\rho w )^2}{2 \\rho ^2} \\right] = (\\gamma - 1) \\frac{u^2 \u0026#43; v^2 \u0026#43; w^2}{2}\\] \\[\\pdv{F_2}{Q_2} = 2 \\frac{(\\rho u)}{\\rho} \u0026#43; \\pdv{p}{Q_2} = 2 (2 - \\gamma) u\\] \\[\\pdv{F_2}{Q_3} = \\pdv{p}{Q_3} = 2(1 - \\gamma) v\\] And so on and so forth for the other elements of \\( \\vec A \\). Once we\u0026rsquo;ve got the flux Jacobian, we can compute the eigenvalues and eigenvectors of \\( \\vec A \\). These will all be real-valued, which tells us that the ideal MHD equations are hyperbolic.\nThe Resistive MHD model can be written as\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\div \\overline{\\vec T} \u0026#43; \\frac{1}{R_m} \\div \\overline{\\vec T_P} = 0\\] We know that \\[\\pdv{\\vec Q}{t} \u0026#43; \\frac{1}{R_m} \\div \\overline{\\vec T_p} = 0\\] If we go through the same process for \\( \\overline{\\vec T_p} \\), we\u0026rsquo;ll find a parabolic system of equations, which is what we would expect from something with \u0026ldquo;resistive\u0026rdquo; in its name. So what sort of system is resistive MHD. The answer is, of course, \u0026ldquo;it depends.\u0026rdquo; Depending on the type of phenomena we expect to see in a particular configuration, we\u0026rsquo;ll have a dominant character associated with either wave phenomena or diffusive phenomena. The dominant character determines the appropriate algorithm to use when solving resistive MHD. For example, a large Reynolds number makes the system more hyperbolic.\n"},{"id":39,"href":"/r/notes/UWAA560/06-spectroscopic-measurements/","title":"Spectroscopic Measurements","section":"Plasma Diagnostics","content":" \\[\\] Spectroscopic Measurements # Plasma Self-Emission # Plasmas emit light (electromagnetic radiation) over a broad range of wavelengths. This can be from the X-ray to UV to visible to IR. The emission depends on the plasma constituents, magnetic field, and plasma properties \\( n_e, T_e, T_i, \\vec v_i \\). We should be able to analyze the self-emission to provide a measurements of these properties. These measurements are both rich in information, and completely non-perturbing. Unfortunately, extracting plasma properties from measurements can be extremely complicated, since the emission is influenced by all of these effects together.\nFree Electron Emission # The primary form of free electron radiation comes from Bremsstrahlung, which is the radiation resulting from the electron\u0026rsquo;s Coulomb interaction with an ion. An electron passing by an ion is deflected through an angle \\( \\theta \\) and radiation is emitted as a result. The degree of reflection depends on the electron\u0026rsquo;s energy and the impact parameter. For a given electron temperature, the impact parameter spans a range up to \\( \\lambda_D \\) and produces a broadband spectrum of radiation.\nConsidering plasma temperatures in the range \\( T_e = 10 eV \\sim 10 keV \\), this emission will be in the soft x-ray range (\\( \\lambda = 0.1 \\sim 100 nm \\)).\nBlackbody radiation is another major source of free electron emission in broadband. Thermal bodies will radiate energy according to\n\\[B(\\nu) = \\frac{\\nu^2}{c^2} \\frac{h \\nu}{e^{h \\nu - T} - 1} \\sim \\frac{\\nu^2 T}{c^2} \\quad (\\text{for low } \\lambda)\\] If we plot the intensity change with frequency, we find that the peak will be near the electron temperature\n\\[T_e \\approx E_\\nu\\] We can measure the x-ray spectrum using filtered detectors. The penetration depth of x-rays depends on their energies, so one can set up a set of detectors behind filters of various thicknesses. With multiple detectors and multiple thicknesses of filter, you can piece together the spectrum. Remember that all photons with high enough energy will pass through the filter, so the resulting ideal acceptance might look like the following:\nTypical materials are metals like Ti, Al, Be, and Ni, and are chosen based on their transmission spectrum. Transmission spectra of real materials are not flat, so you have to convolve the filter transmission spectrum with what you\u0026rsquo;re measuring. Beryllium has a very nice smooth transmission spectrum in the soft x-ray range, but it is very toxic making it difficult to work with. As for the detectors themselves, they must be sufficiently sensitive to x-rays (not all detectors are). X-ray diodes (XRD) are useful, and photomultiplier tubes coupled with an x-ray scintillator are also useful.\nWe know that Bremsstrahlung radiation spectrum shape depends on \\( T_e \\). We can also look at the total Bremsstrahlung radiated power:\n\\[P_{Br} \\approx 5 \\cdot 10^{-37} Z_{eff} n_e ^2 T_e ^{1/2}\\] where \\[Z_{eff} \\equiv \\frac{\\sum_{ions} n_e n_i Z_i ^2}{n_e ^2}\\] Charge neutrality lets us express \\( Z_{eff} \\) as\n\\[Z_{eff} = \\frac{\\sum_i n_i Z_i ^2}{\\sum _i n_i Z_i}\\] For a pure hydrogen plasma \\( Z_{eff} \\) is always 1. Impurities increase \\( Z_{eff} \\), so a measurement is useful to help identify impurity concentrations and resistivity. The measurement requires absolute calibration of the detector and spectrometer, so it\u0026rsquo;s a difficult one to obtain. We can improve the confidence by using corroborating measurements of \\( n_e \\) and \\( T_e \\) from other diagnostics.\nSelf-Emission from Bound Electrons # Electrons still bound to the nucleus will emit radiation as they are excited by collisions, typically with free electrons, and then de-excite to a lower state. The quantum jumps between states produce photons of a well-defined energy, hence line radiation\n\\[E_i - E_j = h \\nu_{ij}\\] In a hydrogen plasma, line radiation is produced only from neutral hydrogen (not ionized). We use H-I to denote emission from the outer-most (I) electron of the hydrogen atom. We often look at impurities, which can be ionized to a higher degree. The emission profile is for a given electron temperature, assuming the ionization population is in thermal equilibrium with the rest of the plasma. For example, C-III indicates that the outer-most electron is the third least-bound, so both of the 2p electrons have been freed: \\[C-III: \\qquad 1s^2 2s^2 (2p^2)\\] We can see that C-III is a Beryllium-like atom. C-V is another common one in high-temperature plasma, and is a Helium-like atom. In thermal equilibrium, the distribution of ions/atoms in any quantum state is given by\n\\[N_i \\propto \\exp \\left( -\\frac{E_i}{T} \\right)\\] and\n\\[\\frac{N_i}{N_j} = \\exp \\left( - \\frac{E_i - E_j}{T} \\right) = \\exp \\left( - \\frac{h \\nu_{ij}}{T} \\right)\\] In general, state \\( E_i \\) can be degenerate, so we add a multiplier \\( g_i \\), \\( g_j \\) in front to correspond with the number of degenerate states. If in thermal equilibrium, that means that the de-excitation rate \\( i \\rightarrow j \\) (emission) is equal to the excitation rate \\( j \\rightarrow i \\).\nIf we assume that these are the only mechanisms involved, the plasma is said to be in coronal equilibrium. Assumption is valid for low-density plasmas, where we have\nno collisional/three-body de-excitation no radiative absorption (optically thin) The line ratio measurements implicitly assume spatial uniformity\nDoppler Broadening # If the radiator is moving, the wavelength shifts according to the relative velocity between the radiator and the viewer.\n\\[\\lambda_{obs} = \\lambda_0 \\left( 1 \\pm \\frac{\\vec v_{i}}{c} \\right)\\] where \\( \\vec v_{i} \\) is the component of the ion velocity towards the observer (spectrometer). Measurements of Doppler broadening are useful for making measurements of the bulk velocity of the plasma. Thermal vibrations will also result in a broadening of the line. The width of the broadening is related to the temperature\n\\[f(\\nu) = \\left( \\frac{m_i}{2 \\pi T_i} \\right)^{3/2} \\exp \\left( - \\frac{ \\frac{1}{2} m_i (v - v_i) ^2}{T_i} \\right)\\] which gives spectral emissivity\n\\[e(\\lambda) = E \\left( \\frac{ m_i}{2 \\pi T_i \\lambda_0 ^2} \\right) ^{1/2} \\exp \\left[- \\frac{(\\lambda - \\lambda_0 \u0026#43; v_i \\lambda_0 / c)^2}{2 T_i \\lambda_0 ^2/m_i c^2} \\right] \u0026#43; B\\] where \\( E \\) is the total emissivity and \\( B \\) is the background signal (e.g. broadband Bremsstrahlung).\n\\[\\Delta \\lambda_{FWHM} = 2 \\lambda_0 \\sqrt{ \\frac{2 \\ln (2) T_i}{m_i c^2}}\\] "},{"id":40,"href":"/r/notes/griffiths/ch1-6/","title":"The Theory of Vector Fields","section":"Griffiths Introduction to Electrodynamics","content":" 1.6: The Theory of Vector Fields # 1.6.1: The Helmholtz Theorem # Ever since Faraday, the laws of electricity and magnetism have been expressed in terms of electric and magnetic fields, E and B. Like many physical laws, these are most compactly expressed as differential equations. Since E and B are vectors, the differential equations naturally involve vector derivatives: divergence and curl. Indeed, Maxwell reduced the entire theory to four equations, specifying respectively the divergence and the curl of E and B.\nMaxwell\u0026rsquo;s formulation raises an important mathematical question: To what extent is a vector function determined by its divergence and curl? In other words, if I tell you that the divergence of F (which stands for E or B, as the case may be) is a specified (scalar) function D,\n\\[\\div \\vec{F} = D\\] and the curl of F is a specified (vector) function C,\n\\[\\curl \\vec{F} = \\vec{C}\\] can you then determine the function F?\nWell\u0026hellip; not quite. For example, as you may have discovered in Prob. 1.20, there are many functions whose divergence and curl are both zero everywhere - the trivial case \\( \\vec{F} = 0 \\), of course, but also \\( \\vec{F} = yz \\vu{x} + zx \\vu{y} + xy \\vu{z}, \\vec{F} = \\sin x \\cosh y \\vu{x} - \\cos x \\sinh y \\vu{y} \\), etc. To solve a differential equation you must also be supplied with appropriate boundary conditions. In electrodynamics we typically require that the fields go to zero \u0026ldquo;at infinity\u0026rdquo; (far away from all charges). With that extra information, the Helmholtz theorem guarantees that the field is uniquely determined by its divergence and curl. (The Helmholtz theorem is discussed in Appendix B.)\n1.6.2: Potentials # If the curl of a vector field (F) vanishes (everywhere), then F can be written as the gradient of a scalar potential (V):\n\\[\\curl \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = - \\grad V \\tagl{1.103}\\] (The minus sign is purely conventional.) That\u0026rsquo;s the essential burden of the following theorem:\nTheorem 1. Curl-less (or \u0026lsquo;irrotational\u0026rsquo;) fields. The following conditions are equivalent (that is, F satisfies one if and only if it satisfies all the others):\n\\[\\tag{a} \\curl \\vec{F} = 0 \\text{ everywhere }\\] \\[\\tag{b} \\int_a ^b \\vec{F} \\cdot \\dd \\vec{l} \\text{ is independent of path, for any given end points}\\] \\[\\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{l} = 0 \\text{ for any closed loop}\\] \\[\\tag{d} \\vec{F} \\text{ is the gradient of some scalar function: } \\vec{F} = - \\grad V\\] The potential is not unique, and any constant can be added to V with impunity, since this will not affect its gradient.\nIf the divergence of a vector field (F) vanishes (everywhere), then F can be expressed as the curl of a vector potential (A):\n\\[\\div \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = \\curl \\vec{A} \\tagl{1.104}\\] That\u0026rsquo;s the main conclusion of the following theorem:\nTheorem 2. Divergence-less (or \u0026lsquo;solenoidal\u0026rsquo;) fields. The following conditions are equivalent:\n\\[\\tag{a} \\div \\vec{F} = 0 \\text{ everywhere}\\] \\[\\tag{b} \\int \\vec{F} \\cdot \\dd \\vec{a} \\text{ is independent of surface, for any given boundary line}\\] \\[\\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{a} = 0 \\text{ for any closed surface.}\\] \\[\\tag{d} \\vec{F} \\text{ is the curl of some vector function: } \\vec{F} = \\curl \\vec{A}\\] The vector potential is not unique - the gradient of any scalar function can be added to A without affecting the curl, since the curl of a gradient is zero.\nIncidentally, in all cases (whatever its curl and divergence may be), a vector field F can be written as the gradient of a scalar plus the curl of a vector:\n\\[\\vec{F} = - \\grad V \u0026#43; \\curl \\vec{A} \\quad \\text{(always)} \\tagl{1.50}\\] In physics, the word field denotes generically any function of position (x, y, z) and time (t). But in electrodynamics two particular fields (E and B) are of such paramount importance as to preempt the term. Thus technically the potentials are also \u0026ldquo;fields,\u0026rdquo; but we never call them that.\n"},{"id":41,"href":"/r/notes/UWAA558/07-equilibrium-for-fusion/","title":"Equilibrium for Fusion","section":"MHD Theory","content":" Equilibrium for Fusion ( \\( \\beta \\) ) # For a fusion device we would like to determine a magnetic configuration that confines plasma while it fuses. At fusion temperatures, the power required to maintain the equilibrium will be substantial. For a device to be useful, the power required to sustain the equilibrium must be less than the power released from fusion. Important loss terms for a confined plasma are transport (thermal conduction primarily) and radiation terms. The scaling factors are \\( P_{Brem} \\sim n^2 T^{1/2} \\) and \\( P_{cycl} \\sim n^2 T^2 \\) for radiation, and \\( P_L \\sim \\frac{3nT}{\\tau_E} \\) for thermal losses.\nWe know that the fusion source term will primarily come from the DT fusion reaction\n\\[\\text{D} \u0026#43; \\text{T} \\rightarrow \\text{He}^4 (3.5\\, MeV) \u0026#43; \\text{n} (14.1\\, MeV)\\] The primary fusion reaction releases an \\( \\alpha \\) -particle and a high-energy neutron. The concept of ignition is that the neutron leaves the plasma, and the \\( \\alpha \\) (with energy \\( E_\\alpha = 3.5 MeV) \\) remains to heat the plasma.\n\\[P_\\alpha = \\frac{1}{4} n^2 \\langle \\sigma v \\rangle E_\\alpha \\qquad \\text{(assuming} \\quad n_D = n_T = n/2 \\text{)}\\] \\[P_\\alpha \u0026gt; P_L \\quad \\rightarrow \\quad n \\tau_E \u0026gt; \\frac{12 T}{E_\\alpha \\langle \\sigma v \\rangle}\\] To sustain fusion, we set the fusion heating term above the thermal loss term. The reaction cross-section \\( \\sigma \\) can be maximized to give the Lawson criterion\n\\[n \\tau_E \u0026gt; 10^{14} s / cm^3\\] The Lawson criterion only applies at fusion temperatures, but it is a useful parameter even outside of ignition since it gives a ratio of fusion power to lost power\n\\( T_i (keV) \\) \\( \\langle \\sigma v \\rangle (cm^3 / s) \\) Required \\( n \\tau_E (s / cm^3) \\) 1 \\( 7 \\cdot 10^{-21} \\) \\( 5 \\cdot 10^{17} \\) 5 \\( 1.4 \\cdot 10^{-17} \\) \\( 1.2 \\cdot 10^{15} \\) 20 \\( 4.3 \\cdot 10^{-16} \\) \\( 1.6 \\cdot 10^{14} \\) 60 \\( 8.7 \\cdot 10^{-16} \\) \\( 2.4 \\cdot 10^{14} \\) We can see that the required \\( n \\tau_E \\) actually has a minimum around \\( 20 keV \\) (at least, as far as the data in the table goes). Even though the maximum cross-section is at a much higher temperature, what we\u0026rsquo;re really concerned with is the ratio of the fusion source term to the thermal loss term, which is linear in temperature.\nMHD equilibrium does not place a limit on the density \\( n \\) . Instead, it places a limit on \\( \\beta \\) in order to achieve equilibrium force-balance \\( (\\beta = 1) \\) \\[\\beta = \\frac{n (T_e \u0026#43; T_i)}{B^2 / 2 \\mu_0} \\rightarrow n = \\frac{ \\beta B^2}{4 \\mu_0 T}\\] In this form, we can more clearly see what our options are to achieve MHD equilibrium. Some devices (large-scale tokamaks) are able to achieve the requisite confinement time at a low \\( \\beta \\) by making use of very strong magnetic fields. Other devices are able to make use of more modest magnetic fields by working at a higher \\( \\beta \\) .\nTherefore,\n\\[\\tau_E \u0026gt; \\frac{1}{\\beta B^2} \\frac{48 \\mu_0}{E_\\alpha} \\frac{T^2}{\\langle \\sigma v \\rangle}\\] The term \\( \\frac{T^2}{\\langle \\sigma v \\rangle} \\) has a minimum at \\( 10-20 keV \\) . At 15 keV and a magnetic field of \\( 5T \\) (many actual components cannot reasonably exceed such magnetic fields) then\n\\[\\tau_E \u0026gt; \\frac{0.1}{\\beta} \\text{s}\\] For a large-scale toroidal device with \\( \\beta = 1\\% \\) , the confinement time \\( \\tau_E \u0026gt; 10s \\) . If we consider a common diffusivity (how fast energy will leave due to thermal conductivity) \\( D_E \\approx 1 m^2 / s\\) , so for a characteristic radius \\( a \\) \\[\\tau_E \\approx \\frac{a^2}{4 D_E} \\rightarrow a \u0026gt; 6.3 \\text{m}\\] This gives you a sense of why low- \\( \\beta \\) devices need to be so large. Instead, if we consider \\( \\beta \\sim 50\\% \\) , \\( \\tau_E \u0026gt; 0.2 \\text{s} \\) and\n\\[\\beta \\sim 50\\% \\rightarrow a \u0026gt; 0.9 \\text{m}\\] When you consider that the cost of a device (to first order) scales with the volume of the device, achieving a high \\( \\beta \\) is very important for fusion equilibrium. However, when we consider MHD stability we are generally forced into lower \\( \\beta \\) to avoid destructive instabilities. Configuration optimization is the process of balancing this trade-off.\nVirial Theorem # Application of the virial theorem to energy balance for the stress tensor \\( \\vec T \\) tells us that MHD equilibria must be supported by externally supplied currents. Many times you\u0026rsquo;ll hear of theoretical designs for compact toroid devices which can maintain stability under their own currents, but they are the MHD stability equivalent of a perpetual motion machine. A compact toroid cannot exist unsupported.\nWriting static equilibrium:\n\\[\\div \\left[ - \\frac{ \\vec B \\vec B}{\\mu_0} \u0026#43; \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0}\\right) \\vec I \\right] = \\div \\vec T = 0\\] If we define the direction of the magnetic field to be \\( \\vu e _B = \\vu z \\) then\n\\[\\vec T = p_\\perp ( \\vec I - \\vu e_B \\vu e_B ) \u0026#43; p_\\parallel \\vu e_B \\vu e_B \\\\ = \\begin{bmatrix} p_\\perp \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; p_\\perp \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; p_\\parallel \\end{bmatrix}\\] where\n\\[p_\\perp = p \u0026#43; \\frac{B^2}{2 \\mu_0}\\] and \\[p_\\parallel = p - \\frac{B^2}{2 \\mu_0}\\] A gradient vector identity gives\n\\[\\div (\\vec r \\cdot \\vec T) = \\vec r \\cdot ( \\div \\vec T) \u0026#43; \\vec T \\cdot \\cdot \\grad \\vec r\\] Integrating this expression over a volume and assuming that the volume contains a confined MHD equilibrium that is self-contained and self-supported:\n\\[\\int_V \\div ( \\vec r \\cdot \\vec T) \\dd V = \\int_V (\\vec r \\cdot \\overbrace{\\cancel{(\\div \\vec T)}}^{\\text{MHD equil.}} \u0026#43; \\vec T \\cdot \\cdot \\grad \\vec r) \\dd V\\] \\[\\grad \\vec r = \\vec I\\] so \\[\\vec T \\cdot \\cdot \\grad \\vec r = p_\\perp \u0026#43; p_\\perp \u0026#43; p_\\parallel \\\\ = 3p \u0026#43; \\frac{B^2}{2 \\mu_0}\\] \\[\\int_V (3p \u0026#43; \\frac{B^2}{2 \\mu_0} ) \\dd V = \\int _V \\div ( \\vec r \\cdot \\vec T) \\dd V = \\oint _S (\\vec r \\cdot \\vec T) \\cdot \\vu n \\dd S \\\\ = \\oint _S \\left[ \\vec r \\cdot \\vec I p_\\perp \u0026#43; \\vec r \\cdot \\vu e_B \\vu e_B (p_\\parallel - p_\\perp) \\right]\\cdot \\vu n \\dd S \\\\ =\\oint \\left[ \\left( \\cancel{p} \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\vu r \\cdot \\vu n - \\frac{B^2}{\\mu_0} (\\vec r \\cdot \\vu e_B)(\\vu e_B \\cdot \\vu n) \\right] \\dd S\\] Beyond where the plasma is contained, the pressure does not contribute \\( p = 0 \\) . If all current sources are contained in the configuration, the magnetic field \\( \\sim 1/r^3 \\) for a dipole, \\( \\sim 1/r^4 \\) for a quadrupole, etc. Therefore the right-hand side will fall off like\n\\[RHS \\propto \\oint_S B^2 r \\dd S \\propto \\left( \\frac{1}{r^3} \\right) ^2 r r^2 \\propto \\frac{1}{r^3} \\text{(dipole)}\\] so \\( RHS \\rightarrow 0 \\) as \\( r \\rightarrow \\infty \\) . But what about the left-hand side? Both of the terms in the volume integral are positive definite, so the LHS must be positive finite and the equality can\u0026rsquo;t possibly hold. The assumption that the plasma is self-contained must be invalid. This tells us that we must have external currents.\nMagnetic Flux Surfaces # The vast difference in thermal conductivity parallel and perpendicular to the magnetic field in a plasma confinement configuration leads to an avoidance of any open field lines. Magnetic equilibria are generally toroidal to eliminate end losses from open configurations. In general fusion confinement devices, magnetic field lines lie on a set of closed nested toroidal surfaces. This means that we can no longer describe any equilibria in a solely 1D geometry. The minor radius is no longer the only important scale length.\nFrom \\( \\vec j \\cross \\vec B = \\grad p \\) , we know that the pressure gradient is perpendicular to \\( \\vec j \\) and \\( \\vec B \\) , and therefore both \\( \\vec B \\) and \\( \\vec j \\) lie on surfaces of uniform pressure. We call these toroidal surfaces either magnetic surfaces or flux surfaces. We can use these surfaces to build a 1-dimensional description.\nAs a brief aside, some geometrical vocabulary will be useful when describing toroidal geometry. A toroid is any surface of revolution with a hole in the middle. A torus is the particular case of a toroid in which the revolved figure is a circle.\nWe will define our global toroidal coordinate system to consist of the major axis \\( (z) \\) , the distance from the major axis \\( (R) \\) , and the azimuthal angle around the major axis \\( (\\phi) \\) .\nWe will also make use of a poloidal coordinate system measured by minor radius (distance from the minor axis) \\( r \\) and the poloidal angle from the minor axis \\( \\theta \\) . We will generally refer to a point on the torus relative to the major axis \\( (R, z, \\phi) \\) , or relative to the minor axis \\( (r, \\theta, \\phi) \\) , or in spherical coordinates \\( (R, \\theta, \\phi) \\) . The major radius \\( R_0 \\) is the distance from the major axis to the minor axis. The minor radius \\( a \\) is the characteristic distance from the minor axis to the exterior of the revolved figure. Usually we will find symmetry under \\( \\phi \\) .\nThe aspect ratio of a torus is the ratio of the major radius to the minor radius.\n\\[A = \\frac{R_0}{a}\\] When we talk about a \u0026ldquo;toroidal surface,\u0026rdquo; we mean a cross-section of the toroidal rotation. When we talk about a \u0026ldquo;poloidal surface\u0026rdquo; we mean a surface which is coplanar with the minor axis:\nThe poloidal flux is determined by the size of the poloidal surface and the poloidal magnetic field:\n\\[\\Psi _p = \\int_{S_p} \\vec B \\cdot \\dd \\vec S\\] and the toroidal flux is determined by the size of a toroidal surface and the toroidal magnetic field:\n\\[\\Psi_t = \\int_{S_t} \\vec B \\cdot \\dd \\vec S\\] Considering the poloidal flux, we can see that if we expand the size of the surface towards the minor radius, the flux will increase until eventually we come to a point where the flux begins to decrease. The position of this maximum is called the magnetic axis, which does not necessarily correspond to the minor axis. In fact, it is generally displaced from the minor axis.\nTo refer back to something more familiar, we\u0026rsquo;ll define the same terms for a cylindrical geometry \\((r, \\theta, z) \\) . An axial surface corresponds with a toroidal surface, and an azimuthal surface corresponds with a poloidal surface:\nIf we consider the trajectory of a single field line, what sorts of surfaces will it trace out? What surface will contain the field line? As it turns out, there are three options:\nRational surface - the field line closes on itself, and it does so after a finite number of revolutions. One way to quickly visualize such a surface is to draw a Poincar puncture plot. Choose a toroidal plane and plot a point wherever the field line punctures the surface. A Poincar puncture plot of a rational surface contains a finite number of points and no continuous curves. Ergodic surface - the field line completely covers an entire surface, which is to say the field line punctures any toroidal surface an infinite number of times. In other words, it never closes on itself and defines an irrational curve. Stochastic region - In this case, there is no definite surface and the field line fills a volume. Generally rational surfaces and ergodic surfaces are largely equivalent, but by introducing a small amount of resistivity a rational surface can lead to magnetic islands. One can imagine the addition of resistivity equivalent to allowing a small degree of motion of the magnetic field lines. In an ergodic surface, a flux surface is defined by a single (irrational) field line. If it moves toward itself in one location it will necessarily move away from itself in another location. But in a rational surface, different field lines can lie on the same constant pressure surface and will tend to move towards each other. By concentrating into magnetic islands, the flux surfaces are now more closely spaced, and the pressure gradient increases (a bad thing!)\nSurface quantities: Since pressure, current (not current density!), and flux (not field!) are constant along a flux surface, it is convenient to use flux \\( \\Psi_p \\) as a coordinate. A particular poloidal flux itself uniquely determines a poloidal surface with constant pressure and current. The flux surface quantities are \\( p, \\, \\Psi_p, \\, \\Psi_t, \\, I_p = \\int_{S_p} \\vec j \\cdot \\dd \\vec S, \\, I_t = \\int_{S_t} \\vec j \\cdot \\dd \\vec S\\) .\nSurface quantities are not independent. The poloidal current \\( I_p \\) affects the toroidal field \\( B_t \\) and toroidal flux \\( \\Psi_t \\) . The toroidal current affects the poloidal field \\( B_p \\) and poloidal flux \\( \\Psi_p \\) .\n\\[B_t = \\vec B \\cdot \\vu \\phi\\] \\[\\vec B_p = B_\\theta \\vu \\theta \u0026#43; B_z \\vu z \\\\ = B_r \\vu R \u0026#43; B_z \\vu z\\] Toroidal Force Balance # Toroidal equilibria solves the end losses of linear equilibrium, but generates a new force which must be balanced. This is a result of the virial theorem.\nPoloidal Fields (Tire Tube Pressure Force) # If you think of a flexible bike tire being inflated, as the pressure within the inner tube increases, the major radius will increase! Why is this? The pressure within the tire is isotropic. As we pump up the tire, the pressure increase causes a force imbalance with the atmospheric pressure that causes the tube to expand. To simplify, we can consider a square tube with inner \u0026ldquo;radius\u0026rdquo; \\( a \\) . The radial force scales with the pressure over the outer and inner surfaces\n\\[F_r = p S_{outer} - p S_{inner} \\\\ \\sim p (R_0 \u0026#43; a) a - p (R_0 - a) a \\sim 2 a^2 p\\] \u0026ldquo;Missing content here on how to balance radial forces for purely poloidal fields (axisymmetric) with a conducting wall\u0026rdquo; Alternatively, we can use vertical external field coils to increase the field strength near the outer wall. They will also tend to center the plasma between the coils, so they have the advantage of preventing the plasma from drifting upwards or downwards\nToroidal Fields # Now let\u0026rsquo;s consider driving a toroidal field in the plasma using a central coil on the major axis. Driving a toroidal field \\( B_\\phi \\) will also create a poloidal current \\( j_\\theta \\) . We know that the driven field will decay as \\( 1/R \\) through the plasma. In actuality, the relationship is not perfectly \\( 1/R \\) because of the generated poloidal current, which will modify the field within the plasma. Depending on the orientation of \\( j_\\theta \\) , it could either increase or decrease the poloidal field within the plasma. These are two different operations by which the plasma interacts with the magnetic field. When the induced plasma current tends to increase the field, we have a paramagnetic effect. In the opposite case (decrease) there is a diamagnetic effect. Generally, plasmas are diamagnetic, but there are certain situations where they become paramagnetic.\nHere again, we have a larger field on the in-bore side than on the out-bore side, so there will be a force imbalance tending to push the plasma towards larger radii. We might consider surrounding the plasma with a conducting wall, as we did previously, but we run into a difficulty determining a current distribution in the wall which would balance the radial effect. Any current distribution must be circular; because of the geometry of a torus such a current distribution would have the same force contribution on both the inside and outside edges. There is no way to stabilize the distribution with purely toroidal fields.\n"},{"id":42,"href":"/r/notes/UWAA543/ch21-2/","title":"Explicit Finite Difference Algorithms","section":"Computational CFD","content":" Explicit Finite Difference Algorithms # Explicit Algorithms # Forward Euler (FTFS) # n+1 --------|--------*--------|-------- n --------|--------*--------*-------- j-1 j j+1 \\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{a \\Delta t}{\\Delta x} (u_{j\u0026#43;1} ^n - u_j ^n)\\] The accuracy is \\( O(\\Delta t, \\Delta x) \\). Previously we saw that the von Neumann stability analysis shows\n\\[a \u0026gt; 0 \\rightarrow \\text{ Unconditionally unstable}\\] \\[a \u0026lt; 0 \\rightarrow \\left| \\frac{ a \\Delta t}{\\Delta x} \\leq 1 \\right| \\quad \\text{ for stability }\\] Forward-Time Centered Space (FTCS) # \\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{a \\Delta t}{\\Delta x} \\frac{u_{j\u0026#43;1} ^n - u_{j-1} ^n}{2}\\] We also found that this is unconditionally unstable.\nLeap-Frog # \\[u_{j}^{n\u0026#43;1} = u_j ^{n-1} - \\frac{a \\Delta t}{\\Delta x} (u_{j\u0026#43;1} ^n - u_{j-1} ^n)\\] n+1 --------|--------*--------|-------- n --------*--------|--------*-------- n-1 --------|--------*--------|-------- j-1 j j+1 Accuracy: \\( O(\\Delta t^2, \\Delta x^2) \\)\nStability: It is marginally stable (\\( G = 1 \\)) if \\( \\left| \\frac{ a \\Delta t}{\\Delta x} \\right| \\leq 1 \\)\nIt is also time-reversible, so we call it a symplectic integrator. This is a good thing! A time-reversible integrator will conserve energy.\nOne big problem with Leap-Frog is the starting problem. How do we compute the very first iteration to get \\( n=1 \\) when we don\u0026rsquo;t have information about \\( n=-1 \\)? We need to \u0026ldquo;start\u0026rdquo; the algorithm by calculating the first step using a different method.\nLax Algorithm # \\[u_j ^{n\u0026#43;1} = \\frac{u_{j\u0026#43;1} ^n \u0026#43; u_{j-1} ^n}{2} - \\frac{a \\Delta t}{\\Delta x} \\left( \\frac{ u_{j\u0026#43;1} ^n - u_{j-1} ^n}{2} \\right)\\] Accuracy: \\( O(\\Delta t, \\Delta x^2/\\Delta t) \\)\nStability: \\( \\left| \\frac{ a \\Delta t}{\\Delta x} \\right| \\leq 1 \\)\nIt may not be consistent if \\( \\Delta t \\rightarrow 0 \\) faster than \\( \\Delta x^2 \\rightarrow 0 \\)\nWe can re-write the Lax algorithm as\n\\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{ a \\Delta t}{\\Delta x} \\left( \\frac{ u_{j\u0026#43;1} ^n - u_{j-1} ^n}{2} \\right) \u0026#43; \\frac{ u_{j\u0026#43;1} ^n - 2 u_j ^n \u0026#43; u_{j-1} ^n}{2}\\] Check out the last term - it\u0026rsquo;s a difference operator for the second derivative. So what PDE do we approximate?\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = \\frac{\\Delta x^2}{\\Delta t} \\pdv{^2 u}{x^2}\\] So we\u0026rsquo;ve added a second-derivative term to the linear advection equation. A second-derivative term acts as an artificial viscosity term, with viscosity \\( \\Delta x^2 / \\Delta t \\).\nLax-Wendroff Algorithm # As we just saw, adding an artificial viscosity term can help to stabilize an unstable algorithm. If we take a Taylor series expansion\n\\[u_j ^{n\u0026#43;1} = u_j ^n \u0026#43; \\Delta t u_t \u0026#43; \\frac{ \\Delta t ^2}{2} u_{tt} \u0026#43; \\frac{\\Delta t^3}{6} u_{ttt} \u0026#43; \\ldots\\] and use our governing equation to replace some of the time derivatives with spatial derivatives\n\\[u_t = - c u_x \\] \\[u_{tt} = c^2 u_{xx}\\] Then we get\n\\[u_{j} ^{n\u0026#43;1} = u_j ^n - c \\Delta t u_x \u0026#43; \\frac{c^2 \\Delta t^2}{2} u_{xx} - \\ldots\\] Substitute into expansion using centered finite difference operators\n\\[\\pdv{u}{x} \\approx \\frac{u_{j\u0026#43;1} ^n - u_{j-1} ^n}{2 \\Delta x} \\qquad \\pdv{^2 u}{x^2} \\approx \\frac{u_{j\u0026#43;1} ^n - 2 u_j ^n \u0026#43; u_{j-1} ^n}{2 \\Delta x^2}\\] \\[\\rightarrow u_j ^{n\u0026#43;1} = u_j ^n - \\frac{ a \\Delta t}{2 \\Delta x} ( u_{j\u0026#43;1} ^n - u_{j-1} ^n) \u0026#43; \\frac{ a^2 \\Delta t^2}{2 \\Delta x ^2}(u_{j\u0026#43;1} ^n - 2 u_j ^n \u0026#43; u_{j-1} ^n)\\] Checking back up the page, we see the FTCS embedded directly in the algorithm, and we\u0026rsquo;ve got the same form of the second derivative but with a different multiplier. Instead of a \\( 1 \\) we\u0026rsquo;ve got \\( \\frac{ a^2 \\Delta t^2}{\\Delta x^2} \\)\nWhat PDE are we approximating?\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = a^2 \\Delta t \\pdv{^2u}{x^2}\\] Again we\u0026rsquo;ve added artificial viscosity, but now the viscosity only depends directly on \\( \\Delta t \\) and goes to zero as \\( \\Delta t \\rightarrow 0 \\).\nWe find that the Lax-Wendroff algorithm is \\( O(\\Delta t^2, \\Delta x^2) \\)\nn+1 --------|--------*--------|-------- n --------*--------*--------*-------- j-1 j j+1 Another advantage of Lax-Wendroff over the Leap-Frog algorithm is that it avoids the starting problem. We don\u0026rsquo;t need any information from step \\( n-1 \\), so it doesn\u0026rsquo;t need to be started. How can that happen?\nWrite Lax-Wendroff as a 2-step method (predictor-corrector). Say we\u0026rsquo;re going to use a Lax algorithm to advance to a midpoint\nn+1 --------|-----------------*-----------------|-------- n+1/2 -----------------*-----------------*----------------- j-1/2 j+1/2 n --------|-----------------*-----------------|-------- j-1 j j+1 \\[u_{j\u0026#43;\\frac{1}{2}} ^{n \u0026#43; \\frac{1}{2}} = \\frac{u^n _{j\u0026#43;1} \u0026#43; u_j ^n}{2} - \\frac{a \\Delta t}{\\Delta x} \\left( \\frac{ u_{j\u0026#43;1} ^n - u_j ^n}{2} \\right)\\] \\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{a \\Delta t}{\\Delta x} \\left( u_{j\u0026#43;1/2} ^{n\u0026#43;1/2} - u_{j-1/2} ^{n\u0026#43;1/2} \\right)\\] That\u0026rsquo;s why Lax-Wendroff is the most popular explicit algorithm for linear PDEs.\n"},{"id":43,"href":"/r/notes/UWAA545/07-finite-difference-models/","title":"Finite Difference Methods for MHD","section":"Computational Methods For Plasmas","content":" Finite Differencing # So for 1D MHD, we want to derive algorithms to advance the governing equations in time that have the form\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{x} = 0\\] We can use the advection equation as a model equation: \\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = 0\\] which has the general solution \\[u(x - at) = \\text{const.} = u(x, t=0)\\] If we apply a forward Euler finite difference operators to our PDE,\n\\[\\frac{u_j ^{n\u0026#43;1} - u^n _j}{\\Delta t} \u0026#43; a \\frac{u _{j\u0026#43;1} ^n - u_{j-1} ^n}{2 \\Delta x} = 0\\] Stability # A brief von Neumann stability analysis shows that this forward Euler differencing is unstable. The way von Neumann stability analysis works, we assume a wave structure for the solution \\[u_j ^n = A^n e^{i k x} = \\underbrace{A ^n}_{\\text{wave amplitude}} \\cdot \\overbrace{e^{i k j \\Delta x}}^{\\text{wave structure}}\\] If we substitute this form into our finite difference expression, we have\n\\[A^{n\u0026#43;1} e^{i k j \\Delta x} = A^n e^{i k j \\Delta x} - \\frac{a \\Delta t}{2 \\Delta x}\\left(A^n e^{i k (j\u0026#43;1) \\Delta x} - A^n e^{i k (j-1) \\Delta x} \\right)\\] We\u0026rsquo;re interested in the growth of the amplitude \\( A^n \\), called the amplification factor \\( G \\): \\[G \\equiv \\frac{|A^{n\u0026#43;1}|}{|A^n|}\\] For forward Euler, we find \\[\\begin{aligned} \\frac{A^{n\u0026#43;1}}{A^n} \u0026amp; = \u0026amp; 1 - \\frac{a \\Delta t}{2 \\Delta x} \\left( e^{i k \\Delta x} - e^{- i k \\Delta x} \\right) \\\\ \u0026amp; = \u0026amp; 1 - \\frac{i a \\Delta t}{\\Delta x} \\sin (k \\Delta x) \\\\ G \u0026amp; = \u0026amp; \\sqrt{\\left( \\frac{A^{n\u0026#43;1}}{A^n} \\right) \\left( \\frac{A^{n\u0026#43;1}}{A^n} \\right)^\\star} \\\\ \u0026amp; = \u0026amp; \\sqrt{1 \u0026#43; \\left( \\frac{a \\Delta t}{\\Delta x}\\right) ^2 \\sin ^2 (k \\Delta x)} \\\\ \u0026amp; \\geq \u0026amp; 1 \\quad \\text{if} \\quad \\Delta t \u0026gt; 0 \\end{aligned}\\] We find that the forward time, centered space (FTCS) algorithm is unconditionally unstable, for any values of \\( k \\), \\( \\Delta t \u0026gt; 0 \\). Not a very good choice for solving the advection equation! However, the FTCS algorithm is an important component of many finite difference schemes which are stable.\nPhysically, the difficulty with the advection term can be seen in the continuity equation\n\\[\\pdv{\\rho}{t} \u0026#43; \\vec v \\cdot \\grad \\rho = - \\rho \\div \\vec v = 0 \\quad \\text{(incompressible)}\\] Using the FTCS difference approach to compute the gradients, we\u0026rsquo;re assuming that the gradient at node \\( j \\) is constant over the time interval \\( n \\rightarrow n + 1 \\). This is only valid if the density profile is linear, and for non-linear profiles the gradient is inaccurate.\nIf we approximate the gradient of the density as\n\\[\\pdv{\\rho}{x} \\approx \\frac{\\rho _{j\u0026#43;1} - \\rho_{j - 1}}{2 \\Delta x}\\] then we get an error that is the difference between the line tangent at \\( j - 1 \\) and the line tangent at \\( j \\). We should actually calculate the gradient at \\( t + \\Delta t / 2 \\) (assuming the flow is in the positive direction). The error leads to an over-estimate of the gradient magnitude, which leads to oscillations, which ultimately leads to instability.\nStable Solution Approaches for Advection Equation # There are a few approaches we can take to come up with a finite differencing scheme which avoids this instability.\nSince the error is oscillating positive and negative, we can add diffusion to smooth out the error. This is the same as applying a time-average of the error to smooth the solution. Spatial smoothing also works:\nNoting that G increases with \\( k \\Delta x \\), the shortest wavelengths grow the fastest, which is why a diffusive term helps smooth out the instability.\nSince the error is caused by computing the gradient at the wrong location, e.g. \\( x_j \\) instead of \\( x_j - v \\Delta t / 2 \\), we can use upwind differencing \\[\\delta _x \\rho _j = \\begin{cases} \\frac{\\rho _j - \\rho _{j-1}}{\\Delta x} \u0026amp; \\quad \u0026amp; \\text{ if} \\quad v \\geq 0 \\\\ \\frac{\\rho_{j\u0026#43;1} - \\rho_j}{\\Delta x} \u0026amp; \\quad \u0026amp; \\text{if } v \u0026lt; 0 \\end{cases}\\] Flux can be calculated as before, but we can apply limits to prevent minima or maxima from developing. Some approaches like this in the literature are total variation diminishing total variation bounded flux corrected transport Eliminate the advection term, or reduce it. If we write out the MHD equations in such a way that the advection term \\( \\vec v \\cdot \\grad \\) is split out from the rest: \\[\\pdv{\\rho}{t} \u0026#43; \\vec v \\cdot \\grad \\rho = - \\rho \\div \\vec v\\] \\[\\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v = ( - \\grad p \u0026#43; \\vec j \\cross \\vec B) / \\rho\\] \\[\\pdv{\\vec B}{t} \u0026#43; \\vec v \\cdot \\grad \\vec B = - \\vec B \\div \\vec v \u0026#43; \\vec B \\cdot \\grad \\vec v\\] \\[\\pdv{p}{t} \u0026#43; \\vec v \\cdot \\grad p = - \\Gamma p \\div \\vec v\\] If we let the grid move at the fluid velocity \\( \\vec v \\)\n\\[\\vec u _{grid} = \\vec v\\] then the advection term becomes\n\\[\\vec v \\cdot \\grad \\rightarrow ( \\vec v - \\vec u _{grid}) \\cdot \\grad\\] which vanishes. Codes like this, expressing the equations of motion in Lagrangian frame of reference, are generally called ALE codes (Arbitrary Lagrangian Eulerian).\nMethod 1: Lax Algorithm # To solve\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{t} = 0\\] we had written the FTCS finite difference scheme:\n\\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{a \\Delta t}{2 \\Delta x} \\left( u_{j\u0026#43;1} ^n - u_{j-1} ^n \\right)\\] We can add diffusion by replacing \\( u_j ^n \\) with a local spatial average, giving us the Lax algorithm:\n\\[u_j ^n \\rightarrow \\frac{u_{j\u0026#43;1} ^n \u0026#43; u_{j - 1} ^n}{2} \\quad \\text{Lax Algorithm}\\] If we apply the same von Neumann stability analysis as before, we find that\n\\[G = \\sqrt{1 - \\left[1 - \\left(\\frac{a \\Delta t}{\\Delta x}\\right)^2 \\right] \\sin ^2 (k \\Delta x)}\\] So we have a stable solution if \\[\\left| \\frac{a \\Delta t}{\\Delta x} \\right| \\leq 1 \\quad \\text{Courant condition}\\] This condition (also called the CFL condition) shows up quite a lot in fluid dynamics algorithms. It effectively limits the time step \\[\\Delta t \\leq \\Delta x / a\\] If we carry the substitution through our finite difference equation, we can see how the Lax method really does add a diffusion term:\n\\[u_j ^{n\u0026#43;1} = \\underbrace{u_j ^n - \\frac{a \\Delta t}{2 \\Delta x} (u_{j\u0026#43;1} ^n - u_{j - 1} ^n)}_{\\text{FTCS}} \u0026#43; \\underbrace{\\left( \\frac{u_{j\u0026#43;1} ^n - 2 u_j ^n \u0026#43; u_{j-1} ^n}{2} \\right)}_{\\text{Lax modification}}\\] Manipulating the terms a bit,\n\\[\\frac{u_j ^{n\u0026#43;1} - u_j ^n}{\\Delta t} \u0026#43; a \\left( \\frac{u_{j\u0026#43;1} ^n - u_{j-1} ^n}{2 \\Delta x} \\right) = \\frac{\\Delta x^2}{2 \\Delta t} \\left( \\frac{u_{j\u0026#43;1} ^n - 2 u_j ^n \u0026#43; u_{j-1} ^n}{\\Delta x ^2} \\right)\\] which approximates the PDE\n\\[\\underbrace{\\pdv{u}{t} \u0026#43; a \\pdv{u}{x}}_{\\text{original PDE}} = \\underbrace{\\frac{\\Delta x ^2}{2 \\Delta t} \\pdv{^2 u}{x^2}}_{\\text{diffusion term}}\\] In the limit of \\( \\Delta x \\rightarrow 0 \\) goes to zero, the diffusion term disappears and we recover the original PDE. It has the unfortunate property that the diffusion term grows as \\( \\Delta t \\rightarrow 0 \\), so we only retain accuracy as long as \\( \\Delta x ^2 \\) goes to zero faster than \\( \\Delta t \\).\nLax-Wendroff Algorithm # Another algorithm known as the Lax-Wendroff algorithm can be less diffusive and is better behaved. It\u0026rsquo;s also derived from a more mathematically rigorous method than the Lax method. We get it through a process called a Cauchy-Kowalevskaya method.\nWe can derive Lax-Wendroff as a 2-step method:\nUse the Lax algorithm to advance the solution from \\( t \\) to \\( t + \\Delta t / 2 \\) Use the leapfrog algorithm to advance from \\( t \\) to \\( t + \\Delta t \\) \\[u_{j \u0026#43; 1/2} ^{n \u0026#43; 1/2} = \\frac{u_{j\u0026#43;1} ^n \u0026#43; u_j ^n}{2} - \\frac{a \\Delta t}{2 \\Delta x} \\left( u_{j \u0026#43; 1} ^n - u_j ^n \\right) \\quad \\text{(Lax advance)}\\] \\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{a \\Delta t}{\\Delta x} \\left(u _{j \u0026#43; 1/2} ^{n \u0026#43; 1/2} - u _{j - 1/2} ^{n \u0026#43; 1/2} \\right) \\quad \\text{(Leapfrog advance)}\\] If we write out the stencil for Lax-Wendroff, we see that the algorithm is properly centered\nFor linear equations, the two steps can be combined, and we can see that it\u0026rsquo;s nothing but the combination of FTCS with a diffusion term\n\\[u_j ^{n\u0026#43;1} = u_j ^n - \\underbrace{\\frac{a \\Delta t}{2 \\Delta x} \\left( u_{j \u0026#43; 1} ^n - u_{j - 1} \\right)}_ {\\text{FTCS}} \u0026#43; \\underbrace{\\frac{a^2 \\Delta t^2}{2 \\Delta x ^2} \\left( u_{j\u0026#43;1} ^n - 2 u_j ^n \u0026#43; u_{j - 1} ^n \\right)} _{\\text{Diffusion}}\\] which approximates the PDE\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = \\frac{a^2 \\Delta t}{2} \\pdv{^2 u}{x ^2}\\] Lax-Wendroff has an accuracy of \\( \\mathcal{O}(\\Delta t ^2, \\Delta x^2) \\). Because of this, it\u0026rsquo;s a very useful algorithm.\nIf the system is non-linear, then we do apply Lax-Wendroff as a 2-step process. For instance, if we look at the form of the MHD equations,\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{x} = 0\\] The first Lax advance will be\n\\[Q_{j \u0026#43; 1/2} ^{n \u0026#43; 1/2} = \\frac{Q_{j \u0026#43; 1} ^n \u0026#43; Q_j ^n}{2} - \\frac{\\Delta t}{2 \\Delta x} \\left( F_{j \u0026#43; 1} ^n - F_j ^n \\right)\\] and the leapfrog advance is\n\\[Q_j ^{n \u0026#43; 1} = Q_j ^n - \\frac{\\Delta t}{\\Delta x} \\left( F_{j \u0026#43; 1/2} ^{n \u0026#43; 1/2} - F_{j - 1/2} ^{n \u0026#43; 1/2} \\right)\\] There is a way to generalize the Lax-Wendroff procedure, which produces the class of Lax-Wendroff-type algorithms. To step from \\( n \\) to \\( n + 1 \\), we first compute the solution at an intermediate step \\( n + \\alpha \\) at some intermediate grid points \\( j + \\beta \\). These algorithms are described by the stencil:\nFor \\( \\alpha = \\beta = \\frac{1}{2} \\), we get the Lax-Wendroff algorithm we just described. There\u0026rsquo;s another useful algorithm known as the MacCormack algorithm, for which \\( \\alpha = 1, \\beta = 0 \\):\n\\[\\overline{Q}_j = Q _j ^n - \\frac{\\Delta t}{\\Delta x} \\left( F_{j \u0026#43; 1} ^n - F_{j} ^n \\right) \\quad \\text{(predictor step)}\\] \\[\\overline{\\overline{Q}}_j = Q_j ^n - \\frac{\\Delta t}{\\Delta x} \\left( \\overline{F}_j - \\overline{F} _{j - 1} \\right) \\quad \\text{(corrector step)}\\] \\[Q_j ^{n \u0026#43; 1} = \\frac{\\overline{Q}_j \u0026#43; \\overline{\\overline{Q}}_j}{2} = \\frac{\\overline{Q}_j \u0026#43; Q_j ^n}{2} - \\frac{\\Delta t}{2 \\Delta x} \\left( \\overline{F}_j - \\overline{F}_{j - 1} \\right)\\] where \\( \\overline{F}_j = F(\\overline{Q}_j) \\). The MacCormack algorithm is also \\( \\mathcal{O}(\\Delta t ^2, \\Delta x ^2) \\). In some cases it can be better (more stable) for multi-dimensional problems.\nWe\u0026rsquo;ve written these algorithms out in 1D. In 3D MHD, we can write out our system of equations as:\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{x} \u0026#43; \\pdv{\\vec G}{y} \u0026#43; \\pdv{\\vec H}{z} = 0\\] We can write the predictor step as\n\\[\\overline{Q} _{i,j,k} = Q^n _{i,j,k} - \\frac{\\Delta t}{\\Delta x} \\left( F _{i\u0026#43;1, j, k} - F _{i, j, k} ^n \\right) - \\frac{\\Delta t}{\\Delta y} \\left( G _{i, j\u0026#43;1, k} - G _{i, j, k} ^n \\right) - \\frac{\\Delta t}{\\Delta z} \\left( H _{i, j, k\u0026#43;1} - H _{i, j, k} ^n \\right)\\] and the corrector step as\n\\[\\begin{aligned} \\overline{\\overline{Q}}_{i, j, k} \u0026amp; = \u0026amp; Q _{i, j, k} ^n - \\frac{\\Delta t}{\\Delta x} \\left( \\overline{F}_{i, j, k} - \\overline{F}_{i-1, j, k} \\right) - \\frac{\\Delta t}{\\Delta y} \\left( \\overline{F}_{i, j, k} - \\overline{F}_{i, j-1, k} \\right) \\\\ \u0026amp; \u0026amp; - \\frac{\\Delta t}{\\Delta z} \\left( \\overline{F}_{i, j, k} - \\overline{F}_{i, j, k-1} \\right) \\end{aligned}\\] \\[Q_{ijk} ^{n \u0026#43; 1} = \\frac{\\overline{Q} \u0026#43; \\overline{\\overline{Q}}}{2} \\] To ensure stability, the domain of dependence must be contained in the numerical domain of dependence. If we plot out the slope of the zone of dependence, the physical domain has a slope \\( |1 / \\lambda_{max}| \\), while the numerical domain has a slope \\( |\\Delta t / \\Delta x| \\):\nwhich gives us the stability condition: \\[\\frac{\\Delta t}{\\Delta x} \\left| \\lambda _{max} \\right| \\leq 1\\] where \\( \\lambda_{max} \\) is the maximum eigenvalue of the flux Jacobian \\( \\pdv{\\vec F}{\\vec Q} \\). In 2D, we have\n\\[\\frac{\\Delta t}{\\Delta x} \\left| \\lambda _{A, max} \\right| \u0026#43; \\frac{\\Delta t}{\\Delta y} \\left| \\lambda _{B, max} \\right| \\leq 1\\] where \\( \\vec A = \\pdv{\\vec F}{\\vec Q} \\) and \\( \\vec B = \\pdv{\\vec G}{\\vec Q} \\).\nIn standard gas dynamics, where the situation is much simpler, the eigenvalues are \\( \\lambda_A = (u, u + v_s, u - v_s) \\), where \\( v_s \\) is the sound speed. The stability condition is then:\n\\[\\Delta t \\leq \\left( \\frac{|u| \u0026#43; v_s}{\\Delta x} \u0026#43; \\frac{|v| \u0026#43; v_s}{\\Delta y} \\right)\\] For high speed flows, which have \\( v \u0026gt; \\lambda \\) for any of the characteristic speeds, sometimes additional diffusion is necessary:\n\\[\\pdv{\\vec Q}{t} \u0026#43; \\pdv{\\vec F}{x} \u0026#43; \\pdv{\\vec G}{y} \u0026#43; \\pdv{\\vec H}{z} = \\sigma \\grad ^2 \\vec Q\\] This diffusivity can be added after we perform an update. For example, for the 2D MacCormack algorithm we can write\n\\[\\left( Q_{i, j} ^{n \u0026#43; 1} \\right)\u0026#39; = Q_{ij} ^{n \u0026#43; 1} \u0026#43; \\Delta t \\sigma \\left[ \\frac{\\left( Q_{i \u0026#43; 1, j} ^{n \u0026#43; 1} - 2 Q_{i, j} ^{n \u0026#43; 1} \u0026#43; Q_{i-1, j} ^{n \u0026#43; 1}\\right)}{\\Delta x^2} \u0026#43; \\frac{\\left( Q_{i, j\u0026#43;1} ^{n \u0026#43; 1} - 2 Q_{i, j} ^{n \u0026#43; 1} \u0026#43; Q_{i, j-1} ^{n \u0026#43; 1}\\right)}{\\Delta y^2}\\right]\\] This helps to smooth out the oscillations that can result from a steep change in \\( Q \\):\nWe can set \\( \\sigma \\) to a constant everywhere, and a typical value might be\n\\[\\frac{\\sigma \\Delta t}{\\Delta x^2} \u0026lt; 0.25\\] You can also make \\( \\sigma \\propto \\div \\vec v \\), so that we only add diffusion in the region of shocks or supersonic flows.\nMethod 2: Upwind Difference Flux # Going back to our model equation can write the advection equation as\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = 0\\] \\[\\pdv{u}{t} \u0026#43; \\pdv{f}{x} = 0 \\qquad (f = au)\\] The simple upwind difference is expressed as\n\\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{a \\Delta t}{\\Delta x} \\begin{cases} (u_j ^n - u_{j-1} ^n) \\quad \u0026amp;\\text{if}\u0026amp; a \\geq 0 \\\\ (u_{j\u0026#43;1} ^n - u_{j} ^n) \\quad \u0026amp;\\text{if}\u0026amp; a \\geq 0 \\end{cases}\\] This works easily enough for the advection equation, but it does not work in general for systems of equations, because we can characteristics which are both positive and negative. We can not, for example, write\n\\[Q_j ^{n\u0026#43;1} = Q_j ^n - \\frac{\\Delta t}{\\Delta x} \\begin{cases} (F_j ^n - F_{j-1} ^n) \\quad \u0026amp;\\text{if} \\quad \\lambda \\geq 0 \\\\ (F_{j\u0026#43;1} ^n - F_{j} ^n) \\quad \u0026amp;\\text{if} \\quad \\lambda \u0026lt; 0 \\end{cases}\\] Because in general \\( \\lambda_i \\) will take both positive and negative values. In gas dynamics, \\( \\lambda = (v, v + v_s, v - v_s) \\). In ideal MHD, we have seven waves:\n\\[\\lambda = v, v \\pm v_A, v \\pm v_{\\text{fast}}, v \\pm v_{\\text{slow}}\\] From a point \\( Q_j ^n \\), waves of each of the characteristics propagate within their own domains:\nIn regions \\( (1) \\) and \\( (2) \\), no information can propagate faster than the fastest characteristic \\( (v \\pm v_f) \\). At any point in time, we can draw a horizontal line to see the extent of each domain. At any point in time, some eigenvalues are positive and some are negative. As drawn, \\( v - v_f \\) and \\( v - v_A \\) are negative, and the rest are positive.\nTo apply an upwind difference, we must split the flux into right- and left-going components\n\\[F = F^\u0026#43; \u0026#43; F^-\\] Re-writing our original PDE:\n\\[\\pdv{Q}{t} \u0026#43; \\pdv{F}{x} = 0\\] \\[\\pdv{Q}{t} \u0026#43; \\pdv{F^\u0026#43;}{x} \u0026#43; \\pdv{F^-}{x} = 0\\] we can now apply upwind differencing\n\\[\\frac{Q_j ^{n\u0026#43;1} - Q_j ^n}{\\Delta t} \u0026#43; \\frac{F_j ^\u0026#43; - F_{j - 1} ^\u0026#43;}{\\Delta x} \u0026#43; \\frac{F_{j\u0026#43;1} ^- - F_j ^- }{\\Delta x} = 0\\] To split \\( F \\) into \\( F^+ \\) and \\( F^- \\), we manipulate the original PDE using the flux Jacobian.\n\\[\\pdv{Q}{t} \u0026#43; \\pdv{F}{Q} \\pdv{Q}{x} = 0\\] \\[\\rightarrow \\pdv{Q}{t} \u0026#43; A^\u0026#43; \\pdv{Q}{x} \u0026#43; A^- \\pdv{Q}{x} = 0\\] where now \\( A^+ \\) and \\( A^- \\) are the flux Jacobians of \\( F^+ \\) and \\( F^- \\). The eigenvalue decomposition of \\( A \\) is\n\\[A X = X \\Lambda\\] where \\( \\Lambda \\) is the diagonal matrix of eigenvalues and \\( X \\) is the matrix of eigenvectors. This means we can write \\( A \\) as\n\\[A = X \\Lambda X^{-1}\\] We can split our PDE into right- and left-going components as:\n\\[\\pdv{Q}{t} \u0026#43; X \\Lambda X^{-1} \\pdv{Q}{x} = 0\\] \\[\\pdv{Q}{t} \u0026#43; X \\Lambda ^\u0026#43; X^{-1} \\pdv{Q}{x} \u0026#43; X \\Lambda ^- X^{-1} \\pdv{Q}{x} = 0\\] where \\( \\Lambda ^+ \\) is the diagonal matrix of only right-going eigenvalues, and \\( \\Lambda^- \\) is a diagonal matrix of only right-going eigenvalues.\nFor gas dynamics, the eigenvalues are \\( v, v \\pm v_s \\) with \\( 0 \u0026lt; v \u0026lt; v_s \\)\n\\[\\Lambda = \\begin{bmatrix} v \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; v \u0026#43; v_s \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; v - v_s \\end{bmatrix} \\\\ = \\Lambda ^\u0026#43; \u0026#43; \\Lambda ^-\\] \\[\\Lambda ^\u0026#43; = \\begin{bmatrix} v \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; v \u0026#43; v_s \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix}\\] \\[\\Lambda ^- = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; v - v_s \\end{bmatrix}\\] Now we can properly upwind difference the flux by defining the flux at grid midpoints, computing \\( F^+ \\) based on \\( F_j \\), and computing \\( F^- \\) based on \\( F_{j+1} \\).\n\\[Q_j ^{n\u0026#43;1} = Q_j ^n - \\frac{\\Delta t}{\\Delta x} \\left( F_{j \u0026#43; 1/2} ^n - F_{j - 1/2} ^n \\right)\\] By defining the numerical flux at the grid midpoints, this naturally leads to a finite volume implementation.\nFinite Volume Method # Finite volume methods differ from finite difference methods in the quantity which we store at each grid point. If we consider a volume centered at grid point \\( j + 1/2 \\), we can integrate over the cell volume to get the average value of the function over the volume.\n\\[\\pdv{}{t} \\underbrace{\\int _\\Omega \\vec Q \\dd \\vec V} _ {\\int _\\Omega \\rho \\dd \\vec V = m _\\Omega} \u0026#43; \\underbrace{\\int _ \\Omega \\div \\vec F \\dd \\vec V = 0} _{\\oint \\dd \\vec S \\cdot \\vec F \\rightarrow \\sum_{\\text{sides}} \\vec S _j \\cdot \\vec F _j }\\] Because the integral is determined by the fluxes, which are defined along the cell edges rather than the grid points, this sort of method is no longer constrained to evenly-spaced quadrilateral grids. Much more complicated polygons are possible, allowing us to avoid the stair-stepping issues at the boundary of uniform grids.\nFluxes are computed using the upwind method\n\\[F_{j \u0026#43; 1/2} = F^- _{j \u0026#43; 1} \u0026#43; F_j ^\u0026#43; = \\vec X \\vec \\Lambda ^- \\vec X ^{-1} Q_{j\u0026#43;1} \u0026#43; \\vec X \\vec \\Lambda^\u0026#43; \\vec X^{-1} Q_j\\] Manipulating this a bit, we can get a more convenient form for evaluating the fluxes. \\[F_{j \u0026#43; 1/2} = \\vec X \\underbrace{\\left( \\frac{ \\vec \\Lambda - |\\vec \\Lambda|}{2} \\right)}_{\\text{negative components}} \\vec X^{-1} Q_{j\u0026#43;1} \u0026#43; \\vec X \\underbrace{\\left( \\frac{\\vec \\Lambda \u0026#43; |\\vec \\Lambda| }{2} \\right)}_{\\text{positive components}} \\vec X^{-1} Q_j \\\\ = \\frac{1}{2} \\left( \\vec X \\vec \\Lambda \\vec X^{-1} Q_{j\u0026#43;1} \u0026#43; \\vec X \\vec \\Lambda \\vec X^{-1} Q_j \\right) - \\frac{1}{2} \\vec X | \\vec \\Lambda| \\vec X^{-1} (Q_{j\u0026#43;1} - Q_j) \\\\ = \\underbrace{\\frac{F_j \u0026#43; F_{j\u0026#43;1}}{2}}_{\\text{Centered flux}} - \\underbrace{\\frac{1}{2} \\left( \\vec X |\\vec \\Lambda| \\vec X ^{-1} \\right) _{j \u0026#43; 1/2} (Q_{j\u0026#43;1} - Q_j)}_{\\text{Flux correction}}\\] To calculate \\( \\vec X |\\vec \\Lambda | \\vec X^{-1} \\) at grid point \\( j + 1/2 \\), we need to perform some sort of average between \\( Q_j \\) and \\( Q_{j+1} \\). The simplest is just:\n\\[Q_{j\u0026#43;1/2} = \\frac{Q_j \u0026#43; Q_{j\u0026#43;1}}{2}\\] which isn\u0026rsquo;t conservative, but works pretty well. There are other averaging methods, like the Roe average, that attempt to preserve conservative properties by propagating a density average through \\( Q \\).\nThis is called an Approximate Riemann Solver. It\u0026rsquo;s composed of two separate steps, a centered flux (unstable FTCS), and a flux correction based on the characteristics of the PDE (stable).\nEquilibrium Calculations # From physical expectations, elliptic equations result from equilibrium calculations and eigenvalue systems. In general, we can write elliptic equations as \\[\\vec A \\vec x = \\vec b\\] where \\( \\vec A \\) may be a nonlinear function of \\( \\vec x \\). For example, if we think of Poisson\u0026rsquo;s equation in 1D, \\[\\dv{^2 \\phi}{x^2} = - \\rho_c\\] and apply finite differencing, \\[\\frac{\\phi_{j\u0026#43;1} - 2 \\phi_j \u0026#43; \\phi_{j-1}}{\\Delta x^2} = - \\rho_j\\] we get a matrix system \\[\\vec A \\vec x = \\frac{1}{\\Delta x^2} \\begin{bmatrix} -2 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \\\\ 1 \u0026amp; -2 \u0026amp; 1 \u0026amp; \\ldots \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\end{bmatrix} \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\ldots \\end{bmatrix} = \\vec b = - \\begin{bmatrix} \\rho_1 \\\\ \\rho_2 \\\\ \\ldots \\end{bmatrix}\\] where \\( \\vec A \\) is a J by J matrix. In equilibrium calculations, this comes up when solving the Grad-Shafranov equation\n\\[\\Delta ^\\star \\psi = I I\u0026#39; - \\mu_0 r^2 p\u0026#39;\\] where \\[I\u0026#39; = \\pdv{I}{\\psi} \\qquad p\u0026#39; = \\pdv{p}{\\psi}\\] and \\[\\Delta ^\\star \\equiv r \\pdv{}{r} \\frac{r}{r} \\pdv{}{r} \u0026#43; \\pdv{^2}{z^2}\\] The Grad-Shafranov equation is a nonlinear equation, and we often assume a power series expansion of the current density and pressure \\[I(\\psi) = I_0 \u0026#43; I_1 \\psi \u0026#43; I_2 \\psi^2 \u0026#43; \\ldots\\] \\[p(\\psi) = p_0 \u0026#43; p_1 \\psi \u0026#43; p_2 \\psi^2 \u0026#43; \\ldots\\] For a force-free equilibrium (\\( p = 0 \\)) with a linear current profile \\[I I\u0026#39; = I_0 I_1 \u0026#43; I_1 ^2 \\psi\\] this makes the Grad-Shafranov equation a linear system. \\[\\vec A = \\Delta ^\\star _{\\text{FD}} \u0026#43; I_1 ^2\\] \\[\\vec x = \\psi_{ij} \\qquad \\vec b = (I_0 I_1) _{ij}\\] where \\( \\Delta ^\\star _{\\text{FD}} \\) is a finite difference representation of the \\( \\Delta ^\\star \\) operator. For a given geometry and poloidal current profile, \\( I_0 \\) and \\( I_1 \\), we can solve \\( \\vec A \\vec x = \\vec b \\) to give \\( \\psi(r, z) \\).\nSolving \\( \\vec A \\vec x = \\vec b \\) can be done using either direct or indirect (iterative) methods. Direct decomposition methods (like Kramer\u0026rsquo;s rule, Gaussian decomposition) require \\( \\mathcal{O}(N^3) \\) operations, where \\( N \\) is the size of the matrix. They also require full matrix storage, even though \\( \\vec A \\) is a sparse (tridiagonal) matrix.\nIterative methods help solve this problem. We begin with a solution which is improved upon until we get convergence. If this converges in fewer operations than a full decomposition, then we\u0026rsquo;ve saved some time and a lot of in-memory storage space.\n\\[x^0 \\rightarrow x^1 \\rightarrow x^2 \\rightarrow \\ldots\\] \\[\\vec x^{n\u0026#43;1} = \\vec x^n \u0026#43; \\vec B^{-1} (\\vec b - \\vec A \\vec x^n) \\\\ = \\vec x^n \u0026#43; \\vec B^{-1} \\vec r^n\\] The technique boils down to a clever choice of \\( \\vec B \\). We want to choose an approximation matrix \\( \\vec B \\) which is very close to \\( \\vec A \\), but is much easier to invert. \\[\\vec B \\approx \\vec A\\] For this kind of iterative method, each iteration requires \\( \\mathcal{O}(N) \\) operations, and converges in fewer than \\( N \\) iterations. The best iterative methods can converge in \\( \\mathcal{O}(N \\ln N) \\) total operations.\nLinear Stability # When we studied plasma waves at the beginning of this course, we performed a linearization of the Vlasov equation about an equilibrium state. This is how we got dispersion relations for waves. We assumed an equilibrium of a uniform plasma, with at most static magnetic fields. For more interesting equilibria with nonzero gradients, we follow the same procedure.\nFirst, we linearize the ideal MHD equations about a static equilibrium (\\( \\vec v_0 = 0 \\)) \\[\\pdv{\\rho_1}{t} = - \\div (\\rho_0 \\vec v_1)\\] \\[\\pdv{p_1}{t} = - \\div (p_0 \\vec v_1) \u0026#43; (1 - \\Gamma) p_0 \\div \\vec v_1\\] \\[\\pdv{\\vec B_1}{t} = \\curl (\\vec v_1 \\cross \\vec B_0)\\] \\[\\rho _0 \\pdv{\\vec v_1}{t} \u0026#43; \\frac{1}{\\mu_0} \\left[(\\curl \\vec B_1) \\cross \\vec B_0 \u0026#43; (\\curl \\vec B_0) \\cross \\vec B_1 \\right]\\] "},{"id":44,"href":"/r/notes/UWAA557/ch11-3/","title":"Oscillations","section":"Physics of Fusion Plasmas","content":" Oscillations # Collision Frequencies # Gases have three degrees of freedom to oscillate, but plasmas have four: the electron fluid adds an additional parameter.\nOne way to see how the electron fluid oscillates, take a region of plasma and displace the electrons by a distance \\( x \\) and release. Then, watch how the electrons oscillate.\nGauss\u0026rsquo;s law gives the resulting electric field (same as a parallel-plate capacitor.\n\\[E = \\frac{n e x}{\\epsilon_0}\\] The restoring force is therefore\n\\[F = \\frac{x n e^2}{\\epsilon_0}\\] We know forces of that form. When released, it will undergo simple harmonic motion\n\\[\\omega = \\sqrt{\\frac{K}{m}} \\qquad K = \\frac{ ne^2}{\\epsilon_0}\\] We define the plasma frequency as\n\\[\\omega_{pe} = \\sqrt{ \\frac{ne^2}{m_e \\epsilon_0}}\\] If we take the displacement out to \\( \\lambda_D \\) then the peak velocity is the electron thermal speed \\( v_e \\)\n\\[\\omega_{pe} \\lambda_D = v_e\\] Taking a look back at the collision frequencies, the electron-electron collision frequency \\( \\nu_{ee} \\) (associated with the energy transfer) is close to the electron-ion collision frequency \\( \\nu_{ei} \\) associated with momentum transfer, and both are about\n\\[\\nu_{ee} \\approx \\nu_{ei} \\approx \\frac{v_e}{\\lambda_D \\Lambda} = \\frac{\\omega_{pe}}{\\Lambda}\\] So the electron-electron collision frequency will be much less than the plasma frequency if \\( \\Lambda \\gg 1 \\)\n\\[\\Lambda \\gg 1 \\rightarrow \\nu_{ee} \\ll \\omega_{pe}\\] Radiation # Cyclotron radiation # As an electron moves through a magnetic field, it takes a helical path. The acceleration is\n\\[\\vec a = \\frac{v^2}{r} = v_{th} \\omega _c\\] Only the component of \\( \\vec a \\) perpendicular to \\( r \\) can cause radiation in the \\( r \\) direction.\n\\( \\theta \\)-pinch # Called theta-pinch because current \\( I \\) is in the \\( \\theta \\) direction. The magnetic field is constant within the plasma, and constant outside the plasma (inside the device).\n\\[\\frac{B_0 ^2}{2 \\mu_0} \\] Z-pinch # Talk about a Z-pinch that\u0026rsquo;s stationary and compressing. Assume T is a constant (burning through). The radiated power is then\n\\[P_{rad} \\propto n^2\\] The ohmic heating goes like \\( j^2 \\)\n\\[P_{ohmic} \\propto j^2\\] So the key parameter in the ratio is \\( j/n \\).\nThere is no \\( B_{\\parallel} \\) so we need to balance the pressure as well.\nConsider ohmically heating a pure Z-pinch like FRC or ZaP.\nFirst, assume that the power radiated per unit volume goes like\n\\[n^2 f (T) = P/V\\] The dependence is true for a fixed impurity fraction for line radiation in steady state, and for Bremsstrahlung.\nIgnore \\( \\ln \\Lambda \\) dependency in ohmic heating so\n\\[P/V = \\eta(T) j^2\\] The pressure balance demands\n\\[\\frac{B^2}{2 \\mu_0} = p = 2 nk T_0\\] For Bremsstrahlung temperature and density dependence are\n\\[P_{rad} = c_1 n^2 T^{1/2}\\] \\[P_{ohm} = c_2 j^2 T^{-3/2}\\] Pressure balance\n\\[n T \\propto B^2 \\propto j^2 r^2\\] \\[nT = c_3 j^2 r^2\\] \\[T = c_e \\frac{j^2 r^2}{n}\\] So if we\u0026rsquo;re going to get any heating, we need the ohmic heating to be greater than the radiation\n\\[c_2 j^2 T^{-3/2} \\geq c_1 n^2 T^{1/2}\\] using\n\\[T = c_3 \\frac{j^2 r^2}{n}\\] \\[\\rightarrow \\frac{c_2}{c_1 c_3 ^2} \u0026gt; j^2 r^4 \\sim I^2\\] Density cancels out, and we\u0026rsquo;re left with a current term (\\( I^2 \\)) called the Pease current\n\\[I \\leq \\approx 1 MA\\] Z-pinch radiatively collapses above 1MA for Bremsstralung radiation. Counter-intuitively, the current must be less than the Pease current in order to continue heating. There is an even lower current for line radiation going up through the burn-through curve.\nSkin Depths in Plasma # Two types of skin depths in plasma: collisional and collisionless\n\\[\\delta = \\sqrt{ \\frac{ 2}{\\mu_0 \\omega \\sigma}} \\quad \\text{Collisional}\\] \\[\\delta = \\frac{c}{\\omega_{pe}} \\quad \\text{Electron collisionless skin depth}\\] \\[\\delta = \\frac{c}{\\omega_{pi}} \\quad \\text{Ion collisionless skin depth}\\] Collisionless skin depth is due to electron inertia, which allows the field to penetrate even if it is very hot and a good conductor. What is magnitude? Look at E production and B shielding\n\\[\\curl E = - \\pdv{B}{t}\\] \\[\\curl B = \\mu_0 j \u0026#43; \\mu_0 \\epsilon_0 \\pdv{E}{t}\\] Electrons accelerate with E\n\\[m \\pdv{v}{t} = E e\\] Doing a wave analysis assuming spatial solutions that decay off as \\( 1 / \\delta \\) and oscillate with frequency \\( \\omega \\), we have\n\\[\\frac{1}{\\delta} \\sim \\grad\\] \\[\\omega \\sim \\pdv{}{t}\\] \\[\\frac{E}{\\delta} = - \\omega B\\] \\[m \\omega v = - e E\\] \\[j = - n e v\\] \\[\\frac{B}{\\delta} = \\mu_0 (- n e v) \u0026#43; \\mu_0 \\epsilon_0 \\omega E = \\frac{E}{\\delta ^2 \\omega}\\] \\[\\frac{E}{\\delta ^2 \\omega} = \\frac{\\mu_0 n e^2}{m} \\frac{E}{\\omega} \u0026#43; \\mu_0 \\epsilon_0 \\omega E\\] That\u0026rsquo;s the dispersion relation for our wave solutions. Plugging in \\( c \\)\n\\[- \\frac{c^2}{\\delta ^2} \\frac{1}{\\omega} = \\frac{n e^2}{\\epsilon_0 m }\\frac{1}{\\omega} \u0026#43; \\omega\\] That lets us identify the electron plasma frequency\n"},{"id":45,"href":"/r/notes/UWAA558/08-1d-equilibria/","title":"1-D Equilibria","section":"MHD Theory","content":" 1-Dimensional Equilibria # The \\( \\theta \\) -pinch # In a \\( \\theta \\) pinch, we have an applied axial field generated by a driven azimuthal current distribution. The way these usually work is that you begin with a plasma generated by some pre-ionization process and zero field. Then you crank up the current to drive an azimuthal current in the plasma (in the opposite direction as the external current).\n\\[\\vec j \\cross \\vec B = \\grad p \\quad \\rightarrow \\quad j_\\theta B_z = \\dv{p}{r}\\] \\[j_\\theta = - \\frac{1}{\\mu_0} \\dv{B_z}{r}\\] \\[\\dv{p}{r} = - \\frac{1}{\\mu_0} B_z \\dv{B_z}{r} = - \\dv{}{r} \\left( \\frac{B_z ^2}{2 \\mu_0} \\right)\\] \\[p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} = \\text{constant} = \\frac{B_0 ^2}{2 \\mu_0}\\] At equilibrium, the magnetic pressure balances the plasma pressure. If we say that the pressure is\n\\[p = p_0 e^{- r^2 / a^2}\\] with \\( p_0 \\) the pressure on-axis, then we can solve for the axial field\n\\[B_z = B_0 (1 - B_0 e^{-r^2/a^2})^{1/2}\\] We can define the peak \\( \\beta \\) to be the ratio of the on-axis pressure to the maximum magnetic field\n\\[\\beta_0 = \\frac{p_0}{B_0 ^2 / 2 \\mu_0}\\] By definition, the peak \\( \\beta \\) will always be \\( \\leq 1 \\) . We can define \\( \\langle \\beta \\rangle \\) \\[\\langle \\beta \\rangle = \\frac{ \\langle p \\rangle }{B_a ^2 / 2 \\mu_0}\\] where \\( B_a \\) is a characteristic field value, typically taken to be at the plasma edge.\n\\[\\langle \\beta \\rangle = \\frac{2 \\mu_0}{B_0 ^2 \\pi a^2} \\int _0 ^a 2 \\pi r p \\, \\dd r\\] \\[= \\frac{2}{a^2} \\int_0 ^a \\frac{ rp}{B_0 ^2/2 \\mu_0} \\dd r = \\frac{2}{a^2} \\int_0 ^a \\left( 1 - \\frac{B_z ^2}{B_0 ^2} \\right) r \\dd r\\] In this form, we can see that because \\( B_z \\) will everywhere be less than \\( B_0 \\) , we can increase \\( \\langle \\beta \\rangle \\) by driving \\( B_z \\) as low as possible. In this particular example, \\( \\langle \\beta \\rangle / \\beta_0 = 63\\% \\) .\nZ-pinch # In the case of a Z-pinch, we only have an applied axial current.\n\\[\\vec j = j_z (r) \\vu z\\] For force balance\n\\[\\vec j \\cross \\vec B = \\grad p \\quad \\rightarrow \\quad - j_z B_\\theta = \\dv{p}{r}\\] \\[j_z = \\frac{1}{\\mu_0} \\frac{1}{r} \\dv{}{r} ( r B_\\theta) \\] \\[- \\dv{p}{r} = \\frac{B_\\theta}{\\mu_0 r} \\dv{}{r} ( r B_\\theta)\\] If we find it convenient we can separate this into a magnetic pressure term\n\\[- \\dv{}{r} \\left( p \u0026#43; \\frac{B_\\theta ^2}{2 \\mu_0} \\right) = \\frac{B_\\theta ^2}{\\mu_0 r}\\] Bennett Profile # An example of an achievable distribution is the Bennett profile, which has a diffuse form\n\\[B_\\theta = \\frac{\\mu_0 I}{2 \\pi} \\frac{r}{r^2 \u0026#43; a^2}\\] \\[j_z = \\frac{I}{\\pi} \\frac{a^2}{(r^2 \u0026#43; a^2) ^2}\\] \\[p = \\frac{\\mu_0 I^2}{8 \\pi ^2} \\frac{a^2}{(r^2 \u0026#43; a^2)^2}\\] Interestingly, \\( j \\propto p \\) . For a uniform temperature, \\( j \\propto n \\) . Since current density is the product of \\( \\vec v \\) and \\( n \\) , this says that we have a uniform drift velocity and all particles are drifting with the same velocity at all points along the profile. If we consider what the equilibrium profile looks like for a Bennett profile:\nSo for \\( r \u0026lt; a \\) we have magnetic tension and pressure which balance the plasma pressure. For \\( r \\geq a \\) we have magnetic tension which balances both plasma pressure and magnetic pressure.\nThe Z-pinch \\( \\langle \\beta \\rangle \\) \\[\\langle \\beta \\rangle \\equiv \\frac{ \\langle p \\rangle}{B_a ^2 / 2 \\mu_0} \\\\ = \\frac{2 \\mu_0}{B_a ^2 \\pi a^2} \\int_0 ^a 2 \\pi r p \\dd r\\] If we multiply the force balance by \\( r^2 \\) and integrate\n\\[\\int_0 ^a r^2 \\dv{p}{r} \\dd r \u0026#43; \\frac{1}{\\mu_0} \\int_0 ^a r B_\\theta \\dv{}{r} (r B_\\theta) \\dd r = 0 \\\\ 0 = \\left[ r^2 p \\right] _0 ^a - \\int_0 ^a p \\dd (r^2) \u0026#43; \\left[ \\frac{(r B_\\theta)^2}{2 \\mu_0} \\right] _0 ^a\\] If we have a discrete pinch such that \\( p(a) = 0 \\) then the first term vanishes.\n\\[\\int_0 ^a 2 r p \\dd r = \\frac{(a B_a)^2}{2 \\mu_0}\\] If we substitute our definition of \\( \\langle \\beta \\rangle \\) , we find \\( \\langle \\beta \\rangle = 1 \\) . For a diffuse pinch in which \\( p(a) \\neq 0 \\) we end up with \\( \\langle \\beta \\rangle \\leq 1 \\) and we have a wall-supported plasma. Ideal confinement ( \\( \\langle \\beta \\rangle = 1 \\) ) is a very nice property and is what makes the Z-pinch configuration so interesting.\nStability Considerations # Instability results if there exists a plasma displacement that leads to a lower energy state. There are several ways to provide stability in the context of MHD. The two most common are magnetic shear and magnetic well.\nMagnetic Shear # In ideal MHD, magnetic field lines can not break or tear. Let\u0026rsquo;s consider some flux surface containing field lines \\( \\vec B_3 \\) . Behind it, we have another flux surface containing field lines \\( B_2 \\) which are not parallel to \\( \\vec B_3 \\) , and the same for \\( \\vec B_1 \\) .\nBecause the field lines are a different angles to each other, these flux surfaces can not interpenetrate. In other words, if the flux surface pressures are \\( P_1 \u0026gt; P_2 \u0026gt; P_3 \\) , we can maintain the pressure gradient and prevent the flux surfaces from moving each other. What prevents the surfaces from achieving a lower energy state is the magnetic shear between flux surfaces.\nWithout shear, the surfaces can interpenetrate and exchange positions. In the case of a toroidal geometry, magnetic shear is defined by the rotational transform \\( \\iota \\) , or by the safety factor\n\\[q = \\frac{2 \\pi}{\\iota} \\] Generally speaking, \\( q \\) is generally referenced for tokamaks and \\( \\iota \\) is referenced for stellarators. Another way of picturing the safety factor in a toroidal geometry is\n\\[q \\equiv \\frac{\\text{no. of windings long way}}{\\text{no. of windings short way}} \\\\ = \\frac{ \\dv{\\psi_t}{V}}{\\dv{\\phi_p}{V}} = \\dv{\\phi_t}{\\phi_p} \\\\ = \\frac{n}{m} = \\frac{\\text{toroidal transits}}{\\text{poloidal transits}}\\] In a cylindrical (1D) geometry it is just\n\\[q = \\frac{\\text{longitudinal transits}}{\\text{azimuthal transits}}\\] Let\u0026rsquo;s calculate the safety factor for a toroidal geometry:\n\\[\\dv{\\phi_p}{V} = \\frac{ B_\\theta 2 \\pi R \\dd r}{2 \\pi R_0 2 \\pi r \\dd r} \\\\ = \\frac{ B_\\theta}{2 \\pi r} \\frac{R}{R_0}\\] \\[\\dv{\\phi_t}{V} = \\frac{B_\\phi 2 \\pi r \\dd r}{2 \\pi R_0 2 \\pi r \\dd r} \\\\ = \\frac{B_\\phi}{2 \\pi R_0}\\] \\[q = \\frac{r B_\\phi}{R B_\\theta}\\] In a cylindrical geometry the analysis is even simpler\n\\[q = 2 \\pi \\frac{r B_z}{L B_\\theta}\\] As a note, it would appear that \\( q \\rightarrow 0 \\) at the magnetic axis as \\( r \\rightarrow 0 \\) , but in general \\( B_\\theta \\rightarrow 0 \\) as well, and the safety factor is generally bounded at \\( r \\rightarrow 0 \\) \\( q \\) is a flux surface quantity.\nWe care about magnetic shear. How does that relate to the safety factor? Magnetic shear is defined as\n\\[s \\equiv 2 \\frac{\\dd q / q}{\\dd V /V} = 2 \\dv{\\ln (q)}{\\ln(V)}\\] Even a uniform \\( B_z \\) or \\( B_\\theta \\) produces a finite magnetic shear because of the way that \\( r \\) and \\( B_\\theta \\) change. The safety factor is often considered synonymous with magnetic shear, and often we don\u0026rsquo;t even compute \\( s \\) .\nShear is generally a stabilizing effect. Interchange between flux surfaces can be prevented/inhibited by shear, or by making it energetically unfavorable. Shear tends to stabilize current-driven instabilities.\nMagnetic Well # As before, we can consider two adjacent flux surfaces \\( B_1, P_1 \\) and \\( B_2, P_2 \\) . If \\( B_2 \u0026gt; B_1 \\) and \\( P_2 \u0026gt; P_1 \\) , the interchange is energetically favorable. But if \\( B_2 \u0026gt; B_1 \\) and \\( P_2 \u0026lt; P_1 \\) then the interchange may be unfavorable without any magnetic shear.\nConsider a plasma confined by an externally applied magnetic field generated by a coil \\( I \\) On the left side, the magnetic field gradient is in the same direction as the plasma pressure gradient, which is a destabilizing configuration. Flux surfaces are able to interchange easily, and the magnetic field is described as having bad curvature. On the right side, the gradients are in the same direction and the magnetic field has a good curvature.\nWe can define the \u0026ldquo;wellness\u0026rdquo; \\( W \\) as\n\\[W \\equiv \\frac{ \\text{total pressure change relative to mag. pressure}}{\\text{relative volume change}} \\\\ = \\frac{\\dd \\langle p \u0026#43; B^2/2 \\mu_0 \\rangle / \\langle B^2/2 \\mu_0 \\rangle}{\\dd V / V}\\] where the angle brackets indicate a quantity integrated along a field line \\[\\langle Q \\rangle \\equiv \\frac{\\int_0 ^L \\frac{ Q \\dd l}{|B|}}{\\int_0 ^l \\frac{\\dd l}{|B|}}\\] For a stabilizing effect, the wellness must be greater than 0. This means that the magnetic pressure must increase faster than the pressure decreases to prevent pressure-driven instabilities.\nSince \\( W \\) is evaluated along a field line, it is also a surface quantity.\nApplication to 1D Equilibria # \\( \\theta \\) -pinch: Since \\( B_\\theta = 0 \\) , \\( q \\rightarrow \\infty \\) , which really just means \\( q \\) is not well defined for a \\( \\theta \\) -pinch. If we consider some small \\( \\delta B_\\theta \\) , we get a very large \\( q \\) . From a magnetic shear perspective, a \\( \\theta \\) -pinch has very large values of shear and very good stability properties.\nThe wellness is\n\\[W = \\frac{V}{\\langle B^2 \\rangle} \\dv{}{V} \\langle 2 \\mu_0 p \u0026#43; B^2 \\rangle \\\\ = \\frac{\\pi r^2 L}{B_z ^2} \\frac{1}{2 \\pi r L} \\dv{}{r} (2 \\mu_0 p \u0026#43; B_z ^2) \\\\ = \\frac{\\mu_0 r}{B_z ^2} \\dv{}{r} \\left( p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} \\right) = 0 \\] so a \\( \\theta \\) -pinch has neutral magnetic well.\nVacuum case:\n\\[W = \\frac{\\mu_0 r}{ B_z ^2} \\dv{}{r} \\left( \\frac{ B_z ^2}{2 \\mu_0} \\right) = 0\\] So vacuum magnetic fields also have neutral wellness. This leads to a general result sometimes referred to as \u0026ldquo;a plasma cannot dig its own well.\u0026rdquo; In other words, by introducing plasma to a magnetic configuration, it cannot make the configuration more stable than it was. Plasmas make stability more challenging, not less.\nZ-pinch:\nSince \\( B_z = 0 \\) , \\( q = 0 \\) and there is no magnetic shear. Even for a small value of \\( \\delta B_z \\) you still get a small \\( q \\) . The magnetic well properties of a Z-pinch are\n\\[W = \\frac{V}{\\langle B^2 \\rangle} \\dv{}{V} \\langle 2 \\mu_0 p \u0026#43; B^2 \\rangle \\\\ = \\frac{\\mu_0 r}{B_\\theta ^2} \\dv{}{r} \\left( p \u0026#43; \\frac{B_\\theta ^2}{2 \\mu_0} \\right) \\\\ = \\frac{\\mu_0 r}{B_\\theta ^2} \\left( - \\frac{ B_\\theta ^2}{ \\mu_0 r} \\right) = -1\\] Recall that \\( W \u0026gt; 0 \\) for stability, so the Z-pinch has negative magnetic well and provides no pressure stability.\nIn summary,\nBoth \\( \\theta \\) - and Z-pinch have high \\( \\beta \\) \\( \\theta \\) -pinch is stable Z-pinch is unstable End losses in a \\( \\theta \\) pinch enormous since \\( k_\\parallel \\gg k_\\perp \\) Screw Pinch # A natural extension is to combine a moderate toroidal field and a moderate poloidal field to produce a screw pinch configuration.\n\\[\\curl \\vec B = \\mu_0 \\vec j\\] \\[\\rightarrow j_\\theta = - \\frac{1}{\\mu_0 } \\dv{B_z}{r} \\] \\[j_z = \\frac{1}{\\mu_0 r} \\dv{}{r} (r B_\\theta)\\] For static MHD equilibrium\n\\[\\vec j \\cross \\vec B = \\grad p\\] \\[\\dv{}{r} \\left( p \u0026#43; \\frac{ B_\\theta ^2 \u0026#43; B_z ^2}{2 \\mu_0} \\right) = - \\frac{B_\\theta ^2}{\\mu_0 r}\\] We can define a toroidal \\( \\beta \\) where \\( B_0 = B_z (a) \\) \\[\\beta_t = \\frac{\\langle p \\rangle }{B_0 ^2 / 2 \\mu_0} \\\\ = \\frac{2 \\mu_0}{ B_0 ^2} \\frac{1}{\\pi a^2} \\int _0 ^a 2 \\pi r p \\dd r\\] and in the poloidal direction with \\( B_{\\theta, a} = B_\\theta (a) = \\frac{\\mu_0 I}{2 \\pi a} \\) \\[\\beta_p = \\frac{\\langle p \\rangle}{B_{\\theta, a} ^2 / 2 \\mu_0} \\\\ = \\frac{8 \\pi ^2 a^2}{ \\mu_0 I_0 ^2} \\left( \\frac{1}{\\pi a^2} \\int _0 ^a 2 \\pi r p \\dd r \\right) \\\\ = \\frac{16 \\pi ^2}{\\mu_0 I_0 ^2} \\int_0 ^a r p \\dd r\\] To proceed we can multiply the force balance by \\( r^2 \\) and integrate\n\\[\\underbrace{\\int_0 ^a r^2 \\pdv{p}{r} \\dd r}_{(1)} \u0026#43; \\underbrace{\\int _0 ^a \\pdv{}{r} \\left( \\frac{ B_\\theta ^2 \u0026#43; B_z ^2 }{2 \\mu_0} \\right) \\dd r}_{(2)} \u0026#43; \\underbrace{\\int_0 ^a r^2 \\frac{B_\\theta ^2}{\\mu_0 r} \\dd r}_{(3)} = 0\\] \\[(1) = \\int_0 ^a r^2 \\dd p = \\left. r^2 p \\right|_0 ^a - \\int_0 ^a p \\dd (r^2) = - \\int_0 ^a 2 r p \\dd r\\] \\[(2) = \\int_0 ^a r^2 \\dd \\left( \\frac{B_\\theta ^2 \u0026#43; B_z ^2}{2 \\mu_0 } \\right) \\\\ = \\left. r^2 \\left( \\frac{ B_\\theta ^2 \u0026#43; B_z ^2}{2 \\mu_0 } \\right) \\right|_0 ^a - \\int_0 ^a r \\left( \\frac{ B_\\theta ^2 \u0026#43; B_z ^2}{\\mu_0} \\right) \\dd r\\] \\[(3) = \\int_0 ^a r \\frac{B_\\theta ^2}{\\mu_0} \\dd r\\] Combining we have\n\\[- \\int_0 ^a 2 r p \\dd r \u0026#43; \\frac{a^2 B_{\\theta, a}^2}{2 \\mu_0} \u0026#43; \\overbrace{\\frac{a^2 B_0 ^2}{2 \\mu_0}}^{B_0 = B_z(r = a)} - \\int_0 ^a r \\frac{B_z ^2}{\\mu_0} \\dd r = 0 \\\\ - \\int_0 ^a 2 r p \\dd r \u0026#43; \\frac{ \\mu_0 I_0 ^2}{8 \\pi ^2} \u0026#43; \\int_0 ^a r \\left( \\frac{ B_\\theta ^2 - B_z ^2}{\\mu_0} \\right) \\dd r = 0\\] Dividing \\( \\int_0 ^a 2 r p \\dd r \\) gives\n\\[\\left[ \\frac{16 \\pi^2}{\\mu_0 I^2} \\int_0 ^a r p \\dd r \\right] ^{-1} \u0026#43; \\left[\\frac{4 \\mu_0}{B_0 ^2 a^2} \\int_0 ^a r p \\dd r \\right] ^{-1} \\frac{2}{a^2} \\int_0 ^a \\left(1 - \\frac{B_z ^2}{B_0 ^2} \\right) r \\dd r = 1\\] or\n\\[\\frac{1}{\\beta_p} \u0026#43; \\frac{\\alpha_t}{\\beta_t} = 1\\] where \\[\\alpha_t = \\frac{2}{a^2} \\int_0 ^a \\left( 1 - \\frac{B_z^2}{B_0 ^2} \\right) r \\dd r\\] is the diamagnetism.\n\\[\\beta _p = \\left( 1 - \\frac{\\alpha_t}{\\beta_t} \\right) ^{-1}\\] If we have a diamagnetic current, then \\( \\alpha \u0026gt; 0 \\) . This maximizes confinement, since we have confinement in the azimuthal field, as well as the axial field. The limit where you have a skin current such that \\( B_z = 0 \\) inside the plasma results in the best confinement and \\( \\alpha _t = 1 \\) .\nLooking at the safety factor,\n\\[q = \\frac{2 \\pi r B_z}{L B_\\theta}\\] If we look at the edge \\( r = a \\) , \\[q_a = \\frac{2 \\pi a B_0}{L B_{\\theta, a}}\\] As it turns out, this value of the edge safety factor is critically important, and for stability we require that \\( q_a \u0026gt; 1 \\) .\nThe magnetic shear of a screw pinch is\n\\[s = 2 \\frac{V}{q} \\dv{q}{V} \\\\ V = \\pi r^2 L \\\\ \\dd V = 2 \\pi L r \\dd r = \\frac{2 V}{r} \\dd r \\\\ s = \\frac{r}{q} \\dv{q}{r}\\] The shear can be adjusted by changing the applied axial field.\nThe magnetic well is\n\\[W = \\frac{V}{B^2} \\dv{(2 \\mu_0 p \u0026#43; B^2)}{V} \\\\ = \\frac{\\mu_0 r}{B^2} \\dv{}{r} \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\\\ = \\frac{\\mu_0 r}{B^2} \\left( - \\frac{B_\\theta ^2}{\\mu_0 r} \\right) \\\\ = - \\frac{B_\\theta ^2}{B^2} \\\\ = - \\frac{B_\\theta ^2}{B_\\theta ^2 \u0026#43; B_z ^2} \\\\ = - \\left( 1 \u0026#43; \\frac{B_z ^2}{B_\\theta ^2} \\right) ^{-1}\\] So the well is always less than zero, but adding \\( B_z \\) improves the well.\nBy combining the properties of \\( \\theta \\) -pinch and Z-pinch, we are able to sacrifice some \\( \\beta \\) to achieve better stability properties. Of course, we have not addressed the end losses in any way; to do that, we need to connect the ends.\n"},{"id":46,"href":"/r/notes/UWAA543/ch21-3/","title":"Finite Difference and Finite Volume Methods","section":"Computational CFD","content":" Finite Difference and Finite Volume Methods # Finite volume methods can easily handle arbitrary geometries. You can use arbitrarily shaped cells, irregular meshes, and even unstructured meshes. With an irregular mesh, the cell shapes can distort. In an unstructured mesh, the cell shape can even change. Why would you ever want that, you may ask? Consider a circular shape in the middle of our domain. We\u0026rsquo;ll have a pretty hard time fitting a regular or even irregular mesh to a circle.\nDifferential Operators # Starting with a 2-dimensional domain, the divergence operator can be discretized as\n\\[\\div F = \\pdv{F_x}{x} \u0026#43; \\pdv{F_y}{y} = \\frac{F_{x_{i\u0026#43;1, j}} - F_{x _{i-1, j}}}{2 \\Delta x} \u0026#43; \\frac{ F_{y_{i, j\u0026#43;1}} - F_{y _{i, j-1}}}{2 \\Delta y}\\] In a finite volume method, we need to integrate the differential operators over a cell volume\n\\[\\int_V \\div \\vec{F} \\dd V = \\oint \\vec{\\dd S} \\cdot \\vec{F}\\] \\[= \\sum_{\\text{faces}} \\vec{F} \\cdot \\vec{\\dd S}\\] \\[= F_{x_{i\u0026#43;1/2}} \u0026#43; \\dd S_{x_{i\u0026#43;1/2}} \u0026#43; F_{y_{j\u0026#43;1/2}} \\dd S _{u_{j\u0026#43;1/2}} \u0026#43; F_{x_{i-1/2}} \u0026#43; \\dd S_{x_{i-1/2}} \\] | j+1/2 | -------------*------------- | | | | i-1/2 * ij * i+1/2 | | | | | | -------------*------------- | j-1/2 | If you have an orthogonal regular grid, \\( \\dd V_{ijk} = \\Delta x \\Delta y \\Delta z \\), then\n\\[\\dd S_{x_{i\u0026#43;1/2}} = \\Delta y \\Delta z\\] \\[F_{x_{i\u0026#43;1/2}} = \\frac{F_x i \u0026#43; F_{x_{i\u0026#43;1}}}{2}\\] and the same for other terms, then \\( \\dd V_{ijk} \\) reduces to the finite difference operator and the finite volume method is exactly equivalent to the finite difference method. In this sense, finite volume algorithms are simply a straightforward extension of finite difference algorithms, where instead of simply looking ahead/behind by some amount \\( \\Delta x \\), we must here take the appropriate dot products to calculate fluxes across faces of oddly-shaped cells. We get to maintain the conservation form\n\\[\\pdv{Q}{t} = \\div F = 0\\] while using finite difference methods. Finite volume methods are some of the most popular for commercial CFD codes.\nThe modified PDE gives the accuracy from the leading order error term, e.g. \\( O(\\Delta x ^2 \\). Because of the way we expand our Taylor series, the leading order error term tends to look like\n\\[\\propto \\pdv{^3 u}{x^3} \\Delta x^2\\] \\[\\propto \\pdv{^2 u}{x^2} \\Delta x\\] One thing we know about wave PDEs is that adding a term with an odd order derivative will introduce dispersive effects, and adding a term with an even order derivative will add diffusive (viscous) effects.\n"},{"id":47,"href":"/r/notes/UWAA545/08-mhd-equilibrium/","title":"MHD Equilibria","section":"Computational Methods For Plasmas","content":" Equilibrium Calculations # From physical expectations, elliptic equations result from equilibrium calculations and eigenvalue systems. In general, we can write elliptic equations as \\[\\vec A \\vec x = \\vec b\\] where \\( \\vec A \\) may be a nonlinear function of \\( \\vec x \\). For example, if we think of Poisson\u0026rsquo;s equation in 1D, \\[\\dv{^2 \\phi}{x^2} = - \\rho_c\\] and apply finite differencing, \\[\\frac{\\phi_{j\u0026#43;1} - 2 \\phi_j \u0026#43; \\phi_{j-1}}{\\Delta x^2} = - \\rho_j\\] we get a matrix system \\[\\vec A \\vec x = \\frac{1}{\\Delta x^2} \\begin{bmatrix} -2 \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \\\\ 1 \u0026amp; -2 \u0026amp; 1 \u0026amp; \\ldots \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\end{bmatrix} \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\ldots \\end{bmatrix} = \\vec b = - \\begin{bmatrix} \\rho_1 \\\\ \\rho_2 \\\\ \\ldots \\end{bmatrix}\\] where \\( \\vec A \\) is a J by J matrix. In equilibrium calculations, this comes up when solving the Grad-Shafranov equation\n\\[\\Delta ^\\star \\psi = I I\u0026#39; - \\mu_0 r^2 p\u0026#39;\\] where \\[I\u0026#39; = \\pdv{I}{\\psi} \\qquad p\u0026#39; = \\pdv{p}{\\psi}\\] and \\[\\Delta ^\\star \\equiv r \\pdv{}{r} \\frac{r}{r} \\pdv{}{r} \u0026#43; \\pdv{^2}{z^2}\\] The Grad-Shafranov equation is a nonlinear equation, and we often assume a power series expansion of the current density and pressure \\[I(\\psi) = I_0 \u0026#43; I_1 \\psi \u0026#43; I_2 \\psi^2 \u0026#43; \\ldots\\] \\[p(\\psi) = p_0 \u0026#43; p_1 \\psi \u0026#43; p_2 \\psi^2 \u0026#43; \\ldots\\] For a force-free equilibrium (\\( p = 0 \\)) with a linear current profile \\[I I\u0026#39; = I_0 I_1 \u0026#43; I_1 ^2 \\psi\\] this makes the Grad-Shafranov equation a linear system. \\[\\vec A = \\Delta ^\\star _{\\text{FD}} \u0026#43; I_1 ^2\\] \\[\\vec x = \\psi_{ij} \\qquad \\vec b = (I_0 I_1) _{ij}\\] where \\( \\Delta ^\\star _{\\text{FD}} \\) is a finite difference representation of the \\( \\Delta ^\\star \\) operator. For a given geometry and poloidal current profile, \\( I_0 \\) and \\( I_1 \\), we can solve \\( \\vec A \\vec x = \\vec b \\) to give \\( \\psi(r, z) \\).\nSolving \\( \\vec A \\vec x = \\vec b \\) can be done using either direct or indirect (iterative) methods. Direct decomposition methods (like Kramer\u0026rsquo;s rule, Gaussian decomposition) require \\( \\mathcal{O}(N^3) \\) operations, where \\( N \\) is the size of the matrix. They also require full matrix storage, even though \\( \\vec A \\) is a sparse (tridiagonal) matrix.\nIterative methods help solve this problem. We begin with a solution which is improved upon until we get convergence. If this converges in fewer operations than a full decomposition, then we\u0026rsquo;ve saved some time and a lot of in-memory storage space.\n\\[x^0 \\rightarrow x^1 \\rightarrow x^2 \\rightarrow \\ldots\\] \\[\\vec x^{n\u0026#43;1} = \\vec x^n \u0026#43; \\vec B^{-1} (\\vec b - \\vec A \\vec x^n) \\\\ = \\vec x^n \u0026#43; \\vec B^{-1} \\vec r^n\\] The technique boils down to a clever choice of \\( \\vec B \\). We want to choose an approximation matrix \\( \\vec B \\) which is very close to \\( \\vec A \\), but is much easier to invert. \\[\\vec B \\approx \\vec A\\] For this kind of iterative method, each iteration requires \\( \\mathcal{O}(N) \\) operations, and converges in fewer than \\( N \\) iterations. The best iterative methods can converge in \\( \\mathcal{O}(N \\ln N) \\) total operations.\nLinear Stability # When we studied plasma waves at the beginning of this course, we performed a linearization of the Vlasov equation about an equilibrium state. This is how we got dispersion relations for waves. We assumed an equilibrium of a uniform plasma, with at most static magnetic fields. For more interesting equilibria with nonzero gradients, we follow the same procedure.\nFirst, we linearize the ideal MHD equations about a static equilibrium (\\( \\vec v_0 = 0 \\)) \\[\\pdv{\\rho_1}{t} = - \\div (\\rho_0 \\vec v_1)\\] \\[\\pdv{p_1}{t} = - \\div (p_0 \\vec v_1) \u0026#43; (1 - \\Gamma) p_0 \\div \\vec v_1\\] \\[\\pdv{\\vec B_1}{t} = \\curl (\\vec v_1 \\cross \\vec B_0)\\] \\[\\rho _0 \\pdv{\\vec v_1}{t} = \\grad p_1 \u0026#43; \\frac{1}{\\mu_0} \\left[(\\curl \\vec B_1) \\cross \\vec B_0 \u0026#43; (\\curl \\vec B_0) \\cross \\vec B_1 \\right]\\] Given an equilibrium (\\( \\rho_0 \\), \\( p_0 \\), \\( \\vec B_0 \\)), the linearized MHD equations can be integrated in time to evolve \\( \\rho_1 \\), \\( p_1 \\), \\( \\vec B_1 \\), \\( \\vec v_1 \\). If we do this and \\( \\vec v_1 \\) grows in time, then the plasma is unstable.\nNotice that \\( \\rho_1 \\) decouples from the other equations, so we don\u0026rsquo;t need to evolve it if we don\u0026rsquo;t want to, or we can evolve it in parallel.\nIt\u0026rsquo;s often more useful to study individual modes. We can assume a form of the perturbation to select specific modes. For arbitrary perturbed quantity \\( g_1 \\),\n\\[g_1(\\vec r, t) = g_1(r, \\theta, z, t) = \\hat g_1 (r, z, t) e^{i m \\theta}\\] where \\( m \\) is the azimuthal mode number. As we\u0026rsquo;ve written it, \\( g_1 \\) is complex-valued, but only because \\( e^{i m \\theta} \\) is a convenient representation of a Fourier mode. We\u0026rsquo;re only interested in the real component.\n\\[\\text{Re}(g_1) \\leftrightarrow g_1(\\theta = 0)\\] \\[\\text{Im}(g_1) \\leftrightarrow g_1(\\theta = - m \\pi / 2)\\] Example: Spheromak Tilt m=1 # As an example, we consider the \\( m=1 \\) tilt mode in the spheromak, a device which consists of a toroidal equilibrium contained in a cylindrical can. The tilt mode, corresponding to \\( m=1 \\), rotates the equilibrium in the \\( r-z \\) plane. The elongation ratio \\( L/R \\) determines the configuration\u0026rsquo;s stability against the tilt mode.\n(stable: L/R=1, unstable: L/R=2)\nPlotting the real part of \\( g \\) in the \\( r-z \\) plane at \\( \\theta = 0 \\) looks something like this:\nWe can fill out the lower half of the plane at \u0026ldquo;\\( r \u0026lt; 0 \\)\u0026rdquo; by setting \\( \\theta = \\pi \\), so we get the negative of the upper half\nTo align the \\( r \\) axes, we need to flip this upside-down before we can put it on the same axes as the upper \\( r-z \\) plane.\nThe assumed form is implemented in the linear equations. Noting:\n\\[\\pdv{}{\\theta} g_1 = i m \\vu g_1 (r, z, t) e^{i m \\theta} \\\\ = i m g_1\\] The remaining terms are approximated with finite differences.\n\\[{\\rho_1} _{i, j} ^{n \u0026#43; 1} = {\\rho_1}_{i, j} ^n - \\left[ \\frac{\\Delta t (r \\rho_0 v_{1, r}) ^n _{i \u0026#43; 1, j} - (r \\rho_0 v_{1, r})^n _{i - 1, j} }{2 r_i \\Delta r} \\right. \\\\ \\left. \\qquad \u0026#43; \\frac{im}{r_i} (\\rho_0 v_{1, \\theta})^n _{i, j} \u0026#43; \\frac{(\\rho_0 v_{1, z})^n _{i, j \u0026#43; 1} - (\\rho_0 v_{1, z})^n _{i, j-1}}{2 \\Delta z} \\right]\\] We can construct the other difference relations for the other perturbed quantities from the linearized ideal MHD equations:\n\\[{p_1}_{ij} ^{n\u0026#43;1} = {p_1}_{ij} ^n \u0026#43; \\Delta t \\left[ \\ldots \\right]^n\\] \\[{\\vec B_1}_{ij} ^{n\u0026#43;1} = {\\vec{B_1}}_{ij} ^n \u0026#43; \\Delta t [ \\ldots ] ^n\\] \\[{\\vec v_1} _{ij} ^{n\u0026#43;1} = {\\vec v_1}_{ij} ^{n\u0026#43;1} \u0026#43; \\Delta t [\\ldots ]^n\\] Normalization # We normalize the PDE\u0026rsquo;s by nondimensionalizing. We do this by choosing characteristic values for density, pressure, magnetic field, velocity\n\\[\\rho_0 = \\tilde{\\rho_0} \\rho ^\\star \\qquad \\text{where} \\quad \\tilde{\\rho_0} \\in [0, 1] \\] \\[\\rho_1 = \\tilde{\\rho_1} \\rho^\\star\\] \\[p_0 = \\tilde{p_0} p^\\star\\] \\[\\vec B_0 = \\tilde{\\vec B} B^\\star\\] \\[\\vec v_1 = \\tilde{\\vec v_1} v^\\star \\qquad v^\\star = L / \\tau\\] We can combine into a single PDE by taking the time derivative of the momentum equation, such that the sound speed and Alfven speed fall out:\n\\[\\pdv{ ^2 \\tilde{\\vec v_1}}{\\tilde t ^2} = \\frac{\\tau ^2}{L^2} \\frac{\\Gamma p^\\star}{\\rho ^\\star} f_a (\\tilde{\\rho}_0, \\tilde{p}_0, L/R, \\tilde{\\vec{v}}_1) \u0026#43; \\frac{\\tau^2}{L^2} \\frac{{B^\\star} ^2}{\\mu_0 \\rho^\\star} f_b(\\tilde{\\rho}_0, \\tilde{\\vec B}_0, L/R, \\tilde{\\vec v}_1)\\] \\[\\frac{\\Gamma p^\\star}{\\rho ^\\star} = v_s ^2 \\qquad \\frac{{B^\\star}^2}{\\mu_0 \\rho ^\\star}\\] here \\( f_a \\) and \\( f_b \\) are some appropriate conversion functions.\nIn our nondimensionalization, we normalize \\( z \\) and \\( r \\) by characteristic lengths \\( L \\) and \\( R \\):\n\\[t = \\tilde t \\tau\\] \\[z = \\tilde z L\\] \\[r = \\tilde r R = \\frac{\\tilde r L}{L/R}\\] Initial Conditions # To solve for the evolution of a perturbation in time, we need initial conditions. The initial condition is both the equilibrium \\( \\rho_0, p_0, \\vec B_0, \\vec v_0 = 0 \\) and the perturbation \\( \\vec v_1 (r, z) \\). For stability analysis, we generally want our perturbation to excite all modes (or as many as possible), and avoid single mode perturbations. The dominant mode will grow the fastest, and we\u0026rsquo;ll see it come out of the excitation.\nYou don\u0026rsquo;t want to perturb a magnetic field that breaks \\( \\div \\vec B_1 = 0 \\). In generall, you can just perturb the velocity \\( \\vec v_1 (r, z) \\), for example like this \\[\\text{Re}(\\vec v_1) \\propto \\sin \\left( \\frac{\\pi r}{R} \\right) \\cos \\left( \\frac{\\pi z}{L} \\right)\\] Because we generally don\u0026rsquo;t expect a radial dependence to be a sin function, \\( \\sin (\\pi r / R) \\) should excite many modes. We also need to meet the boundary conditions, hence using functions with easy to compute zeroes.\nWe the advance the solution in time, and track the evolution of the perturbed kinetic energy\n\\[KE_1(t) = \\int \\frac{1}{2} \\rho_0 \\vec {v_1} ^2 \\dd \\vec V\\] If the equilibrium is unstable, then a dominant mode will grow out of the oscillations with a measurable growth rate \\( \\gamma \\)\nTo study the accuracy order and compare with results, we need to perform a convergence study of the growth rate. We do this by performing the simulation with different grid resolutions. If we plot against some kind of normalized grid resolution \\( J_0 ^2 / J ^2 \\), where \\( J \\) is the most coarse grid resolution. We expect second-order convergence, so plotting the square of the resolution should give us straight lines moving towards zero.\nThe difference between points as we move to higher resolution gives a bound on the error. We may also find solutions that appear to be unstable, but the projected growth rate is zero (or close to it)\nThe eigenfunction \\( \\vec v_1 \\) itself contains useful information, since it describes the unstable behavior. In the m=1 spheromak case, it indicates that the spheromak will tilt. There are other modes that shift the configuration. The difference between these can be seen by looking at the eigenfunction itself.\nThe advantages of this time integration method are:\nSimple to implement. The approach is general to many equilibria. Some of the disadvantages are\nRequires a choice of \\( \\Delta t \\) (eigenvalue problem) Marginal stability boundaries are difficult to determine exactly. Boundary Conditions for Stability Analysis # For a rigid conducting wall,\n\\[\\pdv{}{t} \\vec B_1 \\cdot \\vu n = 0 \\qquad \\text{or, usually} \\quad \\vec B_1 \\cdot \\vu n = 0\\] \\[\\vec v_1 \\cdot \\vu n = 0\\] Scalar quantities (\\( \\rho_1 \\), \\( p_1 \\)) are set to zero at the boundaries.\nThere are also axis conditions in cylindrical geometry, where \\( r = 0 \\). For axis boundaries, we want to assume that all variables are analytic at the axis. That is, there are no singularities, and the solution is differentiable. Then, we want to map from the \\( r \\)-\\( \\theta \\) plane to an \\( x \\)-\\( y \\) plane. The \\( z \\) direction is a trivial one-to-one mapping. Then, we expand in a power series such that:\n\\[\\vec A = \\begin{cases} A_x \u0026amp; = \u0026amp; A \u0026#43; Bx \u0026#43; cy \u0026#43; \\ldots \\\\ A_y \u0026amp; = \u0026amp; D \u0026#43; Ex \u0026#43; Fy \u0026#43; \\ldots \\\\ A_z \u0026amp; = \u0026amp; G \u0026#43; Hx \u0026#43; Jy \u0026#43; Kx^2 \u0026#43; Ly^2 \u0026#43; \\ldots \\end{cases}\\] Ordinarily, you would extend this out to more terms than this, but in this case we happen to already know how many terms we need. Then, map to the cylindrical coordinates:\n\\[A_r = A_x \\cos \\theta \u0026#43; A_y \\sin \\theta \\qquad x = r \\cos \\theta\\] \\[A_\\theta = - A_x \\sin \\theta \u0026#43; A_y \\cos \\theta \\qquad y = r \\sin \\theta\\] Substituting in the expanded quantities gives:\n\\[\\begin{aligned} A_r \u0026amp; =\u0026amp; \\frac{1}{2} ( B \u0026#43; F) r \u0026#43; A \\cos \\theta \u0026#43; D \\sin \\theta \u0026#43; \\frac{1}{2} (B - F) r \\cos (2 \\theta) \\\\ \u0026amp; \u0026amp; \u0026#43; \\frac{1}{2} (C \u0026#43; E) r \\sin (2 \\theta) \u0026#43; \\ldots \\end{aligned}\\] \\[\\begin{aligned} A_\\theta \u0026amp;=\u0026amp; \\frac{1}{2} (E - C) r \u0026#43; D \\cos \\theta - A \\sin \\theta \u0026#43; \\frac{1}{2} (C \u0026#43; E) r \\cos (2 \\theta) \\\\ \u0026amp; \u0026amp; \u0026#43; \\frac{1}{2}(F - B) r \\sin (2 \\theta) \u0026#43; \\ldots \\end{aligned}\\] \\[\\begin{aligned} A_z \u0026amp; = \u0026amp; H \u0026#43; Hr \\cos \\theta \u0026#43; J r \\sin \\theta \u0026#43; K r^2 \\cos ^2 \\theta \\\\ \u0026amp; \u0026amp; \u0026#43; L r^2 \\sin ^2 \\theta \u0026#43; \\ldots \\end{aligned}\\] The solution behavior at the axis \\( r \\rightarrow 0 \\) depends on the \\( \\theta \\) resolution. We assumed \\( A \\propto e^{i m \\theta} \\), or\n\\[\\vec A = \\text{Re}\\left[ \\left(\\vec A ^n \u0026#43; i \\vec A ^i \\right) e^{i m \\theta}\\right] \\\\ = \\vec A^r \\cos (m \\theta) - \\vec A^i \\sin (m \\theta)\\] where \\( \\vec A ^r \\) is the real component of \\( \\vec A \\). If we consider axisymmetric modes, \\( m = 0 \\), based on this expression we have no \\( \\theta \\) dependence \\[\\vec A = \\vec A^r\\] We set the appropriate coefficients to match the expansion. If there is no \\( \\theta \\) dependence, each part of our expansion for \\( A_r \\), \\( A_\\theta \\), \\( A_z \\) which has a \\( \\theta \\) dependence must vanish:\n\\[A = D = B - F = C \u0026#43; E = H = J = K = L = 0\\] so \\[B = F \\qquad \\text{and} \\qquad C = - E\\] which gives\n\\[A_r = Br \\] \\[A_\\theta = Er\\] \\[A_z = G \\] as \\( r \\rightarrow 0 \\). So the boundary conditions at the axis are:\n\\[\\left.A_r \\right|_{\\text{axis}} = 0\\] \\[\\left.A_\\theta \\right|_{\\text{axis}} = 0\\] \\[\\left.A_z \\right|_{\\text{axis}} = \\text{const.} \\rightarrow \\qquad \\left.\\pdv{A_z}{r} \\right| _{\\text{axis}} = 0\\] For \\( m = 1 \\), \\( \\vec A = \\vec A ^r \\cos \\theta - \\vec A^i \\sin \\theta \\)\ngives\n\\[B \u0026#43; F = B - F = C \u0026#43; E = E - C = G = K = L = 0\\] \\[B = F = C = E = 0\\] which gives \\[A_r = A \\cos \\theta \u0026#43; D \\sin \\theta\\] \\[A_\\theta = D \\cos \\theta - A \\sin \\theta\\] \\[A_z = H r \\cos \\theta \u0026#43; J r \\sin \\theta\\] So the boundary conditions are\n\\[\\left. \\pdv{A_r}{r} \\right|_{\\text{axis}} = 0\\] \\[\\left. \\pdv{A_\\theta}{r} \\right|_{\\text{axis}} = 0\\] \\[\\left. A_z \\right|_{\\text{axis}} = 0\\] Note, the real component of the \\( r \\) component is the \\( \\theta \\) component of the imaginary component \\[A_r ^r = A_\\theta ^i\\] and \\[A_\\theta ^r = - A_r ^i\\] We can do the same thing for other mode numbers, and arrive at a general correspondence. Scalar variables map in the same way the axial component \\( A_z \\).\nQuestions??\nSetting up the initial conditions:\nwhat goes in \u0026ldquo;e\u0026rdquo;? calculate B_x, B_y from J_z? "},{"id":48,"href":"/r/notes/UWAA558/09-2d-equilibria/","title":"2D Equilibria","section":"MHD Theory","content":" 2D Equilibria # Let\u0026rsquo;s connect the ends of our 1D equilibria. Doing so is what gives us inherently toroidal configurations. From the 1-dimensional picture:\n\\[\\vec j \\cross \\vec B = j_\\theta B_z - j_z B_\\theta \\\\ = \\grad p = \\dv{p}{r}\\] we move to an axisymmetric 2-dimensional torus, replacing our cylindrical coordinate system with a toroidal one\n\\[\\vec j \\cross \\vec B = \\vec j_\\theta \\cross \\vec B_\\phi \u0026#43; \\vec j_\\phi \\cross \\vec B_\\theta = \\grad p\\] Eventually, the toroidal force balance will lead to the Grad-Shafranov Equation, which tells us how we can solve for a general equilibrium that solves \\( \\vec j \\cross \\vec B = \\grad p \\) .\nLet\u0026rsquo;s consider how we might achieve such a configuration. A toroidal magnetic field can be achieved by driving current through a poloidal coil. A more complicated problem is how to drive toroidal current. In general this is done by means of a transformer, where the plasma itself is the secondary circuit. Driving a time-varying current through the primary induces a toroidal current through the plasma. This is called a transformer drive for current.\nGrad-Shafranov equation # Computing \\( j_\\theta \\) and \\( B_\\phi \\) can be computationally difficult in a toroidal geometry, so let\u0026rsquo;s do some work towards simplifying our force balance expression. The toroidal magnetic vector potential is defined as\n\\[\\vec B_\\theta = \\curl \\vec A_\\phi\\] If we integrate \\( B_\\theta \\) over a poloidal surface, Stokes\u0026rsquo; theorem gives\n\\[\\int _{S_p} \\curl \\vec A_\\phi \\cdot \\dd \\vec S = \\oint \\vec A_\\phi \\cdot \\dd \\vec l \\\\ = \\int _{S_p} B_\\theta \\cdot \\dd \\vec S = \\Psi _p \\] If the equilibrium is axisymmetric, \\( A_\\phi \\) must be uniform along \\( \\dd l \\) , so\n\\[A_\\phi \\vu \\phi \\cdot \\oint \\dd \\vec l = A_\\phi 2 \\pi R = \\Psi _p \\\\ \\rightarrow A_\\phi = \\frac{ \\Psi_p}{R} \\vu \\phi\\] where we absorb the factor of \\( 2 \\pi \\) into the poloidal flux \\( \\Psi _p \\) . After some manipulation, we can relate \\( B_\\theta \\) to the poloidal flux\n\\[\\vec B_\\theta = \\curl \\vec A_\\phi = - \\frac{ \\vu R}{R} \\pdv{\\Psi}{z} \u0026#43; \\frac{\\vu z}{R} \\pdv{\\Psi}{R}\\] \\[\\mu_0 j_\\phi \\cross B_\\theta = (\\curl \\vec B_\\theta) \\cross \\vec B_\\theta \\\\ = \\curl \\vec B_\\theta \\cross \\left( \\grad \\Psi \\cross \\frac{\\vu \\phi}{R} \\right) \\\\ = - \\left[ \\pdv{}{R} \\left( \\frac{1}{R} \\pdv{\\Psi}{R} \\right) \u0026#43; \\frac{1}{R} \\pdv{\\Psi ^2}{z^2} \\right] \\cdot \\left[\\frac{ \\grad \\Psi}{R} ( \\vu \\phi \\cdot \\vu \\phi) - \\frac{ \\phi}{R} \\cancel{(\\grad \\Psi \\cdot \\vu \\phi)} \\right]\\] That gives the first component of \\( \\grad p \\) , now let\u0026rsquo;s do the other one\n\\[\\mu_0 \\vec j_\\theta \\cross \\vec B_\\phi = ( \\curl \\vec B_\\phi) \\cross \\vec B_\\phi \\\\ = \\left[ - \\vu R \\pdv{B_\\phi}{z} \u0026#43; \\vu z \\frac{1}{R} \\pdv{}{R} ( R B_\\phi) \\right] \\cross \\vec B_\\phi \\\\ = - \\frac{B_\\phi}{R} \\left[ \\vu R \\pdv{}{R} (R B_\\phi) \u0026#43; \\vu z \\pdv{}{z} (R B_\\phi) \\right] \\\\ = - \\frac{B_\\phi}{R} \\grad (R B_\\phi)\\] Finally, since pressure is a flux surface quantity we can write\n\\[\\grad p = \\dv{p}{\\Psi} \\grad \\Psi = p\u0026#39; \\grad \\Psi\\] The toroidal force balance now looks like\n\\[\\mu_0 p\u0026#39; \\grad \\Psi = - \\frac{1}{R} \\left( \\pdv{}{R} \\frac{1}{R} \\pdv{\\Psi}{R} \u0026#43; \\frac{1}{R} \\pdv{^2 \\Psi}{z^2} \\right) \\grad \\Psi - \\frac{B_\\phi}{R} \\grad(R B_\\phi)\\] We notice that the only vector quantities here are \\( \\grad \\Psi \\) and \\( \\grad (R B_\\phi) \\) , so \\( \\grad (R B_\\phi) \\) must be parallel to \\( \\grad \\Psi \\) and is a flux surface quantity. We can define our new flux surface quantity as\n\\[F(\\Phi) \\equiv R B_\\phi = \\frac{\\mu_0 I_\\theta}{2 \\pi} = \\frac{\\mu_0}{2 \\pi} \\int_{S_p} \\vec j_\\theta \\cdot \\dd \\vec S\\] \\[\\grad F = \\dv{F}{\\Psi} \\grad \\Psi = F\u0026#39; \\grad \\Psi\\] Now each term in the toroidal force balance has a factor of \\( \\grad \\Psi \\) attached. Let\u0026rsquo;s multiply through by \\( R^2 \\) and factor out the gradient to arrive at the Grad-Shafranov equation:\n\\[R^2 \\mu_0 p\u0026#39; = - \\Delta ^\\star \\Psi - F F\u0026#39;\\] where \\[\\Delta ^\\star \\equiv R \\pdv{}{R} \\frac{1}{R} \\pdv{}{R} \u0026#43; \\pdv{^2}{z^2}\\] To solve the Grad-Shafranov equation, you solve for \\( \\Psi(R, z) \\) , which determines \\( p(\\Psi) \\) and \\( F(\\Psi) \\) , which directly gives you \\( p(R, z) \\) and \\( F(R, z) \\) and completely defines the equilibrium.\nYou can solve for the other terms as well. Since \\( \\vec B_\\theta = \\frac{\\grad \\Psi}{R} \\cross \\vu \\phi \\) \\[\\vec j_\\phi = - \\frac{1}{\\mu_0 R} \\Delta ^\\star \\Psi \\vu \\phi\\] and since \\( \\vec B_\\phi = \\frac{F}{R} \\vu \\phi \\) \\[\\vec j_\\theta = - \\frac{1}{\\mu_0 R} \\grad (R B_\\phi) \\cross \\vu \\phi\\] For the G-S equation to be solvable, you need to specify the equilibrium by specifying \\( p(\\Phi) \\) and \\( F(\\Phi) \\) . In practice, this is usually done by making experimental measurements to determine \\( p \\) and \\( F \\) . A common code that does this is called EFIT, which takes the boundary conditions of the magnetic field and measurements of temperature, density to perform a least-squares fit to solve the G-S equation.\nIn general, the Grad-Shafranov equation leads to a matrix equation\n\\[\\overline \\vec A \\vec \\Psi \u0026#43; \\vec f(\\Psi) = \\vec g\\] Depending on the conditions we place on \\( \\Psi \\) , \\( \\vec f(\\Psi) \\) can be a nonlinear function.\nSolutions to the Grad-Shafranov equation # In the limit that \\( \\vec j \\parallel \\vec B \\) , then \\( \\vec j \\cross \\vec B = 0 = \\grad p \\rightarrow p\u0026#39; = 0 \\) . These are called force-free states. In the G-S equation, the pressure term vanishes and we\u0026rsquo;re left with\n\\[\\Delta ^\\star \\Psi \u0026#43; F F\u0026#39; = 0\\] Spheromaks and RFPs are examples of nearly force-free states in which the current is nearly parallel to the magnetic field. Notice that in completely force-free states, \\( \\langle \\beta \\rangle = 0 \\) .\nAnother interesting limit is the case where \\( F F\u0026#39; \\gg \\Delta ^\\star \\Psi \\) . Now we have \\[\\grad p \\approx \\vec j_\\theta \\cross \\vec B_\\phi\\] which looks like a \\( \\theta \\) -pinch which has been connected at the ends. Remember from the previous section that we can not maintain radial force balance with purely toroidal fields, so the toroidal current is not zero (hence the \\( \\approx \\) ) but is just high enough to maintain radial force balance. This sort of configuration is called a high- \\( \\beta \\) tokamak.\nThe other limit is \\( F F\u0026#39; \\ll \\Delta ^\\star \\Psi \\) \\[\\grad p \\approx \\vec j_\\phi \\cross \\vec B_\\theta\\] which looks like an end-connected z-pinch. This configuration is usually called an Ohmically heated Tokamak, and the majority of currently operating tokamaks operate this way. As we know, a purely poloidal field has very bad stability properties, so \\( \\vec B_\\phi \\) needs to be added to provide stability. The toroidal \\( \\beta \\) is very small \\[\\beta _t \\ll 1 \\qquad \\beta _p \\approx 1\\] Stability Considerations # The same stability factors exist in 2D equilibria that we found for 1D equilibria:\nMagnetic shear - the safety factor \\( q(\\Psi) = \\frac{\\Delta \\phi}{\\Delta \\theta} \\) for \\( \\Delta \\theta = 2 \\pi \\) . We can calculate \\( q \\) more easily by integrating along a flux surface in the poloidal plane:\n\\[q(\\Psi) = \\frac{1}{2 \\pi} \\int_0 ^{\\Delta \\phi} \\dd \\phi \\\\ = \\frac{1}{2 \\pi} \\int _0 ^{2 \\pi} \\dv{\\phi}{\\theta} \\dd \\theta \\\\ = \\frac{1}{2 \\pi} \\int_0 ^{2 \\pi} \\dd \\theta \\left. \\frac{r B_\\phi}{R B_\\theta} \\right|_{\\text{along flux surf.}} \\\\ = \\frac{F(\\Psi)}{2 \\pi} \\oint \\frac{ \\dd l_p}{R^2 B_\\theta} \\qquad \\dd l_p = r \\dd \\theta\\] Magnetic well: similarly we can get the magnetic well factor by integrating around a flux surface in the poloidal plane\n\\[\\langle Q \\rangle = \\frac{\\oint \\frac{Q \\dd l_p}{B_\\theta}}{\\oint \\frac{ \\dd l_p}{B_\\theta}}\\] Shafranov Shift # Remember that when we had an equilibrium which had a toroidal current and a corresponding poloidal magnetic field, and a poloidal magnetic field, then radial force balance will tend to shift the configuration outwards away from the major axis and a conducting wall or external coil will be required to maintain the equilibrium. The radial force balance is really achieved by \\( \\vec j_\\phi \\cross B_p \\) As we move towards the magnetic axis, \\( B_p \\rightarrow 0 \\) by definition. With less poloidal field to balance the radial force imbalance, there is more radial expansion. This means that inner portion of the plasma (inner flux surfaces) must shift radially further to achieve radial force balance.\nThe shift increases with plasma pressure. This effect is further enhanced if we have low poloidal fields, for example in the high- \\( \\beta \\) tokamak configurations.\nLow aspect ratios also enhance the effect. Recall that the radial force imbalance increases with smaller aspect ratio, leading to a larger shift.\n"},{"id":49,"href":"/r/notes/UWAA543/ch21-4/","title":"Implicit Algorithms","section":"Computational CFD","content":" Implicit Algorithms # Stability conditions are often related to the CFL number, and stability analysis of explicit algorithms usually require us to set a limit to small CFL\n\\[\\left| \\frac{a \\Delta x}{\\Delta t} \\right| \\leq 1\\] For a given grid, this effectively sets a limit on the time step. The time step can be no larger than the grid spacing divided by the propagation constant \\( \\Delta t \\leq \\Delta x / |a| \\). As an example, say we have a device of interest that is 1 meter long, and we want a grid resolution such that we can resolve features that are 1mm in size. Say the problem domain is flow over an airfoil\n\\[c_s \\approx 1000 m/s\\] \\[\\Delta x / a \\approx 10^{-6}\\] The time step is limited to less than a microsecond! If the system has any appreciable size, we are probably going to need a lot of compute time to simulate any substantial time scale.\nIf there are boundary layers that need to be resolved, even finer grid resolution is required. The limit on \\( \\Delta t \\) comes from the minimum grid resolution, so non-uniform grid spacings can even further limit the possible time step size.\nSuch strict constraints on the possible time step can make explicit finite difference methods unfeasible. This motivates implicit methods in which we construct the solution based on data that is not yet available\n\\[u^{n\u0026#43;1} = f(u^n, u^{n\u0026#43;1})\\] Implicit FTCS (Backward Euler) # As an example, let\u0026rsquo;s devise an implicit algorithm based on FTCS where we evaluate \\( \\Delta x \\) at time \\( n+1 \\)\n\\[\\frac{u_j ^{n\u0026#43;1} - u_j ^n}{\\Delta t} \u0026#43; a\\frac{ u_{j\u0026#43;1} ^{n\u0026#43;1} - u_{j-1} ^{n\u0026#43;1}}{2 \\Delta x} = 0 \\] \\[\\rightarrow u_j ^{n\u0026#43;1} = u_j - \\frac{a \\Delta t}{\\Delta x} \\left( \\frac{ u_{j\u0026#43;1} ^{n\u0026#43;1} - u_{j-1} ^{n\u0026#43;1}}{2} \\right)\\] Accuracy: \\( O(\\Delta t, \\Delta x^2) \\)\nvon Neumann Stability analysis:\n\\[\\varepsilon_j ^{n\u0026#43;1} = \\varepsilon_j ^n - \\frac{ a \\Delta t}{\\Delta x} \\left( \\frac{ \\varepsilon_{j\u0026#43;1} ^{n\u0026#43;1} - \\varepsilon_{j-1} ^{n\u0026#43;1} }{2} \\right)\\] \\[\\varepsilon_j ^n = V^n e^{i k j \\Delta x}\\] \\[\\frac{ V^{n\u0026#43;1}}{V^n} \\left[ 1 \u0026#43; \\frac{ a \\Delta t}{2\\Delta x} \\left( e^{i k \\Delta x} - e^{- i k \\Delta x} \\right) \\right] = 1\\] \\[\\rightarrow \\left| \\frac{V^{n\u0026#43;1}}{V^n} \\right| = G = \\sqrt{ \\frac{1}{1 \u0026#43; \\frac{a^2 \\Delta t^2}{\\Delta x^2} \\sin ^2(k \\Delta x)}}\\] So the backward Euler method is unconditionally stable! Great, we\u0026rsquo;ve got an algorithm that\u0026rsquo;s unconditionally stable. But how do we actually solve it?\nWe can write backward Euler as a matrix equation \\( \\overline{A} \\overline{x} = \\overline{b} \\) where\n\\( \\overline{A} \\) = operator matrix \\( \\overline{x} \\) = solution vector (\\( u^{n+1} \\)) \\( \\overline{b} \\) = inhomogeneity vector (\\( u^n \\)) Getting to the matrix expression, re-write Backward Euler by putting all of the terms at future times on the left-hand side (in order of increasing \\( j \\) index), and all of the terms at known times on the right-hand side.\n\\[- \\frac{ a \\Delta t}{\\Delta x} u_{j-1} ^{n\u0026#43;1} \u0026#43; u_j ^{n\u0026#43;1} \u0026#43; \\frac{a \\Delta t}{2 \\Delta x} u^{n\u0026#43;1} _{j\u0026#43;1} = u_n ^n\\] \\[\\overline{A} \\overline{x} = \\overline{b}\\] \\[\\begin{bmatrix} 1 \u0026amp; \\left( \\frac{a \\Delta t}{2 \\Delta x} \\right) \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp; 0 \u0026amp; - \\left( \\frac{a \\Delta t}{2 \\Delta x} \\right) \u0026amp; 1 \u0026amp; \\left( \\frac{a \\Delta t}{2 \\Delta x} \\right) \u0026amp; 0 \u0026amp; \\ldots \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp;\\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 0 \u0026amp; - \\left( \\frac{a \\Delta t}{2 \\Delta x} \\right) \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} u_1 ^{n\u0026#43;1} \\\\ \\ldots \\\\ u_{j-1} ^{n\u0026#43;1} \\\\ u_j ^{n\u0026#43;1} \\\\ u_{j\u0026#43;1} ^{n\u0026#43;1} \\\\ \\ldots \\\\ u_J ^{n\u0026#43;1} \\end{bmatrix} = \\begin{bmatrix} u_1 ^{n} \\\\ \\ldots \\\\ u_{j-1} ^{n} \\\\ u_j ^{n} \\\\ u_{j\u0026#43;1} ^{n} \\\\ \\ldots \\\\ u_J ^{n} \\end{bmatrix} \\] In matrix form, it\u0026rsquo;s easy to see how we can get to \\( \\overline{x} \\) from \\( \\overline{b} \\)\n\\[\\rightarrow x = A^{-1} b\\] In this case the operator matrix is tridiagonal, so it can be easily inverted.\nTheta Algorithm # We can get a more accurate form if we combine Backward Euler with FTCS with some multiplier\n\\[u_j ^{n\u0026#43;1} \u0026#43; \\Theta \\frac{ a \\Delta t}{\\Delta x} \\left( \\frac{ u_{j\u0026#43;1} ^{n\u0026#43;1} - u_{j-1} ^{n\u0026#43;1}}{2} \\right) = u_j ^n - (1 - \\Theta) \\frac{ a \\Delta t}{\\Delta x} \\left( \\frac{ u_{j\u0026#43;1}^n - u_{j-1} ^n}{2} \\right) \\] We call this method the \\( \\Theta \\)-method. If we write in terms of our differential operators,\n\\[\\Delta _t u^n \u0026#43;a (1 - \\Theta) \\delta _x u^n \u0026#43; a \\Theta \\delta _x u^{n\u0026#43;1} = 0\\] As we\u0026rsquo;ll see, \\( \\Theta = [0,1] \\) for stability.\n\\( \\Theta = 1 \\): is just Backward Euler \\( \\Theta = 0 \\): we\u0026rsquo;ve just got FTCS \\( \\Theta = 1/2 \\): This is called Crank-Nicolson We find that by using \\( \\Theta \\neq 1 \\), we can improve the performance (condition number of operator matrix) by making the operator matrix more diagonally dominant. It can also reduce diffusion (possibly making it better for shock capture), with the flip-side being an increase in dispersion. It can also increase the temporal accuracy \u0026ndash; we find that for \\( \\Theta = 1/2 \\), the accuracy is \\( O(\\Delta t^2, \\Delta x^2) \\)\n"},{"id":50,"href":"/r/notes/UWAA558/10-equilibrium-of-3d-configurations/","title":"Equilibrium of 3D Configurations","section":"MHD Theory","content":" Equilibrium of 3D Configurations # In 3 dimensions, we lose the axisymmetry that allowed us to reach the Grad-Shefranov equation and we need to solve the full momentum equation in three dimensions. This is not something that we can actually do in this class, and the existing codes that do this are quite sophisticated.\nSome general features of 3D equilibria are:\nNo net toroidal current. This means that they tend to be steady-state configurations. Radial confinement is accomplished by toroidal fields, as in the end-connected \\( \\theta \\) -pinch. As we saw, toroidal fields cannot provide radial confinement in a purely axisymmetric configuration, but radial variation with \\( \\phi \\) can provide confinement. Toroidal effect (radial force balance) is generated by helical magnetic fields. You can do an expansion of the magnetic field into a toroidal component, and a helical component that traces out a twisted shape as you move around the torus. These twisted shapes are what lead to radial confinement. ELMO Bumpy Torus (EBT) # In contrast to most other 3D configurations, even though the EBT is a 3D equilibrium, it has no helical windings.\nSince there are no helical windings, we have to provide radial stability in another way. In the EBT configuration, you also drive hot poloidal electron rings (driven by electron cyclotron resonance) to provide both stability and heating.\nStellarator # The stellarator configuration is composed of a number of helical current lines (generated by helical coils with alternating currents), and a net toroidal field driven by poloidal coils. The direction of the currents alternate, for a total of \\( 2l \\) current lines.\nThe result is a net magnetic field with a ratio such that \\( B_{\\text{helical}} \\gg B_\\phi \\) Stellarators raise some very complicated engineering challenges both in the design and construction of the complicated geometry. It is also very difficult to maintain no net current within the plasma, especially during start-up. As you add plasma, you raise from zero \\( \\beta \\) to a finite \\( \\beta \\) , introducing things like bootstrap currents that need to be balanced.\nTorsatron # Similar to a stellarator, the torsatron does not have alternating currents. All of the helical current lines are in the same direction. There are also no toroidal field coils.\nThe engineering is slightly simpler, but it is slightly less efficient at generating the helical magnetic field.\nThe flux surfaces in stellarators and torsatrons have geometrical cross-sections depending on the number \\( l \\) of helical current lines. About the current lines, the flux surfaces are nearly circular. The flux surfaces within the plasma volume are determined by the separatrix of the helical coil fields.\n"},{"id":51,"href":"/r/notes/UWAA543/ch21-5/","title":"Numerical Boundary Conditions","section":"Computational CFD","content":" Numerical Boundary Conditions # The linear advection equation is\n\\[\\pdv{u}{t} \u0026#43; a \\pdv{u}{x} = 0\\] For it to be a well-posed mathematical problem, you also need the Cauchy data (boundary and initial conditions). From the order of the differential operators, we know that we need one boundary condition (constraint on \\( u(x_b) \\) for all \\( t \\) ) and one initial condition (constraint on \\( u(x) \\) at \\( t = 0 \\)).\nIf we are solving the heat diffusion equation\n\\[\\pdv{T}{t} = \\alpha \\pdv{^2 T}{x^2}\\] Then we need 1 initial condition and 2 boundary conditions.\nBack to the linear advection equation, if we say\n\\[u(t = 0, x) = f(x) \\quad \\text{and} \\quad u(t, x=0) = g_D(t) \\qquad \\text{(Dirichlet BC)}\\] For wave speeds less than 0 (\\( a \u0026lt; 0 \\)), then we would instead specify \\( u(t, x=L) \\).\nThe way boundary conditions are implemented in the Forward Euler algorithm (as an example):\n\\[u_j ^{n\u0026#43;1} = u_j ^n - \\frac{ a \\Delta t}{\\Delta x} (u_j ^n - u_{j-1} ^n)\\] We begin by specifying \\( u_j ^0 = f(x_j) \\), i.e. we populate the solution at \\( t=0 \\). Sweep through the domain starting at \\( j=1 \\) towards \\( j=J \\) \\[u_1 ^{n\u0026#43;1} = u_1 ^n - \\frac{a \\Delta t}{\\Delta x} (u_1 ^n - u_0 ^n)\\] where\n\\[u_0 ^n = g_D(t^n)\\] Notice, at \\( j=J \\) \\[u_J ^{n\u0026#43;1} = u_J ^n - \\frac{a \\Delta t}{\\Delta x} (u_J ^n - u_{J-1} ^n)\\] There is no boundary condition required since there is no data required from outside of our domain. If we do require such data, we\u0026rsquo;ll need to do something special here.\nLeap-Frog BC # Recall our Leap-frog algorithm\n\\[u_j ^{n\u0026#43;1} = u_j ^{n-1} - \\frac{a \\Delta t}{\\Delta x} (u_{j\u0026#43;1} ^n - u_{j-1} ^n)\\] With assumed given initial condition\n\\[u_j ^0 = f(x) = u(x, t=0)\\] What happens on the very first step from \\( n=0 \\) to \\( n=1 \\)?\n\\[u_j ^1 = u_j ^{-1} - \\frac{a \\Delta t}{\\Delta x} (u_{j\u0026#43;1} ^0 - u_{j-1} ^0)\\] We\u0026rsquo;ve got a starting problem: we need to know \\( u_j ^{-1} \\), which is outside of our problem domain. In order to take the first time step, we bootstrap the algorithm by any some other algorithm which is not centered in time. Generally we use Lax-Wendroff to give us an additional initial condition \\( u_j ^1 \\). With both \\( u_j ^0 \\) and \\( u_j ^1 \\), we can proceed with any second-order time-centered algorithm.\nHow about the boundary conditions near the edges of the domain?\n\\[u_1 ^{n\u0026#43;1} = u_1 ^{n-1} - \\frac{a \\Delta t}{\\Delta x} (u_2 ^n - u_0 ^n) \\qquad (x = 0)\\] \\[u_J ^{n\u0026#43;1} = u_J ^{n-1} - \\frac{a \\Delta t}{\\Delta x}(u_{J\u0026#43;1} ^n - u_{J-1} ^n) \\qquad (x = L)\\] We\u0026rsquo;re now being asked for information outside of the problem domain \\( u_0 ^n \\) and \\( u_{J+1} ^n \\). This goes back to the modified PDE that we\u0026rsquo;re actually solving. The linear advection equation is first-order in \\( x \\) so we only need a single boundary condition to solve it, but the modified PDE that we are solving with Leap-frog has a second-order diffusive term, so we require an additional boundary condition.\nOur solution to the boundary problem is called a Numerical Boundary Scheme. As a zeroth-order scheme, we can just extend our solution at the boundary out one grid point\n\\[(j = J)\\qquad u_{J\u0026#43;1} ^n = u_J \\rightarrow u_J ^{n\u0026#43;1} - \\frac{ a \\Delta t}{\\Delta x} (u_J ^n - u_{J-1} ^n)\\] We could also make use of a first-order extrapolation\n\\[u_{J\u0026#43;1} ^n = 2 u_J ^n - u_{J-1} ^n \\rightarrow u_J ^{n\u0026#43;1} = u_J ^{n-1} - \\frac{2 a \\Delta t}{\\Delta x} (u_J ^n - u_{J-1} ^n)\\] Another method that\u0026rsquo;s particularly well suited to leap-frog is\n\\[u_{J\u0026#43;1} ^n = u_J ^{n-1}\\] The advantage of this method is that in a centered scheme it preserves information flow. It assumes that information from \\( (n-1, J) \\) will flow to \\( (n, J+1) \\), and we can use that information to compute \\( (n+1, J) \\). This helps to prevent unphysical reflections at the boundary.\nAnother concept which addresses unphysical boundary effects that we will cover later is called PML (Perfectly Matched Layer).\nNBS are needed in all algorithms which use central difference operators.\nNeumann Boundary Conditions # Instead of specifying the value of \\( u \\) at the boundary, we can specify the value of \\( \\pdv{u}{x} \\) instead\n\\[\\left.\\pdv{u}{x}\\right|_{x=0} = g_N(t)\\] \\[\\rightarrow \\frac{u_1 ^n - u_0 ^n}{\\Delta x} = g_N(t) \\rightarrow u_0 ^n = u_1 ^n - \\Delta x g_N(t)\\] What happens when we put this into the Forward Euler algorithm\n\\[u_1 ^{n\u0026#43;1} = u_1 ^n - \\frac{a \\Delta t}{\\Delta x} \\left(u_1 ^n - \\left[u_1 ^n - \\Delta x g_N(t)\\right]\\right) = u_1 ^n - a \\Delta t g_N (t)\\] So the spatial difference has been entirely replaced by the boundary condition, which is about what we would expect. It means that we can essentially remove \\( j=1 \\) from our algorithm, since we can write down the value of \\( u_1 ^n \\) for all \\( n \\) only using the BC and IC.\nNumerical BC in Implicit Algorithms # The way we implement boundary conditions in implicit algorithms is by incorporating the BC into the operator matrix. Looking at the \\( \\theta \\)-method:\n\\[- \\theta \\frac{a \\Delta t}{\\Delta x} u_{j-1} ^{n\u0026#43;1} \u0026#43; u_j ^{n\u0026#43;1} \u0026#43; \\theta \\frac{a \\Delta t}{\\Delta x} u_{j\u0026#43;1} ^{n\u0026#43;1} = u_j ^n (1 - \\theta) \\frac{a \\Delta t}{2 \\Delta x} (u_{j\u0026#43;1} ^n - u_{j-1} ^n)\\] Consider \\( j=1 \\), and define \\( \\nu \\equiv \\frac{a \\Delta t}{2 \\Delta x} \\) to clean things up\n\\[(j = 1) \\quad u_1 ^{n\u0026#43;1} \u0026#43; \\theta \\nu u_2 ^{n\u0026#43;1} = u_1 ^n - (1 - \\theta) \\nu u_2 ^n \u0026#43; \\nu [\\theta u_0 ^{n\u0026#43;1} \u0026#43; (1 - \\theta) u_0 ^n]\\] When we have Dirichlet boundary conditions, the terms in square brackets \\( u_0 ^{n+1}, u_0 ^n \\) are the ones we would replace by our boundary condition. Writing out the operator matrix for the algorithm, the boundary condition is just an additional term we add\n\\[\\begin{bmatrix} 1 \u0026amp; \\theta \u0026amp; \\nu \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \\\\ -\\theta \\nu \u0026amp; 1 \u0026amp; \\theta \\nu \u0026amp; 0 \u0026amp; \\ldots \\\\ \\ldots \\end{bmatrix} \\begin{bmatrix} u_1 ^{n\u0026#43;1} \\\\ u_2 ^{n\u0026#43;1} \\\\ \\ldots \\end{bmatrix} \\] \\[= \\begin{bmatrix} 1 \u0026amp; -(1-\\theta) \\nu \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\ldots \\\\ (1-\\theta) \\nu \u0026amp; 1 \u0026amp; -(1-\\theta) \\nu \u0026amp; 0 \\ldots \\\\ \\ldots \\end{bmatrix} \\begin{bmatrix} u_1 ^n \\\\ u_2 ^n \\\\ \\ldots \\end{bmatrix} \u0026#43; \\begin{bmatrix} \\nu \\theta g_D(t^{n\u0026#43;1}) \u0026#43; \\nu (1 - \\theta ) g(t^n) \\\\ 0 \\\\ 0 \\\\ \\ldots \\end{bmatrix}\\] Note that now \\( u_0 \\) does not appear in the solution vector, so whenever we need to go and plot the solution, we need to add the initial condition back in.\nNow consider the Neumann BC for the \\( \\theta \\)-algorithm.\n\\[u_0^n = u_1 ^n - \\Delta x g_N (t)\\] At \\( j=1 \\)\n\\[\\theta \\frac{a \\Delta t}{\\Delta x} (\\Delta x g_N(t^{n\u0026#43;1})- \\theta\\frac{ a \\Delta t}{2 \\Delta x} u_1 ^{n\u0026#43;1}\u0026#43; u_1 ^{n\u0026#43;1} \u0026#43; \\theta \\frac{ a \\Delta t}{2 \\Delta x} u_2 ^{n\u0026#43;1} = u_1 ^n - (1 - \\theta) \\frac{a \\Delta t}{2 \\Delta x} (u_2 ^n - u_1 ^n \u0026#43; \\Delta x g_N (t^n))\\] Notice that\n\\[\\theta \\frac{a \\Delta t}{\\Delta x} (\\Delta x g_N(t^{n\u0026#43;1})- \\theta\\frac{ a \\Delta t}{2 \\Delta x} u_1 ^{n\u0026#43;1} = - \\theta \\frac{ a \\Delta t}{2\\Delta x} u_0 ^{n\u0026#43;1}\\] So if we look at our operator matrix,\n\\[\\begin{bmatrix} 1 - \\theta \\nu \u0026amp; \\theta \\nu \u0026amp; 0 \u0026amp; 0 \\ldots \\\\ - \\theta \\nu \u0026amp; 1 \u0026amp; \\theta \\nu \u0026amp; 0 \\ldots \\\\ \\ldots \\end{bmatrix} \\begin{bmatrix} u_1 ^{n\u0026#43;1} \\\\ u_2 ^{n\u0026#43;1}\\\\ \\ldots \\end{bmatrix} \\begin{bmatrix} u_1 ^{n\u0026#43;1} \\\\ u_2 ^{n\u0026#43;1} \\\\ \\ldots \\end{bmatrix}\\] \\[= \\begin{bmatrix} 1 \u0026#43; (1 - \\theta)\\nu \u0026amp; -(1-\\theta) \\nu \u0026amp; 0 \u0026amp; \\ldots \\\\ (1 - \\theta) \\nu \u0026amp; 1 \u0026amp; - (1 - \\theta) \\nu \u0026amp; 0 \\ldots \\\\ \\ldots \\end{bmatrix} \u0026#43; \\begin{bmatrix} \u0026#43; - \\theta \\nu \\Delta x g_N(t^{n\u0026#43;1}) - (1 - \\theta) \\nu \\Delta x g_N(t^n) \\\\ 0 \\\\ \\ldots \\end{bmatrix}\\] In comparison with Dirichlet BC, now we\u0026rsquo;ve actually changed both the operator matrix and the inhomogeneity vector (instead of just adding a term to the inhomogeneity vector).\nSince the \\( \\theta \\)-method uses central difference operators, NBS is also required at the other end of the domain at \\( j=J \\). We have the same options to choose from (0-th order or 1st-order extrapolation).\n\\[u_{J\u0026#43;1} ^n = u_J ^n \\qquad \\text{0th Order}\\] \\[\\rightarrow - \\theta \\nu u_{J-1} ^{n\u0026#43;1} \u0026#43; ( 1 \u0026#43; \\theta \\nu ) u_J^{n\u0026#43;1} = u_J ^n - (1 - \\theta) \\nu( u_J ^n - u_{J-1} ^n)\\] \\[A = \\begin{bmatrix} \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp; 0 - \\theta \u0026amp; \\nu \u0026amp; 1 \u0026amp; \\theta \\nu \\\\ \\ldots \u0026amp; \\ldots \u0026amp; 0 \u0026amp; - \\theta \\nu \u0026amp; 1 \u0026#43; \\theta nu \\end{bmatrix}\\] There is again no row for \\( u_{J+1} \\). How about 1st order:\n\\[u_{J\u0026#43;1} ^n = 2 u_J ^n - u_{J-1}^n\\] \\[\\rightarrow -2 \\theta \\nu u_{J-1} ^{n\u0026#43;1} \u0026#43; (1 \u0026#43; 2 \\theta \\nu ) u_J ^{n\u0026#43;1} = u_J ^n - 2 (1 - \\theta) \\nu (u_J ^n - u_{J-1} ^n) \\] \\[A = \\begin{bmatrix} \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ \\ldots \u0026amp; - \\theta \\nu \u0026amp; 1 \u0026amp; \\theta \\nu \\\\ \\ldots \u0026amp; 0 \u0026amp; - 2 \\theta \\nu \u0026amp; 1 \u0026#43; 2 \\theta \\nu \\end{bmatrix}\\] Periodic Boundary Conditions # Periodic boundary conditions alter the operator matrix in a fairly straightforward manner. We simply set\n\\[u_0 = u_J \\qquad u_{J\u0026#43;1} = u_1\\] \\[A = \\begin{bmatrix} 1 \u0026amp; \\theta \\nu \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 0 \u0026amp; - \\theta \\nu \\\\ -\\theta \\nu \u0026amp; 1 \u0026amp; \\theta \\nu \u0026amp; 0 \u0026amp; \\ldots\u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 0 \\\\ \\ldots \u0026amp; \\ldots\u0026amp; \\ldots\u0026amp; \\ldots \\\\ 0 \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp;- \\theta \\nu \u0026amp; 1 \u0026amp; \\theta \\nu \\\\ \\theta \\nu \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; 0 \u0026amp; - \\theta \\nu \u0026amp; 1 \\end{bmatrix}\\] "},{"id":52,"href":"/r/notes/UWAA558/11-mhd-stability/","title":"MHD Stability","section":"MHD Theory","content":" MHD Stability # Equilibrium is simply a balance of forces that results in a steady state. Beyond equilibrium, stability is the tendency of a perturbation to return to equilibrium, rather than increasing. We are very interested in analyzing the stability of MHD equilibria, including the plasma dynamics, so we need to use the complete ideal MHD model. The MHD equations are non-linear, which means that any evolution/dynamics are also going to be non-linear. We can define the initial deviation from equilibrium to be a linear phenomenon. As usual, we perform this linearization by letting \\( Q(r, t) = Q_0 \u0026#43; Q_1(r, t) \\) with \\( Q_1 \\) being a small first-order perturbation. Since the equilibrium is both time and space independent, the general form of the perturbation is\n\\[Q_1(r, t) = \\vu Q_1 e^{-i (\\omega t - \\vec k \\cdot \\vec r)}\\] \\[\\grad p_0 = \\vec j_0 \\cross \\vec B_0\\] \\[p = p_0 \u0026#43; p_1 \\qquad \\rho = \\rho_0 \u0026#43; \\rho_1\\] \\[\\vec j = \\vec j_0 \u0026#43; \\vec j_1 \\qquad \\vec B = \\vec B_0 \u0026#43; \\vec B_1\\] and for a static equilibrium \\[\\vec v = \\vec v_1\\] In our momentum equations of the perturbed quantities, we assume that the static equilibrium holds, so most of the equilibrium terms drop out. We can define a velocity displacement \\( \\vec \\xi = \\int_0 ^t \\vec v_1 \\dd t \\) . As we integrate the field and pressure in time,\n\\[\\pdv{B_1}{t} = \\curl (\\vec v_1 \\cross \\vec B_0)\\] \\[\\int \\pdv{B_1}{t} = \\vec B_1 = \\curl \\int \\vec v_1 \\cross \\vec B_0 \\dd t \\\\ = \\curl (\\vec \\xi \\cross \\vec B_0)\\] If we do the same for the pressure equation, we get\n\\[p_1 = - \\vec \\xi \\cdot \\grad p_0 - \\Gamma p_0 \\div \\vec \\xi\\] where \\( \\Gamma \\) is the ratio of specific heats, to avoid confusion with typical perturbation growth rate \\( \\gamma \\) .\nIf we combine all of these together, substituting into the momentum equation, we can express the perturbation entirely in terms of \\( \\vec \\xi \\) and the equilibrium properties:\n\\[\\rho_0 \\pdv{ ^2 \\vec \\xi }{t^2} = \\grad (\\vec \\xi \\cdot \\grad p_0 \u0026#43; \\Gamma p_0 \\div \\vec \\xi)\\\\ \u0026#43; \\frac{1}{\\mu_0} \\left[(\\curl \\vec B_0) \\cross \\curl (\\vec \\xi \\cross \\vec B_0) \\right] \\\\ \u0026#43; \\frac{1}{\\mu_0} \\left[ \\curl \\curl ( \\vec \\xi \\cross \\vec B_0) \\cross \\vec B_0 \\right]\\] We define the right-hand-side as the linearized forcing function of our equilibrium \\[\\rho_0 \\pdv{ ^2 \\vec \\xi }{t^2} = \\vec F(\\vec \\xi _i , p_0, \\vec B_0)\\] For a linear force function, we can also write it in terms of a spring constant tensor\n\\[\\rho_0 \\pdv{ ^2 \\vec \\xi }{t^2} = \\vec F(\\vec \\xi) = - \\overline \\vec K \\cdot \\vec \\xi\\] We can determine the stability behavior of a configuration by specifying an initial condition\n\\[\\vec \\xi (t = 0) = 0 \\qquad \\text{and} \\qquad \\left. \\pdv{\\xi}{t} \\right| _{t = 0} = f(\\vec r)\\] and boundary conditions. A boundary condition may be a rigid wall \\[\\vec \\xi \\cdot \\vu n |_{wall} = 0\\] One way we can tell whether a given solution is unstable is to assume a variation of the form \\[\\vec \\xi \\propto e^{-i \\omega t}\\] If \\( \\omega^2 \u0026gt; 0 \\) , the displacement will oscillate in time without growth, and if \\( \\omega^2 \u0026lt; 0 \\) then the displacement will grow. In other words, if \\( \\omega \\) is real, then the mode is stable, and if \\( \\omega \\) is imaginary then the mode is unstable. The eigenvalue equation to be solved is\n\\[- \\omega ^2 \\rho_0 \\vec \\xi = \\vec F(\\vec \\xi)\\] which we can write as a matrix equation \\[\\overline \\vec A\\, \\overline X = \\lambda \\overline X\\] \\[\\frac{1}{\\rho_0} \\vec F (\\vec \\xi) = - \\omega ^2 \\vec \\xi\\] For any arbitrary linear forcing function, we might get an infinite number of eigenvalues. How do we know which ones to look at? It turns out that the linearized force function \\( \\vec F(\\vec \\xi) \\) has the property of being self-adjoint, so \\[\\int \\vec \\eta \\cdot \\vec F(\\vec \\xi) \\dd V = \\int \\xi \\cdot \\vec F( \\vec \\eta) \\dd V\\] where \\( \\vec \\eta \\) and \\( \\vec \\xi \\) are arbitrary displacements that satisfy the same boundary conditions. If \\( \\vec F \\) is self-adjoint, then the system is Hermitian, which guarantees that we get real eigenvalues ( \\( \\omega^2 \\) ) , orthogonal eigenfunctions, and most importantly we are guaranteed to have an ordered spectrum of eigenvalues. That is to say \\( \\omega_0 ^2 \u0026lt; \\omega _1 ^2 \u0026lt; \\omega _2 ^2 \u0026lt; \\ldots \\) . This means that the eigenvalue of the lowest mode is guaranteed to be the most negative, and therefore dictates the stability of the system. If the lowest eigenvalue is negative, then the system is necessarily unstable, and if the lowest eigenvalue is positive, then we are guaranteed that all modes are stable.\nBecause \\( \\vec F \\) is self-adjoint, we can make use of the energy principle to write the variation in the sum of the kinetic and potential energy as:\n\\[0 = \\dv{}{t} \\left[ \\frac{1}{2} \\int \\rho_0 \\left( \\pdv{\\vec \\xi}{t} \\right) ^2 \\dd V - \\frac{1}{2} \\int \\vec \\xi \\cdot \\vec F ( \\vec \\xi) \\dd V \\right]\\] The kinetic energy term will always be positive, so we can formulate the stability based on the potential energy, often called a \\( \\delta W \\) approach \\[\\delta W = - \\frac{1}{2} \\int \\vec \\xi \\cdot \\vec F ( \\vec \\xi) \\dd V\\] is the change in potential energy due to a displacement \\( \\xi \\) . If the potential energy decreases due to a displacement \\( \\xi \\) , then the kinetic energy must necessarily increase, so \\( \\delta W \u0026lt; 0 \\) indicates instability.\nWe can write the change in kinetic energy for our normal mode decomposition as \\[\\delta T = \\frac{1}{2} \\int \\rho _0 \\left( \\pdv{\\xi}{t} \\right) ^2 \\dd V = - \\frac{1}{2} \\omega ^2 \\int \\rho _0 \\vec \\xi ^ \\star \\cdot \\vec \\xi \\dd V \\\\ = - \\delta W = \\frac{1}{2} \\int \\vec \\xi ^\\star \\cdot \\vec F(\\vec \\xi) \\dd V\\] \\[\\omega^2 = \\frac{- \\int \\xi ^\\star \\cdot \\vec F \\dd V}{\\int \\rho_0 \\xi ^\\star \\cdot \\xi \\dd V} = \\frac{\\delta W}{\\frac{1}{2} \\int \\rho_0 \\xi ^\\star \\cdot \\xi \\dd V}\\] The denominator is strictly positive, so the sign of \\( \\omega^2 \\) is determined by the sign of \\( \\delta W \\) \\[\\delta W \u0026lt; 0 \\rightarrow \\omega^2 \u0026lt; 0 \\rightarrow \\text{unstable} \\\\ \\delta W \u0026gt; 0 \\rightarrow \\omega^2 \u0026gt; 0 \\rightarrow \\text{stable}\\] Analyzing the form of \\( \\delta W \\) (within the plasma volume)\n\\[\\delta W = \\frac{1}{2} \\int_{plasma} \\dd V \\Gamma p_0(\\div \\vec \\xi) ^2 \u0026#43; \\vec \\xi \\cdot \\grad p_0 (\\div \\vec \\xi) \\qquad \\qquad \\\\ \\qquad \\qquad \u0026#43; \\frac{1}{\\mu_0} \\left[ \\curl ( \\vec \\xi \\cross \\vec B_0) \\right]^2 \\\\ \\qquad \\qquad - \\frac{1}{\\mu_0} \\left[\\vec \\xi \\cross ( \\curl \\vec B_0) \\right] \\cdot \\left[ \\curl ( \\vec \\xi \\cross \\vec B_0) \\right]\\] Generally speaking, the plasma volume does not extend to infinity, and we care very much about the boundary. The total \\( \\delta W \\) is the sum of that in the plasma volume \\( \\delta W_F \\) , the surface \\( \\delta W_S \\) , and the vacuum region \\( \\delta W_V \\) . The vacuum term looks like\n\\[\\delta W_V = \\frac{1}{2} \\int _{vac} \\dd V \\frac{ (\\curl ( \\vec \\xi \\cross \\vec B_0))^2}{\\mu_0} = \\int_{vac} \\dd V \\frac{\\vec B_1 ^2}{\\mu_0} \u0026gt; 0\\] so the vacuum term is always positive, and has a stabilizing influence. The surface contribution offsets this\n\\[\\delta W_S = \\frac{1}{2} \\oint \\dd S ( \\vu n \\cdot \\vec \\xi) ^2 \\left[ \\left[ \\grad \\left( p_0 \u0026#43; \\frac{B_0^2}{2 \\mu_0} \\right) \\right] \\right] \\cdot \\vu n\\] Instabilities can be characterized as:\nInternal/fixed boundary \\( \\delta W = \\delta W_F \\) External/free boundary \\( \\delta W = \\delta W_F \u0026#43; \\delta W_S \u0026#43; \\delta W_V \\) The plasma portion can be re-written slightly as\n\\[\\delta W_F = \\frac{1}{2} \\int \\dd V \\frac{ |B_{1, \\perp}|^2}{\\mu_0} \\quad \\leftarrow \\text{Shear Alfven} \\\\ \u0026#43; \\mu_0 \\left| \\frac{B_{1, \\parallel}}{\\mu_0} - \\frac{B_0 \\xi \\cdot \\grad p_0}{B_0} ^2 \\right|^2 \\quad \\leftarrow \\text{Fast magnetosonic} \\\\ \u0026#43; \\Gamma p_0 |\\div \\xi|^2 \\quad \\leftarrow \\text{Acoustic}\\\\ \u0026#43; \\frac{\\vec j_0 \\cdot \\vec B_0}{B_0 ^2} (\\vec B_0 \\cross \\vec \\xi) \\cdot \\vec B_1 \\quad \\leftarrow \\text{Current-driven (kink)} \\\\ - 2 ( \\vec \\xi \\cdot \\grad p_0)(\\vec \\xi \\cdot \\vec \\kappa) \\quad \\leftarrow \\text{pressure-driven (interchange/balooning)}\\] where \\( \\vec \\kappa \\) is the curvature vector \\( \\vu e_B \\cdot \\grad \\vu e_B \\) . If we look at each of these terms, the first three terms are all going to be stabilizing effects, which means that all instability is going to come from the last two terms, the current-driven instability term and the pressure-driven instability term.\nGoing back to the screw pinch, \\[\\dv{p}{r} = j_\\theta B_z - j_z B_\\theta\\] we have current in the same direction as magnetic field ( \\( j_\\theta \\) with \\( B_\\theta \\) and \\( j_z \\) with \\( B_z \\) ), so kink instabilities are possible. We also have a pressure gradient, so interchange instabilities are also possible.\nAs a concrete example, look at the pressure driven instability term in a Z-pinch. \\[\\kappa = - \\frac{ \\vu r}{r}\\] \\[\\vec \\xi = \\xi _r \\vu r\\] \\[\\delta W_{F, pressure} = \\int \\dd V \\xi _r \\dv{p_0}{r} \\frac{\\xi_r}{r} \\\\ = \\int \\dd V \\frac{2 \\xi _r ^2}{r} \\dv{p_0}{r}\\] In a Z-pinch, it is always the case that \\( \\dv{p_0}{r} \u0026lt; 0 \\) . As shown by Kadomtsev (1965) it turns out that these modes can be stabilized by adding \\( B_z \\) , but this also introduces kink modes.\nGoing back to our stabilizing quantities of wellness and shear, current-driven instabilities are generally managed through shear, and pressure-driven instabilities are stabilized by well.\n"},{"id":53,"href":"/r/notes/griffiths/ch2-1/","title":"The Electric Field","section":"Griffiths Introduction to Electrodynamics","content":" 2.1: The Electric Field # 2.1.1: Introduction # The fundamental problem electrodynamics hopes to solve is this (Fig 2.1): We have some electric charges \\( q_1, q_2, q_3, \\ldots \\) (call them source charges); what force do they exert on another charge, \\( Q \\) (call it the test charge)? The positions of the source charges are given (as functions of time); the trajectory of the test particle is to be calculated. In general, both the source charges and the test charge are in motion.\nThe solution to this problem is facilitated by the principle of superposition, which states that the interaction between any two charges is completely unaffected by the presence of others. This means that to determine the force on Q, we can first compute the force \\( \\vec{F_1} \\), due to \\( q_1 \\) alone (ignoring all the others); then we compute the force \\( \\vec{F_2} \\), due to \\( q_2 \\) alone, and so in. Finally, we take the vector sum of all these individual forces: \\( \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\vec{F_3} + \\ldots \\) Thus, if we can find the force on Q due to a single source charge \\( q \\), we are, in principle, done (the rest is just a question of repeating the same operation over and over, and adding it all up)\nThe principle of superposition may seem \u0026ldquo;obvious\u0026rdquo; to you, but it did not have to be so simple: if the electromagnetic force were proportional to the square of the total source charge, for instance, the principle of superposition would not hold, since \\( (q_1 + q_2)^2 \\neq q_1 ^2 + q_2 ^2 \\) (there would be \u0026ldquo;cross terms\u0026rdquo; to consider). Superposition is not a logical necessity, but an experimental fact.\nWell, at first sight this looks very easy: Why don\u0026rsquo;t I just write down the formula for the force on Q due to q, and be done with it? I could, and in Chapter 10 I shall, but you would be shocked to see it at this stage, for not only does the force on Q depend on the separation distance \\( \\gr \\) between the charges (Fig 2.2), it also depends on both their velocities and on the acceleration of \\( q \\). Moreover, it is not the position, velocity, and acceleration of \\( q \\) right now that matter: electromagnetic \u0026ldquo;news\u0026rdquo; travels at the speed of light, so what concerns Q is the position, velocity, and acceleration q had at some earlier time, when the message left.\nTherefore, in spite of the fact that the basic question (\u0026ldquo;What is the force on Q due to q?\u0026rdquo;) is easy to state, it does not pay to confront it head on; rather, we shall go at it by stages. In the meantime, the theory we develop will allow for the solution of more subtle electromagnetic problems that do not present themselves in quite this simple format. To begin with, we shall consider the special case of electrostatics in which all the source charges are stationary (though the test charge may be moving).\n2.1.2: Coulomb\u0026rsquo;s Law # What is the force on a test charge Q due to a single point charge q, that is at rest a distance \\( \\gr \\) away? The answer (based on experiments) is given by Coulomb\u0026rsquo;s Law:\n\\[\\vec{F} = \\frac{1}{4 \\pi \\epsilon_0}\\frac{q Q}{\\gr^2} \\vu{\\vec{\\gr}} \\label{2.1} \\tag{2.1}\\] The constant \\( \\epsilon_0 \\) is called (ludicrously) the permittivity of free space. In SI units, where force is in newtons (N), distance in meters (m), and charge in coulombs (C),\n\\[\\epsilon_0 = 8.85 \\times 10^{-12} \\frac{C^2}{N \\cdot m ^2} \\] In words, the force is proportional to the product of the charges and inversely proportional to the square of the separation distance. As always (Sect 1.1.4), \\( \\vec{\\gr} \\) is the separation vector from \\( \\vec{r\u0026rsquo;} \\) (the location of q) to \\( \\vec{r} \\) (the location of Q):\n\\[\\vec{\\gr} = \\vec{r} - \\vec{r}\u0026#39;\\] \\( \\gr \\) is its magnitude, and \\( \\vu{\\gr} \\) is its direction. The force points along the line from q to Q; it is repulsive if q and Q have the same sign, and attractive if their signs are opposite.\nCoulomb\u0026rsquo;s law and the principle of superposition constitute the physical input for electrostatics - the rest, except for some special properties of matter, is mathematical elaboration of these fundamental rules.\n2.1.3: The Electric Field # If we have several point charges \\( q_1, q_2, \\ldots , q_n \\) at distances \\( \\gr_1 \\gr_2 \\ldots, \\gr_n \\) from Q, the total force on Q is evidently\n\\[\\begin{aligned} \\vec{F} \u0026amp; = \u0026amp; \\vec{F_1} \u0026#43; \\vec{F_2} \u0026#43; \\ldots \\\\ \u0026amp; = \u0026amp; \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 Q}{\\gr_1 ^2} \\vu{\\gr}_1 \u0026#43; \\frac{q_2 Q}{\\gr_2 ^2} \\vu{\\gr}_2 \u0026#43; \\ldots \\right) \\\\ \u0026amp; = \u0026amp; \\frac{Q}{4 \\pi \\epsilon _0} \\left( \\frac{q_1}{\\gr ^2 _1} \\vu{\\gr_1} \u0026#43; \\frac{q_2}{\\gr _2 ^2}\\vu{\\gr_2} \u0026#43; \\ldots \\right) \\end{aligned}\\] or\n\\[\\vec{F} = Q \\vec{E} \\label{2.3} \\tag{2.3}\\] where\n\\[\\vec{E}(\\vec{r}) \\equiv \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^n \\frac{q_i}{\\gr_{i}^2} \\vu{\\gr_i} \\label{2.4} \\tag{2.4}\\] E is called the electric field of the source charges. Notice that it is a function of position (r), because the separation vectors \\( \\gr_i \\) depend on the location of the field point P (Fig 2.3). But it makes no reference to the test charge Q. The electric field is a vector quantity that varies from point to point and is determined by the configuration of source charges; physically, \\( \\vec{E}(\\vec{r}) \\) is the force per unit charge that would be exerted on a test charge, if you were to place one at P.\nWhat exactly is an electric field? I have deliberately begun with what you might call the \u0026ldquo;minimal\u0026rdquo; interpretation of E, as an intermediate step in the calculation of electric forces. But I encourage you to think of the field as a \u0026ldquo;real\u0026rdquo; physical entity, filling the space around electric charges. Maxwell himself came to believe that electric and magnetic fields are stresses and strains in an invisible primordial jellylike \u0026ldquo;ether.\u0026rdquo; Special relativity has forced us to abandon the notion of either, and with it Maxwell\u0026rsquo;s mechanical interpretation of electromagnetic fields. (It is even possible, although cumbersome, to formulate classical electrodynamics as an \u0026ldquo;action-at-a-distance\u0026rdquo; theory, and dispense with the field concept altogether.) I can\u0026rsquo;t tell you, then, what a field is \u0026ndash; only how to calculate it and what it can do for you once you\u0026rsquo;ve got it.\nExample 2.1 # Q Find the electric field a distance z above the midpoint between two equal charges (q), a distance d apart (Fig. 2.4a) A Let \\( \\vec{E_1} \\) be the field of the left charge alone, and \\( \\vec{E_2} \\) that of the right charge alone (Fig. 2.4b). Adding them (vectorially), the horizontal components cancel and the vertical components conspire\n\\[E_z = 2 \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{\\gr ^2} \\cos \\theta\\] Here \\( \\gr = \\sqrt{z^2 + (d/2)^2} \\) and \\( \\cos \\theta = z / \\gr \\), so\n\\[\\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2qz}{\\left[ z^2 \u0026#43; (d/2)^2 \\right]^{3/2}} \\vu{z} \\] Check: When \\( z \\gg d \\) you\u0026rsquo;re so far away that it just looks like a single charge \\( 2q \\), so the field should reduce to \\( \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2q}{z^2} \\vu{z} \\). And it does, just set \\( d \\rightarrow 0 \\) in the formula).\n2.1.4: Continuous Charge Distributions # Our definition of the electric field (Eq. \\( \\eqref{2.4} \\) ) assumes that the source of the field is a collection of discrete point charges \\( q_i \\). If, instead, the charge is distributed continuously over some region, the sum becomes an integral (Fig 2.5a):\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr ^2} \\vu{\\gr} \\dd{q}\\] If the charge is spread out along a line (Fig. 2.5b), with charge-per-unit-length \\( \\lambda \\) then \\( \\dd{q} = \\lambda \\dd{l}\u0026rsquo; \\) (where \\( \\dd{l}\u0026rsquo; \\) ) is an element of length along the line); if the charge is smeared out over a surface (Fig. 2.5c) with charge-per-unit-area \\( \\sigma \\), then \\( \\dd{q} = \\sigma \\dd{a}\u0026rsquo; \\) (where \\( \\dd{a\u0026rsquo;} \\) ) is an element of area on the surface); and if the charge fills a volume (Fig 2.5d), with charge-per-unit-volume \\( \\rho \\), then \\( \\dd{q} = \\rho\\dd{\\tau\u0026rsquo;} \\) (where \\( \\dd{\\tau\u0026rsquo;} \\) is an element of volume):\n\\[dq \\rightarrow \\lambda \\dd{l\u0026#39;} \\sim \\sigma \\dd{a\u0026#39;} \\sim \\rho \\dd{\\tau\u0026#39;}\\] Thus the electric field of a line charge is\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r\u0026#39;})}{\\gr ^2} \\vu{\\gr} \\dd{l\u0026#39;}\\] for a surface charge,\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r\u0026#39;})}{\\gr ^2} \\vu{\\gr} \\dd{a\u0026#39;}\\] and for a volume charge,\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r\u0026#39;})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau\u0026#39;} \\label{2.8} \\tag{2.8}\\] Equation \\( \\eqref{2.8} \\) itself is often referred to as \u0026ldquo;Coulomb\u0026rsquo;s law,\u0026rdquo; because it is such a short step from the original, and because a volume charge is in a sense the most general and realistic case. Please note carefully the meaning of \\( \\gr \\) in these formulas. Originally, in \\( \\eqref{2.4} \\), \\( \\gr_i \\) stood for the vector from the source charge \\( q_i \\) to the field point r. Correspondingly, in Eq.s 9-11, \\( \\gr \\) is the vector from \\( \\dd{q} \\) to the field point \\( \\vec{r} \\).\nWarning: the unit vector \\( \\vu{\\gr} \\) is not constant: its direction depends on the source point \\( \\vec{r\u0026rsquo;} \\), and hence it cannot be taken outside the integrals (9-11). In practice, you must work with Cartesian components (\\( \\vu{x}, \\vu{y}, \\vu{z} \\) are constant, and do come out) , even if you use curvilinear coordinates to perform the integration.\nExample 2.2 # Q Find the electric field a distance z above the midpoint of a straight line segment of length \\( 2L \\) that carries a uniform line charge \\( \\lambda \\) (Fig. 2.6). A TODO!\n"},{"id":54,"href":"/r/notes/griffiths/ch2-2/","title":"Divergence and Curl of Electrostatic Fields","section":"Griffiths Introduction to Electrodynamics","content":" 2.2: Divergence and Curl of Electrostatic Fields # 2.2.1 Field Lines, Flux, and Gauss\u0026rsquo; Law # In principle, we are done with the subject of electrostatics. Eq. 2.8 tells us how to compute the field of a charge distribution, and Eq. 2.3 tells us what the force on a charge Q placed in this field will be. Unfortunately, as you may have discovered, the integrals involved in computing E can be formidable, even for reasonably simple charge distributions. Much of the rest of electrostatics is devoted to assembling a bag of tools and tricks for avoiding these integrals. It all begins with the divergence and curl of E. I shall calculate the divergence of E directly from Eq. 2.8 in section 2.2.2, but first I want to show you a more qualitative, and perhaps more illuminating, intuitive approach.\nLet\u0026rsquo;s begin with the simplest possible case: a single point charge q, situated at the origin:\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{\\vec{r}} \\tag{2.10} \\label{2.10}\\] To get a \u0026ldquo;feel\u0026rdquo; for this field, I might sketch a few representative vectors, as in Fig. 2.12a. Because the field falls off like \\( 1/r^2 \\), the vectors get shorter as you go farther away from the origin; they always point radially outward. But there is a nicer way to represent this field, and that\u0026rsquo;s to connect up the arrows, to form field lines (Fig. 2.12b).\nYou might think that I have thereby thrown away information about the strength of the field, which was contained in the length of the arrows. But actually I have not. The magnitude of the field is indicated by the density of the field lines: it\u0026rsquo;s strong near the center where the field lines are close together, and weak farther out, where they are relatively far apart.\nIn truth, the field-line diagram is deceptive, when I draw it on a two-dimensional surface, for the density of lines passing through a circle of radius r is the total number divided by the circumference (\\( n / 2 \\pi r \\)), which goes like \\( (1/r) \\), not \\( (1/r^2) \\). But if you imagine the model in three dimensions (a pincushion with needles sticking out in all directions), then the density of lines is the total number divided by the area of the sphere \\( (n/4 \\pi r^2) \\), which does go like \\( (1/r^2) \\).\nSuch diagrams are also convenient for representing more complicated fields. Of course, the number of lines you draw depends on how lazy you are (and how sharp your pencil is), though you ought to include enough to get an accurate sense of the field, and you must be consistent: if \\( q \\) gets 8 lines, then \\( 2q \\) deserves 16. And you must space them fairly - they emanate from a point charge symmetrically in all directions. Field lines begin on positive charges and end on negative ones; they cannot simply terminate in midair, though they may extend out to infinity. Moreover, field lines can never cross - at the intersection the field would have two different directions at once! With all this in mind, it is easy to sketch the field of any simple configuration of point charges: Begin by drawing the lines in the neighborhood of each charge, and then connect them up or extend them to infinity (Figs. 2.13 and 2.14)\nIn this model, the flux of E through a surface S,\n\\[\\Phi_E \\equiv \\int _S \\vec{E} \\cdot \\dd{\\vec{a}} \\label{2.11} \\tag{2.11}\\] is a measure of the \u0026ldquo;number of lines\u0026rdquo; passing through S. I put this in quotes because of course we can only draw a representative sample of field lines - the total number would be infinite. But for a given sampling rate the flux is proportional to the number of lines drawn, because the field strength, remember, is proportional to the density of field lines (the number per unit area), and hence \\( \\vec{E} \\cdot \\dd{\\vec{a}} \\) is proportional to the number of lines passing through the infinitesimal area \\( \\dd{\\vec{a}} \\). (The dot product picks out the component of \\( \\dd{\\vec{a}} \\) along the direction of E, as indicated in Fig 2.15. It is the area in the plane perpendicular to E that we have in mind when we say that the density of field lines is the number per unit area).\nThis suggests that the flux through any closed surface is a measure of the total charge inside. For the field lines that originate on a positive charge must either pass out through the surface or else terminate on a negative charge inside (Fig 2.16a). On the other hand, a charge outside the surface will contribute nothing to the total flux, since its field lines pass in one side and out the other (Fig 2.16b). This is the essence of Gauss\u0026rsquo;s law. Now let\u0026rsquo;s make it quantitative.\nIn the case of a point charge q at the origin, the flux of E through a spherical surface or radius r is\n\\[\\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\int \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r^2} \\vu{r} \\right) \\cdot \\left( r^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\vu{r} \\right) = \\frac{1}{\\epsilon_0} q \\label{2.12} \\tag{2.12}\\] Notice that the radius of the sphere cancels out, for while the surface area goes up as \\( r^2 \\), the field goes down as \\( 1/r^2 \\), so the product is constant. In terms of the field-line picture, this makes good sense, since the same number of field lines pass through any sphere centered at the origin, regardless of its size. In fact, it didn\u0026rsquo;t have to be a sphere - any closed surface, whatever its shape, would be pierced by the same number of field lines. Evidently, the flux through any surface enclosing the charge is \\( q / \\epsilon_0 \\).\nNow suppose that instead of a single charge at the origin, we have a bunch of charges scattered about. According to the principle of superposition, the total field is the (vector) sum of all the individual fields:\n\\[\\vec{E} = \\sum _{i = 1} ^\\nu \\vec{E}_i\\] The flux through a surface that encloses them all is\n\\[\\oint \\vec{E} \\cdot \\dd{\\vec{l}} = \\sum _{i = 1}^n \\left( \\oint \\vec{E_i} \\cdot \\dd{\\vec{a}} \\right) = \\sum_{i = 1}^n \\left( \\frac{1}{\\epsilon_0} q_i \\right)\\] For any closed surface, then\n\\[\\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} \\label{2.13} \\tag{2.13}\\] where \\( Q_{enc} \\) is the total charge enclosed within the surface. This is the quantitative statement of Gauss\u0026rsquo;s law. Although it contains no information that was not already present in Coulomb\u0026rsquo;s law plus the principle of superposition, it is of almost magical power, as you will see in Sect. 2.2.3. Notice that it all hinges on the \\( 1/r^2 \\) character of Coulomb\u0026rsquo;s law; without that, the crucial cancellation of the r\u0026rsquo;s in \\( \\eqref{2.12} \\) would not take place, and the total flux of E would depend on the surface chosen, not merely on the total charge enclosed. Other \\( 1/r^2 \\) forces (I am thinking particularly of Newton\u0026rsquo;s law of universal gravitation) will obey \u0026ldquo;Gauss\u0026rsquo;s laws\u0026rdquo; of their own, and the applications we develop here carry over directly.\nAs it stands, Gauss\u0026rsquo;s law is an integral equation, but we can easily turn it into a differential one by applying the divergence theorem:\n\\[\\oint_{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau}\\] Rewriting \\( Q_{enc} \\) in terms of the charge density \\( \\rho \\) we have\n\\[Q_{enc} = \\int_{\\mathscr{V}} \\rho \\dd{\\tau}\\] So Gauss\u0026rsquo;s law becomes\n\\[\\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} = \\int_{\\mathscr{V}} \\left( \\frac{\\rho}{\\epsilon_0} \\dd{\\tau} \\right)\\] And since this holds for any volume, the integrands must be equal:\n\\[\\nabla \\cdot \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\label{2.14} \\tag{2.14}\\] Equation \\( \\eqref{2.14} \\) carries the same message as \\( \\eqref{2.13} \\); it is Gauss\u0026rsquo;s law in differential form. The differential version is tidier, but the integral form has the advantage that it accommodates point, line, and surface charges more naturally.\n2.2.2: The Divergence of E # Let\u0026rsquo;s go back now, and calculate the divergence of \\( \\vec{E} \\) directly from Eq. 2.8:\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4\\pi\\epsilon_0} \\int_{\\text{all space}} \\frac{\\vu{\\gr}}{\\gr ^2} \\rho(\\vec{r}\u0026#39;) \\dd{\\tau\u0026#39;} \\label{2.15}\\] (Originally the integration was over the volume occupied by the charge, but I may as well extend it to all space, since \\( \\rho = 0 \\) in the exterior region anyway.) Noting that the r-dependence is contained in \\( \\gr = r - r\u0026rsquo; \\), we have\n\\[\\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int \\vec{\\nabla} \\cdot \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \\] We calculated this divergence in Section 1.5:\n\\[\\div{\\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right)} = 4 \\pi \\delta ^3(\\gr)\\] Thus\n\\[\\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int 4 \\pi \\delta^3(\\vec{r} - \\vec{r\u0026#39;}) \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} = \\frac{1}{\\epsilon_0} \\rho(\\vec{r}) \\label{2.16} \\tag{2.16}\\] which is Gauss\u0026rsquo;s law in differential form \\( \\eqref{2.14} \\). To recover the integral form \\( \\eqref{2.13} \\) we run the previous argument in reverse - integrate over a volume and apply the divergence theorem:\n\\[\\int_{\\mathscr{V}} \\div{\\vec{E}} \\dd{\\tau} = \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} \\int_{\\mathscr{V}} \\rho \\dd{\\tau} = \\frac{1}{\\epsilon_0} Q_{enc}\\] 2.2.3: Applications of Gauss\u0026rsquo;s Law # I must interrupt the theoretical development at this point to show you the extraordinary power of Gauss\u0026rsquo;s law, in integral form. When symmetry permits, it affords by far the quickest and easiest way of computing electric fields. I\u0026rsquo;ll illustrate the method with a series of examples.\nExample 2.3 # Q Find the field outside a uniformly charged solid sphere of radius R and total charge q A Imagine a spherical surface at radius \\( r \u0026gt; R \\) (Fig. 2.18). This is called a Gaussian surface in the trade. Gauss\u0026rsquo;s law says that\n\\[\\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc}\\] and in this case \\( Q_{enc} = q \\). At first glance this doesn\u0026rsquo;t seem to get us very far, because the quantity we want (E) is buried inside the surface integral. Luckily, symmetry allows us to extract E from under the integral sign: E certainly points radially outward, as does \\( \\dd{\\vec{a}} \\), so we can drop the dot product\n\\[\\int_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{S}} | \\vec{E} | da\\] and the magnitude of E is constant over the Gaussian surface, so it comes outside the integral:\n\\[\\int_{S} | E | da = |E| \\int_{S} da = E 4 \\pi r^2\\] Thus\n\\[|\\vec{E}|4\\pi r^2 = \\frac{1}{\\epsilon_0} q\\] or\n\\[\\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r}\\] Notice a remarkable feature of this result: the field outside the sphere is exactly the same as it would have been if all the charge had been concentrated at the center.\nGauss\u0026rsquo;s law is always true, but not always useful. If \\( \\rho \\) had not been uniform (or at any rate, not spherically symmetrical), or if I had chosen some other shape for my Gaussian surface, it would have still been true that the flux of \\( \\vec{E} \\) is \\( q / \\epsilon_0 \\), but \\( \\vec{E} \\) would not have pointed in the same direction as \\( \\dd{\\vec{a}} \\), and its magnitude would not have been constant over the surface, and without that I cannot get \\( |\\vec{E}| \\) outside the integral. Symmetry is crucial to this application of Gauss\u0026rsquo;s law. As far as I know, there are only three kinds of symmetry that work:\nSpherical symmetry. Make your Gaussian survace a concentric sphere. Cylindrical symmetry. Make your Gaussian surface a coaxial cylinder. Plane symmetry. Use a Gaussian \u0026ldquo;pillbox\u0026rdquo; that straddles the surface. Although 2 and 3 technically require infinitely long cylinders, and planes extending to infinity, we shall often use them to get approximate answers for \u0026ldquo;long\u0026rdquo; cylinders or \u0026ldquo;large\u0026rdquo; planes, at points far from the edges.\nExample 2.4 # Q A long cylinder (Fig 2.21) carries a charge density that is proportional to the distance from the axis: \\( \\rho = ks \\) for some constant \\( k \\). Find the electric field inside this cylinder.\nA Draw a Gaussian cylinder of length l and radius s. For this surface, Gauss\u0026rsquo;s law states\n\\[\\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc}\\] The enclosed charge is\n\\[\\begin{aligned} Q_{enc} \u0026amp; = \u0026amp; \\int \\rho \\dd{\\tau} \\\\ \u0026amp; = \u0026amp; \\int(ks\u0026#39;)(s\u0026#39; \\dd{s\u0026#39;} \\dd{\\phi} \\dd{z}) \\\\ \u0026amp; = \u0026amp; 2 \\pi k l \\int_{0}^{s} s\u0026#39;^2 \\dd{s\u0026#39;} \\\\ \u0026amp; = \u0026amp; \\frac{2}{3} \\pi k l s^3 \\end{aligned}\\] (I used the volume element appropriate to cylindrical coordinates, and integrated \\( \\phi \\) from \\( 0 \\) to \\( 2\\pi \\), \\( \\dd{z} \\) from \\( 0 \\) to \\( l \\). I put a prime on the integration variable \\( s\u0026rsquo; \\) to distinguish it from the radius \\( s \\) of the Gaussian surface.)\nNow, symmetry dictates that \\( \\vec{E} \\) must point radially outward, so for the curved portion of the Gaussian cylinder we have:\n\\[\\int \\vec{E} \\cdot \\dd{\\vec{a}} = \\int | \\vec{E}| da = | \\vec{E}| \\int da = |\\vec{E} 2 \\pi s l\\] while the two ends contribute nothing (here \\( \\vec{E} \\) is perpendicular to \\( \\dd{\\vec{a}} \\)). Thus,\n\\[|\\vec{E} | 2 \\pi s l = \\frac{1}{\\epsilon_0} \\frac{2}{3} \\pi k l s^3\\] or, finally,\n\\[\\vec{E} = \\frac{1}{3\\epsilon_0} k s^2 \\vu{s}\\] Example 2.5 # Q An infinite plane carries a uniform surface charge \\( \\sigma \\). Find its electric field. A Draw a Gaussian pillbox, extending equal distances above and below the plane (Fig. 2.22). Apply Gauss\u0026rsquo;s law to this surface:\n\\[\\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc}\\] In this case, \\( Q = \\sigma A \\), where A is the area of the lid of the pillbox. By symmetry, \\( \\vec{E} \\) points away from the plane (upward for points above, downward for points below). So the top and bottom surfaces yield\n\\[\\int \\vec{E} \\cdot \\dd{\\vec{a}} = 2 A |\\vec{E}|,\\] whereas the sides contribute nothing. Thus\n\\[2 A | \\vec{E} | = \\frac{1}{\\epsilon_0} \\sigma A \\] or\n\\[\\vec{E} = \\frac{\\sigma}{2 \\epsilon_0} \\vu{n}\\] where \\( \\vu{n} \\) is a unit vector pointing away from the surface. In Prob 2.6, you obtained this same result by a much more laborious method.\nIt seems surprising, at first, that the field of an infinite plane is independent of how fara away you are. What about the \\( 1/r^2 \\) in Coulomb\u0026rsquo;s law? The point is that as you move farther and farther away from the plane, more and more charge comes into your \u0026ldquo;field of view,\u0026rdquo; and this compensates for the diminishing influence of any particular piece. The electric field of a sphere falls off like \\( 1/r^2 \\); the electric field of an infinite line falls off like \\( 1/r \\); and the electric field of an infinite plane does not fall off at all (you cannot escape from an infinite plane).\nAlthough the direct use of Gauss\u0026rsquo;s law to compute fields is limited to cases of spherical, cylindrical, and planar symmetry, we can put together combinations of objects posessing such symmetry, even though the arrangement as a whole is not symmetrical. For example, invoking the principle of superposition, we could find the field in the vicinity of two uniformly charged parallel cylinders, or a sphere near an infinite charged plane.\nExample 2.6 # Q Two infinite parallel planes carry equal but opposite uniform charge densities \\( \\pm \\sigma \\) (Fig 2.23). Find the field in each of the three regions: (i) to the left of both, (ii) between them, (iii) to the right of both.\nA The left plate produces a field \\( (1/2 \\epsilon_0)\\sigma \\), which points away from it (Fig. 2.24) to the left in region in (i) and to the right in regions (ii) and (iii). The right plate, being negatively charged, produces a field \\( (1/2 \\epsilon_0)\\sigma \\) which points toward it - to the right in regions (i) and (ii) and to the left in region (iii). The two fields cancel in regions (i) and (iii); they conspire in region (ii). Conclusion: The field between the plates is \\( \\sigma / \\epsilon_0 \\), and points to the right; elsewhere it is zero. 2.2.4: The Curl of E # I\u0026rsquo;ll calculate the curl of \\( \\vec{E} \\) as I did the divergence in Sect 2.2.1, by studying first the simplest possible configuration: a point charge at the origin. In this case\n\\[ \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} \\] Now, a glance at Fig 2.12 should convince you that the curl of this field has to be zero, but I suppose we ought to come up with something a little more rigorous than that. What if we calculate the line integral of this field from some point \\( \\vec{a} \\) to some other point \\( \\vec{b} \\) (Fig 2.29):\n\\[\\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}}\\] In spherical coordinates, \\( \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\phi} \\vu{\\phi} \\), so\n\\[\\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\dd{r}\\] Therefore,\n\\[\\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{a}^{b} \\frac{q}{r^2} \\dd{r} \\\\ = \\left.\\frac{-1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\right|_{r_a} ^{r_b} \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r_a} - \\frac{q}{r_b} \\right)\\] The integral around a closed path is evidently zero (for then \\( r_a = r_b \\) ):\n\\[\\oint \\vec{E} \\cdot \\dd\\vec{l} = 0 \\label{2.19} \\tag{2.19}\\] and hence, applying Stokes\u0026rsquo; theorem\n\\[\\curl{\\vec{E}} = 0 \\label{2.20} \\tag{2.20}\\] Now, I proved eqs. \\( \\eqref{2.19} \\) and \\( \\eqref{2.20} \\) only for the field of a single point charge at the origin, but these results make no reference to what is, after all, a perfectly arbitrary choice of coordinates; they hold no matter where the charge is located. Moreover, if we have many charges, the principle of superposition states that the total field is a vector sum of their individual fields:\n\\[ \\vec{E} = \\vec{E_1} \u0026#43; \\vec{E_2} \u0026#43; \\ldots \\] so\n\\[\\curl{\\vec{E}} = \\curl{(\\vec{E_1} \u0026#43; \\vec{E_2} \u0026#43; \\ldots)} = (\\curl{\\vec{E_1}}) \u0026#43; (\\curl{\\vec{E_2}}) \u0026#43; \\ldots = 0\\] Thus, Eqs. \\( \\eqref{2.19} \\) and \\( \\eqref{2.20} \\) hold for any static charge distribution whatever.\n"},{"id":55,"href":"/r/notes/griffiths/ch2-3/","title":"Electric Potential","section":"Griffiths Introduction to Electrodynamics","content":" 2.3: Electric Potential # 2.3.1: Introduction to Potential # The electric field E is not just any old vector function. It is a very special kind of vector function: one whose curl id zero. \\( \\vec{E} = y \\vu{x} \\) , for example, could not possibly be an electrostatic field; no set of charges, regardless of their sizes and positions, could ever produce such a field. We\u0026rsquo;re going to exploit this special property of electric fields to reduce a vector problem (finding E) to a much simpler scalar problem. The first theorem in Sect 1.6.2 asserts that any vector whose curl is zero is equal to the gradient of some scalar. What I\u0026rsquo;m going to do now amounts to a proof of that claim, in the context of electrostatics.\nBecause \\( \\nabla \\cross \\vec{E} = 0\\) , the line integral of E around any closed loop is zero (that follows from Stokes\u0026rsquo; theorem). Because \\( \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 \\), the line integral of E from point a to point b is the same for all paths (otherwise you could go out along path (i) and return along path (ii) - Fig 2.30 - and obtain \\( \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\neq 0 \\) ). Because the line integral is independent of path, we can define a function\n\\[V(\\vec{r}) \\equiv - \\int _{O} ^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\label{2.21} \\tag{2.21}\\] Here \\( O \\) is some standard reference point on which we have agreed beforehand; V then depends only on the point \\( \\vec{r} \\). It is called the electric potential.\nThe potential difference between two points a and b is\n\\[\\begin{aligned} V(\\vec{b}) - V(\\vec{a}) \u0026amp; = \u0026amp; -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \u0026#43; \\int_{O}^{\\vec{a}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ \u0026amp; = \u0026amp; -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} - \\int_{\\vec{a}}^{O} \\vec{E}\\cdot \\dd{\\vec{l}} \\\\ \u0026amp; = \u0026amp; - \\int_{\\vec{a}} ^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \\end{aligned} \\tagl{2.22}\\] Now, the fundamental theorem for gradients states that\n\\[V(\\vec{b}) - V(\\vec{a}) = \\int_{\\vec{a}} ^{\\vec{b}} (\\grad{V}) \\cdot \\dd{\\vec{l}}\\] so\n\\[\\int_{\\vec{a}}^{\\vec{b}} (\\grad{V})\\cdot \\dd{\\vec{l}} = - \\int_{\\vec{a}}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}}\\] Since, finally, this is true for any points a and b, the integrands must be equal:\n\\[\\vec{E} = - \\grad{V} \\label{2.23} \\tag{2.23}\\] Equation \\( \\eqref{2.23} \\) is the differential version of \\( \\eqref{2.21} \\); it says that the electric field is the gradient of a scalar potential, which is what we set out to prove.\nNotice the subtle but crucial role played by path independence (or, equivalently, the fact that \\( \\nabla \\times \\vec{E} = 0 \\) ) in this argument. If the line integral of E depended on the path taken, then the \u0026ldquo;definition\u0026rdquo; of V \\( \\eqref{2.21} \\) would be nonsense. It simply would not define a function, since changing the path would alter the value of \\( V(\\vec{r}) \\). By the way, don\u0026rsquo;t let the minus sign in \\( \\eqref{2.23} \\) distract you; it carries over from \\( \\eqref{2.21} \\) and is largely a matter of convention.\n2.3.2: Comments on Potential # The name. The word \u0026ldquo;potential\u0026rdquo; is a hideous misnomer because it inevitably reminds you of potential energy. This is particularly insidious, because there is a connection between \u0026ldquo;potential\u0026rdquo; and \u0026ldquo;potential energy,\u0026rdquo; as you will see in Sect 2.4. I\u0026rsquo;m sorry that it is impossible to escape this word. The best I can do is to insist once and for all that \u0026ldquo;potential\u0026rdquo; and \u0026ldquo;potential energy\u0026rdquo; are completely different terms and should, by all rights, have different names. Incidentially, a surface over which the potential is constant is called an equipotential.\nAdvantage of the potential formulation. If you know V, you can easily get E - just take the gradient: \\( \\vec{E} =- \\grad{V} \\). This is quite extraordinary when you stop to think about it, for E is a vector quantity (three components), but V is a scalar (one component). How can one function possibly contain all the information that three independent functions carry? The answer is that the three components of E are not really as independent as they look; in fact, they are explicitly interrelated by the very condition we started with,\\( \\nabla \\times \\vec{E} = 0 \\). In terms of components,\n\\[\\pdv{E_x}{y} = \\pdv{E_y}{x}, \\qquad \\pdv{E_z}{y} = \\pdv{E_y}{z}, \\qquad \\pdv{E_x}{z} = \\pdv{E_z}{x}\\] This brings us back to my observation at the beginning of Sect 2.3.1: E is a very special kind of vector.What the potential formulation does is to exploit this feature to maximum advantage, reducing a vector problem to a scalar one, in which there is no need to fuss with components.\nThe reference point \\( \\mathscr{O} \\). There is an essential ambiguity in the definition of potential, since the choice of reference point \\( \\mathscr{O} \\) was arbitrary. Changing reference points amounts to adding a constant K to the potential:\n\\[V\u0026#39;(r) = -\\int_{\\mathscr{O}\u0026#39;}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - \\int_{\\mathscr{O}\u0026#39;} ^{\\mathscr{O}} \\vec{E} \\cdot \\dd{\\vec{l}} - \\int_{\\mathscr{O}}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = K \u0026#43; V(\\vec{r})\\] where K is the line integral of E from the old reference point \\( \\mathscr{O} \\) to the new one \\( \\mathscr{O}\u0026rsquo; \\). Of course, adding a constant to V will not affect the potential difference between two points, since the K\u0026rsquo;s cancel out. Nor does the ambiguity affect the gradient of V:\n\\[\\grad{V\u0026#39;} = \\grad{V} \\] since the derivative of a constant is zero. That\u0026rsquo;s why all such V\u0026rsquo;s, differing only in their choice of reference point, correspond to the same field E\nPotential as such carries no real physical significance, for at any given point we can adjust its value at will by suitable relocation of \\( \\mathscr{O} \\). In this sense, it is rather like altitude: if I ask you how high Denver is, you will probably tell me its height above sea level, because that is a convenient and traditional reference point. But we could as well agree to measure altitude above Washington, DC, or Greenwich, or wherever. That would add (or rather, subtract) a fixed amount from all our sea-level readings, but it wouldn\u0026rsquo;t change anything about the real world. The only quantity of interest is the difference in altitude between two points, and that is the same whatever your reference level.\nHaving said this, however, there is a \u0026ldquo;natural\u0026rdquo; spot to use for \\( \\mathscr{O} \\) in electrostatics - analogous to sea level for altitude - and that is a point infinitely far from the charge. Ordinarily, then, we s\u0026quot;set the zero of potential at infinity.\u0026quot; (Since \\( V(\\mathscr{O}) = 0 \\), choosing a reference point is equivalent to selecting a place where \\( V \\) is to be zero.) But I must warn you that there is one special circumstance in which this convention fails: when the charge distribution itself extends to infinity. The symptom of trouble, in such cases, is that the potential blows up. For instance, the field of a uniformly charged plane is \\( (\\sigma / 2 \\epsilon_0) \\vu{n} \\), as we found in Ex 2.5; if we naively put \\( \\mathscr{O} = \\infty \\), then the potential at height z above the plane becomes\n\\[V(z) = - \\int_{\\infty}^{z}\\frac{1}{2\\epsilon_0} \\sigma \\dd{z} = - \\frac{1}{2\\epsilon_0} \\sigma(z - \\infty)\\] The remedy is simply to choose some other reference point (in this example you might use a point on the plane). Notice that the difficulty occurs only in textbook problems; in \u0026ldquo;real life\u0026rdquo; there is no such thing as a charge distribution that goes on forever, and we can always use infinity as our reference point.\nPotential obeys the superposition principle. The original superposition principle pertains to the force on a test charge Q. It says that the total force on Q is the vector sum of the forces attributable to the source charges individually:\n\\[\\vec{F} = \\vec{F_1} \u0026#43; \\vec{F_2} \u0026#43; \\ldots\\] Dividing through by Q, we see that the electric field, too, obeys the superposition principle:\n\\[\\vec{E} = \\vec{E_1} \u0026#43; \\vec{E_2} \u0026#43; \\ldots \\label{2.38}\\] Integrating from the common reference point to \\( \\vec{r} \\), it follows that the potential also satisfies such a principle:\n\\[V = V_1 \u0026#43; V_2 \u0026#43; \\ldots\\] That is, the potential at any given point is the sum of the potentials due to all the source charges separately. Only this time it is an ordinary sum, not a vector sum, which makes it a lot easier to work with.\nUnits of Potential. In our units, force is measured in newtons and charge in coulombs, so electric fields are in newtons per coulomb. Accordingly, potential is newton-meters per coulomb, or joules per coulomb. A joule per coulomb is a volt.\n2.3.3: Poisson\u0026rsquo;s Equation and Laplace\u0026rsquo;s Equation # We found in Sect 2.3.1 that the electric field can be written as the gradient of a scalar potential\n\\[\\vec{E} = - \\grad{V}\\] The question arises, what do the divergence and curl of E,\n\\[\\div{\\vec{E}} = \\frac{\\rho}{\\epsilon_0} \\qquad \\text{ and } \\qquad \\curl{\\vec{E}} = 0\\] look like, in terms of V? Well, \\( \\div{\\vec{E}} = \\div(-\\grad{V}) = -\\laplacian{V} \\), so, apart from that persistent minus sign, the divergence of E is the Laplacian of V. Gauss\u0026rsquo;s law, then, says\n\\[\\laplacian{V} = -\\frac{\\rho}{\\epsilon_0} \\label{2.24}\\] This is known as Poisson\u0026rsquo;s equation. In regions where there is no charge, so \\( \\rho = 0 \\), Poisson\u0026rsquo;s equation reduces to Laplace\u0026rsquo;s equation,\n\\[\\laplacian{V} = 0 \\label{2.25}\\] We\u0026rsquo;ll explore this equation more fully in Chapter 3.\nSo much for Gauss\u0026rsquo;s law. What about the curl law? This says that\n\\[\\curl{\\vec{E}} = \\curl(-\\grad{V}) = 0\\] But that\u0026rsquo;s no condition on V - curl of gradient is always zero. Of course, we used the curl law to show that E could be expressed as the gradient of a scalar, so it\u0026rsquo;s not really surprising that this works out: \\( \\curl{\\vec{E}} = 0 \\) permits our definition of V; in return, \\( \\vec{E} = - \\grad{V} \\) guarantees \\( \\curl{\\vec{E}} = 0 \\). It only takes one differential equation (Poisson\u0026rsquo;s) to determine V, because V is a scalar. For \\( \\vec{E} \\) we needed two, the divergence and the curl.\n2.3.4: The potential of a Localized Charge Distribution # I defined V in terms of \\( \\vec{E} \\eqref{2.21} \\). Ordinarily, though, it\u0026rsquo;s E that we\u0026rsquo;re looking for (if we already knew E, there wouldn\u0026rsquo;t be much point in calculating V). The idea is that it might be easier to get V first, and then calculate E by taking the gradient. Typically, then, we know where the charge is (that is, we know \\( \\rho \\)), and we want to find V. Now, Poisson\u0026rsquo;s equation relates V and \\( \\rho \\), but unfortunately it\u0026rsquo;s \u0026ldquo;the wrong way round\u0026rdquo;: it would give us \\( \\rho \\) if we knew V, whereas we want V, knowing \\( \\rho \\). What we must do, then, is \u0026ldquo;invert\u0026rdquo; Poisson\u0026rsquo;s equation. That\u0026rsquo;s the program for this section, although I shall do it by roundabout means, beginning, as always, with a point charge at the origin.\nThe electric field is \\( \\vec{E} = (1 / 4 \\pi \\epsilon_0)(1 / r^2) \\vu{r} \\), and \\( \\dd{\\vec{l}} = \\dd{r} \\vu{r} \\), and \\( \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\theta} \\vu{\\phi} \\), so\n\\[\\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\dd{r} \\] Setting the reference point at infinity, the potential of a point charge q at the origin is\n\\[V(r) = - \\int_{\\mathscr{O}} ^r \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = \\frac{-1}{4 \\pi \\epsilon_0} \\int_{\\infty}^r \\frac{q}{(r\u0026#39;) ^2} \\dd{r\u0026#39;} \\\\ = \\left.\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r\u0026#39;} \\right| ^r _{\\infty} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\] (You see here the advantage of using infinity for the reference point: it kills the lower limit on the integral.) Notice the sign of V; presumably the conventional minus sign in the definition was chosen in order to make the potential of a positive charge come out positive. It is useful to remember that regions of positive charge are potential \u0026ldquo;hills,\u0026rdquo; and electric field points \u0026ldquo;downhill\u0026rdquo; from plus toward minus.\nIn general, the potential of a point charge q is\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} \\] where \\( \\gr \\), as always, is the distance from \\( q \\) to \\( \\vec{r} \\) (Fig 2.32). Invoking the superposition principle, then, the potential of a collection of charges is\n\\[V(r) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1} ^n \\frac{q_i}{\\gr _i} \\] or, for a continuous distribution,\n\\[V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}\u0026#39;)}{\\gr} \\dd{\\tau\u0026#39;} \\label{2.29} \\tag{2.29}\\] This is the equation we were looking for, telling us how to compute V when we know \\( \\rho \\); it is, if you like, the \u0026ldquo;solution\u0026rdquo; to Poisson\u0026rsquo;s equation, for a localized charge distribution. Compare \\( \\eqref{2.29} \\) with the corresponding formula for the electric field in terms of \\( \\rho \\):\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r\u0026#39;})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau\u0026#39;}\\] The main point is that the pesky unit vector \\( \\vu{\\gr} \\) is gone, so there is no need to fuss with components. The potentials of line and surface charges are\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r\u0026#39;})}{\\gr} \\dd{l\u0026#39;} \\qquad \\text{ and } \\qquad V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r\u0026#39;})}{\\gr} \\dd{a\u0026#39;}\\] I should warn you that everything in this section is predicated on the assumption that the reference point is at infinity. This is hardly apparent in \\( \\eqref{2.29} \\), but remember that we got the equation from the potential of a point charge at the origin, \\( (1/4 \\pi \\epsilon_0) (q / r) \\), which is valid only when \\( \\mathscr{O} = \\infty \\). If you try to apply these formulas to one of those artificial problems in which the charge itself extends to infinity, the integral will diverge.\n2.3.5: Boundary Conditions # In the typical electrostatic problem you are given a source charge distribution \\( \\rho \\), and you want to find the electric field \\( \\vec{E} \\) it produces. Unless the symmetry of the problem allows a solution by Gauss\u0026rsquo;s law, it is generally to your advantage to calculate the potential first, as an intermediate step. These are the three fundamental quantities of electrostatics: \\( \\rho \\), \\( \\vec{E} \\), and \\( V \\). We have, in the course of our discussion, derived all six formulas interrelating them. These equations are neatly summarized in Fig. 2.35. We began with just two experimental observations: (1) the principle of superposition - a broad general rule applying to all electromagnetic forces, and (2) Coulomb\u0026rsquo;s law - the fundamental law of electrostatics. From these, all else followed.\nYou may have noticed, in studying the exercises in this chapter, that the electric field always undergoes a discontinuity when you cross a surface charge \\( \\sigma \\). In fact, it is a simple matter to find the amount by which E changes at such a boundary. Suppose we draw a wafer-thin Gaussian pillbox, extending just barely over the edge in each direction (Fig. 2.36). Gauss\u0026rsquo;s law says that\n\\[\\oint _{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} = \\frac{1}{\\epsilon_0} \\sigma A\\] where A is the area of the pillbox lid. If \\\\( \\sigma \\\\) varies from point to point or the surface is curved, we can simply pick A to be extremely small. Now, the sides of the pillbox contribute nothing to the flux, in the limit as the thickness \\\\( \\epsilon \\\\) goes to zero, so we are left with \\[E_{above}^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\label{2.31} \\tag{2.31}\\] where \\( E_{above}^{\\perp} \\) denotes the component of \\( \\vec{E} \\) that is perpendicular to the surface immediately above, and \\( E_{below} ^{\\perp} \\) is the same, only just below the surface. For consistency, let \u0026ldquo;upward\u0026rdquo; be the positive direction for both. Conclusion: the normal component of \\( \\vec{E} \\) is discontinuous by an amount \\( \\sigma / \\epsilon_0 \\) at any boundary. In particular, where there is no surface charge, \\( \\vec{E}^{\\perp} \\) is continuous, as for instance at the surface of a uniformly charged solid sphere.\nThe tangential component of \\( \\vec{E} \\), by contrast, is always continuous. For if we apply Eq. 2.19,\n\\[\\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0\\] to the thin rectangular loop of Fig 2.37, the ends give nothing (as \\( \\epsilon \\rightarrow 0 \\)), and the sides give \\( (E_{above} ^{\\parallel} l - E_{below} ^{\\parallel} l) \\), so\n\\[\\vec{E}_{above} ^{\\parallel} = \\vec{E}_{below} ^{\\parallel} \\label{2.32} \\tag{2.32}\\] where \\( \\vec{E}^{\\parallel} \\) stands for the components of \\( \\vec{E} \\) parallel to the surface.\nThe boundary conditions on \\( \\vec{E} \\) (Eqs. \\( \\eqref{2.31} \\) and \\( \\eqref{2.32} \\)) can be combined into a single formula:\n\\[\\vec{E}_{above} - \\vec{E}_{below} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.33}\\] where \\( \\vu{n} \\) is a unit vector perpendicular to the surface, pointing from \u0026ldquo;below\u0026rdquo; to \u0026ldquo;above.\u0026rdquo;\nThe potential, meanwhile, is continuous across any boundary (Fig 2.38), since\n\\[V_{above} - V_{below} = -\\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}}\\] as the path length shrinks to zero, so too does the integral\n\\[V_{above} = V_{below} \\label{2.34} \\tag{2.34}\\] However, the gradient of V inherits the discontinuity in \\( \\vec{E} \\), since \\( \\vec{E} - \\grad{V} \\), so\n\\[\\grad{V}_{above} - \\grad{V}_{below} = - \\frac{\\sigma}{\\epsilon_0} \\vu{n}\\] or more conveniently\n\\[\\pdv{V_{above}}{n} - \\pdv{V_{below}}{n} = - \\frac{1}{\\epsilon_0} \\sigma \\label{2.36} \\tag{2.36}\\] where\n\\[\\pdv{V}{n} = \\grad{V} \\cdot \\vu{n}\\] denotes the normal derivative of V (that is, the rate of change in the direction perpendicular to the surface).\nPlease note that these boundary conditions relate the fields and potentials just above and just below the surface. For example, the derivatives in \\( \\eqref{2.36} \\) are the limiting values as we approach the surface from either side.\n"},{"id":56,"href":"/r/notes/griffiths/ch2-4/","title":"Work and Energy in Electrostatics","section":"Griffiths Introduction to Electrodynamics","content":" 2.4: Work and Energy in Electrostatics # 2.4.1: The Work it Takes to Move a Charge # Suppose you have a stationary configuration of source charges, and you want to move a test charge Q from point a to point b (Fig. 2.39). Question: how much work will you have to do? At any point along the path, the electric force on Q is \\( \\vec{F} = Q \\vec{E} \\); the force you must exert, in opposition to the electric force, is \\( -Q\\vec{E} \\). The work you do is therefore\n\\[W = \\int_{a}^{b} \\vec{F} \\cdot \\dd{\\vec{l}} \\\\ = - Q \\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - Q[V(b) - V(a)]\\] Notice that the answer is independent of the path you take from a to b; in mechanics, then, we would call the electrostatic force \u0026ldquo;conservative.\u0026rdquo; Dividing through by Q, we have\n\\[V(b) - V(a) = \\frac{W}{Q} \\] In words, the potential difference between points a and b is equal to the work per unit charge required to carry a particle from a to b. In particular, if you want to bring Q in from far away and stick it at point r, the work you must do is\n\\[W = Q[V(\\vec{r}) - V(\\infty)],\\] so if you have set the reference point at infinity,\n\\[W = Q V(\\vec{r}) \\label{2.39} \\tag{2.39}\\] In this sense, potential is potential energy (the work it takes to create a system) per unit charge (just as the field is force per unit charge).\n2.4.2: The Energy of a Point Charge Distribution # How much work would it take to assemble an entire collection of point charges? Imagine bringing in the charges, one by one, from far away (Fig 2.40). The first charge \\( q_1 \\) takes no work, since there is no field to fight against. Now bring in \\( q_2 \\). According to \\( \\eqref{2.39} \\) this will cost you \\( q_2 V_1(\\vec{r}_2) \\), where \\( V_1 \\) is the potential due to \\( q_1 \\) , and \\( \\vec{r}_2 \\) is the place we\u0026rsquo;re putting \\( q_2 \\):\n\\[W_2 = \\frac{1}{4 \\pi \\epsilon_0} q_2 \\left( \\frac{q_1}{\\gr_{12}} \\right)\\] (\\( \\gr_{12} \\) is the distance between \\( q_1 \\) and \\( q_2 \\), once they are in position). As you bring in each charge, nail it down in its final location, so it doesn\u0026rsquo;t move when you bring in the next charge. Now bring in \\( q_3 \\). This requires work \\( q_3 V_{1,2}(\\vec{r}3) \\), where \\( V{1,2} \\) is the potential due to charges \\( q_1 \\) and \\( q_2 \\), namely \\( (1 / 4 \\pi \\epsilon_0) (q_1 / \\gr_{13} + q_2 / \\gr_{23} ) \\). Thus\n\\[W_3 = \\frac{1}{4 \\pi \\epsilon_0} q_3 \\left( \\frac{q_1}{\\gr_{13}} \u0026#43; \\frac{q_2}{\\gr_{23}} \\right)\\] Similarly, the extra work to bring in \\( q_4 \\) will be\n\\[W_4 = \\frac{1}{4 \\pi \\epsilon_0} q_4 \\left( \\frac{q_1}{\\gr_{14}} \u0026#43; \\frac{q_2}{\\gr_{24}} \u0026#43; \\frac{q_3}{\\gr_{34}} \\right)\\] The total work necessary to assemble the first four charges, then, is\n\\[W = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 q_2}{\\gr_{12}} \u0026#43; \\frac{q_1 q_3}{\\gr_{13}} \u0026#43; \\frac{q_1 q_4}{\\gr_{14}} \u0026#43; \\frac{q_2 q_3}{\\gr_{23}} \u0026#43; \\frac{q_2 q_4}{\\gr_{24}} \u0026#43; \\frac{q_3 q_4}{\\gr_{34}} \\right)\\] You see the general rule: Take the product of each pair of charges, divide by their separation distance, and add it all up:\n\\[W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1} ^{n} \\sum_{j \u0026gt; i} ^n \\frac{q_i q_j}{\\gr_{ij}} \\] The stipulation \\( j \u0026gt; i \\) is to remind you not to count the same pair twice. A nicer way to accomplish this is intentionally to count each pair twice, and then divide by 2:\n\\[W = \\frac{1}{8 \\pi \\epsilon_0} \\sum_{i = 1} ^n \\sum_{j \\neq i} \\frac{q_i q_j}{\\gr_{ij}} \\] (we must still avoid \\( i = j \\), of course). Notice that in this form the answer plainly does not depend on the order in which you assemble the charges, since every pair occurs in the sum.\nFinally, let\u0026rsquo;s pull out the factor \\( q_i \\):\n\\[W = \\frac{1}{2} \\sum_{i = 1}^n q_i \\left( \\sum_{j \\neq i} ^n \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_j}{\\gr_{ij}} \\right)\\] The term in parentheses is the potential at point \\( \\vec{r_i} \\) (the position of \\( q_i \\) ) due to all the other charges - all of them, now, not just the ones that were present at some stage during the assembly. Thus,\n\\[W = \\frac{1}{2} \\sum_{i = 1} ^n q_i V(\\vec{r_i}) \\label{2.42} \\tag{2.42}\\] That\u0026rsquo;s how much work it takes to assemble a configuration of point charges; it\u0026rsquo;s also the amount of work you\u0026rsquo;d get back if you dismantled the system. In the meantime, it represents energy stored in the configuration (\u0026ldquo;potential\u0026rdquo; energy, if you insist, though for obvious reasons I prefer to avoid that word in this context).\n2.4.3: The Energy of a Continuous Charge Distribution # For a volume charge density \\( \\rho \\), \\( \\eqref{2.42} \\) becomes\n\\[W = \\frac{1}{2} \\int \\rho V \\dd{\\tau} \\label{2.43} \\tag{2.43}\\] There is a lovely way to write this result, in which \\( \\rho \\) and \\( V \\) are eliminated in favor of \\( \\vec{E} \\). First, use Gauss\u0026rsquo;s law to express \\( \\rho \\) in terms of \\( \\vec{E} \\)\n\\[\\rho = \\epsilon_0 \\div{\\vec{E}} \\qquad \\text{so,} \\qquad W = \\frac{\\epsilon_0}{2} \\int (\\div{\\vec{E}}) V \\dd{\\tau}\\] Now, use integration by parts to transfer the derivative from \\( \\vec{E} \\) to \\( V \\):\n\\[W = \\frac{\\epsilon_0}{2} \\left[ - \\int \\vec{E} \\cdot (\\grad{V}) \\dd{\\tau} \u0026#43; \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right]\\] But \\( \\grad{V} = - \\vec{E} \\), so\n\\[ W = \\frac{\\epsilon_0}{2} \\left( \\int_{\\mathscr{V}} E^2 \\dd{\\tau} \u0026#43; \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right) \\label{2.44} \\tag{2.44} \\] But what volume is this we\u0026rsquo;re integrating over? Let\u0026rsquo;s go back to the formula we started with, \\( \\eqref{2.43} \\). From its derivation, it is clear that we should integrate over the region where the charge is located. But actually, any larger volume would do just as well: The \u0026ldquo;extra\u0026rdquo; territory we throw in will contribute nothing to the integral, since \\( \\rho = 0 \\) out there. With this in mind, we return to \\( \\eqref{2.44} \\). What happens here, as we enlarge the volume beyond the minimum necessary to trap all the charge? Well, the integral of \\( E^2 \\) can only increase (the integrand being positive); evidently the surface integral must decrease accordingly to leave the sum intact. (In fact, at large distances from the charge, \\( E \\) goes like \\( 1 / r^2 \\) and \\( V \\) like \\( 1/r \\), while the surface area grows like \\( r^2 \\); roughly speaking, then, the surface integral goes down like \\( 1/r \\). Please understand: \\( \\eqref{2.44} \\) gives you the correct energy W, whatever volume you use (as long as it encloses all the charge), but the contribution of the volume integral goes up, and that of the surface integral goes down, as you take larger and larger volumes. In particular, why not integrate over all space? Then the surface integral goes to zero, and we are left with\n\\[W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} \\quad \\text{(all space)} \\label{2.45} \\tag{2.45}\\] Example 2.9 # Q Find the energy of a uniformly charged spherical shell of total charge \\( q \\) and radius \\( R \\) A Use \\( \\eqref{2.43} \\) in the version appropriate to surface charges\n\\[W = \\frac{1}{2} \\sigma V \\dd{a}\\] Now, the potential at the surface of this sphere is \\( (1/4 \\pi \\epsilon_0)q/R \\) (a constant), so\n\\[W = \\frac{1}{8\\pi \\epsilon_0} \\frac{q}{R} \\int \\sigma \\dd{a} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} \\] Solution 2 Use \\( \\eqref{2.45} \\). Inside the sphere, \\( \\vec{E} = 0 \\); outside:\n\\[\\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} \\quad \\text{so} \\quad E^2 = \\frac{q^2}{(4 \\pi \\epsilon_0)^2 r^4} \\] Therefore,\n\\[W_{tot} = \\frac{\\epsilon_0}{2 (4 \\pi \\epsilon_0)^2}\\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) \\\\ = \\frac{1}{32 \\pi ^2 \\epsilon_0} q^2 4 \\pi \\int_{R} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} \\] 2.4.4: Comments on Electrostatic Energy # A perplexing \u0026ldquo;inconsistency\u0026rdquo;\nEquation \\( \\eqref{2.45} \\) clearly implies that the energy of a stationary charge distribution is always positive. On the other hand, \\( \\eqref{2.42} \\) (from which \\( \\eqref{2.45} \\) was in fact derived), can be positive or negative. For instance, according to \\( \\eqref{2.42} \\) the energy of two equal but opposite charges a distance \\( \\gr \\) apart is \\( -(1/4 \\pi \\epsilon_0) (q^2/\\gr) \\). What\u0026rsquo;s gone wrong? Which equation is correct?\nThe answer is that both are correct, but they speak to slightly different questions. Equation \\( \\eqref{2.42} \\) does not take into account the work necessary to make the point charges in the first place; we started with point charges and simply found the work required to bring them together. This is wise strategy, since \\( \\eqref{2.45} \\) indicates that the energy of a point charge is in fact infinite\n\\[ W = \\frac{\\epsilon_0}{2(4 \\pi \\epsilon_0)^2} \\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) = \\frac{q^2}{8 \\pi \\epsilon_0} \\int_{0} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\infty \\] Equation \\( \\eqref{2.45} \\) is more complete, in the sense that it tells you the total energy stored in a charge configuration, but \\( \\eqref{2.42} \\) is more appropriate when you\u0026rsquo;re dealing with point charges, because we prefer (for good reason!) to leave out that portion of the total energy that is attributable to the fabrication of the point charges themselves. In practice, after all, the point charges (electrons, say) are given to us ready-made; all we do is move them around. Since we did not put them together, and we cannot take them apart, it is immaterial how much work the process would involve. (Still, the infinite energy of a point charge is a recurring source of embarrassment for electromagnetic theory, afflicting the quantum version as well as the classical. We shall return to the problem in Chapter 11).\nNow, you may wonder where the inconsistency crept into an apparently water-tight derivation. The \u0026ldquo;flaw\u0026rdquo; lies between \\( \\eqref{2.42} \\) and \\( \\eqref{2.43} \\): in the former, \\( V(\\vec{r_i}) \\) represents the potential due to all the other charges, but not \\( q_i \\), whereas in the latter, \\( V(\\vec{r}) \\) is the full potential. For a continuous distribution, there is no distinction, since the amount of charge right at the point \\( \\vec{r} \\) is vanishingly small, and its contribution to the potential is zero. But in the presence of point charges you\u0026rsquo;d better stick with \\( \\eqref{2.42} \\).\nWhere is the energy stored? Equations \\( \\eqref{2.43} \\) and \\( \\eqref{2.45} \\) offer two different ways of calculating the same thing. The first is an integral over the charge distribution, the second is an integral over the field. These can involve completely different regions. For instance, in the case of a spherical shell, the charge is confined to the surface, whereas the electric field is everywhere outside this surface. Where is the energy, then? Is it stored in the field, as \\( \\eqref{2.45} \\) seems to suggest, or is it stored in the charge, as \\( \\eqref{2.43} \\) implies? At the present stage this is simply an unanswerable question: I can tell you what the total energy is, and I can provide you with several different ways to compute it, but it is impertinent to worry about where the energy is located. In the context of radiation theory (Chapter 11) it is useful (and in general relativity it is essential) to regard the energy as stored in the field, with a density\n\\[\\frac{\\epsilon_0}{2} E^2 = \\text{ energy per unit volume} \\label{2.46} \\tag{2.46}\\] But in electrostatics one could just as well say it is stored in the charge, with a density \\( \\frac{1}{2} \\rho V \\). The difference is purely a matter of bookkeeping.\nThe superposition principle. Because electrostatic energy is quadratic in the fields, it does not obey a superposition principle. The energy of a compound system is not the sum of the energies of its parts considered separately - there are also \u0026ldquo;cross terms\u0026rdquo;:\n\\[\\begin{aligned} W_{tot} \u0026amp; = \u0026amp; \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} = \\frac{\\epsilon_0}{2} \\int (\\vec{E_1} \u0026#43; \\vec{E_2})^2 \\dd{\\tau} \\\\ \u0026amp; = \u0026amp; \\frac{\\epsilon_0}{2} \\int (E_1 ^2 \u0026#43; E_2 ^2 \u0026#43; 2 \\vec{E_1} \\cdot \\vec{E_2}) \\dd{\\tau} \\\\ \u0026amp; = \u0026amp; W_1 \u0026#43; W_2 \u0026#43; \\epsilon_0 \\int \\vec{E_1} \\cdot \\vec{E_2} \\dd{\\tau} \\end{aligned}\\] For example, if you double the charge everywhere, you quadruple the total energy.\n"},{"id":57,"href":"/r/notes/griffiths/ch2-5/","title":"Conductors","section":"Griffiths Introduction to Electrodynamics","content":" 2.5: Conductors # 2.5.1: Basic Properties # In an insulator, such as glass or rubber, each electron is on a short leash, attached to a particular atom. In a metallic conductor, by contrast, one or more electrons per atom are free to roam. (In liquid conductors such as salt water, it is ions that do the moving). A perfect conductor would contain an unlimited supply of free charges. In real life there are no perfect conductors, but metals come pretty close, for most purposes.\nFrom this definition, the basic electrostatic properties of ideal conductors immediately follow:\n(i) E = 0 inside a conductor. Why? Because if there were any field, those free charges would move, and it wouldn\u0026rsquo;t be electrostatics any more. Hmm\u0026hellip; that\u0026rsquo;s hardly a satisfactory explanation; maybe all it proves is that you can\u0026rsquo;t have electrostatics when conductors are present. We had better examine what happens when you put a conductor into an external electric field \\( \\vec{E_0} \\) (Fig. 2.42). Initially, the field will drive any free positive charges to the right, and negative ones to the left. (In practice, it\u0026rsquo;s the negative charges - electrons - that do the moving, but when they depart, the right side is left with a net positive charge - the stationary nuclei - so it doesn\u0026rsquo;t really matter which charges move; the effect is the same). When they come to the edge of the material, the charges pile up: plus on the right side, minus on the left. Now, these induced charges produce a field of their own, \\( \\vec{E_1} \\), which, as you can see from the figure, is in the opposite direction to \\( \\vec{E_0} \\). That\u0026rsquo;s the crucial point, for it means that the field of the induced charges tends to cancel the original field. Charge will continue to flow until this cancellation is complete, and the resultant field inside the conductor is precisely zero. The whole process is practically instantaneous.\n(ii) \\( \\rho = 0 \\) inside a conductor. This follows from Gauss\u0026rsquo;s law: if E is zero, so also is \\( \\rho \\). There is still charge around, but exactly as much plus as minus, so the net charge density in the interior is zero.\n(iii) Any net charge resides on the surface. That\u0026rsquo;s the only place left.\n(iv) A conductor is an equipotential. For if a and b are any two points within (or at the surface of) a given conductor, \\( V(b) - V(a) = - \\int _{a} ^{b} \\vec{E} \\cdot \\dd{\\vec{l}} = 0 \\), and hence \\( V(a) = V(b) \\).\n(v) E is perpendicular to the surface, just outside a conductor. Otherwise, as in (i), charge will immediately flow around the surface until it kills off the tangential component (Fig. 2.43). (Perpendicular to the surface, charge cannot flow, of course, since it is confined to the conducting object.)\nI think it is astonishing that the charge on a conductor flows to the surface. Because of their mutual repulsion, the charges naturally spread out as much as possible, but for all of them to go to the surface seems like a waste of the interior space. Surely we could do better, from the point of view of making each charge as possible from its neighbors, to sprinkle some of them throughout the volume. Well, it simply is not so. You do best to put all the charge on the surface, and this is true regardless of the size or shape of the conductor.\nThe problem can also be phrased in terms of energy. Like any other free dynamical system, the charge on a conductor will seek the configuration that minimizes its potential energy. What property (iii) asserts is that the electrostatic energy of a solid object (with specified shape and total charge) is a minimum when that charge is spread over the surface. For instance, the energy of a sphere is \\( (1 / 8 \\pi \\epsilon_0)(q^2 / R) \\) if the charge is uniformly distributed over the surface, as we found in Ex 2.9, but it is greater \\( (3/20 \\pi \\epsilon_0)(q^2 / R) \\) if the charge is uniformly distributed throughout the volume (Prob. 2.34).\n2.5.2: Induced Charges # If you hold a charge +q near an uncharged conductor (Fig 2.44), the two will attract one another. The reason for this is that q will pull minus charges over to the near side and repel plus charges to the far side (Another way to think of it is that the charge moves around in such a way as to kill off the field of q for points inside the conductor, where the total field must be zero.) Since the negative induced charge is closer to q, there is a net force of attraction. (In chapter 3 we will calculate this force explicitly, for the case of a spherical conductor.)\nWhen I speak of the field, charge, or potential \u0026ldquo;inside\u0026rdquo; a conductor, I mean in the \u0026ldquo;meat\u0026rdquo; of the conductor. If there is some hollow cavity in the conductor, and within that cavity you put some charge, then the field in the cavity will not be zero. But in a remarkable way the cavity and its contents are electrically isolated from the outside world by the surrounding conductor (Fig. 2.45). No external fields penetrate the conductor; they are canceled at the outer surface by the induced charge there. Similarly, the field due to charges within the cavity is canceled, for all exterior points, by the induced charge on the inner surface. However, the compensating charge left over on the outer surface of the conductor effectively \u0026ldquo;communicates\u0026rdquo; the presence of q to the outside world. The total charge induced on the cavity wall is equal and opposite to the charge inside, for if we surround the cavity with a Gaussian surface, all points of which are in the conductor (Fig 2.45), \\( \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = 0 \\), and hence (by Gauss\u0026rsquo;s law) the net enclosed charge must be zero. But \\( Q_{enc} = q + q_{induced} \\), so \\( q_{induced} = - q \\). Then if the conductor as a whole is electrically neutral, there must be a charge +q on its outer surface.\nExample 2.10 # An uncharged spherical conductor centered at the origin has a cavity of some weird shape carved out of it (Fig. 2.46). Somewhere within the cavity is a charge q. Question: What is the field outside the sphere?\nSolution At first glance, it would appear that the answer depends on the shape of the cavity and the location of the charge. But that\u0026rsquo;s wrong: the answer is\n\\[\\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r}\\] regardless. The conductor conceals from us all information concerning the nature of the cavity, revealing only the total charge it contains. How can this be? Well, the charge +q induces an opposite charge -q on the wall of the cavity, which distributes itself in such a way that its field cancels that of q, for all points exterior to the cavity. Since the conductor carries no net charge, this leaves +q to distribute itself uniformly over the surface of the sphere. (It\u0026rsquo;s uniform because the asymmetrical influence of the point charge +q is negated by that of the induced charge -q on the inner surface.) For points outside the sphere, then, the only thing that survives is the field of the leftover +q, uniformly distributed over the outer surface.\nIt may occur to you that in one respect this argument is open to challenge: There are actually three fields at work here: \\( \\vec{E_q}, \\vec{E_{induced}} \\), and \\( \\vec{E_{leftover}} \\). All we know for certain is that the sum of the three is zero inside the conductor, yet I claimed that the first two alone cancel, while the third is separately zero there. Moreover, even if the first two cancel within the conductor, who is to say they still cancel for points outside? They do not, after all, cancel for points inside the cavity. I cannot give you a completely satisfactory answer at the moment, but this much at least is true: there exists a way of distributing -q over the inner surface so as to cancel the field of q at all exterior points. For that same cavity could have been carved out of a huge spherical conductor with a radius of 27 miles or light years or whatever. In that case, the leftover +q on the outer surface is simply too far away to produce a significant field, and the other two fields would have to accomplish the cancellation by themselves. So we know they can do it\u0026hellip; but are we sure they choose to? Perhaps for small spheres nature prefers some complicated three-way cancellation? Nope: As we\u0026rsquo;ll see in the uniqueness theorems of Chapter 3, electrostatics is very stingy with its options; there is always precisely one way - no more - of distributing the charge on a conductor so as to make the field inside zero. Having found a possible way, we are guaranteed that no alternative exists, even in principle.\nIf a cavity surrounded by conducting material is itself empty of charge, then the field within the cavity is zero. For any field line would have to begin and end on the cavity wall, going from a plus charge to a minus charge (Fig 2.47). Letting that field line be part of a closed loop, the rest of which is entirely inside the conductor (where E = 0), the integral \\( \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\) is distinctly positive, in violation of Eq. 2.19. It follows that \\( E = 0 \\) within an empty cavity, and there is in vact no charge on the surface of the cavity. (This is why you are relatively safe inside a metal car during a thunderstorm - you may get cooked, if lightning strikes, but you will not be electrocuted. The same principle applies to the placement of sensitive apparatus inside a grounded Faraday cage, to shield out stray electric fields. In practice, the enclosure doesn\u0026rsquo;t even have to be solid conductor - chicken wire will often suffice.)\n2.5.3: Surface Charge and the Force on a Conductor # Because the field inside a conductor is zero, boundary condition Eq. 2.33 requires that the field immediately outside is\n\\[ \\vec{E} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.48} \\tag{2.48} \\] consistent with our earlier conclusion that the field is normal to the surface. In terms of potential, Eq. 2.36 yields\n\\[\\sigma = - \\epsilon_0 \\pdv{V}{n} \\label{2.49} \\tag{2.49}\\] These equations enable you to calculate the surface charge on a conductor, if you can determine \\( \\vec{E} \\) or \\( V \\); we shall use them frequently in the next chapter.\nIn the presence of an electric field, a surface charge will experience a force; the force per unit area, \\( \\vec{f} \\), is \\( \\sigma \\vec{E} \\). But there\u0026rsquo;s a problem here, for the electric field is discontinuous at a surface charge, so what are we supposed to use: \\( \\vec{E}{above}, \\vec{E}{below} \\), or something in between? The answer is that we should use the average of the two\n\\[ \\vec{f} = \\sigma \\vec{E}_{average} = \\frac{1}{2} \\sigma (\\vec{E}_{above} \u0026#43; \\vec{E}_{below}) \\label{2.50} \\tag{2.50}\\] Why the average? The reason is very simple, thought the telling makes it sound complicated: Let\u0026rsquo;s focus our attention on a tiny patch of surface surrounding the point in question (Fig. 2.50). Make it small enough so it is essentially flat and the surface in question is essentially constant. The total field consists of two parts - that attributable to the patch itself, and that due to everything else (other regions of the surface, as well as any external sources that may be present)\n\\[\\vec{E} = \\vec{E}_{patch} \u0026#43; \\vec{E}_{other}\\] Now, the patch cannot exert a force on itself, any more than you can lift yourself by standing in a basket and pulling up on the handles. The force on the patch, then, is exclusively due to \\( \\vec{E}_{other} \\), and this suffers no discontinuity (if we removed the patch, the field in the \u0026ldquo;hole\u0026rdquo; would be perfectly smooth). The discontinuity is due entirely to the charge on the patch, which puts out a field \\( (\\sigma / 2 \\epsilon_0) \\) on either side, pointing away from the surface. Thus,\n\\[ \\vec{E}_{above} = \\vec{E}_{other} \u0026#43; \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ \\vec{E}_{below} = \\vec{E}_{other} - \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ \\] and hence\n\\[\\vec{E}_{other} = \\frac{1}{2} (\\vec{E}_{above} \u0026#43; \\vec{E}_{below}) = \\vec{E}_{average}\\] Averaging is really just a device for removing the contribution of the patch itself.\nThat argument applies to any surface charge; in the particular case of a conductor, the field is zero inside and \\( (\\sigma / \\epsilon_0)\\vu{n} \\) outside (\\( \\eqref{2.48} \\), so the average is \\( (\\sigma / 2 \\epsilon_0) \\vu{n} \\), and the force per unit area is\n\\[ f = \\frac{1}{2 \\epsilon_0} \\sigma ^2 \\vu{n} \\label{2.51} \\tag{2.51} \\] This amounts to an outward electrostatic pressure on the surface, tending to draw the conductor into the field, regardless of the sign of \\( \\sigma \\). Expressing the pressure in terms of the field just outside the surface\n\\[ P = \\frac{\\epsilon_0}{2} E^2 \\] 2.5.4: Capacitors # Suppose we have two conductors, and we put charge +Q on one and -Q on the other (Fig 2.51). Since V is constant over a conductor, we can speak unambiguously of the potential difference between them:\n\\[V = V_{\u0026#43;} - V_{-} = - \\int_{(-)}^{(\u0026#43;)} \\vec{E} \\cdot \\dd{\\vec{l}}\\] We don\u0026rsquo;t know how the charge distributes itself over the two conductors, and calculating the field would be a nightmare, if their shapes are complicated, but this much we do know: \\( \\vec{E} \\) is proportional to \\( Q \\). For \\( \\vec{E} \\) is given by Coulomb\u0026rsquo;s law:\n\\[\\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr^2} \\vu{\\gr} \\dd{\\tau}\\] so if you double \\( \\rho \\), you double \\( \\vec{E} \\). Wait a minute! How do we know that doubling Q (and also -Q) simply doubles \\( \\rho \\)? Maybe the charge moves around into a completely different configuration, quadrupling \\( \\rho \\) in some places and halving it in others, just so the total charge on the conductor is doubled. The fact is that this concern is unwarranted - doubling Q does double \\( \\rho \\) everywhere; it doesn\u0026rsquo;t shift charge around. The proof will come in Chapter 3; for now you\u0026rsquo;ll have to trust me.\nSince \\( \\vec{E} \\) is proportional to Q, so also is V. The constant of proportionality is called the capacitance of the arrangement\n\\[C \\equiv \\frac{Q}{V} \\label{2.53} \\tag{2.53}\\] Capacitance is a purely geometrical quantity, determined by the sizes, shapes, and separation of the two conductors. In SI units, C is measured in farads (F); a farad is a coulomb-per-volt. Actually this turns out to be inconveniently large; more practical units are the microfarad (\\( 10^{-6} F \\)) and the picofarad (\\( 10^{-12} F \\))\nNotice that V is, by definition, the potential of the positive conductor less that of the negative one; likewise, Q is the charge of the positive conductor. Accordingly, capacitance is an intrinsically positive quantity. By the way, you will occasionally hear someone speak of the capacitance of a single conductor. In this case the \u0026ldquo;second conductor\u0026rdquo; is an imaginary spherical shell of infinite radius surrounding the one conductor. It contributes nothing to the field, so the capacitance is given by \\( \\eqref{2.53} \\), where V is the potential with infinity as the reference point.\nExample 2.11 # Q Find the capacitance of a parallel-plate capacitor consisting of two metal surfaces of area A held a distance d apart (Fig. 2.52)\nA If we put +Q on the top and -Q on the bottom, they will spread out uniformly over the two surfaces, provided the area is reasonably large and the separation small. The surface charge density, then, is \\( \\sigma = Q / A \\) on the top plate, and so the field, according to Ex. 2.6, is \\( (1 / \\epsilon_0) Q / A \\). The potential difference between the plates is therefore\n\\[V = \\frac{Q}{A \\epsilon_0} d\\] and hence\n\\[C = \\frac{A \\epsilon_0}{d} \\label{2.54} \\tag{2.54}\\] If, for instance, the plates are square with sides 1 cm long, and they are held 1 mm apart, then the capacitance is \\( 9 \\times 10^{-13} F \\)\nExample 2.12 # Q Find the capacitance of two concentric spherical metal shells, with radii a and b. A Place charge +Q on the inner sphere, and -Q on the outer one. The field between the spheres is\n\\[\\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vec{r}\\] so the potential difference between them is\n\\[V = - \\int_{b}^{a} \\vec{E} \\cdot \\dd{\\vec{l}} = - \\frac{Q}{4 \\pi \\epsilon_0} \\int_{b}^a \\frac{1}{r^2} \\dd{r} = \\frac{Q}{4 \\pi \\epsilon_0} \\left( \\frac{1}{a} - \\frac{1}{b} \\right)\\] As promised, V is proportional to Q; the capacitance is\n\\[C = \\frac{Q}{V} = 4 \\pi \\epsilon_0 \\frac{ab}{(b - a)} \\] To \u0026ldquo;charge up\u0026rdquo; a capacitor, you have to remove electrons from the positive plate and carry them to the negative plate. In doing so, you fight against the electric field, which is pulling them back toward the positive conductor and pushing them away from the negative one. How much work does it take, then, to charge the capacitor up to a final amount \\( Q \\)? Suppose that at some intermediate stage in the process the charge on the positive plate is \\( q \\), so that the potential difference is \\( q / C \\). According to Eq. 2.38, the work you must do to transport the next piece of charge \\( dq \\) is\n\\[\\dd{W} = \\left( \\frac{q}{C} \\right) \\dd{q}\\] The total work necessary, then, to go from \\( q = 0 \\) to \\( q = Q \\), is\n\\[W = \\int_{0} ^Q \\left( \\frac{q}{C} \\dd{q} \\right) = \\frac{1}{2} \\frac{Q^2}{C} \\] or, since \\( Q = CV \\),\n\\[ W = \\frac{1}{2} C V^2 \\label{2.55} \\tag{2.55} \\] where \\( V \\) is the final potential of the capacitor.\n"},{"id":58,"href":"/r/notes/griffiths/ch3-1/","title":"Laplace's Equation","section":"Griffiths Introduction to Electrodynamics","content":" 3.1: Laplace\u0026rsquo;s Equation # 3.1.1: Introduction # The primary task of electrostatics is to find the electric field of a given stationary charge distribution. In principle, this purpose is accomplished by Coulomb\u0026rsquo;s law, in the form of\n\\[\\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r\u0026#39;})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau\u0026#39;} \\label{3.1}\\] Unfortunately, integrals of this type can be difficult to calculate for any but the simplest charge configurations. Occasionally we can get around this by exploiting symmetry and using Gauss\u0026rsquo;s law, but ordinarily the best strategy is first to calculate the potential, V, which is given by the somewhat more tractable\n\\[V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}\u0026#39;)}{\\gr} \\dd{\\tau\u0026#39;} \\tagl{3.2}\\] Still, even this integral is often too tough to handle analytically. Moreover, in problems involving conductors \\( \\rho \\) itself may not be known in advance; since charge is free to move around, the only thing we control directly is the total charge (or perhaps the potential) of each conductor.\nIn such cases, it is fruitful to recast the problem in differential form, using Poisson\u0026rsquo;s equation\n\\[\\laplacian{V} = - \\frac{1}{\\epsilon_0} \\rho \\label{3.3}\\] which, together with appropriate boundary conditions, is equivalent to \\( \\eqref{3.2} \\). Very often, in fact, we are interested in finding the potential in a region where \\( \\rho = 0 \\). (If \\( \\rho = 0 \\) everywhere, of course, then \\( V = 0 \\), and there is nothing further to say - that\u0026rsquo;s not what I mean. There may be plenty of charge elsewhere, but we\u0026rsquo;re confining our attention to places where there is no charge.) In this case, Poisson\u0026rsquo;s equation reduces to Laplace\u0026rsquo;s equation\n\\[\\laplacian{V} = 0 \\label{3.4}\\] or, written out in Cartesian coordinates,\n\\[\\frac{\\partial^2{V}}{\\partial{x^2}} \u0026#43; \\frac{\\partial^2 V}{\\partial{y^2}} \u0026#43; \\frac{\\partial^2{V}}{\\partial{z^2}} = 0 \\label{3.5}\\] This formula is so fundamental to the subject that one might almost say electrostatics is the study of Laplace\u0026rsquo;s equation. At the same time, it is a ubiquitous equation, appearing in such diverse branches of physics as gravitation and magnetism, the theory of heat, and the study of soap bubbles. In mathematics, it plays a major role in analytic function theory. To get a feel for Laplace\u0026rsquo;s equation and its solutions (which are called harmonic functions), we shall begin with the one- and two-dimensional versions, which are easier to picture, and illustrate all the essential properties of the three-dimensional case.\n3.1.2: Laplace\u0026rsquo;s Equation in One Dimension # Suppose V depends on only one variable, x. Then Laplace\u0026rsquo;s equation becomes\n\\[\\frac{d^2 V}{dx^2} = 0\\] The general solution is\n\\[V(x) = mx \u0026#43; b \\label{3.6} \\tag{3.6}\\] the equation for a straight line. It contains two undetermined constants (m and b), as is appropriate for a second-order (ordinary) differential equation. They are fixed, in any particular case, by the boundary conditions of that problem. For instance, it might be specified that \\( V = 4 \\) at \\( x = 1 \\) and \\( V = 0 \\) at \\( x = 5 \\). In that case, \\( m = -1 \\) and \\( b = 5 \\), so \\( V = - x + 5 \\) (See Fig. 3.1)\nI want to call your attention to two features of this result; they may seem silly and obvious in one dimension, where I can write down the general solution explicitly, but the analogs in two and three dimensions are powerful and by no means obvious:\nV(x) is the average of \\( V(x + a) \\) and \\( V(x - a) \\) for any a: \\[V(x) = \\frac{1}{2} [V(x \u0026#43; a) \u0026#43; V(x-a)]\\] Laplace\u0026rsquo;s equation is a kind of averaging instruction; it tells you to assign to the point x the average of the values to the left and to the right of x. Solutions to Laplace\u0026rsquo;s equation are, in this sense, as boring as they could possibly be, and yet fit the end points properly.\nLaplace\u0026rsquo;s equation tolerates no local maxima or minima; extreme values of V must occur at the end points. Actually, this is a consequence of (1), for if there were a local maximum, V would be greater at that point than on either side, and therefore could not be the average. (Ordinarily, you expect the second derivative to be negative at a maximum and positive at a minimum. Since Laplace\u0026rsquo;s equation requires, on the contrary, that the second derivative is zero, it seems reasonable that solutions should exhibit no extrema. However, this is not a proof, since there exist functions that have maxima and minima at points where the second derivative vanishes: \\( x^4 \\) for example, has such a minimum point at \\( x=0 \\)). 3.1.3: Laplace\u0026rsquo;s Equation in Two Dimensions # If V depends on two variables, Laplace\u0026rsquo;s equation becomes\n\\[\\frac{\\partial^2{V}}{\\partial{x^2}} \u0026#43; \\frac{\\partial^2{V}}{\\partial{y^2}} = 0 \\] This is no longer an ordinary differential equation (that is, one involving ordinary derivatives only); it is a partial differential equation. As a consequence, some of the simple rules you may be familiar with do not apply. For instance, the general solution to this equation doesn\u0026rsquo;t contain just two arbitrary constants - or, for that matter, any finite number - despite the fact that it\u0026rsquo;s a second order equation. Indeed, one cannot write down a \u0026ldquo;general solution\u0026rdquo; (at least, not in a closed form like \\( \\eqref{3.6} \\)). Nevertheless, it is possible to deduce certain properties common to all solutions.\nIt may help to have a physical example in mind. Picture a thin rubber sheet (or a soap film) stretched over some support. For definiteness, suppose you take a cardboard box, cut a wavy line all the way around, and remove the top part (Fig. 3.2). Now glue a tightly stretched rubber membrane over the box, so that it fits like a drum head (it won\u0026rsquo;t be a flat drumhead, of course, unless you choose to cut the edges off straight). Now, if you lay out the coordinates (x, y) on the bottom of the box, the height V(x, y) of the sheet above the point (x, y) will satisfy Laplace\u0026rsquo;s equation. (The one-dimensional analog would be a rubber band stretched between two points. Of course, it would form a straight line.)\nActually, the equation satisfied by a rubber sheet is\n\\[\\frac{\\partial}{\\partial{x}} \\left( g \\pdv{V}{y} \\right) \u0026#43; \\frac{\\partial}{\\partial{y}} \\left( g \\pdv{V}{y} \\right) = 0, \\\\ \\qquad \\text{ where } g = \\left[ 1 \u0026#43; \\left( \\pdv{V}{x} \\right)^2 \u0026#43; \\left( \\pdv{V}{y} \\right)^2 \\right]^{-1/2}\\] Harmonic functions in two dimensions have the same properties we noted in one dimension:\nThe value of V at a point (x, y) is the average of those around the point. More precisely, if you draw a circle of any radius R about the point (x, y), the average value of V on the circle is equal to the value at the center: \\[V(x, y) = \\frac{1}{2 \\pi R} \\oint_{\\text{circle}} = V \\dd{l} \\] (This, incidentally, suggests the method of relaxation, on which computer solutions to Laplace\u0026rsquo;s equation are based: Starting with specified values for V at the boundary, and reasonable guesses for V on a grid of interior points, the first pass reassigns to each point the average of its nearest neighbors. The second pass repeats this process, using the corrected values, and so on. After a few iterations, the numbers begin to settle down, so that subsequent passes produce negligible changes, and a numerical solution to Laplace\u0026rsquo;s equation, with the given boundary values, has been achieved.)\nV has no local maxima or minima; all extrema occur at the boundaries. (As before, this follows from (1)). Again, Laplace\u0026rsquo;s equation picks the most featureless function possible, consistent with the boundary conditions: no hills, no valleys, just the smoothest conceivable surface. For instance, if you put a ping-pong ball on the stretched rubber sheet of Fig 3.2, it will roll over to one side and fall off - it will not find a \u0026ldquo;pocket\u0026rdquo; somewhere to settle into, for Laplace\u0026rsquo;s equation allows no such dents in the surface. From a geometrical point of view, just as a straight line is the shortest distance between two points, so a harmonic function in two dimensions minimizes the surface area spanning the given boundary line. 3.1.4: Laplace\u0026rsquo;s Equation in Three Dimensions # In three dimensions I can neither provide you with an explicit solution (as in one dimension) nor offer a suggestive physical example to guide your intuition (as I did in two dimensions). Nevertheless, the same two properties remain true, and this time I will sketch a proof.\nFor a proof that does not rely on Coulomb\u0026rsquo;s law (only on Laplace\u0026rsquo;s equation), see Prob. 3.37\nThe value of V at point r is the average value of V over a spherical surface of radius R centered at r: \\[V(\\vec{r}) = \\frac{1}{4 \\pi R^2} \\oint_{\\text{sphere}} V \\dd{a}\\] As a consequence, V can have no local maxima or minima; the extreme values of V must occur at the boundaries (For if V had a local maximum at r, then by the very nature of the maximum I could draw a sphere around r over which all the values of V - and a fortiori the average - would be less than at r.) Proof: V is a solution to the three-dimensional Laplace\u0026rsquo;s equation. Then the value of V at point r is the average value of V over a spherical surface of radius R centered at r\nLet\u0026rsquo;s begin by calculating the average potential over a spherical surface of radius R due to a single point charge q located outside the sphere. We may as well center the sphere at the origin and choose coordinates so that q lies on the z-axis (Fig 3.3).\nThe potential at a point on the surface is\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} \\] where\n\\[\\gr ^2 = z^2 \u0026#43; R^2 - 2z R \\cos \\theta\\] so\n\\[V_{\\text{ave}} = \\frac{1}{4\\pi R^2} \\frac{1}{4 \\pi \\epsilon_0} \\int [z^2 \u0026#43; R^2 - 2 z R \\cos \\theta]^{-1/2} R^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\\\ = \\left. \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2 z R} \\sqrt{z^2 \u0026#43; R^2 - 2 z R \\cos\\theta} \\right|_{0} ^{\\pi} \\\\ = \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2zR} [(z \u0026#43; R) - (z-R)] = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{z} \\] But this is precisely the potential due to q at the center of the sphere! By the superposition principle, the same goes for any collection of charges outside the sphere: their average potential over the sphere is equal to the net potential they produce at the center\n3.1.5: Boundary Conditions and Uniqueness Theorems # Laplace\u0026rsquo;s equation does not by itself determine V; in addition, suitable boundary conditions must be supplied. This raises a delicate question: What are appropriate boundary conditions, sufficient to determine the answer and yet not so strong as to generate inconsistencies? The one-dimensional case is easy, for here the general solution \\( V = mx + b \\) contains two arbitrary constants, and we therefore require two boundary conditions. We might, for instance, specify the value of the function at each end, or we might give the value of the function and its derivative at one end, or the value at one end and the derivative at the other, and so on. But we cannot get away with just the value or just the derivative at one end - this is insufficient information. Nor would it do to specify the derivatives at both ends - this would be either redundant (if the two are equal) or inconsistent (if they are not).\nIn two or three dimensions we are confronted by a partial differential equation, and it is not so obvious what would constitute acceptable boundary conditions. Is the shape of a taut rubber membrane, for instance, uniquely determined by the frame over which it is stretched, or, like a canning jar lid, can it snap from one stable configuration to another? The answer, as I think your intuition would suggest, that V is uniquely determined by its value at the boundary (canning jars evidently do not obey Laplace\u0026rsquo;s equation). However, other boundary conditions can also be used (see Prob. 3.5). The proof that a proposed set of boundary conditions will suffice is usually presented in the form of a uniqueness theorem. There are many such theorems for electrostatics, all sharing the same basic format - I\u0026rsquo;ll show you the two most useful ones:\nFirst Uniqueness Theorem: The solution to Laplace\u0026rsquo;s equation in some volume \\( \\mathscr{V} \\) is uniquely determined if V is specified on the boundary surface \\( \\mathscr{S} \\).\nIn Fig. 3.5 I have drawn such a region and its boundary. (There could also be \u0026ldquo;islands\u0026rdquo; inside, so long as V is given on all their surfaces; also, the outer boundary could be at infinity, where V is ordinarily taken to be zero.)\nProof: Suppose there were two solutions to Laplace\u0026rsquo;s equation:\n\\[\\laplacian{V_1} = 0 \\quad \\text{and} \\quad \\laplacian{V_2} = 0\\] both of which assume the specified value on the surface. I want to prove that they must be equal. The trick is to look at their difference:\n\\[V_3 \\equiv V_1 - V_2\\] This obeys Laplace\u0026rsquo;s equation (obviously)\n\\[\\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = 0\\] and it takes the value zero on all boundaries (since \\( V_1 \\) and \\( V_2 \\) are equal there). But Laplace\u0026rsquo;s equation allows no local maxima or minima - all extrema occur on the boundaries. So the maximum and minimum of \\( V_3 \\) are both zero. Therefore \\( V_3 \\) must be zero everywhere, and hence\n\\[V_1 = V_2\\] Example 3.1 # Q Show that the potential is constant inside an enclosure completely surrounded by conducting material, provided there is no charge within the enclosure. A The potential on the cavity wall is some constant \\( V_0 \\) (that\u0026rsquo;s item (iv) in Sect. 2.5.1), so the potential inside is a function that satisfies Laplace\u0026rsquo;s equation and has the constant value \\( V_0 \\) at the boundary. It doesn\u0026rsquo;t take a genius to think of one solution to this problem: \\( V = V_0 \\) everywhere. The uniqueness theorem guarantees that this is the only solution. (It follows that the field inside an empty cavity is zero - the same result we found in Sect. 2.5.2 on rather different grounds.) The uniqueness theorem is a license to your imagination. It doesn\u0026rsquo;t matter how you come by your solution; if (a) it satisfies Laplace\u0026rsquo;s equation and (b) it has the correct value on the boundaries, then it\u0026rsquo;s right. You\u0026rsquo;ll see the power of this argument when we come to the method of images.\nIncidentally, it is easy to improve on the first uniqueness theorem: I assumed there was no charge inside the region in question, so the potential obeyed Laplace\u0026rsquo;s equation, but we may as well throw in some charge (in which case V obeys Poisson\u0026rsquo;s equation).\nCorollary: The potential in a volume \\( \\mathscr{V} \\) is uniquely determined if (a) the charge density throughout the region, and (b) the value of V on all boundaries, are specified\nThe argument is the same, only this time\n\\[\\laplacian{V_1} = -\\frac{1}{\\epsilon_0} \\rho, \\qquad \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho \\] so\n\\[\\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho \u0026#43; \\frac{1}{\\epsilon_0} \\rho = 0\\] Once again the difference \\( (V_3 \\equiv V_1 - V_2) \\) satisfies Laplace\u0026rsquo;s equation and has the value zero on all boundaries, so \\( V_3 = 0 \\) and hence \\( V_1 = V_2 \\)\n3.1.6: Conductors and the Second Uniqueness Theorem # The simplest way to set the boundary conditions for an electrostatic problem is to specify the value of V on all surfaces surrounding the region of interest. And this situation often occurs in practice: In the laboratory, we have conductors connected to batteries, which maintain a given potential, or to ground, which is the experimentalist\u0026rsquo;s word for \\( V = 0 \\). However, there are other circumstances in which we do not know the potential at the boundary, but rather the charges on various conducting surfaces. Suppose I put \\( Q_a \\) on the first conductor, \\( Q_b \\) on the second conductor, and so on - I\u0026rsquo;m not telling you how the charge distributes itself over each conducting surface, because as soon as I put it on, it moves around in a way I do not control. And for good measure, let\u0026rsquo;s say there is some specified charge density \\( \\rho \\) in the region between the conductors. Is the electric field now uniquely determined? Or are there perhaps a number of different ways the charges could arrange themselves on their respective conductors, each leading to a different field?\nSecond uniqueness theorem: In a volume \\( \\mathscr{V} \\) surrounded by conductors and containing a specified charge density \\( \\rho \\), the electric field is uniquely determined if the total charge on each conductor is given (Fig. 3.6). (The region as a whole can be bounded by another conductor, or else unbounded.)\nProof: Suppose there are two fields satisfying the conditions of the problem. Both obey Gauss\u0026rsquo;s law in differential form in the space betwen the conductors:\n\\[\\div{\\vec{E_1}} = \\frac{1}{\\epsilon_0} \\rho, \\qquad \\div{\\vec{E_2}} = \\frac{1}{\\epsilon_0} \\rho\\] And both obey Gauss's law in integral form for a Gaussian surface enclosing each conductor \\[\\oint_{i_{th} \\text{ conducting surface}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\\\] Likewise, for the outer boundary (whether this is just inside an enclosing conductor at infinity),\n\\[\\oint_{\\text{outer boundary}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} \\\\ \\oint_{\\text{outer boundary}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot}\\] As before, we examine the difference\n\\[\\vec{E_3} \\equiv \\vec{E_1} - \\vec{E_2}\\] which obeys\n\\[\\div{\\vec{E_3}} = 0 \\label{3.7} \\tag{3.7}\\] in the region between the conductors, and\n\\[\\oint \\vec{E_3} \\cdot \\dd{\\vec{a}} = 0 \\label{3.8} \\tag{3.8}\\] over each boundary surface. Now there is one final piece of information we must exploit: Although we do not know how the charge \\( Q_i \\) distributes itself over the _i_th conductor, we do know that each conductor is an equipotential, and hence \\( V_3 \\) is a constant (not necessarily the same constant) over each conducting surface. (It need not be zero, for the potentials \\( V_1 \\) and \\( V_2 \\) may not be equal - all we know for sure is that both are constant over any given conductor.) Next comes a trick. Invoking the product rule\n\\[\\div{(V_3 \\vec{E_3})} = V_3 (\\div{\\vec{E_3}}) \u0026#43; \\vec{E_3} \\cdot (\\grad{V_3}) = - (E_3)^2\\] Here I have used \\( \\eqref{3.7} \\) and \\( \\vec{E_3} = - \\grad{V_3} \\). Integrating this over \\( \\mathscr{V} \\) and applying the divergence theorem to the left side:\n\\[\\int_{\\mathscr{V}} \\div{(V_3 \\vec{E_3})} \\dd{\\tau} = \\oint_{S} V_3 \\vec{E_3} \\cdot \\dd{\\vec{a}} = - \\int_{\\mathscr{V}} (E_3)^2 \\dd{\\tau}\\] The surface integral covers all boundaries of the region in question - the conductors and outer boundary. Now \\( V_3 \\) is a constant over each surface (if the outer boundary is invinity, \\( V_3 = 0 \\) there), so it comes outside each integral, and what remains is zero, according to \\( \\eqref{3.8} \\). Therefore\n\\[ \\int_{\\mathscr{V}}(E_3)^2 \\dd{\\tau} = 0 \\] The integrand is never negative, so the only way the integral can vanish is if \\( E_3 = 0 \\) everywhere. Consequently, \\( \\vec{E_1} = \\vec{E_2} \\) and the theorem is proved.\nThis proof was not easy, and there is a real danger that the theorem itself will seem more plausible to you than the proof. In case you think the second uniqueness theorem is \u0026ldquo;obvious,\u0026rdquo; consider this example of Purcell\u0026rsquo;s: Figure 3.7 shows a simple electrostatic configuration, consisting of four conductors with charges \\( \\pm Q \\), situated so that the plusses are near the minuses. It all looks very comfortable. Now, what happens if we join them in pairs, by tiny wires, as indicated in Fig. 3.8? Since the positive charges are very near negative charges (which is where they like to be) you might well guess that nothing will happen - the configuration looks stable.\nWell, that sounds reasonable, but it\u0026rsquo;s wrong. The configuration in Fig. 3.8 is impossible. For there are now effectively two conductors, and the total charge on each is zero. One possible way to distribute zero charge over these conductors is to have no accumulation of charge anywhere, and hence zero field everywhere (Fig. 3.9). By the second uniqueness theorem, this must be the solution: The charge will flow down the tiny wires, canceling itself off.\n"},{"id":59,"href":"/r/notes/griffiths/ch3-2/","title":"The Method of Images","section":"Griffiths Introduction to Electrodynamics","content":" 3.2: The Method of Images # 3.1.1: The Classic Image Problem # Suppose a point charge q is held a distance d above an infinite grounded conducting plane (Fig. 3.10). Question: what is the potential in the region above the plane? It\u0026rsquo;s not just \\( (1/4 \\pi \\epsilon_0) q / \\gr \\), for q will induce a certain amount of negative charge on the nearby surface of the conductor; the total potential is due in part to q directly, and in part to this induced charge. But how can we possibly calculate the potential, when we don\u0026rsquo;t know how much charge is induced or how it is distributed?\nFrom a mathematical point of view, our problem is to solve Poisson\u0026rsquo;s equation in the region \\( z \u0026gt; 0 \\), with a single point charge q at \\( (0, 0, d) \\), subject to the boundary conditions\n\\( V = 0 \\) when \\( z = 0 \\) (since the conducting plane is grounded) \\( V \\rightarrow 0 \\) far from the charge (that is, for \\( x^2 + y^2 + z^2 \\gg d^2 \\) The first uniqueness theorem (actually, its corollary) guarantees that there is only one function that meets these requirements. If by trick or clever guess we can discover such a function, it\u0026rsquo;s got to be the answer. Trick: Forget about the actual problem; we\u0026rsquo;re going to study a completely different situation. This new configuration consists of two point charges, \\( +q \\) at \\( (0, 0, d) \\) and \\( -q \\) at \\( (0, 0, -d) \\), and no conducting plane (Fig. 3.11). For this configuration I can easily write down the potential:\n\\[V(x, y, z) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; (z - d)^2 }} - \\frac{q}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; (z \u0026#43; d)^2}} \\right] \\label{3.9} \\tag{3.9}\\] It follows that\n\\( V = 0 \\) when \\( z = 0 \\) \\( V \\rightarrow 0 \\) for \\( x^2 + y^2 + z^2 \\gg d^2 \\) and the only charge in the region \\( z \u0026gt; 0 \\) is the point charge \\( +q \\) at \\( (0, 0, d) \\). But these are precisely the conditions of the original problem! Evidently the second configuration happens to produce exactly the same potential as the first configuration, in the \u0026ldquo;upper\u0026rdquo; region \\( z \\geq 0 \\). (The \u0026ldquo;lower\u0026rdquo; region, \\( z \u0026lt; 0 \\), is completely different, but who cares? The upper part is all we need.) Conclusion: The potential of a point charge above an infinite grounded conductor is given by \\( \\eqref{3.9} \\), for \\( z \u0026gt; 0 \\).\nNotice the crucial role played by the uniqueness theorem in this argument: without it, no one would believe this solution, since it was obtained for a completely different charge distribution. But the uniqueness theorem certifies it: If it satisfies Poisson\u0026rsquo;s equation in the region of interest, and assumes the correct value at the boundaries, then it must be right.\n3.2.2: Induced Surface Charge # Now that we know the potential, it is a straightforward matter to compute the surface charge \\( \\sigma \\) induced on the conductor. According to Eq. 2.49,\n\\[\\sigma = - \\epsilon_0 \\pdv{V}{n}\\] where \\( \\partial V / \\partial n \\) is the normal derivative of V at the surface. In this case the normal direction is the z direction, so\n\\[\\sigma = \\left. - \\epsilon_0 \\pdv{V}{z} \\right|_{z = 0}\\] From Eq. 3.9\n\\[\\pdv{V}{z} = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{-q (z - d)}{[x^2 \u0026#43; y^2 \u0026#43; (z - d)^2]^{3/2}} \u0026#43; \\frac{q(z \u0026#43; d)}{[x^2 \u0026#43; y^2 \u0026#43; (z \u0026#43; d)^2 ]^{3/2}} \\right]\\] so\n\\[\\sigma(x, y) = \\frac{-qd}{2 \\pi (x^2 \u0026#43; y^2 \u0026#43; d^2)^{3/2}} \\label{3.10} \\tag{3.10} \\] As expected, the induced charge is negative (assuming q is positive) and greatest at \\( x = y = 0 \\).\nWhile we\u0026rsquo;re at it, let\u0026rsquo;s compute the total induced charge\n\\[Q = \\int \\sigma \\dd{a}\\] This integral, over the xy plane, could be done in Cartesian coordinates, with \\( \\dd{a} = \\dd{x} \\dd{y} \\), but it\u0026rsquo;s easier to use polar coordinates \\( (r, \\phi) \\), with \\( r^2 = x^2 + y^2 \\) and \\( \\dd{a} = r \\dd{r} \\dd{\\phi} \\). Then\n\\[\\sigma(r) = \\frac{-qd}{2 \\pi (r^2 \u0026#43; d^2)^{3/2}} \\] and\n\\[Q = \\int_{0} ^{2\\pi} \\int_{0} ^{\\infty} \\frac{-qd}{2 \\pi (r^2 \u0026#43; d^2)^{3/2}} r \\dd{r} \\dd{\\phi} = \\left. \\frac{qd}{\\sqrt{r^2 \u0026#43; d^2}} \\right|_{0} ^{\\infty} = -q \\label{3.11} \\tag{3.11}\\] The total charge induced on the plane is \\( -q \\), as (with benefit of hindsight) you can perhaps convince yourself it had to be.\n3.2.3: Force and Energy # The charge \\( q \\) is attracted toward the plane, because of the negative induced charge. Let\u0026rsquo;s calculate the force of attraction. Since the potential in the vicinity of \\( q \\) is the same as in the analog problem (the one with \\( +q \\) and \\( -q \\) but no conductor), so also is the field and, therefore, the force\n\\[\\vec{F} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{(2d)^2} \\vu{z} \\label{3.12} \\tag{3.12}\\] Beware: It is easy to get carried away, and assume that everything is the same in the two problems. Energy, however, is not the same. With the two point charges and no conductor, Eq. 2.42 gives\n\\[W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{2d} \\] But for a single charge and conducting plane, the energy is half this\n\\[W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\label{3.14} \\tag{3.14} \\] Why half? Think of the energy stored in the fields (Eq. 2.45):\n\\[W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau}\\] In the first case, both the upper region \\( (z \u0026gt; 0) \\) and the lower region \\( (z \u0026lt; 0) \\) contribute, and by symmetry they contribute equally. But in the second case, only the upper region contains a nonzero field, and hence the energy is half as great.\nOf course, one could also determine the energy by calculating the work required to bring \\( q \\) in from infinity. The force required (to oppose the electrical force in \\( \\eqref{3.12} \\) is \\( (1 / 4 \\pi \\epsilon_0)(q^2/4z^2) \\vu{z} \\), so\n\\[\\begin{aligned} W \u0026amp; = \u0026amp; \\int _{\\infty} ^{d} \\vec{F} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{\\infty} ^d \\frac{q^2}{4z^2} \\dd{z} \\\\ \u0026amp; = \u0026amp; \\frac{1}{4 \\pi \\epsilon_0} \\left. \\left( - \\frac{q^2}{4z} \\right) \\right|_{\\infty} ^d = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\end{aligned}\\] As I move \\( q \\) toward the conductor, I do work only on q. It is true that induced charge is moving in over the conductor, but this costs me nothing, since the whole conductor is at potential zero. By contrast, if I simultaneously bring in two point charges (with no conductor), I do work on both of them, and the total is (again) twice as great.\n3.2.4: Other Image Problems # The method just described is not limited to a single point charge; any stationary charge distribution near a grounded conducting plane can be treated in the same way, by introducing its mirror image - hence the name method of images. (Remember that the image charges have the opposite sign; this is what guarantees that the xy plane will be at potential zero.) There are also some exotic problems that can be handled in similar fashion; the nicest of these is the following.\nExample 3.2 # Q A point charge q is situated a distance a from the center of a grounded conducting sphere of radius R (Fig. 3.12). Find the potential outside the sphere\nA Examine the completely different configuration, consisting of the point charge q together with another point charge\n\\[q\u0026#39; = - \\frac{R}{a} q \\label{3.15} \\tag{3.15}\\] placed a distance\n\\[b = \\frac{R^2}{a} \\label{3.16} \\tag{3.16}\\] to the right of the center of the sphere (Fig 3.13). No conductor, now - just the two point charges. The potential of this configuration is\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr} \u0026#43; \\frac{q\u0026#39;}{\\gr \u0026#39;} \\right) \\label{3.17} \\tag{3.17}\\] where \\( \\gr \\) and \\( \\gr\u0026rsquo; \\) are the distances from q and q\u0026rsquo;, respectively. Now, it happens (see Prob. 3.8) that this potential vanishes at all points on the sphere, and therefore fits the boundary conditions for our original problem, in the exterior region.\nConclusion: \\( \\eqref{3.17} \\) is the potential of a point charge near a grounded conducting sphere. (Notice that b is less than R, so the \u0026ldquo;image\u0026rdquo; charge q\u0026rsquo; is safely inside the sphere - you cannot put image charges in the region where you are calculating V; that would change \\( \\rho \\), and you\u0026rsquo;d be solving Poisson\u0026rsquo;s equation with the wrong source.) In particular, the force of attraction between the charge and the sphere is\n\\[F = \\frac{1}{4\\pi \\epsilon_0} \\frac{q q\u0026#39;}{(a - b)^2} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2 R a}{(a^2 - R^2)^2} \\label{3.18} \\tag{3.18} \\] The method of images is delightfully simple\u0026hellip; when it works. But it is as much an art as a science, for you must somehow think up just the right \u0026ldquo;auxiliary\u0026rdquo; configuration, and for most shapes this is forbiddingly complicated, if not impossible.\n"},{"id":60,"href":"/r/notes/griffiths/ch3-3/","title":"Separation of Variables","section":"Griffiths Introduction to Electrodynamics","content":" 3.3: Separation of Variables # In this section we shall attack Laplace\u0026rsquo;s equation directly, using the method of separation of variables, which is the physicist\u0026rsquo;s favorite tool for solving partial differential equations. The method is applicable in circumstances where the potential \\( (V) \\) or the charge density \\( (\\sigma) \\) is specified on the boundaries of some region, and we are asked to find the potential in the interior. The basic strategy is very simple: We look for solutions that are products of functions, each of which depends on only one of the coordinates. The algebraic details, however, can be formidable, so I\u0026rsquo;m going to develop the method through a sequence of examples. We\u0026rsquo;ll start with Cartesian coordinates and then do spherical coordinates (I\u0026rsquo;ll leave the cylindrical case for you to tackle on your own, in Prob 3.24).\n3.3.1: Cartesian Coordinates # Example 3.3 # Q Two infinite grounded metal plates lie parallel to the xz plane, one at \\( y = 0 \\), the other at \\( y = a \\) (Fig. 3.17). The left end, at \\( x = 0 \\), is closed off with an infinite strip insulated from the two plates, and maintained at a specific potential \\( V_0(y) \\). Find the potential inside this \u0026lsquo;slot.\u0026rsquo;\nA The configuration is independent of z, so this is really a two-dimensional problem. In mathematical terms, we must solve Laplace\u0026rsquo;s equation,\n\\[\\frac{\\partial ^2 V}{\\partial{x^2}} \u0026#43; \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 \\label{3.20} \\tag{3.20}\\] subject to the boundary conditions\n(i) \\( V = 0 \\) when \\( y = 0 \\) (ii) \\( V = 0 \\) when \\( y = a \\) (iii) \\( V = V_0(y) \\) when \\( x = 0 \\) (iv) \\( V \\rightarrow 0 \\) as \\( x \\rightarrow \\infty \\) (The latter, although not explicitly stated in the problem, is necessary on physical grounds: as you get farther and farther away from the \u0026ldquo;hot\u0026rdquo; strip at \\( x = 0 \\), the potential should drop to zero.) Since the potential is specified on all boundaries, the answer is uniquely determined.\nThe first step is to look for solutions in the form of products:\n\\[V(x, y) = X(x)Y(y) \\tagl{3.22}\\] On the face of it, this is an absurd restriction - the overwhelming majority of solutions to Laplace\u0026rsquo;s equation do not have such a form. For example, \\( V(x, y) = (5x + 6y) \\) satisfies the equation, but you can\u0026rsquo;t express it as the product of a function of x times a function of y. Obviously, we\u0026rsquo;re only going to get a tiny subset of all possible solutions by this means, and it would be a miracle if one of them happened to fit the boundary conditions of our problem\u0026hellip; But hang on, because the solutions we do get are very special, and it turns out that by pasting them together we can construct the general solution.\nAnyway, putting \\( \\eqref{3.22} \\) into \\( \\eqref{3.20} \\) we obtain\n\\[Y \\frac{d^2X}{dx^2} \u0026#43; X \\frac{d^2 Y}{dy^2} = 0 \\] The next step is to \u0026ldquo;separate the variables\u0026rdquo; (that is, collect all the x-dependence into one term and all the y-dependence into another). Typically, this is accomplished by dividing through by V:\n\\[\\frac{1}{X} \\frac{d^2 X}{dx^2} \u0026#43; \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = 0 \\tagl{3.23}\\] Here the first term depends only on x and the second term only on y; in other words, we have an equation of the form\n\\[f(x) \u0026#43; g(y) = 0 \\label{3.24} \\tag{3.24}\\] Now, there\u0026rsquo;s only one way this could possibly be true: f and g must both be constant. For what if \\( f(x) \\) changed, as you vary x - then if we held y fixed and fiddled with x, the sum \\( f(x) + g(y) \\) would change, in violation of \\( \\eqref{3.24} \\), which says it\u0026rsquo;s always zero. (That\u0026rsquo;s a simple but somehow rather elusive argument; don\u0026rsquo;t accept it without due thought, because the whole method rides on it.)\nIt follows from \\( \\eqref{3.23} \\), then, that\n\\[\\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 \\quad \\text{ and } \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2, \\quad \\text{ with } C_1 \u0026#43; C_2 = 0 \\tagl{3.25}\\] One of these constants is positive, the other negative (or perhaps both are zero). In general, one must investigate all possibilities; however in our particular problem we need \\( C_1 \\) positive and \\( C_2 \\) negative, for reasons that will appear in a moment. Thus\n\\[\\frac{d^2X}{dx^2} = k^2 X, \\qquad \\frac{d^2 Y}{dy^2} = - k^2 Y \\tagl{3.26} \\] Notice what has happened: A partial differential equation has been converted into two ordinary differential equations. The advantage of this is obvious - ordinary differential equations are a lot easier to solve. Indeed:\n\\[X(x) = A e^{kt} \u0026#43; B e^{-kt}, \\qquad Y(y) = C \\sin ky \u0026#43; D \\cos ky\\] so\n\\[V(x, y) = (A e^{kt} \u0026#43; B e^{-kt})(C \\sin ky \u0026#43; D \\cos ky) \\tagl{3.27}\\] This is the appropriate separable solution to Laplace\u0026rsquo;s equation; it remains to impose the boundary conditions, and see what they tell us about the constants. To begin at the end, condition (iv) requires tha A equal zero. Absorbing B into C and D, we are left with\n\\[V(x, y) = e^{-kx} (C\\sin ky \u0026#43; D \\cos ky)\\] Condition (i) now demands that D equal zero\n\\[V(x, y) = C ^{-kx} \\sin ky \\tagl{3.28}\\] Meanwhile (ii) yields \\( \\sin ka = 0 \\), from which it follows that\n\\[k = \\frac{n \\pi}{a} \\quad (n = 1, 2, 3, \\ldots) \\tagl{3.29}\\] (At this point you can see why I chose \\( C_1 \\) positive and \\( C_2 \\) negative: If X were sinusoidal, we could never manage for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both 0 and a. Incidentally, \\( n = 0 \\) is no good, for in that case the potential vanishes everywhere. And we have already excluded negative n\u0026rsquo;s) That\u0026rsquo;s as far as we can go, using separable solutions, and unless \\( V_0(y) \\) just happens to have the form \\( \\sin(n \\pi / a) \\) for some integer n, we simply can\u0026rsquo;t fit the final boundary condition at \\( x = 0 \\). But now comes the crucial step that redeems the method: Separation of variables has given us an infinite family of solutions (one for each n), and whereas none of them by itself satisfies the final boundary condition, it is possible to combine them in a way that does. Laplace\u0026rsquo;s equation is linear, in the sense that if \\( V_1, V_2, V_3, \\ldots \\) satisfy it, so does any linear combination, for\n\\[\\laplacian{V} = \\alpha_1 \\laplacian{V_1} \u0026#43; \\alpha_2 \\laplacian{V_2} \u0026#43; \\ldots = 0 \\alpha_1 \u0026#43; 0 \\alpha_2 \u0026#43; \\ldots = 0\\] Exploiting this fact, we can patch together the separable solutions \\( \\eqref{3.28} \\) to construct a much more general solution:\n\\[V(x, y) = \\sum_{n=1} ^{\\infty} C_n e^{-n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.30}\\] This still satisfies three of the boundary conditions; the question is, can we (by astute choice of the coefficients \\( C_n \\)) fit the final boundary condition (iii)?\n\\[V(0, y) = \\sum_{n=1} ^{\\infty} C_n \\sin (n \\pi y / a) = V_0(y) \\tagl{3.31}\\] Well, you may recognize this sum - it\u0026rsquo;s a Fourier sine series. And Dirichlet\u0026rsquo;s theorem guarantees that virtually any function \\( V_0(y) \\) - it can even have a finite number of discontinuities - can be expanded in such a series. But how do we actually determine the coefficients \\( C_n \\), buried as they are in that infinite sum? The device for accomplishing this is so lovely it deserves a name - I call it Fourier\u0026rsquo;s trick, though it seems Euler had used essentially the same idea somewhat earlier. Here\u0026rsquo;s how it goes: Multiply \\( \\eqref{3.31} \\) by \\( \\sin(n\u0026rsquo; \\pi y /a) \\) (where \\( n\u0026rsquo; \\) is a positive integer), and integrate from 0 to a:\n\\[\\sum_{n=1} ^{\\infty} C_n \\int_{0} ^{a} \\sin(n \\pi y / a) \\sin(n\u0026#39; \\pi y/a) \\dd{y} = \\int_{0} ^a V_0(y) \\sin (n\u0026#39; \\pi /a) \\dd{y} \\tagl{3.32}\\] You can work out the integral on the left yourself; the answer is\n\\[\\int_{0} ^a \\sin (n \\pi y /a) \\sin (n\u0026#39; \\pi y / a) \\dd{y} = \\begin{cases} 0 \u0026amp; \\quad \\text{if } n\u0026#39; \\neq n \\\\ \\frac{a}{2} \u0026amp; \\quad \\text{if } n\u0026#39; = n \\end{cases} \\tagl{3.33}\\] Thus all the terms in the series drop out, save only the one where \\( n = n\u0026rsquo; \\), and the left side of \\( \\eqref{3.32} \\) reduces to \\( (a/2)C_{n\u0026rsquo;} \\). Conclusion:\n\\[C_n = \\frac{2}{a} \\int_{0}^a V_0(y) \\sin (n \\pi y /a) \\dd{y} \\tagl{3.34}\\] That does it: \\( \\eqref{3.30} \\) is the solution, with coefficients given by \\( eqref{3.34} \\). As a concrete example, suppose the strip at \\( x = 0 \\) is a metal plate with constant potential \\( V_0 \\) (remember, it\u0026rsquo;s insulated from the grounded plates at \\( y = 0 \\) and \\( y = a \\). Then\n\\[C_n = \\frac{2V_0}{a} \\int_0 ^a \\sin (n \\pi y / a) \\dd y \\\\ = \\frac{2 V_0}{n \\pi} (1 - \\cos n \\pi) = \\begin{cases} 0 \u0026amp; \\quad \\text{if n is even } \\\\ \\frac{4 V_0}{n \\pi} \u0026amp; \\quad \\text{if n is odd} \\end{cases} \\tagl{3.35}\\] Thus\n\\[V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n = 1, 3, 5, \\ldots} \\frac{1}{n} e^{- n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.36}\\] Figure 3.18 is a plot of this potential; Fig. 3.10 shows how the first few terms in the Fourier series combine to make a better and better approximation to the constant \\( V_0 \\): (a) is the \\( n=1 \\) term only, (b) includes n up to 5, (c) is the sum of the first 10 terms, and (d) is the sum of the first 100 terms.\nIncidentally, the infinite series in Eq. 3.36 can be summed explicitly (try your hand at it if you like); the result is \\[V(x, y) = \\frac{2V_0}{\\pi} \\tan^{-1} \\left( \\frac{\\sin(\\pi y / a)}{\\sinh(\\pi x /a )} \\right) \\tagl{3.37}\\] In this form, it is easy to check that Laplace\u0026rsquo;s equation is obeyed and the four boundary conditions are satisfied\nThe success of this method hinged on two extraordinary properties of the separable solutions \\( \\eqref{3.28} \\) and \\( \\eqref{3.29} \\): completeness and orthogonality. A set of functions \\( f_n(y) \\) is said to be complete if any other function \\( f(y) \\) can be expressed as a linear combination of them:\n\\[f(y) = \\sum_{n=1} ^{\\infty} C_n f_n(y) \\tagl{3.38}\\] The functions \\( \\sin (n \\pi y/a) \\) are complete on the interval \\( 0 \\leq y \\leq a \\). It was this fact, guaranteed by Dirichlet\u0026rsquo;s theorem, that assured us \\( \\eqref{3.31} \\) could be satisfied, given the proper choice of the coefficients \\( C_n \\). (The proof of completeness, for a particular set of functions, is an extremely difficult business, and I\u0026rsquo;m afraid physicists tend to assume it\u0026rsquo;s true and leave the checking to others.) A set of functions is orthogonal if the integral of the product of any two different members of the set is zero:\n\\[\\int_0 ^a f_n(y) f_{n\u0026#39;} (y) \\dd{y} = 0 \\quad \\text{for } n\u0026#39; \\neq n\\] The sine functions are orthogonal \\( \\eqref{3.33} \\); that is the property on which Fourier\u0026rsquo;s trick is based, allowing us to kill off all terms but one in the infinite series and thereby solve for the coefficients \\( C_n \\) (Proof of orthogonality is generally quite simple, either by direct integration or by analysis of the differential equation from which the functions came.)\nExample 3.4 # Q Two infinitely-long grounded metal plates, again at \\( y=0 \\) and \\( y=a \\) are connected at \\( x= \\pm b \\) by metal strips maintained at a constant potential \\( V_0 \\), as shown in Fig. 3.20 (a thin layer of insulation at each corner prevents them from shorting out). Find the potential inside the resulting rectangular pipe.\nA Once again, the configuration is independent of z. Our problem is to solve Laplace\u0026rsquo;s equation\n\\[\\frac{\\partial ^2 V}{\\partial{x^2}} \u0026#43; \\frac{\\partial ^2 V}{\\partial{y^2}} = 0\\] subject to the boundary conditions\n(i) \\( V = 0 \\) when \\( y = 0 \\) (ii) \\( V = 0 \\) when \\( y = a \\) (iii) \\( V = V_0 \\) when \\( x = b \\) (iv) \\( V = V_0 \\) when \\( x = -b \\) The argument runs as before, up to \\( \\eqref{3.27} \\):\n\\[V(x, y) = (A e^{kt} \u0026#43; B e^{-kt})(C \\sin ky \u0026#43; D \\cos ky)\\] This time, however, we cannot set \\( A = 0 \\); the region in question does not extend to \\( x = \\infty \\), so \\( e^{kx} \\) is perfectly acceptable. On the other hand, the situation is symmetric with respect to x, so \\( V(-x, y) = V(x, y) \\), and it follows that \\( A = B \\). Using\n\\[e^{kx} \u0026#43; e^{-kx} = 2 \\cosh kx\\] and absorbing \\( 2A \\) into \\( C \\) and \\( D \\), we have\n\\[V(x, y) = \\cosh kx (C \\sin ky \u0026#43; D\\cos ky)\\] Boundary conditions (i) and (ii) require, as before, that \\( D = 0 \\) and \\( k = n\\pi /a \\), so\n\\[V(x, y) = C \\cosh (n \\pi x /a )\\sin(n \\pi y/a) \\tagl{3.41}\\] Because \\( V(x, y) \\) is even in x, it will automatically meet conditions (iv) if it fits (iii). It remains, therefore, to construct the general linear combination\n\\[V(x, y) = \\sum _{n=1}^{\\infty} C_n \\cosh (n \\pi x / a) \\sin(n \\pi y /a)\\] and pick the coefficients \\( C_n \\) in such a way as to satisfy condition (iii):\n\\[V(b, y) = \\sum_{n=1}^{\\infty} C_n \\cosh (n \\pi b /a) \\sin(n \\pi y/a) = V_0\\] This is the same problem in Fourier analysis that we faced before; I quote the result from \\( \\eqref{3.35} \\);\n\\[C_n \\cosh (n \\pi b/a) = \\begin{cases} 0 \u0026amp; \\quad \\text {if n is even} \\\\ \\frac{4 V_0}{n \\pi} \u0026amp; \\quad \\text{if n is odd} \\end{cases}\\] Conclusion: The potential in this case is given by\n\\[V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n=1, 3, 5\\ldots} \\frac{1}{n} \\frac{\\cosh(n \\pi x/a)}{\\cosh(n \\pi b/a)} \\sin(n \\pi y/a) \\tagl{3.42}\\] This function is shown in Fig. 3.21\nExample 3.5 # Q An infinitely long rectangular metal pipe (sides a and b) is grounded, but one end, at \\( x = 0 \\), a \u0026lsquo;hot\u0026rsquo; plate is maintained at a specified potential \\( V_0(y, z) \\), as indicated in Fig. 3.22. Find the potential inside the pipe.\nA This is genuinely a three-dimensional problem,\n\\[\\frac{\\partial ^2 V}{\\partial{x^2}} \u0026#43; \\frac{\\partial ^2 V}{\\partial{y^2}} \u0026#43; \\frac{\\partial ^2 V}{\\partial{z^2}} = 0 \\tagl{3.43}\\] subject to the boundary conditions\n(i) \\( V = 0 \\) when \\( y = 0 \\) (ii) \\( V = 0 \\) when \\( y = a \\) (iii) \\( V = 0 \\) when \\( z = 0 \\) (iv) \\( V = 0 \\) when \\( z = b \\) (v) \\( V \\rightarrow 0 \\) as \\( x \\rightarrow \\infty \\) (vi) \\( V = V_0(y, z) \\) when \\( x = 0 \\) As always, we look for solutions that are products:\n\\[V(x, y, z) = X(x)Y(y)Z(z) \\tagl{3.45}\\] Putting this into \\( \\eqref{3.43} \\) and dividing by V, we find\n\\[\\frac{1}{X} \\frac{d^2 X}{dx^2} \u0026#43; \\frac{1}{Y} \\frac{d^2 Y}{dy^2} \u0026#43; \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = 0\\] It follows that\n\\[\\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 , \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2 , \\quad \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = C_3 , \\text{ with } C_1 \u0026#43; C_2 \u0026#43; C_3 = 0\\] Our previous experience in Ex. 3.3 suggests that \\( C_1 \\) must be positive, \\( C_2 \\) and \\( C_3 \\) negative. Setting \\( C_2 = -k^2 \\) and \\( C_3 = -l^2 \\), we have \\( C_1 = k^2 + l^2 \\), and hence\n\\[\\frac{d^2 X}{dx^2} = (k^2 \u0026#43; l^2)X, \\quad \\frac{d^2 Y}{dy^2} = -k^2 Y, \\quad \\frac{d^2 Z}{dz^2} = -l^2 Z \\tagl{3.46}\\] Once again, separation of variables has turned a partial differential equation into ordinary differential equations. The solutions are\n\\[\\begin{aligned} X(x) \u0026amp; = A e^{\\sqrt{k^2 \u0026#43; l^2} x} \u0026#43; B e^{- \\sqrt{k^2 \u0026#43; l^2} x} \\\\ Y(y) \u0026amp; = C \\sin ky \u0026#43; D \\cos ky \\\\ Z(z) \u0026amp; = E \\sin lz \u0026#43; F \\cos lz \\end{aligned}\\] Boundary condition (v) implies \\( A =0 \\), (i) gives \\( D = 0 \\), and (iii) yields \\( F = 0 \\) whereas (ii) and (iv) require that \\( k = n\\pi /a \\) and \\( l =m \\pi /b \\), where n and m are positive integers. Combining the remaining constants, we are left with\n\\[V(x, y, z) = C e^{-\\pi \\sqrt{(n/a)^2 \u0026#43; (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) \\tagl{3.47}\\] This solution meets all the boundary conditions except (vi). It contains two unspecified integers (n and m), and the most general linear combination is a double sum\n\\[V(x, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C e^{-\\pi \\sqrt{(n/a)^2 \u0026#43; (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.48}\\] We hope to fit the remaining boundary condition,\n\\[V(0, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.49}\\] by appropriate choice of the coefficients \\( C_{n, m} \\). To determine these constants, we multiply by \\(\\sin(n\u0026rsquo; n \\pi y/a) \\sin(m\u0026rsquo; \\pi z / b)\\), where \\( n\u0026rsquo; \\) and \\( m\u0026rsquo; \\) are arbitrary positive integers, and integrate\n\\[\\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C_{n, m} \\int_0 ^a \\sin (n \\pi y/a) \\sin(n\u0026#39; \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\sin (m\u0026#39; \\pi z/b) \\dd{z} \\\\ = \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin(n\u0026#39; \\pi y/a) \\sin(m\u0026#39; \\pi z/b) \\dd{y} \\dd{z}\\] Quoting \\( \\eqref{3.33} \\), the left side is \\( (ab/4) C_{n\u0026rsquo;, m\u0026rsquo;} \\), so\n\\[C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin (n \\pi y/a) \\sin(m\\pi z/b) \\dd{y} \\dd{z} \\tagl{3.50}\\] Equation \\( \\eqref{3.48} \\), with the coefficients given by \\( \\eqref{3.50} \\), is the solution to our problem.\nFor instance, if the end of the tube is a conductor at constant potential \\( V_0 \\),\n\\[C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\sin(n \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\dd{z} \\\\ = \\begin{cases} 0 \u0026amp; \\qquad \\text{if n or m is even} \\\\ \\frac{16 V_0}{\\pi^2 nm} \u0026amp; \\qquad \\text{if n and m are odd} \\end{cases} \\tagl{3.51} \\] In this case,\n\\[V(x, y, z) = \\frac{16V_0}{\\pi^2} \\sum_{n,m=1,3,5,\\ldots} ^{\\infty} \\frac{1}{nm} e^{-\\pi \\sqrt{(n/a)^2 \u0026#43; (m/b)^2}x} \\sin(n \\pi y/a) \\sin(m \\pi z/b) \\tagl{3.52}\\] Notice that successive terms decrease rapidly; a reasonable approximation would be obtained by keeping only the first few.\n3.3.2: Spherical Coordinates # In the examples considered so far, Cartesian coordinates were clearly appropriate, since the boundaries were planes. For round objects, spherical coordinates are more natural. In the spherical system, Laplace\u0026rsquo;s equation reads:\n\\[\\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) \u0026#43; \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) \u0026#43; \\frac{1}{r^2\\sin ^2 \\theta} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\tagl{3.53} \\] I shall assume the problem has azimuthal symmetry, so that V is independent of \\( \\phi \\); In that case, \\( \\eqref{3.53} \\) reduces to\n\\[\\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) \u0026#43; \\frac{1}{\\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) = 0 \\tagl{3.54}\\] As before, we look for solutions that are products:\n\\[V(r, \\theta) = R(r) \\Theta (\\theta) \\tagl{3.55}\\] Putting this into \\( \\eqref{3.54} \\), and dividing by V,\n\\[\\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) \u0026#43; \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = 0 \\tagl{3.56}\\] Since the first term depends only on r, and the second only on \\( \\theta \\), it follows that each must be a constant:\n\\[ \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l\u0026#43;1), \\quad \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l\u0026#43;1) \\tagl{3.57}\\] Here \\( l(l+1) \\) is just a fancy way of writing the separation constant, whose convenience will appear shortly. As always, separation of variables has converted a partial differential equation into ordinary differential equations. The radial equation,\n\\[\\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l\u0026#43;1)R \\tagl{3.58}\\] has the general solution\n\\[R(r) = A r^l \u0026#43; \\frac{B}{r^{l\u0026#43;1}} \\tagl{3.59} \\] as you can easily check; A and B are the two arbitrary constants to be expected in the solution of a second-order differential equation. But the angular equation,\n\\[\\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l\u0026#43;1) \\sin \\theta \\Theta \\tagl{3.60}\\] is not so simple. The solutions are Legendre polynomials in the variable \\( \\cos \\theta \\):\n\\[\\Theta (\\theta ) = P_l (\\cos \\theta ) \\tagl{3.61}\\] \\( P_l (x) \\) is most conveniently defined by the Rodrigues formula:\n\\[P_l(x) \\equiv \\frac{1}{2^l l!}\\left( \\dv{}{x} \\right)^l (x^2 - 1)^l \\tagl{3.62}\\] The first few Legendre polynomials are listed:\nLegendre Polynomials \\( P_0 \\) - \\(P_5 \\)\n\\[\\begin{aligned} P_0(x) \u0026amp; = 1 \\\\ P_1(x) \u0026amp; = x \\\\ P_2(x) \u0026amp; = (3x^2 - 1)/2 \\\\ P_3(x) \u0026amp; = (5x^3 - 3x)/2 \\\\ P_4(x) \u0026amp; = (35x^4 - 30x^2 \u0026#43; 3)/8 \\\\ P_5(x) \u0026amp; = (63x^5 - 70x^3 \u0026#43; 15x)/8 \\end{aligned}\\] Notice that \\( P_l(x) \\) is (as the name suggests) an l-th-order polynomial in x; it contains only even powers if l is even, and only odd powers if l is odd. The factor in front \\( (1/2^l l! ) \\) was chosen in order that\n\\[P_l(1) = 1 \\tagl{3.63}\\] The Rodrigues formula obviously only works for nonnegative integer values of l. Moreover, it provides us with only one solution. But \\( \\eqref{3.60} \\) is second-order, and it should possess two independent solutions for every value of l. It turns out that these \u0026ldquo;other solutions\u0026rdquo; blow up at \\( \\theta = 0 \\) and/or \\( \\theta = \\pi \\), and are therefore unacceptable on physical grounds. For instance, the second solution for \\( l=0 \\) is\n\\[\\Theta(\\theta) = \\ln \\left( \\tan \\frac{\\theta}{2} \\right) \\tagl{3.64}\\] You might want to check for yourself that this satisfies \\( \\eqref{3.60} \\).\nIn the case of azimuthal symmetry, then, the most general separable solution to Laplace\u0026rsquo;s equation, consistent with minimal physical requirements, is\n\\[V(r, \\theta) = \\left( A r^l \u0026#43; \\frac{B}{r^{l\u0026#43;1}} \\right) P_l(\\cos \\theta)\\] (There was no need to include an overall constant in \\( \\eqref{3.61} \\) because it can be absorbed into A and B at this stage.) As before, separation of variables yields an infinite set of solutions, one for each l. The general solution is the linear combination of separable solutions:\n\\[V(r, \\theta) = \\sum_{l=0} ^{\\infty} \\left( A r^l \u0026#43; \\frac{B}{r^{l\u0026#43;1}} \\right) P_l(\\cos \\theta) \\tagl{3.65}\\] The following examples illustrate the power of this important result.\nExample 3.6 # Q The potential \\( V_0(\\theta) \\) is specified on the surface of a hollow sphere, of radius R. Find the potential inside the sphere. A In this case, \\( B_l = 0 \\) for all l, otherwise the potential would blow up at the origin. Thus,\n\\[V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{3.66}\\] At \\( r = R \\) this must match the specified function \\( V_0(\\theta) \\):\n\\[V(R, \\theta) = \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.67}\\] Can this equation be satisfied, for an appropriate choice of coefficients \\( A_l \\)? Yes: The Legendre polynomials (like the sines) constitute a complete set of functions, on the interval \\( -1 \\leq x \\leq 1 (0 \\leq \\theta \\leq \\pi) \\). How do we determine the constants? Again, by Fourier\u0026rsquo;s trick, for the Legendre polynomials (like the sines) are orthogonal functions:\n\\[\\begin{aligned} \\int_{-1}^1 P_l(x) P_{l\u0026#39;}(x) \\dd{x} \u0026amp; = \\int_0 ^\\pi P_l(\\cos \\theta) P_{l\u0026#39;}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\\\ \u0026amp; = \\begin{cases} 0, \u0026amp; \\quad \\text{if } l\u0026#39; \\neq l \\\\ \\frac{2}{2l \u0026#43;1} , \u0026amp; \\quad \\text{if } l\u0026#39; = l \\end{cases} \\end{aligned} \\tagl{3.68}\\] Thus, multiplying \\( \\eqref{3.67} \\) by \\( P_{l\u0026rsquo;}(\\cos \\theta) \\sin \\theta \\) and integrating, we have\n\\[A_{l\u0026#39;} R^{l\u0026#39;} \\frac{2}{2l\u0026#39; \u0026#43; 1} = \\int_{0} ^\\pi V_0(\\theta) P_{l\u0026#39;}(\\cos \\theta) \\sin \\theta \\dd{\\theta}\\] or\n\\[A_l = \\frac{2l\u0026#43;1}{2R^l} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.69}\\] \\( \\eqref{3.66} \\) is the solution to our problem, with the coefficients given by \\( \\eqref{3.69} \\). It can be difficult to evaluate integrals of the form \\( \\eqref{3.69} \\) analytically, and in practice it is often easier to solve \\( \\eqref{3.67} \\) \u0026ldquo;by eyeball.\u0026rdquo; For instance, suppose we are told that the potential on the sphere is\n\\[V_0(\\theta) = k \\sin^2 (\\theta/2) \\tagl{3.70}\\] where k is constant. Using the half-angle formula, we rewrite this as\n\\[V_0(\\theta) = \\frac{k}{2}(1 - \\cos \\theta) = \\frac{k}{2} [P_0(\\cos \\theta) - P_1 (\\cos \\theta)] \\] Putting this into \\( \\eqref{3.67} \\), we read off immediately that \\( A_0 = k/2 \\), \\( A_1 = -k/(2R) \\), and all other \\( A_l \\)\u0026rsquo;s vanish. Therefore\n\\[V(r, \\theta) = \\frac{k}{2} \\left[ r^0 P_{0}(\\cos \\theta) - \\frac{r^1}{R} P_1 (\\cos \\theta) \\right] = \\frac{k}{2} \\left( 1 - \\frac{r}{R} \\cos \\theta \\right) \\tagl{3.71}\\] Example 3.7 # Q The potential \\( V_0(\\theta) \\) is again specified on the surface of a sphere of radius R, but this time we are asked to find the potential outside, assuming there is no charge there. A In this case it\u0026rsquo;s the \\( A_l \\)\u0026rsquo;s that must be zero (or else V would not go to zero at \\( \\infty \\)), so\n\\[V(r, \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l\u0026#43;1}} P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.72}\\] Multiplying by \\( P_{l\u0026rsquo;}(\\cos \\theta) \\sin \\theta \\) and integrating - exploiting, again, the orthogonality relation 3.68 - we have\n\\[\\frac{B_{l\u0026#39;}}{R^{l\u0026#39;\u0026#43;1}} \\frac{2}{2l\u0026#39; \u0026#43; 1} = \\int_0 ^\\pi V_0(\\theta) P_{l\u0026#39;}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\] or\n\\[B_l = \\frac{2l \u0026#43; 1}{2} R^{l\u0026#43;1} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.73}\\] \\( \\eqref{3.72} \\), with the coefficients given by \\( \\eqref{3.73} \\), is the solution to our problem.\nExample 3.8 # Q An uncharged metal sphere of radius R is placed in an otherwise uniform electric field \\( \\vec{E} = E_0 \\vu{z} \\). The field will push positive charge to the \u0026rsquo;northern\u0026rsquo; surface of the sphere, and - symmetrically - negative charge to the \u0026lsquo;southern\u0026rsquo; surface (Fig. 3.24). This induced charge, in turn, distorts the field in the neighborhood of the sphere. Find the potential in the region outside the sphere. A The sphere is an equipotential - we may as well set it to zero. Then by symmetry the entire xy plane is at potential zero. This time, however, V does not go to zero at large z. In fact, far from the sphere the field is \\( E_0 \\vu{z} \\) and hence\n\\[V \\rightarrow - E_0 z \u0026#43; C\\] Since \\( V = 0 \\) in the equatorial plane, the constant C must be zero. Accordingly, the boundary conditions for this problem are\n(i) \\( V = 0 \\) when \\( r = R \\) (ii) \\( V \\rightarrow - E_0 r \\cos \\theta \\) for \\( r \\gg R \\) We must fit these boundary conditions with a function of the form \\( \\eqref{3.65} \\). The first condition yields \\[A_l R^l \u0026#43; \\frac{B_l}{R^{l\u0026#43;1}} = 0\\] or\n\\[B_l = -A_l R^{2l\u0026#43;1} \\tagl{3.75}\\] so\n\\[V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l \\left( r^l - \\frac{R^{2l\u0026#43;1}}{r^{l\u0026#43;1}} \\right) P_l(\\cos \\theta)\\] For \\( r \\gg R \\), the second term in parentheses is negligible, and therefore the condition (ii) requires that\n\\[\\sum_{l=0}^\\infty A_l R^{l} P_l (\\cos \\theta) = - E_0 r \\cos \\theta\\] Evidently only one term is present: \\( l = 1 \\). In fact, since \\( P_1(\\cos \\theta) = \\cos \\theta \\) we can read off immediately\n\\[A_1 = - E_0, \\qquad \\text{ all other }A_l\u0026#39;s \\text{ zero}\\] Conclusion:\n\\[V(r, \\theta) = - E_0 \\left( r - \\frac{R^3}{r^2} \\right) \\cos \\theta \\tagl{3.76}\\] The first term \\( (-E_0 r \\cos \\theta) \\) is due to the external field; the contribution attributable to the induced charge is\n\\[E_0 \\frac{R^3}{r^2} \\cos \\theta\\] If you want to know the induced charge density, it can be calculated in the usual way:\n\\[\\sigma(\\theta) = - \\epsilon_0 \\left. \\pdv{V}{r} \\right|_{r = R} = \\epsilon_0 E_0 \\left. \\left( 1 \u0026#43; 2 \\frac{R^3}{r^3} \\right) \\cos \\theta \\right|_{r = R} = 3 \\epsilon_0 E_0 \\cos \\theta \\tagl{3.77}\\] As expected, it is positive in the \u0026rsquo;northern\u0026rsquo; hemisphere \\( 0 \\leq \\theta \\leq \\pi /2 \\) and negative in the \u0026lsquo;southern\u0026rsquo; \\( \\pi/2 \\leq \\theta \\leq \\pi \\).\nExample 3.9 # Q A specified charge density \\( \\sigma_0(\\theta) \\) is glued over the surface of a spherical shell of radius R. Find the resulting potential inside and outside the sphere. A You could, of course, do this by direct integration:\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma_0}{\\gr} \\dd{a}\\] but separation of variables is often easier. For the interior region, we have\n\\[V(r, \\theta) = \\sum_{l = 0}^\\infty A_l r^l P_l (\\cos \\theta) \\quad (r \\leq R) \\tagl{3.78}\\] (no \\( B_l \\) terms - they blow up at the origin); in the exterior region\n\\[V(r, \\theta) = \\sum_{l=0}^\\infty \\frac{B_l}{r^{l\u0026#43;1}} P_l(\\cos \\theta) \\quad (r \\geq R) \\tagl{3.79}\\] (no \\( A_l \\) terms - they don\u0026rsquo;t go to zero at infinity). These two functions must be joined together by the appropriate boundary conditions at the surface itself. First, the potential is continuous at \\( r = R \\) (Eq. 2.34):\n\\[\\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l\u0026#43;1}} P_l(\\cos \\theta) \\tagl{3.80} \\] It follows that the coefficients of like Legendre polynomial are equal:\n\\[B_l = A_l R^{2l\u0026#43;1} \\tagl{3.81}\\] (To prove that formally, multiply both sides of \\( \\eqref{3.80} \\) by \\( P_{l\u0026rsquo;} (\\cos \\theta)\\sin \\theta \\) and integrate from \\( 0 \\) to \\( \\pi \\), using the orthogonality relation \\( \\eqref{3.68} \\) .) Second, the radial derivative of V suffers a discontinuity at the surface (Eq. 2.36):\n\\[\\left. \\left( \\pdv{V_{out}}{r} - \\pdv{V_{in}}{r} \\right) \\right|_{r = R} = - \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.82}\\] Thus,\n\\[- \\sum_{l=0}^\\infty (l\u0026#43;1) \\frac{B_l}{R^{l\u0026#43;2}} P_l(\\cos \\theta) - \\sum_{l=0}^\\infty l A_l R^{l-1} P_l(\\cos \\theta) = - \\frac{1}{\\epsilon_0} \\sigma_0 (\\theta)\\] or, using \\( \\eqref{3.81} \\),\n\\[\\sum_{l=0}^\\infty (2l\u0026#43;1) A_l R^{l-1} P_l(\\cos \\theta) = \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.83}\\] From here, the coefficients can be determined using Fourier\u0026rsquo;s trick\n\\[A_l = \\frac{1}{2 \\epsilon_0 R^{l-1}} \\int_0 ^\\pi \\sigma_0 (\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.84}\\] Equations 3.78 and 3.79 constitute the solution to our problem, with the coefficients given by \\( \\eqref{3.81} \\) and \\( \\eqref{3.84} \\). For instance, if\n\\[\\sigma_0(\\theta) = k \\cos \\theta = k P_1 (\\cos \\theta) \\tagl{3.85}\\] for some constant k, then all the \\( A_l \\)\u0026rsquo;s are zero except for \\( l = 1 \\), and\n\\[A_1 = \\frac{k}{2 \\epsilon_0} \\int_0 ^\\pi [P_1(\\cos \\theta)]^2 \\sin \\theta \\dd{\\theta} = \\frac{k}{3\\epsilon_0} \\] The potential inside the sphere is therefore\n\\[V(r, \\theta) = \\frac{k}{3 \\epsilon_0} r \\cos \\theta \\quad (r \\leq R) \\tagl{3.86}\\] whereas outside the sphere\n\\[V(r, \\theta) = \\frac{kR^3}{3 \\epsilon_0} \\frac{1}{r^2} \\cos \\theta \\quad (r \\geq R) \\tagl{3.87}\\] In particular, if \\( \\sigma_0(\\theta) \\) is the induced charge on a metal sphere in an external field \\( E_0(\\vu{z}) \\), so that \\( k = 3 \\epsilon_0 E_0 \\eqref{3.77} \\), then the potential inside is \\( E_0 r \\cos \\theta = E_0 z \\), and the field is \\( -E_0 \\vu{z} \\) - exactly right to cancel off the external field, as of course it should be. Outside the sphere the potential due to this surface charge is\n\\[E_0 \\frac{R^3}{r^2} \\cos \\theta\\] consistent with our conclusion in Example 3.8.\n"},{"id":61,"href":"/r/notes/griffiths/ch3-4/","title":"Multipole Expansion","section":"Griffiths Introduction to Electrodynamics","content":" 3.4: Multipole Expansion # 3.4.1: Approximate Potentials at Large Distances # If you are very far away from a localized charge distribution, it \u0026ldquo;looks\u0026rdquo; like a point charge, and the potential is - to good approximation - \\( (1/4 \\pi \\epsilon_0) Q/r \\), where \\( Q \\) is the total charge. We have often used this as a check on formulas for V. But what if \\( Q \\) is zero? You might reply that the potential is then approximately zero, and of course, you\u0026rsquo;re right in a sense (indeed, the potential at large r is pretty small even if Q is not zero). But we\u0026rsquo;re looking for something a bit more informative than that.\nExample 3.10 # Q A (physical) electric dipole consists of two equal and opposite charges \\( (\\pm q) \\) separated by a distance d. Find the approximate potential at points far from the dipole A Let \\( \\gr_- \\) be the distance from \\( -q \\) and \\( \\gr_{+} \\) be the distance from \\( +q \\) (Fig. 3.26).\nThen \\[V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr_{\u0026#43;}} - \\frac{q}{\\gr_{-}} \\right)\\] and (from the law of cosines),\n\\[\\gr_{\\pm} ^2 = r^2 \u0026#43; (d/2)^2 \\mp r d \\cos \\theta = r^2 \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \u0026#43; \\frac{d^2}{4r^2} \\right)\\] We\u0026rsquo;re interested in the regime \\( r \\gg d \\), so the third term is negligible, and the binomial expansion yields\n\\[\\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \\right) ^{-1/2} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right) \\] Thus\n\\[\\frac{1}{\\gr_\u0026#43;} - \\frac{1}{\\gr_{-}} \\approx \\frac{d}{r^2} \\cos \\theta \\] and hence\n\\[V(r) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd \\cos \\theta}{r^2} \\tagl{3.90} \\] The potential of a dipole goes like \\( 1/r^2 \\) at large r; as we might have anticipated, it falls off more rapidly than the potential of a point charge. If we put together a pair of equal and opposite dipoles to make a quadrupole, the potential goes like \\( 1/r^3 \\); for back-to-back quadrupoles (an octopole), it goes like \\( 1/r^4 \\), and so on. Figure 3.27 summarizes the hierarchy; for completeness I have included the electric monopole (point charge), whose potential, of course, goes like \\( 1/r \\)\nExample 3.10 pertains to a very special charge configuration. I propose now to develop a systematic expansion for the potential of any localized charge distribution, in powers of \\( 1/r \\). Figure 3.28 defines the relevant variables; the potential at r is given by\n\\[V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr} \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \\tagl{3.91}\\] Using the law of cosines, \\[\\gr ^2 = r^2 \u0026#43; (r\u0026#39;)^2 - 2r r\u0026#39; \\cos \\alpha = r^2 \\left[ 1 \u0026#43; \\left( \\frac{r\u0026#39;}{r} \\right)^2 - 2 \\left( \\frac{r\u0026#39;}{r} \\right)\\cos \\alpha \\right]\\] where \\( \\alpha \\) is the angle between \\( \\vec{r} \\) and \\( \\vec{r\u0026rsquo;} \\). Thus,\n\\[ \\gr = r \\sqrt{1 \u0026#43; \\epsilon} \\tagl{3.92} \\] with\n\\[\\epsilon \\equiv \\left( \\frac{r\u0026#39;}{r} \\right)\\left( \\frac{r\u0026#39;}{r} - 2 \\cos \\alpha \\right)\\] For points well outside the charge distribution, \\( \\epsilon \\) is much less than 1, and this invites a binomial expansion:\n\\[\\frac{1}{\\gr} = \\frac{1}{r} (1 \u0026#43; \\epsilon)^{-1/2} = \\frac{1}{r} \\left( 1 - \\frac{1}{2} \\epsilon \u0026#43; \\frac{3}{8} \\epsilon^2 - \\frac{5}{16} \\epsilon^3 \u0026#43; \\ldots \\right) \\tagl{3.93}\\] or, in terms of \\( r, r\u0026rsquo; \\), and \\( \\alpha \\),\n\\[\\begin{aligned} \\frac{1}{\\gr} \u0026amp; = \\frac{1}{r} \\left[ 1 - \\frac{1}{2} \\left( \\frac{r\u0026#39;}{r} \\right) \\left( \\frac{r\u0026#39;}{r} - 2 \\cos \\alpha \\right) \u0026#43; \\frac{3}{8} \\left( \\frac{r\u0026#39;}{r} \\right)^2 \\left( \\frac{r\u0026#39;}{r} - 2 \\cos \\alpha \\right)^2 \\right. \\\\ \u0026amp; \\qquad \\left. - \\frac{5}{16} \\left( \\frac{r\u0026#39;}{r} \\right)^3 \\left( \\frac{r\u0026#39;}{r} - 2 \\cos \\alpha \\right)^3 \u0026#43; \\ldots \\right] \\\\ \u0026amp; = \\frac{1}{r} \\left[ 1 \u0026#43; \\left( \\frac{r\u0026#39;}{r} \\right)(\\cos \\alpha) \u0026#43; \\left( \\frac{r\u0026#39;}{r} \\right) \\left( \\frac{3 \\cos ^2 \\alpha - 1}{2} \\right) \\right. \\\\ \u0026amp; \\qquad \\left. \\left( \\frac{r\u0026#39;}{r} \\right)^3 \\left( \\frac{5\\cos ^3 \\alpha - 3 \\cos \\alpha}{2} \\right) \u0026#43; \\ldots \\right] \\end{aligned}\\] In the last step, I have collected together like powers of \\( (r\u0026rsquo;/r) \\); surprisingly, their coefficients (the terms in parentheses) are Legendre polynomials! The remarkable result is that\n\\[\\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r\u0026#39;}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{3.94}\\] Substituting this back into \\( \\eqref{3.91} \\), and noting that \\( r \\) is constant, as far as the integration is concerned, I conclude that\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n\u0026#43;1)}} \\int (r\u0026#39;) P_n(\\cos \\alpha) \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \\tagl{3.95} \\] or, more explicitly,\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{r} \\int \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \u0026#43; \\frac{1}{r^2} \\int r\u0026#39; \\cos \\alpha \\rho(\\vec{r\u0026#39;})\\dd{\\tau\u0026#39;} \\right. \\\\ \u0026#43; \\left. \\frac{1}{r^3} \\int (r\u0026#39;)^2 \\left( \\frac{3}{2} \\cos^2 \\alpha - \\frac{1}{2} \\right) \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \u0026#43; \\ldots \\right] \\tagl{3.96}\\] This is the desired result - the multipole expansion of \\( V \\) in powers of \\( 1/r \\). The first term \\( (n=0) \\) is the monopole contribution (it goes like \\( 1/r \\)); the second \\( (n=1) \\) is the dipole (it goes like \\( 1/r^2 \\) ); the third is quadrupole; the fourth octopole, and so on. Remember that \\( \\alpha \\) is the angle between \\( \\vec{r} \\) and \\( \\vec{r\u0026rsquo;} \\), so the integrals depend on the direction to the field point. If you are interested in the potential along the \\( z\u0026rsquo; \\) axis (or - putting it the other way round - if you orient your \\( \\vec{r\u0026rsquo;} \\) coordinates so the \\( z\u0026rsquo; \\) axis lies along \\( \\vec{r} \\)), then \\( \\alpha \\) is the usual polar angle \\( \\theta\u0026rsquo; \\). As it stands, \\( \\eqref{3.95} \\) is exact, but it is useful primarily as an approximation scheme: the lowest nonzero term in the expansion provides the approximate potential at large r, and the successive terms tell us how to improve the approximation if greater precision is required.\n3.4.2: The Monopole and Dipole Terms # Ordinarily, the multipole expansion is dominated (at large r) by the monopole term:\n\\[V_{mon} (\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r} \\] where \\( Q =\\int \\rho \\dd{\\tau} \\) is the total charge of the configuration. This is just what we expect for the approximate potential at large distances from the charge. For a point charge at the origin, \\( V_{mon} \\) is the exact potential, not merely a first approximation at large r; in this case, all the higher multipoles vanish. If the total charge is zero, the dominant term in the potential will be the dipole (unless, of course, it also vanishes):\n\\[V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\int r\u0026#39; \\cos \\alpha \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \\] Since \\( \\alpha \\) is the angle between \\( r\u0026rsquo; \\) and \\( r \\) (Fig 2.38),\n\\[r\u0026#39; \\cos \\alpha = \\vu{r} \\cdot \\vec{r\u0026#39;}\\] and the dipole potential can be written more succinctly:\n\\[V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\vu{r} \\cdot \\int \\vec{r\u0026#39;} \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;}\\] This integral (which does not depend on \\( \\vec{r} \\) ) is called the dipole moment of the distribution:\n\\[\\vec{p} \\equiv \\int \\vec{r\u0026#39;} \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \\tagl{3.98}\\] and the dipole contribution to the potential simplifies to\n\\[V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\tagl{3.99} \\] The dipole moment is determined by the geometry (size, shape, and density) of the charge distribution. \\( \\eqref{3.98} \\) translates in the usual way (Sect 2.1.4) for point, line, and surface charges. Thus, the dipole moment for a collection of point charges is\n\\[\\vec{p} = \\sum_{i=1} ^n q_i \\vec{r\u0026#39;}_i \\tagl{3.100}\\] For a physical dipole (equal and opposite charges \\( \\pm q \\)),\n\\[\\vec{p} = q\\vec{r\u0026#39;_\u0026#43;} - q \\vec{r_- \u0026#39; } = q(\\vec{r\u0026#39; _\u0026#43;} - \\vec{r\u0026#39;_-}) = q \\vec{d} \\tagl{3.101}\\] where \\( \\vec{d} \\) is the vector from the negative charge to the positive one (Fig. 3.29).\nIs this consistent with what we got in Example 3.10? Yes: If you put \\( \\eqref{3.101} \\) into \\( \\eqref{3.99} \\), you recover \\( \\eqref{3.90} \\). Notice, however, that this is only the approximate potential of the physical dipole - evidently there are higher multipole contributions. Of course, as you go farther and farther away, \\( V_{dip} \\) becomes a better and better approximation, since the higher terms die off more rapidly with increasing \\( r \\). By the same token, at a fixed \\( r \\) the dipole approximation improves as you shrink the separation distance \\( d \\). To construct a perfect dipole whose potential is given exactly by \\( \\eqref{3.99} \\), you\u0026rsquo;d have to let \\( d \\) approach zero. Unfortunately you then lose the dipole term too, unless you simultaneously arrange for q to go to infinity! A physical dipole becomes a pure dipole, then, in the rather artificial limit \\( d \\rightarrow 0. q \\rightarrow \\infty \\), with the product \\( qd = p \\) held fixed. When someone uses the word \u0026ldquo;dipole,\u0026rdquo; you can\u0026rsquo;t always tell whether they mean a physical dipole (with finite separation between the charges) or an ideal dipole. If in doubt, assume that d is small enough that you can safely apply \\( \\eqref{3.99} \\).\nDipole moments are vectors, and they add accordingly: if you have two dipoles \\( \\vec{p_1} \\) and \\( \\vec{p_2} \\), the total dipole moment is \\( \\vec{p_1} + \\vec{p_2} \\). For instance, with four charges at the corners of a square, as shown in Fig. 3.30, the net dipole moment is zero. You can see this by combining the charges in pairs or by adding up the four contributions individually using \\( \\eqref{3.100} \\). This is a quadrupole, as I indicated earlier, and its potential is dominated by the quadrupole term in the multipole expansion.\n3.4.3: Origin of Coordinates in Multipole Expansions # I mentioned earlier that a point charge at the origin constitutes a \u0026ldquo;pure\u0026rdquo; monopole. If it is not at the origin, it\u0026rsquo;s no longer a pure monopole. For instance, the charge in Fig. 3.32 has a dipole moment \\( \\vec{p} = q d \\vu{y} \\), and a corresponding dipole term in its potential. The monopole potential \\( (1/4 \\pi \\epsilon_0) q/r \\) is not quite correct for this configuration; rather, the exact potential is \\( (1/4 \\pi \\epsilon_0) q/\\gr \\). The multipole expansion is, remember, a series in inverse powers of r (the distance to the origin), and when we expand \\( 1/\\gr \\), we get all powers, not just the first.\nSo moving the origin (or, what amounts to the same thing, moving the charge) can radically alter a multipole expansion. The monopole moment \\( Q \\) does not change, since the total charge is obviously independent of the coordinate system. (In Fig. 3.32, the monopole term was unaffected when we moved q away from the origin - it\u0026rsquo;s just that it was no longer the whole story: a dipole term - and for that matter all higher poles - appeared as well.) Ordinarily, the dipole moment does change when you shift the origin, but there is an important exception: If the total charge is zero, then the dipole moment is independent of the choice of origin. For suppose we displace the origin by an amount \\( \\vec{a} \\) (Fig. 3.33). The new dipole moment is then\n\\[\\begin{aligned} \\vec{p_2} \u0026amp; = \\int \\vec{r\u0026#39;} \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} = \\int (\\vec{r\u0026#39;} - \\vec{a} ) \\rho (\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} \\\\ \u0026amp; = \\int \\vec{r\u0026#39;} \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} - \\vec{a} \\int \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;} = \\vec{p} - Q \\vec{a} \\end{aligned}\\] In particular, if \\( Q = 0 \\), the \\( \\vec{p_2} = \\vec{p} \\). So if someone asks for the dipole moment in Fig 3.34(a), you can answer with confidence \u0026ldquo;\\( q \\vec{d} \\),\u0026rdquo; but if you\u0026rsquo;re asked for the dipole moment in Fig 3.34(b), the appropriate response would be \u0026ldquo;With respect to what origin?\u0026rdquo;\n3.4.4: The Electric Field of a Dipole # So far we have only worked with potentials. Now I would like to calculate the electric field of a (perfect) dipole. If we choose coordinates so that \\( \\vec{p} \\) is at the origin and points in the z direction (Fig. 3.36), then the potential at \\( r, \\theta \\) is \\( \\eqref{3.99} \\):\n\\[V_{dip} (r, \\theta) = \\frac{\\vu{r} \\cdot \\vec{p}}{4 \\pi \\epsilon_0 r^2} = \\frac{p \\cos \\theta}{4 \\pi \\epsilon_0 r^2} \\tagl{3.102} \\] To get the field, we take the negative gradient of \\( V \\):\n\\[\\begin{aligned} E_r \u0026amp; = - \\pdv{V}{r} = \\frac{2 p \\cos \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\theta \u0026amp; = - \\frac{1}{r} \\pdv{V}{\\theta} = \\frac{p \\sin \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\phi \u0026amp; = - \\frac{1}{r \\sin \\theta} \\pdv{V}{\\phi} = 0 \\end{aligned}\\] Thus,\n\\[\\vec{E_{dip}} (r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3}(2 \\cos \\theta \\vu{r} \u0026#43; \\sin \\theta \\vu{\\theta}) \\tagl{3.103} \\] This formula makes explicit reference to a particular coordinate system (spherical) and assumes a particular orientation for \\( \\vec{p} \\) (along z). It can be recast in a coordinate-free form, analogous to the potential in \\( \\eqref{3.99} \\) - See problem 3.36.\nNotice that the dipole falls off as the inverse cube of r; the monopole field \\( (Q / 4 \\pi \\epsilon_0 r^2) \\vu{r} \\) goes as the inverse square, of course. Quadrupole fields go like \\( 1/r^4 \\), octopole like \\( 1/r^5 \\), and so on. (This merely reflects how the respective potentials fall off - the gradient introduces another factor of \\( 1/r \\) ).\nFigure 3.37(a) shows the field lines of a \u0026ldquo;pure\u0026rdquo; dipole \\( \\eqref{3.103} \\). For comparison, I have also sketched the field lines for a \u0026ldquo;physical\u0026rdquo; dipole, in Fig 3.37(b). Notice how similar the two pictures become if you blot out the central region; up close, however, they are entirely different. Only for points \\( r \\gg d \\) does \\( \\eqref{3.103} \\) represent a valid approximation to the field of a physical dipole. As I mentioned earlier, this regime can be reached either by going to large \\( r \\) or by squeezing the charges very close together.\n"},{"id":62,"href":"/r/notes/griffiths/ch4-1/","title":"Polarization","section":"Griffiths Introduction to Electrodynamics","content":" 4.1: Polarization # 4.1.1: Dielectrics # This chapter is all about what happens to an electric field when you take matter into account. Matter, of course, comes in many varieties - phase, composition, state, etc. - and depending upon which type of matter we\u0026rsquo;re dealing with, the electrostatic field response can be very different. Nevertheless, most everyday objects belong (at least, to good approximation) to one of two large classes: conductors and insulators (or dielectrics). We have already gone over what happens to an electrostatic field in a conductor; the \u0026ldquo;unlimited\u0026rdquo; free charges within a conductor distribute themselves through the material so as to form an equipotential. In practice, this usually means that many electrons (one or two per atom, in a typical metal) are not associated with a particular nucleus, but roam around at will. In dielectrics, by contrast, all charges are attached to specific atoms or molecules - they cannot escape their leash, and can only move a bit within the atom or molecule. Such microscopic displacements are not as dramatic as the wholesale rearrangement of charge in a conductor, but their cumulative effects account for the characteristic behavior of dielectric materials. There are actually two principal mechanisms by which electric fields can distort the charge distribution of a dielectric atom or molecule: stretching and rotating. In the next two sections I\u0026rsquo;ll discuss these processes.\n4.1.2: Induced Dipoles # Say we have a totally neutral atom and place it in an electric field E? What happens? At first guess, you might think \u0026ldquo;Nothing at all! The atom is not charged, so the field has no effect on it.\u0026rdquo; That\u0026rsquo;s incorrect. Although the atom as a whole is electrically neutral (just like the dipoles we looked at in the last chapter), there is a positively charged core (the nucleus) and negatively charged electron(s) surrounding it. These two regions of charge within the atom are influenced by the field: the nucleus is pushed in the direction of the field, and the electrons the opposite way. In principle, if the field is large enough, it can pull the atom apart completely, \u0026ldquo;ionizing\u0026rdquo; it (the substance then becomes a conductor). With less extreme fields, however, an equilibrium is soon established, for if the center of the electron cloud does not coincide with the nucleus, these positive and negative charges attract one another, and that holds the atom together. The two opposing forces - E pulling the electrons and nucleus apart, and their mutual attraction drawing them back together - reach a balance, leaving the atom polarized, with plus charge shifted slightly one way, and minus the other. The atom now has a tiny dipole moment p, which points in the same direction as E. Typically, this induced dipole moment is approximately proportional to the field (as long as the latter is not too strong):\n\\[\\vec{p} = \\alpha \\vec{E} \\tagl{4.1}\\] The constant of proportionality \\( \\alpha \\) is called atomic polarizability. Its value depends on the detailed structure of the atom in question. Table 4.1 lists some experimentally determined atomic polarizabilities.\nExample 4.1 # Q A primitive model for an atom consists of a point nucleus (+q) surrounded by a uniformly charged spherical cloud (-q) of radius a (Fig 4.1). Calculate the atomic polarizability of such an atom.\nA In the presence of an external field E, the nucleus will be shifted slightly to the right and the electron cloud to the left, as shown in Fig 4.2. (Because the actual displacements involved are extremely small, as you\u0026rsquo;ll see in Prob 4.1, it is reasonable to assume that the electron cloud retains its spherical shape.) Say that equilibrium occurs when the nucleus is displaced a distance d from the center of the sphere. At that point, the external field pushing the nucleus to the right exactly balances the internal field pulling it to the left: \\( E = E_e \\), where \\( E_e \\) is the field produced by the electron cloud. Now the field at a distance d from the center of a uniformly charged sphere is\n\\[E_e = \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{a^3} \\] At equilibrium, then\n\\[E = \\frac{1}{4 \\pi \\epsilon_0 } \\frac{qd}{a^3}, \\quad \\text{ or } p = qd = (4 \\pi \\epsilon_0 a^3 ) E \\] The atomic polarizability is therefore\n\\[\\alpha = 4 \\pi \\epsilon_0 a^3 = 3 \\epsilon_0 v \\tagl{4.2}\\] where v is the volume of the atom. Although this atomic model is extremely crude, the result \\( \\eqref{4.2} \\) is not too bad - it\u0026rsquo;s accurate to within a factor of four or so for many simple atoms.\nFor molecules the situation is not quite so simple, because frequently they polarize more readily in some directions than in others. Carbon dioxide (Fig 4.3), for instance, has a polarizability of \\( 4.5 \\times 10^{-40} \\) when you apply the field along the axis of the molecule, but only \\( 2 \\times 10^{-40} \\) for fields perpendicular to this direction. When the field is at some angle to the axis, you must first resolve it into parallel and perpendicular components, and multiply each component by the pertinent polarizability\n\\[\\vec{p} = a_{\\perp} E_{\\perp} \u0026#43; \\alpha_{\\parallel} E_{\\parallel} \\] In this case, the induced dipole moment may not even be in the same direction as E. And \\( CO_2 \\) is relatively simple, as molecules go, since at least the atoms arrange themselves in a straight line; for a completely asymmetrical molecule, \\( \\eqref{4.1} \\) is replaced by the most general linear relation between E and p:\n\\[ \\begin{aligned} p_x = \\alpha_{xx} E_x \u0026#43; \\alpha_{xy} E_y \u0026#43; \\alpha_{xz} E_z\\\\ p_y = \\alpha_{yx} E_x \u0026#43; \\alpha_{yy} E_y \u0026#43; \\alpha_{yz} E_z\\\\ p_z = \\alpha_{zx} E_x \u0026#43; \\alpha_{zy} E_y \u0026#43; \\alpha_{zz} E_z \\end{aligned} \\tagl{4.3} \\] The set of nine constants \\( \\alpha_{ij} \\) constitute the polarizability tensor for the molecule. Their values depend on the orientation of the axes you use, though it is always possible to choose \u0026ldquo;principal\u0026rdquo; axes such that all off-diagonal terms vanish, leaving just three nonzero polarizabilities.\n4.1.3: Alignment of Polar Molecules # The neutral atom discussed in section 4.1.2 had no dipole moment to start with - p was entirely induced by the applied field. Some molecules have built-in, permanent dipole moments. In the water molecule, for example, the electrons tend to cluster around the oxygen atom (Fig 4.4), and since the molecule is bent at \\( 105^{\\circ} \\), this leaves a negative charge at the vertex and a positive charge on the opposite side. (The dipole moment of water is unusually large: \\( 6.1 \\times 10^{-30} C \\cdot m \\); in fact, this is what accounts for its effectiveness as a solvent.) What happens when such molecules (called polar molecules) are placed in an electric field?\nIf the field is uniform, the force on the positive end, \\( \\vec{F_+} = q \\vec{E} \\), exactly cancels the force on the negative end, \\( \\vec{F_-} = - q \\vec{E} \\) (Fig 4.5). However, there will be a torque:\n\\[\\vec{N } = (\\vec{r_{\u0026#43;}} \\cross \\vec{F_\u0026#43;}) \u0026#43; (\\vec{r_{-}} \\cross \\vec{F_{-}}) \\\\ = \\left[ ( \\vec{d}/2) \\cross (q \\vec{E}) \\right] \u0026#43; \\left[ ( -\\vec{d}/2) \\cross (- q \\vec{E}) \\right] = q \\vec{d} \\cross \\vec{E}\\] Thus a dipole \\( \\vec{p} = q \\vec{d} \\) in a uniform field \\( \\vec{E} \\) experiences a torque\n\\[\\vec{N} = \\vec{p} \\cross \\vec{E} \\tagl{4.4}\\] Notice that N is in such a direction as to line p up parallel to E; a polar molecule that is free to rotate will swing around until it points in the direction of the applied field.\nIf the field is nonuniform, so that \\( \\vec{F_{+}} \\) does not exactly balance \\( \\vec{F_-} \\), there will be a net force on the dipole, in addition to the torque. Of course, E must change rather abruptly for there to be significant variation in the space of one molecule, so this is not ordinarily a major consideration in discussing the behavior of dielectrics. Nevertheless, the formula for the force on a dipole in a nonuniform field is of some interest:\n\\[\\vec{F} = \\vec{F_{\u0026#43;}} \u0026#43; \\vec{F_-} = q(\\vec{E_\u0026#43;} - \\vec{E_-}) = q(\\Delta \\vec{E})\\] where \\( \\Delta \\vec{E} \\) represents the difference between the field at the plus end and the field at the minus end. Assuming the dipole is very short, we may use Eq 1.35 to approximate the small change in E\n\\[\\Delta \\vec{E} = (\\vec{d} \\cdot \\grad ) \\vec{E}\\] and therefore\n\\[\\vec{F} = ( \\vec{p} \\cdot \\grad) \\vec{E} \\tagl{4.5}\\] For a \u0026ldquo;perfect\u0026rdquo; dipole of infinitesimal length, \\( \\eqref{4.4} \\) gives the torque about the center of the dipole even in a nonuniform field; about any other point, \\( \\vec{N} = ( \\vec{p} \\cross \\vec{E}) + (\\vec{r} \\cross \\vec{F}) \\).\n4.1.4: Polarization # In the previous two sections, we have considered the effect of an external electric field on an individual atom or molecule. We are now in a position to answer (quantitatively) the original question: What happens to a piece of dielectric material when it is placed in an electric field? If the substance consists of neutral atoms (or nonpolar molecules), the field will induce in each a tiny dipole moment, pointing in the same direction as the field. If the material is made up of polar molecules, each permanent dipole will experience a torque, tending to line it up along the field direction. (Random thermal motions compete with this process, so the alignment is never complete, especially at higher temperatures, and disappears almost at once when the field is removed.)\nNotice that these two mechanisms produce the same basic result: a lot of little dipoles pointing along the direction of the field - the material becomes polarized. A convenient measure of this effect is\n\\[\\vec{P} = \\text{ dipole moment per unit volume }\\] which is called the polarization. From now on we shall not worry much about how the polarization got there. Actually, the two mechanisms I described are not as clear-cut as I tried to pretend. Even in polar molecules there will be some polarization by displacement (though generally it is a lot easier to rotate a molecule than to stretch it, so the second mechanism dominates). It\u0026rsquo;s even possible in some materials to \u0026ldquo;freeze in\u0026rdquo; polarization, so that it persists after the field is removed. But let\u0026rsquo;s forget for a moment about the cause of the polarization, and let\u0026rsquo;s study the field that a chunk of polarized material itself produces. Then in section 4.3 we\u0026rsquo;ll put it all together: the original field, which was responsible for P, plus the new field, which is due to P.\n"},{"id":63,"href":"/r/notes/griffiths/ch4-2/","title":"The Field of a Polarized Object","section":"Griffiths Introduction to Electrodynamics","content":" 4.2: The Field of a Polarized Object # 4.2.1: Bound Charges # Suppose we have a piece of polarized material - that is, an object containing a lot of microscopic dipoles lined up. The dipole moment per unit volume P is given. Question: What is the field produced by this object (not the field that may have caused the polarization, but the field the polarization itself causes)? Well, we know what the field of an individual dipole looks like, so why not chop the material up into infinitesimal dipoles and integrate to get the total? As usual, it\u0026rsquo;s easier to work with the potential. For a single dipole p (Eq. 3.99)\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{\\gr}}{\\gr ^2} \\tagl{4.8}\\] where \\( \\vec{\\gr} \\) is the vector from the dipole to the point at which we are evaluating the potential (Fig 4.8). In the present context, we have a dipole moment \\( \\vec{p} = \\vec{P} \\dd \\tau\u0026rsquo; \\) in each volume element \\( \\dd \\tau\u0026rsquo; \\), so the total potential is\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_{V} \\frac{\\vec{P}(\\vec{r\u0026#39;}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau\u0026#39;\\] That does it, in principle. But a little sleight of hand casts this integral into a much more illuminating form. Observing that \\[\\grad \u0026#39; \\left( \\frac{1}{\\gr} \\right) = \\frac{\\vu{\\gr}}{\\gr ^2} \\] where the differentiation is with respect to the source coordinates (r\u0026rsquo;), we have\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\vec{P} \\cdot \\grad \u0026#39; \\left( \\frac{1}{\\gr} \\right) \\dd \\tau\u0026#39;\\] Peeling the \\( \\grad \\) leftwards with integration by parts, we have\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\int _V \\grad\u0026#39; \\cdot \\left( \\frac{\\vec{P}}{\\gr} \\right) \\dd \\tau\u0026#39; - \\int _V \\frac{1}{\\gr} ( \\grad\u0026#39; \\cdot \\vec{P} ) \\dd \\tau\u0026#39; \\right]\\] The left-hand integral is a volume integral of a divergence, so with Gauss\u0026rsquo;s law\n\\[V = \\frac{1}{4 \\pi \\epsilon_0 } \\oint _S \\frac{1}{\\gr } \\vec{P} \\cdot \\dd \\vec{a\u0026#39;} - \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\frac{1}{\\gr} ( \\grad\u0026#39; \\cdot \\vec{P} ) \\dd \\tau\u0026#39; \\tagl{4.10}\\] The first term looks like the potential of a surface charge\n\\[\\sigma_b \\equiv \\vec{P} \\cdot \\vu{n} \\tagl{4.11}\\] while the second term looks like the potential of a volume charge\n\\[\\rho_b \\equiv - \\div \\vec{P} \\tagl{4.12}\\] With these definitions, \\( \\eqref{4.10} \\) becomes\n\\[V(\\vec{r} = \\frac{1}{4 \\pi \\epsilon_0} \\oint_S \\frac{\\sigma_b}{\\gr} \\dd a\u0026#39; \u0026#43; \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\frac{\\rho_b}{\\gr} \\dd \\tau\u0026#39; \\tagl{4.13}\\] What this means is that the potential (and hence also the field) of a polarized object is the same as that produced by a volume charge density \\( \\rho_b = - \\div \\vec{P} \\) plus a surface charge density \\( \\sigma_b = \\vec{P} \\cdot \\vu{n} \\). Instead of integrating the contributions of all the infinitesimal dipoles, we could just find those bound charges, and then calculate the fields they produce, in the same way we calculate the field of any other volume and surface charges.\nExample 4.2 # Q Find the electric field produced by a uniformly polarized sphere of radius R\nA We may as well choose the z axis to coincide with the direction of polarization (Fig 4.9). The volume bound charge density \\( \\rho_b \\) is zero, since \\( \\vec{P} \\) is uniform. The surface bound charge density is then\n\\[\\sigma_b = \\vec{P} \\cdot \\vu{n} = P \\cos \\theta\\] where \\( \\theta \\) is the usual spherical coordinate. What we want, then is the field produced by a charge density \\( P \\cos \\theta \\) plastered over the surface of a sphere. We happen to have already computed that potential in Exercise 3.9:\n\\[V(r, \\theta) = \\begin{cases} \\frac{P}{3 \\epsilon_0} r \\cos \\theta, \u0026amp; \\qquad \\text{ for } r \\leq R \\\\ \\frac{P}{3 \\epsilon_0} \\frac{R^3}{r^2} \\cos \\theta, \u0026amp; \\qquad \\text{ for } r \\geq R \\end{cases}\\] Since \\( r \\cos \\theta = z \\), the field inside the sphere is uniform:\n\\[\\vec{E} = - \\grad V = - \\frac{P}{3 \\epsilon_0} \\vu{z} = - \\frac{1}{3 \\epsilon_0} \\vec{P} , \\quad \\text{ for } r \u0026lt; R \\tagl{4.14}\\] This is a pretty remarkable result, and will be very useful in what follows. Outside the sphere the potential is the same as that of a perfect dipole at the origin\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} , \\quad \\text{ for } r \\geq R \\tagl{4.15}\\] whose dipole moment is, not surprisingly, equal to the total dipole moment of the sphere: \\[\\vec{p} = \\frac{4}{3} \\pi R^3 \\vec{P} \\tagl{4.16}\\] The field of the uniformly polarized sphere is shown in Fig 4.10.\n4.2.2: Physical Interpretation of Bound Charges # In the last section we found that the field of a polarized object is identical to the field that would be produced by a certain distribution of \u0026ldquo;bound charges,\u0026rdquo; \\( \\sigma_b \\) and \\( \\rho_b \\). But this conclusion emerged in the course of abstract manipulations on the integral in Eq. 4.9, and left us with no clue as to the physical meaning of these bound charges. Indeed, some authors give you the impression that bound charges are in some sense \u0026ldquo;fictitious\u0026rdquo; - mere bookkeeping devices used to facilitate the calculation of fields. Nothing could be further from the truth: \\( \\rho_b \\) and \\( \\sigma_b \\) represent perfectly genuine accumulations of charge. In this section I\u0026rsquo;ll explain how polarization leads to these charge distributions.\nThe basic idea is very simple: Suppose we have a long string of dipoles, as shown in Fig. 4.11. Along the line, the head of one effectively cancels the tail of its neighbor, but at the ends there are two charges left over: plus at the right end and minus at the left. It is as if we had peeled off an electron at one end and carried it all the way down to the other end, though in fact no single electron made the whole trip - a lot of tiny displacements add up to one large one. We call the net charge at the ends a bound charge to remind ourselves that it cannot be removed; in a dielectric every electron is attached to a specific atom or molecule. But apart from that, bound charge is no different from any other kind.\nTo calculate the actual amount of bound charge resulting from a given polarization, examine a \u0026ldquo;tube\u0026rdquo; of dielectric parallel to P. The dipole moment of the tiny chunk shown in Fig. 4.12 is \\( P(Ad) \\), where A is the cross-sectional area of the tube and dis the length of the chunk. In terms of the charge (q) at the end, this same dipole moment can be written \\( qd \\). The bound charge that piles up at the right end of the tube is therefore\n\\[q = PA\\] If the ends have been sliced off perpendicularly, the surface charge density is\n\\[\\sigma_b = \\frac{q}{A} = P\\] For an oblique cut (Fig 4.13), the charge is still the same, but \\( A = A_{end} \\cos \\theta \\) so\n\\[\\sigma_b = \\frac{1}{A_{end}} = P \\cos \\theta = \\vec{P} \\cdot \\vu{n}\\] The effect of the polarization, then, is to paint a bound charge \\( \\sigma_b = \\vec{P} \\cdot \\vu{n} \\) over the surface of the material. This is exactly what we found by more rigorous means in Sect. 4.2.1. But now we know where the bound charge comes from.\nIf the polarization is nonuniform, we get accumulations of bound charge within the material, as well as on the surface. A glance at Fig. 4.14 suggests that a diverging P results in a pileup of negative charge. Indeed, the net bound charge \\( \\int \\rho_b \\dd \\tau \\) in a given volume is equal and opposite to the amount that has been pushed out through the surface. The latter (by the same reasoning we used before) is \\( \\vec{P} \\cdot \\vu{n} \\) per unit area, so\n\\[\\int_v \\rho_b \\dd \\tau = - \\oint _S \\vec{P} \\cdot \\dd \\vec{a} = - \\int_V (\\div \\vec{P}) \\dd \\tau\\] Since this is true for any volume, we have\n\\[\\rho_b = - \\div \\vec{P}\\] confirming, again, the more rigorous conclusion of Sect. 4.2.1.\nExample 4.3 # Q There is another way of analyzing the uniformly polarized sphere, which nicely illustrates the idea of bound charge. What we have, really, is two spheres of charge: a positive sphere and a negative sphere. Without polarization the two are superimposed and cancel completely. But when the material is uniformly polarized, all the plus charges move slightly upward (the z direction) and all the minus charges move slightly downward (Fig 4.15). The two spheres no longer overlap perfectly: at the top there\u0026rsquo;s a \u0026lsquo;cap\u0026rsquo; of leftover positive charge and at the bottom a cap of negative charge. This \u0026rsquo;leftover\u0026rsquo; charge is precisely the bound surface charge \\( \\sigma_b \\)\nA In Prob 2.18, you calculated the field in the region of overlap between two uniformly charged spheres; the answer was\n\\[\\vec{E} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1 \\vec{d}}{R^3} \\] where q is the total charge of the positive sphere, d is the vector from the negative center to the positive center, and R is the radius of the sphere. We can express this in terms of the polarization of the sphere, \\( \\vec{p} = q \\vec{d} = (\\frac{4}{3} \\pi R^3 )\\vec{P} \\), as\n\\[\\vec{E} = - \\frac{1}{3 \\epsilon_0} \\vec{P}\\] Meanwhile, for points outside, it is as though all charge on each sphere were concentrated at the respective center. We have, then, a dipole, with potential\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\] (Remember that d is some small fraction of an atomic radius; Fig 4.15 is grossly exaggerated). These answers agree, of course, wtih the results from Ex 4.2.\n4.2.3: The Field Inside a Dielectric # I have been sloppy about the distinction between \u0026ldquo;pure\u0026rdquo; dipoles and \u0026ldquo;physical\u0026rdquo; dipoles. In developing the theory of bound charges, I assumed we were working with the pure kind - indeed, I stated in Eq 4.8 the formula for the potential of a perfect dipole. And yet an actual polarized dielectric consists of physical dipoles, albeit extremely tiny ones. What is more, I presumed to represent discrete molecular dipoles by a continuous density function P. How should I justify this method? Outside the dielectric there is no real problem: here we are far away from the molecules (\\( \\gr \\) is many times greater than the separation distance between plus and minus charges), so the dipole potential dominates overwhelmingly and the detailed \u0026ldquo;graininess\u0026rdquo; of the source is blurred by distance. Inside the dielectric, however, we can hardly pretend to be far from all the dipoles, and the procedure I used in Sect. 4.2.1 is open to serious challenge.\nIn fact, when you stop to think about it, the electric field inside matter must be fantastically complicated, on the microscopic level. If you happen to be very near an electron, the field is gigantic, whereas a short distance away it may be small or may point in a totally different direction. Moreover, an instant later, as the atoms move about, the field will have altered entirely. This true microscopic field would be utterly impossible to calculate, nor would it be of much interest if you could. Just as, for macroscopic purposes, we regard water as a continuous fluid, ignoring its molecular structure, so also we can ignore the microscopic bumps and wrinkles in the electric field inside matter, and concentrate on the macroscopic field. This is defined as the average field over regions large enough to contain many thousands of atoms (so that the uninteresting microscopic fluctuations are smoothed over), and yet small enough to ensure that we do not wash out any significant large-scale variations in the field. (In practice, this means we must average over regions much smaller than the dimensions of the object itself.) Ordinarily, the macroscopic field is what people mean when they speak of \u0026ldquo;the\u0026rdquo; field inside matter.\nIt remains to show that the macroscopic field is what we actually obtain when we use the methods of Sect. 4.2.1. The argument is subtle, so hang on. Suppose I want to calculate the macroscopic field at some point r within a dielectric. I know I must average the true (microscopic) field over an appropriate volume, so let me draw a small sphere about r, of radius, say, a thousand times the size of a molecule. The macroscopic field at r, then, consists of two parts: the average field over the sphere due to all charges outside, plus the average due to all charges inside:\n\\[\\vec{E} = \\vec{E}_{out} \u0026#43; \\vec{E}_{in}\\] You proved in problem 3.47 that the average field over a sphere produced by charges outside is equal to the field they produce at the center, so \\( \\vec{E_{out}} \\) is the field at r due to the dipoles exterior to the sphere. These are far enough away that we can safely use Eq 4.9\n\\[V_{out} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{outside} \\frac{\\vec{P}(\\vec{r\u0026#39;}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau\u0026#39; \\tagl{4.17} \\] The dipoles inside the sphere are too close to treat in this fashion. But fortunately all we need is their average field, which we already know (Eq 3.105)\n\\[\\vec{E}_{in} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p}}{R^3} \\] regardless of the details of the charge distribution within the sphere. The only relevant quantity is the total dipole moment, \\( \\vec{p} = (\\frac{4}{3} \\pi R^3 )\\vec{P} \\):\n\\[\\vec{E_{in}} = - \\frac{1}{3 \\epsilon_0} \\vec{P} \\tagl{4.18}\\] Now, by assumption, the sphere is small enough that P does not vary significantly over its volume, so the term left out of the integral in \\( \\eqref{4.17} \\) corresponds to the field at the center of a uniformly polarized sphere, to wit: \\( -(1/3\\epsilon_0 )\\vec{P} \\eqref{4.14}\\) . But this is precisely what \\( \\vec{E_{in}} \\) puts back in! The macroscopic field, then, is given by the potential\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\vec{P}(\\vec{r\u0026#39;}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau\u0026#39; \\tagl{4.19} \\] where the integral runs over the entire volume of the dielectric. This is, of course, what we used under the assumption of perfect dipoles in Sect 4.2.1; without realizing it, we were correctly calculating the averaged, macroscopic field, for points inside the dielectric.\nNotice that this argument all revolves around the curious fact that the average field over any sphere (due to the charge inside) is the same as the field at the center of a uniformly polarized sphere with the same total dipole moment. This means that no matter how crazy the actual microscopic charge configuration, we can replace it with a nice smooth distribution of perfect dipoles, if all we care about is the macroscopic (average) field. Incidentally, while the argument ostensibly relies on the spherical shape I chose to average over, the macroscopic field is certainly independent of the geometry of the averaging region, and this is reflected in the final answer \\( \\eqref{4.19} \\). Presumably one could reproduce the same argument for a cube or ellipsoid or whatever by performing some more grueling calculations.\n"},{"id":64,"href":"/r/notes/griffiths/ch4-3/","title":"The Electric Displacement","section":"Griffiths Introduction to Electrodynamics","content":" 4.3: The Electric Displacement # 4.3.1: Gauss\u0026rsquo;s Law in the Presence of Dielectrics # In Section 4.2 we found that the effect of polarization is to produce accumulations of (bound) charge, \\( \\rho_b = - \\div \\vec{P} \\) within the dielectric and \\( \\sigma_b = \\vec{P} \\cdot \\vu{n} \\) on the surface. The field due to polarization of the medium is just the field of this bound charge. We are now ready to put it all together: the field attributable to bound charge plus the field due to everything else (which, for want of a better term, we call free charge, \\( \\rho_f \\)). The free charge might consist of electrons on a conductor or ions embedded in the dielectric material or whatever; any charge, in other words, that is not a result of polarization. Within the dielectric, the total charge density can be written\n\\[\\rho = \\rho_b \u0026#43; \\rho_f \\tagl{4.20}\\] and Gauss\u0026rsquo;s law reads\n\\[\\epsilon_0 \\div \\vec{E} = \\rho = \\rho_b \u0026#43; \\rho_f = - \\div \\vec{P} \u0026#43; \\rho_f\\] where E is now the total field, not just that portion generated by polarization.\nIt is convenient to combine the two divergence terms:\n\\[\\div (\\epsilon_0 \\vec{E} \u0026#43; \\vec{P}) = \\rho_f\\] The expression in parentheses is known as the electric displacement and is designated by the letter D:\n\\[\\vec{D} \\equiv \\epsilon_0 \\vec{E} \u0026#43; \\vec{P} \\tagl{4.21}\\] In terms of D, Gauss\u0026rsquo;s law then reads\n\\[\\div \\vec{D} = \\rho_f \\tagl{4.22}\\] or in integral form\n\\[\\oint \\vec{D} \\cdot \\dd \\vec{a} = Q_{f_{enc}} \\tagl{4.23}\\] where \\( Q_{f_{enc}} \\) denotes the total free charge enclosed in the volume. This is a particularly useful way to express Gauss\u0026rsquo;s law, in the context of dielectrics, because it makes reference only to the free charges, and free charge is the stuff we control. Bound charge comes along for the ride: when we put the free charge in place, a certain polarization automatically arises, by the mechanisms of Sect 4.1, and this polarization produces the bound charge. In a typical problem, therefore, we know \\( \\rho_f \\), but we do not (initially) know \\( \\rho_b \\); \\( \\eqref{4.23} \\) lets us go right to work with the information at hand. In particular, whenever the requisite symmetry is present, we can immediately calculate \\(\\vec D\\) by the standard Gauss\u0026rsquo;s law methods.\nExample 4.4 # Q A long straight wire, carrying uniform line charge \\( \\lambda \\), is surrounded by rubber insulation out to a radius a (Fig 4.17). Find the electric displacement.\nA Drawing a cylindrical Gaussian surface, of radius s and length L, and applying \\( \\eqref{4.23} \\) we find\n\\[D( 2\\pi s L) = \\lambda L\\] Therefore\n\\[\\vec{D} = \\frac{\\lambda}{2 \\pi s} \\vu{s} \\tagl{4.24}\\] Notice that this formula holds both within the insulation and outside it. In the latter region, \\( \\vec{P} = 0 \\) so\n\\[\\vec{E} =\\frac{1}{\\epsilon_0} \\vec{D} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s}, \\quad \\text{ for } s \u0026gt; a\\] Inside the rubber, the electric field cannot be determined, since we do not know P.\nHold on a tick! We got all the way to a field we can calculate by Gauss\u0026rsquo;s law, but we have left out the surface bound charge \\( \\sigma_b \\). What happened to it? To be more precise, \\( \\eqref{4.22} \\) works within a dielectric, but we cannot apply Gauss\u0026rsquo;s law precisely at the boundary of the dielectric, because the local \\( \\rho_b \\) blows up there, taking \\( \\div \\vec{E} \\) with it. The polarization drops abruptly to zero outside the material, so its derivative is a delta function. The surface bound charge is precisely this term, so in this sense it is actually included in \\( \\rho_b \\), but we ordinarily prefer to handle it separately as \\( \\sigma_b \\). We could even picture the edge of the dielectric as having some finite thickness, within which the polarization drops off to zero (which is probably a more realistic model anyway), in which case there is no \\( \\sigma_b \\), \\( \\rho_b \\) varies rapidly but smoothly, and Gauss\u0026rsquo;s law can safely be applied everywhere. In any case, we can use \\( \\eqref{4.23} \\) safely without fear of this \u0026ldquo;defect.\u0026rdquo;\n4.3.2: A Deceptive Parallel # Our expression for the divergence of the displacement looks just like Gauss\u0026rsquo;s law, only the total charge density \\( \\rho \\) is replaced by the free charge density \\( \\rho_f \\), and \\( \\vec{D} \\) is substituted for \\( \\epsilon_0 \\vec{E} \\). For this reason, you may be tempted to conclude that D is \u0026ldquo;just like\u0026rdquo; E (apart from the factor \\( \\epsilon_0 \\)), except that its source is \\( \\rho_f \\) instead of \\( \\rho \\). That is, it\u0026rsquo;s tempting to say \u0026ldquo;To solve problems involving dielectrics, you just forget all about the bound charge - calculate the field as you ordinarily would, only call the answer D instead of E.\u0026rdquo; This reasoning is seductive, but the conclusion is false; in particular there is no \u0026ldquo;Coulomb\u0026rsquo;s law\u0026rdquo; for D:\n\\[\\vec{D}(\\vec{r}) \\neq \\frac{1}{4 \\pi} \\int \\frac{\\vu{\\gr}}{\\gr ^2} \\rho_f(\\vec{r\u0026#39;}) \\dd \\tau\u0026#39;\\] This is because the divergence alone is insufficient to determine a vector field; you need to know its curl as well. One tends to forget this in the case of electrostatics because we usually don\u0026rsquo;t care about the curl of E anyway. But the curl of D is not always zero, even in electrostatics, since there is no reason, in general, to suppose that the curl of P vanishes:\n\\[\\curl \\vec{D} = \\epsilon_0 (\\curl \\vec{E}) \u0026#43; (\\curl \\vec{P}) = \\curl \\vec{P} \\tagl{4.25}\\] Sometimes it does, but more often it does not. The bar electret of Prob 4.11 is one example of this: here there is no free charge anywhere, so if you really believe that the only source of D is \\( \\rho_f \\) you will be forced to conclude that \\( \\vec{D} = 0 \\) everywhere, and hence that \\( \\vec{E} = (-1 / \\epsilon_0) \\vec{P} \\) inside and \\( \\vec{E} = 0 \\) outside the electret, which is obviously wrong. And because \\( \\curl \\vec{D} \\neq 0 \\) in general, D cannot be expressed as the gradient of a scalar - there is no \u0026ldquo;potential\u0026rdquo; for D.\nAdvice: When you are asked to compute the electric displacement, first look for symmetry. If the problem exhibits spherical, cylindrical, or plane symmetry, then you can get D directly from Eq. 4.23 by the usual Gauss\u0026rsquo;s law methods. (Evidently in such cases \\( \\curl \\vec{P} \\) is automatically zero, but since symmetry alone dictates the answer, you\u0026rsquo;re not really obliged to worry about the curl.) If the requisite symmetry is absent, you\u0026rsquo;ll have to think of another approach, and, in particular, you must not assume that D is determined exclusively by the free charge.\n4.3.3: Boundary Conditions # The electrostatic boundary conditions we had in Sect 2.3 can be re-cast in terms of D. \\( \\eqref{4.23} \\) tells us the discontinuity in the component perpendicular to an interface:\n\\[D_{above} ^{\\perp} - D_{below} ^{\\perp} = \\sigma_f \\tagl{4.26}\\] while \\( \\eqref{4.25} \\) gives the discontinuity in parallel components:\n\\[\\vec{D}_{above} ^{\\parallel} - \\vec{D}_{below} ^{\\parallel} = \\vec{P}_{above} ^{\\parallel} - \\vec{P}_{below} ^{\\parallel} \\tagl{4.27}\\] In the presence of dielectrics, these are sometimes more useful than the corresponding boundary conditions on E (Eqs 2.31 and 2.32):\n\\[E_{above} ^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\tagl{4.28}\\] and\n\\[\\vec{E}_{above} ^{\\parallel} - \\vec{E}_{below} ^{\\parallel} = 0 \\tagl{4.29}\\] "},{"id":65,"href":"/r/notes/griffiths/ch4-4/","title":"The Linear Dielectrics","section":"Griffiths Introduction to Electrodynamics","content":" 4.4: Linear Dielectrics # 4.4.1: Susceptibility, Permittivity, Dielectric Constant # In the first few sections of this chapter we did not commit ourselves as to the cause of P; we dealt only with the effects of polarization. From the qualitative essence of 4.1, though, we know that the polarization of a dielectric ordinarily results from an electric field, which lines up the atomic or molecular dipoles. For many substances, in fact, the polarization is proportional to the field, provided E is not too strong:\n\\[\\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\tagl{4.30}\\] The constant of proportionality, \\( \\chi_e \\), is called the electric susceptibility of the medium (a factor of \\( \\epsilon_0 \\) has been extracted to make \\( \\chi_e \\) dimensionless). The value of \\( \\chi_e \\) depends on the microscopic structure of the substance in question (and also on external conditions such as temperature). I shall call materials that obey \\( \\eqref{4.30} \\) linear dielectrics.\nIn modern optical applications, especially, nonlinear materials have become increasingly important. For these there is a second term relating P to E - typically a cubic term. In general, Eq 4.30 can be regarded as the first (nonzero) term in the Taylor expansion of P in powers of E.\nNote that E in \\( \\eqref{4.30} \\) is the total field; it may be due in part to free charges and in part to the polarization itself. If, for instance, we put a piece of dielectric into an external field \\( \\vec{E_0} \\), we cannot compute P directly from the linear susceptibility relation; the external field will polarize the material, and this polarization will produce its own field, which then contributes to the total field, and this in turn modifies the polarization, which\u0026hellip; Breaking out of this infinite regress is not always easy. You\u0026rsquo;ll see some examples in a moment. The simplest approach is to begin with the displacement, at least in those cases where D can be deduced directly from the free charge distribution. In linear media we have\n\\[\\vec{D} = \\epsilon_0 \\vec{E} \u0026#43; \\vec{P} = \\epsilon_0 \\vec{E} \u0026#43; \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 (1 \u0026#43; \\chi_e) \\vec{E} \\tagl{4.31}\\] so D is also proportional to E\n\\[\\vec{E} = \\epsilon \\vec{E} \\tagl{4.32}\\] where\n\\[\\epsilon \\equiv \\epsilon_0 (1 \u0026#43; \\chi_e) \\tagl{4.33}\\] This new constant \\( \\epsilon \\) is called the permittivity of the material. (In vacuum, where there is no matter to polarize, the susceptibility is zero, and the permittivity is \\( \\epsilon_0 \\). That\u0026rsquo;s why \\( \\epsilon_0 \\) is called the permittivity of free space. I dislike the term, for it suggest that the vacuum is just a special kind of linear dielectric, in which the permittivity happens to have the value \\( 8.85 \\times 10^{-12} C^2 / N \\cdot m^2 \\) .) If you remove a factor of \\( \\epsilon_0 \\), the remaining dimensionless quantity\n\\[\\epsilon_r = 1 \u0026#43; \\chi _e = \\frac{\\epsilon}{\\epsilon_0} \\tagl{4.34}\\] is called the relative permittivity, or dielectric constant, of the material. Dielectric constants for some common substances are listed in Table 4.2. (Notice that \\( \\epsilon_r \\) is greater than 1, for all ordinary materials.) Of course, the permittivity and the dielectric constant do not convey any information that was not already available in the susceptibility, nor is there anything essentially new in Eq 4.32: the physics of linear dielectrics is all contained in \\( \\eqref{4.30} \\)\nExample 4.5 # Q A metal sphere of radius a carries a charge Q (Fig 4.20). It is surrounded, out to radius b, by linear dielectric material of permittivity \\( \\epsilon \\). Find the potential at the center (relative to infinity).\nA To compute V, we need to know E; to find E, we might first try to locate the bound charge; we could get the bound charge from P, but we can\u0026rsquo;t calculate P unless we already know E. What we do know is the free charge, and our arrangement is spherically symmetric, so we can go straight for D using Eq 4.23:\n\\[\\vec{D} = \\frac{Q}{4 \\pi r^2} \\vu{r}, \\quad \\text{ for all points } r \u0026gt; a\\] Inside the conducting sphere, all our electrostatic fields are zero. We then obtain E via Eq 4.32:\n\\[\\vec{E} = \\begin{cases} \\frac{Q}{4 \\pi \\epsilon r^2} \\vu{r} \u0026amp; \\quad \\text{ for } a \u0026lt; r \u0026lt; b \\\\ \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r} \u0026amp; \\quad \\text{ for } r \u0026gt; b \\end{cases}\\] We get the potential at the center by integrating E\n\\[V = - \\int _{\\infty} ^0 \\vec{E} \\cdot \\dd \\vec{l} = \\\\ - \\int _{\\infty} ^b \\left( \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\right) \\dd r - \\int_b ^a \\left( \\frac{Q}{4 \\pi \\epsilon r^2} \\right) \\dd r\\\\ = \\frac{Q}{4 \\pi } \\left( \\frac{1}{\\epsilon_0 b} \u0026#43; \\frac{1}{\\epsilon a} - \\frac{1}{\\epsilon b} \\right)\\] In this case, we didn\u0026rsquo;t need to compute the polarization or the bound charge explicitly, but we can easily do so now that we have E:\n\\[\\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon r^2} \\vu{r}\\] within the dielectric, so that\n\\[\\rho_b = - \\div \\vec{P} = 0\\] and\n\\[\\sigma_b = \\vec{P} \\cdot \\vu{n} = \\begin{cases} \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon b^2} \u0026amp; \\qquad \\text{ at the outer surface } \\\\ \\frac{- \\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon a^2} \u0026amp; \\qquad \\text{ at the inner surface } \\end{cases}\\] Notice that the surface bound charge at a is negative (\\( \\vu{n} \\) points outward with respect to the dielectric, which is \\( + \\vu{n} \\) at b, but \\( -\\vu{r} \\) at a). This is natural, since the charge on the metal sphere attracts its opposite in all the dielectric molecules. It is this layer of negative charge that reduces the field, within the dielectric, from \\( 1 / 4 \\pi \\epsilon_0 (Q / r^2) \\vu{r} \\) to \\( 1 / 4 \\pi \\epsilon (Q / r^2) \\vu{r} \\). In this respect, a dielectric is rather like an imperfect conductor: on a conducting shell the induced surface charge would be such as to cancel out the field of Q entirely in the region \\( a \u0026lt; r \u0026lt; b \\); the dielectric does the best it can, but the cancellation is only partial.\nSince linear dielectrics give us cases where P and D are proportional to E, you might suppose that linear dielectrics escape the defect in the parallel between E and D. Does it not follow that their curls, like E\u0026rsquo;s, must vanish? Unfortunately, it does not, for the line integral of P around a closed path that straddles the boundary between one type of material and another need not be zero, even though the integral of E around the same loop must be. The reason is that the proportionality factor \\( \\epsilon_0 \\chi_e \\) is different on the two sides. For instance, at the interface between a polarized dielectric and the vacuum (Fig 4.21), P is zero on one side but not on the other. Around this loop, \\( \\oint \\vec{P} \\cdot \\dd \\vec{l} \\neq 0 \\), and hence, by Stokes\u0026rsquo; theorem, the curl of P cannot vanish everywhere within the loop (in fact, it is infinite at the boundary).\nOf course, if space is entirely filled with a homogeneous linear dielectric, then this objection is void; in this rather special circumstance\n\\[\\div \\vec{D} = \\rho_f \\quad \\text{and} \\quad \\curl \\vec{D} = 0\\] so D can be found from the free charge just as though the dielectric were not there:\n\\[\\vec{D} = \\epsilon_0 \\vec{E_{vac}}\\] where \\( \\vec{E_{vac}} \\) is the field the same charge distribution would produce in the absence of any dielectric. According to \\( \\eqref{4.32} \\) and \\( \\eqref{4.34} \\), therefore,\n\\[\\vec{E} = \\frac{1}{\\epsilon} \\vec{D} = \\frac{1}{\\epsilon_r} \\vec{E_{vac}} \\tagl{4.35}\\] Conclusion: when all space is filled with a homogeneous linear dielectric, the field everywhere is simply reduced by a factor of one over the dielectric constant. (Actually it\u0026rsquo;s not necessary for the dielectric to fill all space; in regions where the field is zero anyway, it can hardly matter whether the dielectric is present or not, since there\u0026rsquo;s no polarization in any event.)\nFor example, if a free charge q is embedded in a large dielectric, the field it produces is\n\\[\\vec{E} = \\frac{1}{4 \\pi \\epsilon} \\frac{q}{r^2} \\vu{r} \\tagl{4.36}\\] (that\u0026rsquo;s \\( \\epsilon \\), not \\( \\epsilon_0 \\)), and the force it exerts on nearby charges is reduced accordingly. But it\u0026rsquo;s not that there is anything wrong with Coulomb\u0026rsquo;s law; rather, the polarization of the medium partially \u0026ldquo;shields\u0026rdquo; the charge, by surrounding it with bound charge of the opposite sign (Fig 4.22)\nExample 4.6 # Q A parallel-plate capacitor (Fig 4.23) is filled with insulating material of dielectric constant \\( \\epsilon_r \\). What effect does this have on its capacitance?\nA Since the field is confined to the space between the plates, the dielectric will reduce E, and hence also the potential difference V, by a factor \\( 1 / \\epsilon_r \\). Accordingly, the capacitance \\( C = Q / V \\) is increased by a factor of the dielectric constant\n\\[C = \\epsilon_r C_{vac} \\tagl{4.37}\\] This is, in fact, a common way to beef up a capacitor\nA crystal is generally easier to polarize in some directions than others, and in this case Eq 4.30 is replaced by the general linear relation\n\\[\\begin{aligned} P_x \u0026amp; = \\epsilon_0 (\\chi_{e,xx} E_x \u0026#43; \\chi_{e, xy} E_y \u0026#43; \\chi_{e, xz} E_z) \\\\ P_y \u0026amp; = \\epsilon_0 (\\chi_{e,yx} E_x \u0026#43; \\chi_{e, yy} E_y \u0026#43; \\chi_{e, yz} E_z) \\\\ P_z \u0026amp; = \\epsilon_0 (\\chi_{e,zx} E_x \u0026#43; \\chi_{e, zy} E_y \u0026#43; \\chi_{e, zz} E_z) \\\\ \\end{aligned} \\tagl{4.38}\\] just as Eq. 4.1 was superseded by Eq. 4.3 for asymmetrical molecules. The nine coefficients constitute the susceptibility tensor\n4.4.2: Boundary Value Problems with Linear Dielectrics # In a (homogeneous isotropic) linear dielectric, the bound charge density is proportional to the free charge density\n\\[\\rho_b = - \\div \\vec{P} = - \\div \\left( \\epsilon_0 \\frac{\\chi_e}{\\epsilon} \\vec{D} \\right) = - \\left( \\frac{\\chi_e}{1 \u0026#43; \\chi_e} \\right) \\rho_f \\tagl{4.39}\\] In particular, unless free charge is actually embedded in the material, \\( \\rho = 0 \\) and any net charge must reside at the surface. Within such a dielectric, then, the potential obeys Laplace\u0026rsquo;s equation, and all the machinery of Chapter 3 carries over. It is convenient, however, to rewrite the boundary conditions in a way that makes reference only to the free charge. Equation 4.26 says\n\\[\\epsilon_{above} E_{above} ^{\\perp} - \\epsilon_{below} E_{below} ^{\\perp} = \\sigma_f \\tagl{4.40}\\] or, in terms of the potential,\n\\[\\epsilon_{above} \\pdv{V_{above}}{n} - \\epsilon_{below} \\pdv{V_{below}}{n} = - \\sigma_f \\tagl{4.41}\\] whereas the potential itself is, of course, continuous (Eq 2.34):\n\\[V_{above} = V_{below} \\tagl{4.42}\\] Example 4.7 # Q A sphere of homogeneous linear dielectric material is placed in an otherwise uniform electric field \\( \\vec{E_0} \\) (Fig 4.27). Find the electric field inside the sphere\nA This is very similar to Ex 3.8, in which an uncharged conducting sphere was introduced into a uniform field. In that case, the field of the induced charge canceled \\( \\vec{E_0} \\) within the sphere. In a dielectric, the cancellation from the bound charge is incomplete.\nOur problem is to solve Laplace\u0026rsquo;s equation, for \\( V_{in}(r, \\theta) \\) when \\( r \\leq R \\) and \\( V_{out}(r, \\theta) \\) when \\( r \\geq R \\), subject to the boundary conditions\n\\[\\tag{i} V_{in} = V_{out} \\qquad \\text{ at } r = R\\] \\[\\tag{ii} \\epsilon \\pdv{V_{in}}{r} = \\epsilon_0 \\pdv{V_{out}}{r} \\qquad \\text{ at } r = R\\] \\[\\tag{iii} V_{out} \\rightarrow - E_0 r \\cos \\theta \\qquad \\text{ for } r \\gg R\\] (The second of these follows from Eq 4.41, since there is no free charge at the surface.) Inside the sphere, Eq 3.65 says\n\\[V_{in}(r, \\theta) = \\sum_{l=0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{4.44}\\] outside the sphere, in view of (iii), we have\n\\[V_{out}(r, \\theta) = - E_0 r \\cos \\theta \u0026#43; \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l\u0026#43;1}} P_l(\\cos \\theta) \\tagl{4.45}\\] Boundary condition (i) requires that\n\\[\\sum_{l=0} ^{\\infty} A_l R^l P_l(\\cos \\theta) = - E_0 R \\cos \\theta \u0026#43; \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l\u0026#43;1}} P_l(\\cos \\theta)\\] so\n\\[A_l R^l = \\frac{B_l}{R_{l\u0026#43;1}}, \\qquad \\text{ for } l \\neq 1 \\\\ A_1 R = - E_0 R \u0026#43; \\frac{B_1}{R^2} \\tagl{4.46} \\] Meanwhile, condition (ii) yields\n\\[\\epsilon_r \\sum_{l=0} ^\\infty l A_l R^{l-1} P_l (\\cos \\theta) = - E_0 \\cos \\theta - \\sum_{l=0} ^\\infty \\frac{(l\u0026#43;1) B_l}{R^{l\u0026#43;2}} P_l(\\cos theta)\\] so\n\\[\\begin{aligned} \\epsilon_r l A_l R^{l-1} \u0026amp; = - \\frac{(l\u0026#43;1) B_l }{B^{l\u0026#43;2}} , \\text{ for } l \\neq 1 \\\\ \\epsilon_r A_1 \u0026amp; = - E_0 - \\frac{2 B_1}{R^3} \\end{aligned} \\tagl{4.47}\\] It follows that\n\\[A_l = B_l = 0 \\qquad \\text{ for } l \\neq 1\\\\ A_1 = - \\frac{3}{\\epsilon_r \u0026#43; 2} E_0 \\quad B_1 = \\frac{\\epsilon_r - 1}{\\epsilon_r \u0026#43; 2} R^3 E_0 \\tagl 4.48\\] Evidently\n\\[V_{in} (r, \\theta) = - \\frac{3 E_0}{\\epsilon_r \u0026#43; 2} r \\cos \\theta = - \\frac{3E_0}{\\epsilon_r \u0026#43; 2} z\\] We should be used to finding that the field within a polarized sphere is uniform, but it\u0026rsquo;s still a surprising result:\n\\[\\vec{E} = \\frac{3}{\\epsilon_r \u0026#43; 2} \\vec{E_0} \\tagl{4.49}\\] Example 4.8 # Q Suppose the entire region below the plane \\( z = 0 \\) in Fig 4.28 is filled with uniform linear dielectric material of susceptibility \\( \\chi_e \\). Calculate the force on a point charge q situated a distance d above the origin.\nA The surface bound charge on the xy plane is of opposite sign to q, so the force will be attractive. (In view of Eq 4.39, there is no volume bound charge.) Let us first calculate \\( \\sigma_b \\), using \\( \\eqref{4.11} \\) and \\( \\eqref{4.30} \\):\n\\[\\sigma_b = \\vec{P} \\cdot \\vu{n} = P_z = \\epsilon_0 \\chi_e E_z\\] where \\( E_z \\) is the z-component of the total field inside the dielectric, at \\( z = 0 \\). This field is due in part to q and in part to the bound charge itself. From Coulomb\u0026rsquo;s law, the former contribution is\n\\[- \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{(r^2 \u0026#43; d^2)} \\cos \\theta = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 \u0026#43; d^2)^{3/2}} \\] where \\( r = \\sqrt{x^2 + y^2} \\) is the distance from the origin. We can immediately read off the z-component of the field of the bound charge using Gauss\u0026rsquo;s law as \\( - \\sigma_b / 2 \\epsilon_0 \\). Thus,\n\\[\\sigma_b = \\epsilon_0 \\chi_e \\left[ - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 \u0026#43; d^2)^{3/2}} - \\frac{\\sigma_b}{2 \\epsilon_0} \\right]\\] which we can solve for \\( \\sigma_b \\)\n\\[\\sigma_b = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e \u0026#43; 2} \\right) \\frac{qd}{(r^2 \u0026#43; d^2)^{3/2}} \\tagl{4.50} \\] Apart from the factor \\( \\chi_e / (\\chi_e + 2) \\) this is exactly the same as the induced charge on an infinite conducting plane under similar circumstances (Eq 3.10). Evidently the total bound charge is\n\\[q_b = - \\left( \\frac{\\chi_e}{\\chi_e \u0026#43; 2} \\right)q \\tagl{4.51}\\] We could, of course, get the field of \\( \\sigma_b \\) by direct integration\n\\[\\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\left( \\frac{\\vu{\\gr}}{\\gr ^2} \\sigma_b \\dd a \\right)\\] But, as in the case of the conducting plane, there is a nicer solution by the method of images. If we replace the dielectric by a single point charge \\( q_b \\) at the image position (0, 0, -d), we have\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43;(z-d)^2}} \u0026#43; \\frac{q_b}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; (z \u0026#43; d)^2}} \\right] \\tagl{4.52} \\] in the region \\( z \u0026gt; 0 \\). Meanwhile, a charge \\( (q + q_b) \\) at (0, 0, d) yields the potential\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q \u0026#43; q_b}{\\sqrt{x^2 \u0026#43; y^2 \u0026#43; (z - d)^2}} \\right] \\tagl{4.53} \\] for the region \\( z \u0026lt; 0 \\). Taken together, \\( \\eqref{4.52} \\) and \\( \\eqref{4.53} \\) constitute a function that satisfies Poisson\u0026rsquo;s equation with a point charge q at (0, 0, d), which goes to zero at infinity, which is continuous at the boundary \\( z = 0 \\), and whose normal derivative exhibits the discontinuity appropriate to a surface charge \\( \\sigma_b \\) at \\( z = 0 \\):\n\\[- \\epsilon_0 \\left( \\left. \\pdv{V}{z} \\right| _{z = 0\u0026#43;} - \\left. \\pdv{V}{z} \\right|_{z = 0-} \\right) = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e \u0026#43; 2} \\right)\\] Accordingly, this is the correct potential for our problem. In particular, the force on q is:\n\\[\\vec{F} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q q_b}{(2d)^2} \\vu{z} = - \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{\\chi_e}{\\chi_e \u0026#43; 2} \\right) \\frac{q^2}{4 d^2} \\vu{z} \\tagl{4.54}\\] I do not claim to have provided a compelling motivation for \\( \\eqref{4.52} \\) and \\( \\eqref{4.53} \\) - like all image solutions, this one owes its justification to the fact that it works: it solves Poisson\u0026rsquo;s equation, and it meets the boundary conditions. Still, discovering an image solution is not entirely a matter of guesswork. There are at least two \u0026ldquo;rules of the game\u0026rdquo;: (1) You must never put an image charge into the region where you\u0026rsquo;re computing the potential. (2) The image charges must add up to the correct total in each region.\n4.4.3: Energy in Dielectric Systems # It takes work to charge up a capacitor (Eq 2.55):\n\\[W = \\frac{1}{2} C V^2\\] If the capacitor is filled with linear dielectric, its capacitance exceeds the vacuum value by a factor of the dielectric constant\n\\[C = \\epsilon_r C_{vac}\\] as we found in Ex. 4.6. Evidently, the work necessary to charge a dielectric-filled capacitor is increased by the same factor. The reason is pretty clear: you have to pump more (free) charge, to achieve a given potential, because part of the field is canceled off by the bound charges.\nIn chapter 2 we got a general formula for the energy stored in any electrodynamic system\n\\[W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd \\tau \\tagl{4.55} \\] The case of the dielectric-filled capacitor suggests that this should be changed to\n\\[W = \\frac{\\epsilon_0}{2} \\int \\epsilon_r E^2 \\dd \\tau = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau\\] in the presence of linear dielectrics. To prove it, suppose the dielectric material is fixed in position, and we bring in the free charge, a bit at a time. As \\( \\rho_f \\) is increased by an amount \\( \\Delta \\rho_f \\), the polarization will change and with it the bound charge distribution; but we\u0026rsquo;re interested only in the work done on the incremental free charge:\n\\[\\Delta W = \\int (\\Delta \\rho_f) V \\dd \\tau \\tagl{4.56}\\] Since \\( \\div \\vec{D} = \\rho_f, \\Delta \\rho_f = \\div (\\Delta \\vec{D}) \\), where \\( \\Delta \\vec{D} \\) is the resulting change in D, so\n\\[\\Delta W = \\int [ \\div ( \\Delta D) ] V \\dd \\tau\\] Now\n\\[\\div [ (\\Delta D) V ] = [ \\div (\\Delta \\vec{D})] V \u0026#43; \\Delta \\vec{D} \\cdot (\\grad V)\\] and hence, integrating by parts,\n\\[\\Delta W = \\in \\div [ (\\Delta \\vec{D}) V] \\dd \\tau \u0026#43; \\int ( \\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau\\] The divergence theorem turns the first term into a surface integral, which vanishes if we integrate over all space. Therefore, the work done is equal to\n\\[\\Delta W = \\int (\\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau \\tagl{4.57}\\] So far, this applies to any material. In the specific case of a linear dielectric,\n\\[\\frac{1}{2} \\Delta ( \\vec{D} \\cdot \\vec{E} ) = \\frac{1}{2} \\Delta (\\epsilon E^2) = \\epsilon (\\Delta \\vec{E}) \\cdot \\vec{E} = ( \\Delta \\vec{D}) \\cdot \\vec{E}\\] (for infinitesimal increments). Thus\n\\[\\Delta W = \\Delta \\left( \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\right)\\] The total work done, then, as we build the free charge up from zero to the final configuration is\n\\[W = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\tagl{4.58}\\] as anticipated.\nIt may puzzle you that Eq. 4.55, which we derived quite generally in Chapter 2, does not seem to apply in the presence of dielectrics, where it is replaced by Eq. 4.58. The point is not that one or the other of these equations is wrong, but rather that they address somewhat different questions. The distinction is subtle, so let\u0026rsquo;s go right back to the beginning: What do we mean by \u0026ldquo;the energy of a system\u0026rdquo;? Answer: It is the work required to assemble the system. Very well - but when dielectrics are involved, there are two quite different ways one might construe this process:\nWe bring in all the charges (free and bound), one by one, with tweezers, and glue each one down in its proper final location. If this is what you mean by \u0026ldquo;assemble the system,\u0026rdquo; then Eq. 4.55 is your formula for the energy stored. Notice, however, that this will not include the work involved in stretching and twisting the dielectric molecules (if we picture the positive and negative charges as held together by tiny springs, it does not include the spring energy, \\( \\frac{1}{2} k x^2 \\), associated with polarizing each molecule). With the unpolarized dielectric in place, we bring in the free charges, one by one, allowing the dielectric to respond as it sees fit. If this is what you mean by \u0026ldquo;assemble the system\u0026rdquo; (and ordinarily it is, since free charge is what we actually push around), then Eq. 4.58 is the formula you want. In this case the \u0026ldquo;spring\u0026rdquo; energy is included, albeit indirectly, because the force you must apply to the free charge depends on the disposition of the bound charge; as you move the free charge, you are automatically stretching those \u0026ldquo;springs.\u0026rdquo; Example 4.9 # Q A sphere of radius R is filled with material of dielectric constant \\( \\epsilon_r \\) and uniform embedded free charge \\( \\rho_f \\). What is the energy of this configuration? A From Gauss\u0026rsquo;s law, the displacement is\n\\[\\vec{D}(r) = \\begin{cases} \\frac{\\rho_f}{3} \\vec{r} \u0026amp; \\qquad ( r \u0026lt; R) \\\\ \\frac{\\rho_f}{3} \\frac{R^3}{r^2} \\vu{r} \u0026amp; \\qquad ( r \u0026gt; R ) \\end{cases}\\] So the electric field is\n\\[\\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} \u0026amp; \\qquad ( r \u0026lt; R) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{R^3}{r^2} \\vu{r} \u0026amp; \\qquad ( r \u0026gt; R ) \\end{cases}\\] The purely electrostatic energy is\n\\[\\begin{aligned} W \u0026amp; = \\frac{\\epsilon_0}{2} \\left[ \\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) ^2 \\int _0 ^R r^2 4 \\pi r^2 \\dd r \u0026#43; \\left( \\frac{\\rho_f}{3 \\epsilon_0} \\right)^2 R^6 \\int_R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ \u0026amp; = \\frac{2 \\pi}{9 \\epsilon_0} \\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r ^2} \u0026#43; 1 \\right) \\end{aligned}\\] But the total energy (Eq 4.58) is\n\\[\\begin{aligned} W_2 \u0026amp; = \\frac{1}{2} \\left[ \\left( \\frac{\\rho_f}{3} \\right)\\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) \\int_0 ^R r^2 4 \\pi r ^2 \\dd r \u0026#43; \\left( \\frac{\\rho_f R^3}{3} \\right) \\left( \\frac{\\rho_f R^3}{3 \\epsilon_0} \\right) \\int _R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ \u0026amp; = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} \u0026#43; 1 \\right) \\end{aligned}\\] Notice that \\( W_1 \u0026lt; W_2 \\) - that\u0026rsquo;s because \\( W_1 \\) does not include the energy involved in stretching the molecules.\nLet\u0026rsquo;s check that \\( W_2 \\) is the work done on the free charge in assembling the system. We start with the (uncharged, unpolarized) dielectric sphere, and bring in the free charge in infinitesimal installments (dq), filling out the sphere layer by layer. When we have reached radius \\( r\u0026rsquo; \\) , the electric field is\n\\[\\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} \u0026amp; \\quad (r \u0026lt; r\u0026#39;) \\\\ \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\frac{(r\u0026#39;) ^3}{r^2} \\vu{r} \u0026amp; \\quad (r\u0026#39; \u0026lt; r \u0026lt; R ) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{(r\u0026#39;) ^3}{r^2} \\vu{r} \u0026amp; \\quad ( r \u0026gt; R) \\end{cases}\\] The work required to bring the next dq in from infinity to \\( r\u0026rsquo; \\) is\n\\[\\begin{aligned} \\dd W \u0026amp; = - \\dd q \\left[ \\int_{\\infty} ^R \\vec{E} \\cdot \\dd \\vec{l} \u0026#43; \\int _R ^{r\u0026#39;} \\vec{E} \\cdot \\dd \\vec{l} \\right] \\\\ \u0026amp; = - \\dd q \\left[ \\frac{\\rho_f (r\u0026#39;) ^3}{3 \\epsilon_0} \\int_{\\infty} ^R \\frac{1}{r^2} \\dd r \u0026#43; \\frac{\\rho_f (r\u0026#39;) ^3}{3 \\epsilon_0 \\epsilon_r} \\int _{R} ^{r\u0026#39;} \\frac{1}{r^2} \\dd r \\right] \\\\ \u0026amp; = \\frac{\\rho_f (r\u0026#39;) ^3}{3 \\epsilon_0} \\left[ \\frac{1}{R} \u0026#43; \\frac{1}{\\epsilon_r} \\left( \\frac{1}{r\u0026#39;} - \\frac{1}{R} \\right) \\right] \\dd q \\end{aligned}\\] This increases the radius \\( (r\u0026rsquo;) \\)\n\\[\\dd q = \\rho_f 4 \\pi (r\u0026#39;) ^2 \\dd r\u0026#39;\\] so the total work done, in going from \\( r\u0026rsquo;=0 \\) to \\( r\u0026rsquo; = R \\) is\n\\[\\begin{aligned} W \u0026amp; = \\frac{4 \\pi \\rho_f ^2}{3 \\epsilon_0} \\left[ \\frac{1}{R} \\left( 1 - \\frac{1}{ \\epsilon_r} \\right) \\int_0 ^R (r\u0026#39;) ^5 \u0026#43; \\frac{1}{\\epsilon_r} \\int_0 ^R (r\u0026#39;) ^4 \\dd r\u0026#39; \\right] \\\\ \u0026amp; = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} \u0026#43; 1 \\right) = W_2 \\end{aligned}\\] Evidently the energy \u0026ldquo;stored in the springs\u0026rdquo; is\n\\[W_{sprint} = W_2 - W_1 = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho _f ^2 R^5 (\\epsilon_r - 1) \\] I would like to confirm this in an explicit model. Picture the dielectric as a collection of tiny proto-dipoles, each consisting of +q and -q attached to a spring of constant k and equilibrium length 0, so in the absence of any field the positive and negative ends coincide. One end of each dipole is nailed in position (like the nuclei in a solid), but the other end is free to move in response to any imposed field. Let \\( \\dd \\tau \\) be the volume assigned to each proto-dipole (the dipole itself may occupy only a small portion of this space).\nWith the field turned on, the electric force on the free end is balanced by the spring force; the charges separate by a distance \\( d: qE = kd \\). In our case\n\\[\\vec{E} = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r}\\] The resulting dipole moment is \\( p = qd \\) and the polarization is \\( P = p / \\dd \\tau \\) so\n\\[k = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r d^2} P r \\dd \\tau\\] The energy of this particular spring is\n\\[\\dd W_{spring} = \\frac{1}{2} k d^2 = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} P r \\dd \\tau\\] and hence the total is\n\\[W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\int P r \\dd \\tau\\] Now\n\\[\\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 \\chi_e \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\vec{r} = \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} \\vec{r}\\] so\n\\[W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} 4 \\pi \\int_0 ^R r^4 \\dd r = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho_f ^2 R^5 (\\epsilon_r - 1) \\] and it works out perfectly.\nIt is sometimes alleged that Eq. 4.58 represents the energy even for nonlinear dielectrics, but this is false: To proceed beyond Eq. 4.57, one must assume linearity. In fact, for dissipative systems the whole notion of \u0026ldquo;stored energy\u0026rdquo; loses its meaning, because the work done depends not only on the final configuration but on how it got there. If the molecular \u0026ldquo;springs\u0026rdquo; are allowed to have some friction, for instance, then \\( W_{spring} \\) can be made as large as you like, by assembling the charges in such a way that the spring is obliged to expand and contract many times before reaching its final state. In particular, you get nonsensical results if you try to apply Eq. 4.58 to electrets, with frozen-in polarization (see Prob. 4.27).\n4.4.4: Forces on Dielectrics # Just as a conductor is attracted into an electric field (Eq. 2.51), so too is a dielectric - and for essentially the same reason: the bound charge tends to accumulate near the free charge of the opposite sign. But the calculation of forces on dielectrics can be surprisingly tricky. Consider, for example, the case of a slab of linear dielectric material, partially inserted between the plates of a parallel-plate capacitor (Fig. 4.30). We have always pretended that the field is uniform inside a parallel-plate capacitor, and zero outside. If this were literally true, there would be no net force on the dielectric at all, since the field everywhere would be perpendicular to the plates. However, there is in reality a fringing field around the edges, which for most purposes can be ignored but in this case is responsible for the whole effect. (Indeed, the field could not terminate abruptly at the edge of the capacitor, for if it did, the line integral of E around the closed loop shown in Fig. 4.31 would not be zero.) It is this nonuniform fringing field that pulls the dielectric into the capacitor.\nFringing fields are notoriously difficult to calculate; luckily, we can avoid this altogether, by the following ingenious method. Let W be the energy of the system - it depends, of course, on the amount of overlap. If I pull the dielectric out an infinitesimal distance dx, the energy is changed by an amount equal to the work done:\n\\[\\dd W = F_{me} \\dd x \\tagl{4.59}\\] where \\( F_{me} \\) is the force I mus exert, to counteract the electrical force F on the dielectric. Thus, the electrical force on the slab is\n\\[F = - \\dv{W}{x} \\tagl{4.60}\\] Now, the energy stored in the capacitor is\n\\[W = \\frac{1}{2} C V^2 \\tagl{4.61}\\] and the capacitance in this case is\n\\[C = \\frac{\\epsilon_0 w}{d} (\\epsilon_r l - \\chi_e x) \\tagl{4.62}\\] where l is the length of the plates (Fig 4.30). Let\u0026rsquo;s assume that the total charge on the plates is held constant \\( (Q = CV) \\), as the dielectric moves. In terms of Q,\n\\[W = \\frac{1}{2} \\frac{Q^2}{C} \\tagl{4.63}\\] so\n\\[F = - \\dv{W}{x} = \\frac{1}{2} \\frac{Q^2}{C^2} \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.64}\\] But\n\\[\\dv{C}{x} = - \\frac{\\epsilon_0 \\chi_e w}{d} \\] and hence\n\\[F = - \\frac{\\epsilon_0 \\chi_e w}{2d} V^2 \\tagl{4.65}\\] (The minus sign indicates that the force is in the negative x direction; the dielectric is pulled into the capacitor.)\nIt is a common error to use Eq. 4.61 (with V constant), rather than Eq. 4.63 (with Q constant), in computing the force. One then obtains\n\\[F = - \\frac{1}{2} V^2 \\dv{C}{x}\\] which is off by a sign. It is, of course, possible to maintain the capacitor at a fixed potential, by connecting it up to a battery. But in that case the battery also does work as the dielectric moves; instead of Eq. 4.59, we now have\n\\[\\dd W = F_{me} \\dd x \u0026#43; V \\dd Q \\tagl{4.66}\\] where \\( V \\dd Q \\) is the work done by the battery. It follows that\n\\[F = - \\dv{W}{x} \u0026#43; V \\dv{Q}{x} = - \\frac{1}{2} V^2 \\dv{C}{x} \u0026#43; V^2 \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.67}\\] the same as before, with the correct sign.\nPlease understand: The force on the dielectric cannot possibly depend on whether you plan to hold Q constant or V constant - it is determined entirely by the distribution of charge, free and bound. It\u0026rsquo;s simpler to calculate the force assuming constant Q, because then you don\u0026rsquo;t have to worry about work done by the battery; but if you insist, it can be done correctly either way.\nNotice that we were able to determine the force without knowing anything about the fringing fields that are ultimately responsible for it! Of course, it\u0026rsquo;s built into the whole structure of electrostatics that \\( \\curl \\vec{E} = 0 \\), and hence that the fringing fields must be present; we\u0026rsquo;re not really getting something for nothing here - just cleverly exploiting the internal consistency of the theory. The energy stored in the fringing fields themselves (which was not accounted for in this derivation) stays constant, as the slab moves; what does change is the energy well inside the capacitor, where the field is nice and uniform.\n"},{"id":66,"href":"/r/notes/griffiths/ch5-1/","title":"The Lorentz Force Law","section":"Griffiths Introduction to Electrodynamics","content":" 5.1: The Lorentz Force Law # 5.1.1: Magnetic Fields # Remember the basic problem of classical electrodynamics: We have a collection of charges \\( q_1, q_2, q_3, \\ldots \\) (the \u0026ldquo;source\u0026rdquo; charges), and we want to calculate the force they exert on some other charge Q (the \u0026ldquo;test\u0026rdquo; charge). According to the principle of superposition, it is sufficient to find the force of a single source charge - the total is then the vector sum of all the individual forces. Up to now, we have confined our attention to the simplest case, electrostatics, in which the source charge is at rest (though the test charge need not be). The time has come to consider the forces between charges in motion.\nTo give you some sense of what is in store, imagine that I set up the following demonstration: Two wires hang from the ceiling, a few centimeters apart; when I turn on a current, so that it passes up one wire and back down the other, the wires jump apart - they evidently repel one another (Fig. 5.2(a)). How do we explain this? You might suppose that the battery (or whatever drives the current) is actually charging up the wire, and that the force is simply due to the electrical repulsion of like charges. But this is incorrect. I could hold up a test charge near these wires, and there would be no force on it, for the wires are in fact electrically neutral. (It\u0026rsquo;s true that electrons are flowing down the line - that\u0026rsquo;s what a current is - but there are just as many stationary plus charges as moving minus charges on any given segment.) Moreover, if I hook up my demonstration so as to make the current flow up both wires (Fig. 5.2(b)), they are found to attract! What\u0026rsquo;s going on here?\nWhatever force accounts for the attraction of parallel currents and the repulsion of anti-parallel ones is not electrostatic in nature. It is our first encounter with a magnetic force. Whereas a stationary charge produces only an electric field E in the space around it, a moving charge generates, in addition, a magnetic field B. In fact, magnetic fields are a lot easier to detect, in practice - all you need is a Boy Scout compass. How these devices work is irrelevant at the moment; it is enough to know that the needle points in the direction of the local magnetic field. Ordinarily, this means north, in response to the earth\u0026rsquo;s magnetic field, but in the laboratory, where typical fields may be hundreds of times stronger than that, the compass indicates the direction of whatever magnetic field is present.\nNow, if you hold up a tiny compass in the vicinity of a current-carrying wire, you quickly discover a very peculiar thing: The field does not point toward the wire, nor away from it, but rather it circles around the wire. In fact, if you grab the wire with your right hand-thumb in the direction of the current-your fingers curl around in the direction of the magnetic field (Fig. 5.3). How can such a field lead to a force of attraction on a nearby parallel current? At the second wire, the magnetic field points into the page (Fig. 5.4), the current is upward, and yet the resulting force is to the left! It\u0026rsquo;s going to take a strange law to account for these directions.\n5.1.2: Magnetic Forces # In fact, this contribution of directions is just right for a cross product: the magnetic force on a charge Q, moving with velocity v in a magnetic field B is\n\\[\\vec{F}_{mag} = Q(\\vec{v} \\cross \\vec{B}) \\tagl{5.1}\\] This is known as the Lorentz force law. In the presence of both electric and magnetic fields, the net force on Q would be\n\\[\\vec{F} = q[ \\vec{E} \u0026#43; ( \\vec{V} \\cross \\vec{B} ) ] \\tagl{5.2}\\] I do not pretend to have derived \\( \\eqref{5.1} \\), of course; it is a fundamental axiom of the theory, whose justification is to be found in experiments such as those I described in the previous section.\nOur main job now is to calculate the magnetic field B (and for that matter the electric field E as well; the rules are more complicated when the source charges are in motion). But before we proceed, it is worthwhile to take a closer look at the Lorentz force law itself; it is a peculiar law, and it leads to some truly bizarre particle trajectories.\nExample 5.1: Cyclotron Motion # Q We have a charged particle moving in a constant magnetic field. What is the general form of the trajectory?\nA The archetypical motion of a charged particle in a magnetic field is circular, with the magnetic force providing the centripetal acceleration. In Fig 5.5, a uniform magnetic field points into the page; if the charge Q moves counterclockwise, with speed v, around a circle of radius R, the magnetic force points inward, and has a fixed magnitude QvB - just right to sustain uniform circular motion:\n\\[QvB = m \\frac{v^2}{R} \\quad \\text{ or } \\quad p = Q B R \\tagl{5.3}\\] where m is the particle\u0026rsquo;s mass, and \\( p = mv \\) is its momentum. Equation 5.3 is known as the cyclotron formula because it describes the motion of a particle in a cyclotron - the first of the modern particle accelerators. It also suggests a simple experimental technique for finding the momentum of a charged particle: send it through a region of known magnetic field, and measure the radius of its trajectory. This is in fact the standard means for determining the momenta of elementary particles.\nI assumed that the charge moves in a plane perpendicular to \\( \\vec B \\). If it starts out with some additional speed \\( v_{\\parallel} \\) parallel to \\( \\vec B \\) , this component of the motion is unaffected by the magnetic field, and the particle moves in a helix (Fig 5.6). The radius is given by Eq 5.3, but the velocity in question is now the component perpendicular to \\( \\vec B \\), \\( v_{\\perp} \\)\nExample 5.2: Cycloid Motion # Q A more exotic trajectory occurs if we include a uniform electric field, at right angles to the magnetic one. Suppose, for instance, that B points in the x-direction, and E in the z-direction, as shown in Fig. 5.7. A positive charge is released from the origin; what path will it follow? A Let\u0026rsquo;s think it through qualitatively, first. Initially, the particle is at rest, so the magnetic force is zero, and the electric field accelerates the charge in the z-direction. As it picks up speed, a magnetic force develops which, according to Eq. 5.1, pulls the charge around to the right. The faster it goes, the stronger \\( F_{B} \\) becomes; eventually, it curves the particle back around towards the y axis. At this point the charge is moving against the electrical force, so it begins to slow down - the magnetic force then decreases, and the electrical force takes over, bringing the particle to rest at point a, in Fig. 5.7. There the entire process commences anew, carrying the particle over to point b, and so on. Now let\u0026rsquo;s do it quantitatively. There being no force in the x-direction, the position of the particle at any time t can be described by the vector \\( (0, y(t), z(t) ) \\); the velocity is therefore\n\\[\\vec{v} = (0, \\dot{y}, \\dot{z})\\] Thus,\n\\[\\vec{v} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} \u0026amp; \\vu{y} \u0026amp; \\vu{z} \\\\ 0 \u0026amp; \\dot{y} \u0026amp; \\dot{z} \\\\ B \u0026amp; 0 \u0026amp; 0 \\end{vmatrix} = B \\dot{z} \\vu{y} - B \\dot{y} \\vu{z}\\] and hence, applying Newton\u0026rsquo;s second law,\n\\[\\vec{F} = Q(\\vec{E} \u0026#43; \\vec{v} \\cross \\vec{B}) = Q (E \\vu{z} \u0026#43; B \\dot{z} \\vu{y} - B \\dot{y} \\vu{z}) = m \\vec{a} = m(\\ddot{y} \\vu{y} \u0026#43; \\ddot{z} \\vu{z})\\] Treating the y and z components separately,\n\\[QB \\dot{z} = m \\ddot{y} \\qquad Q E - Q B \\dot{y} = m \\ddot{z}\\] Let\u0026rsquo;s define a frequency\n\\[\\omega = \\frac{Q B}{m} \\tagl{5.4}\\] (This is the cyclotron frequency, at which the particle would revolve in the absence of any electric field.) Then the equations of motion take the form\n\\[\\ddot{y} = \\omega \\dot{z} \\qquad \\ddot{z} = \\omega \\left( \\frac{E}{B} - \\dot{y} \\right) \\tagl{5.5}\\] We\u0026rsquo;re left with a straightforward ODE, with solution\n\\[\\begin{aligned} y(t) \u0026amp; = C_1 \\cos \\omega t \u0026#43; C_2 \\sin \\omega t \u0026#43; (E / B) t \u0026#43; C_3 \\\\ z(t) \u0026amp; = C_2 \\cos \\omega t - C_1 \\sin \\omega t \u0026#43; C_4 \\end{aligned} \\tagl{5.6}\\] But the particle started from rest (\\( \\dot{y}(0) = \\dot{z}(0) = 0 ) \\) at the origin \\( ( y(0) = z(0) = 0) \\); these four conditions determine the constants \\( C_1, C_2, C_3, C_4 \\):\n\\[y(t) = \\frac{E}{\\omega B} (\\omega t - \\sin \\omega t) \\qquad z(t) = \\frac{E}{\\omega B} (1 - \\cos \\omega t) \\tagl{5.7}\\] In this form, the answer is not terribly enlightening, but if we let\n\\[R \\equiv \\frac{E}{\\omega B} \\tagl{5.8}\\] and eliminate the sines and cosines by exploiting the trigonometric identity \\( \\sin ^2 \\omega t + \\cos ^2 \\omega t = 1 \\), we find that\n\\[(y - R \\omega t)^2 \u0026#43; (z - R)^2 = R^2 \\tagl{5.9}\\] This is the formula for a circle, of radius R, whose center \\( (0, R \\omega t, R) \\) travels in the y-direction at constant speed\n\\[u = \\omega R = \\frac{E}{B} \\tagl{5.10} \\] The particle moves as though it were a spot on the rim of a wheel rolling along the y-axis. The curve generated in this way is called a cycloid. Notice that the overall motion is not in the direction of E, as you might suppose, but perpendicular to it.\nExample 5.2b: Mass Spectrometer # Mass Spectrometer\nSuppose we have particles of several isotopes of a known element, and we wish to know exactly which mass isotopes are present (and separate them out) A mass spectrometer is an instrument which can measure the masses and relative concentrations of atoms and molecules. It makes use of the basic magnetic force on a moving charged particle. First, we ionize the particle, giving it a known net charge. We accelerate the particles through a known voltage into a constant magnetic field perpendicular to the velocity of the particle. The charged particles now undergo cyclotron motion (as we just described) of radius given by\n\\[m v = q B R \\rightarrow v^2 = \\frac{q^2 B^2 R^2}{m^2} \\] We know that the energy per unit charge imparted by our known voltage difference is\n\\[q | \\Delta V| = \\frac{1}{2} m v^2 \\rightarrow v^2 = \\frac{2 q | \\Delta V|}{m} \\] We can measure the radius of the cyclotron motion by simply putting a detector wall 1/2 of the way around the circular motion, such that the ions will strike the detector a distance \\( 2R \\) from the output nozzle of the accelerating voltage. Putting our known quantities (\\( \\Delta V, B )\\) together with the measured radius of the cyclotron motion, we get\n\\[\\frac{m}{q} = \\frac{B^2 R^2}{2 | \\Delta V|} \\] One implication of the Lorentz force law deserves special attention:\nMagnetic forces do no work\nFor the magnetic force is, by definition, always perpendicular to the path of motion. Magnetic forces may alter the direction in which a particle moves, but they cannot speed it up or slow it down. The fact that magnetic forces do no work is an elementary and direct consequence of the Lorentz force law, but there are many situations where it appears so manifestly false that one\u0026rsquo;s confidence is bound to waver. When a magnetic crane lifts the carcass of a junked car, for instance, something is obviously doing work, and it seems perverse to deny that the magnetic force is responsible. Well, perverse or not, deny it we must, and it can be a very subtle matter to figure out who does deserve the credit in such circumstances. We\u0026rsquo;ll see a cute example in the next section, but the full story will have to wait until we hit the key conservation laws much later.\n5.1.3: Currents # The current in a wire is the charge per unit time passing a given point. By definition, negative charges moving to the left count the same as positive ones to the right. This conveniently reflects the physical fact that almost all phenomena involving moving charges depend on the product of charge and velocity - if you reverse the signs of q and v, you get the same answer, so it doesn\u0026rsquo;t really matter which you have. (The Lorentz force law is a case in point; the Hall effect (Prob. 5.41) is a notorious exception.) In practice, it is ordinarily the negatively charged electrons that do the moving - in the direction opposite to the electric current. To avoid the petty complications this entails, I shall often pretend it\u0026rsquo;s the positive charges that move, as in fact everyone assumed they did for a century or so after Benjamin Franklin established his unfortunate convention. Current is measured in coulombs-per-second, or amperes (A):\n\\[1 \\text{ A} = 1 \\text{ C/s} \\tagl{5.12}\\] A line charge \\( \\lambda \\) traveling down a wire at speed v (Fig 5.9) constitutes a current\n\\[I = \\lambda v \\tagl{5.13}\\] because a segment of length \\( v \\Delta t \\), carrying charge \\( \\lambda v \\Delta t \\), passes point P in a time interval \\( \\Delta t \\). Current is actually a vector\n\\[\\vec{I} = \\lambda \\vec{v} \\tagl{5.14}\\] Because the path of the flow is dictated by the shape of the wire, one doesn\u0026rsquo;t ordinarily bother to display the direction of I explicitly, but when it comes to surface and volume currents we cannot afford to be so casual, and for the sake of notational consistency it is a good idea to acknowledge the vectorial character of currents right from the start. A neutral wire, of course, contains as many stationary positive charges as mobile negative ones. The former do not contribute to the current-the charge density \\( \\lambda \\) in Eq. 5.13 refers only to the moving charges. In the unusual situation where both types move, I = \\( \\lambda_{+} \\vec{v}{+} + \\lambda{-}\\vec{v}_{-} \\) . The magnetic force on a segment of current-carrying wire is\n\\[\\vec{F}_B = \\int (\\vec{v} \\cross \\vec{B}) \\dd q = \\int (\\vec{v} \\cross \\vec{B}) \\lambda \\dd l = \\int (\\vec{I} \\cross \\vec{B}) \\dd l \\tagl{5.15}\\] Inasmuch as I and dl both point in the same direction, we can just as well write this as\n\\[\\vec{F}_B = \\int I ( \\dd \\vec{l} \\cross \\vec{B}) \\tagl{5.16}\\] Typically, the current is constant (in magnitude) along the wire, and in that case, I comes outside the integral:\n\\[\\vec{F}_B = I \\int ( \\dd \\vec{l} \\cross \\vec{B}) \\tagl{5.17}\\] Example 5.3 # Q A rectangular loop of wire, supporting mass m, hangs vertically with one end in a uniform magnetic field B, which points into the page in the shaded region of Fig 5.10. For what current I, in the loop, would the magnetic force upward exactly balance the gravitational force downward?\nA First of all, the current must circulate clockwise, in order for \\( (\\vec{I} \\cross \\vec{b}) \\) in the horizontal segment to point upward. The force is\n\\[F_B = I B a\\] where a is the width of the loop. The magnetic forces on the two vertical segments cancel. For \\( F_{mag} \\) to balance the weight (mg), we must therefore have\n\\[I = \\frac{mg}{Ba} \\tagl{5.18}\\] The weight just hangs there, suspended in mid-air!\nWhat happens if we now increase the current? Then the upward force exceeds the downward force of gravity, and the loop rises, lifting the weight. Somebody\u0026rsquo;s doing work, and it sure looks as though the magnetic force is responsible. Indeed, one is tempted to write\n\\[W_{mag} = F_{mag} h = I B a h \\tagl{5.19}\\] where h is the distance the loop rises. But we know that magnetic forces never do work, so what\u0026rsquo;s going on here?\nWell, when the loop starts to rise, the charges in the wire are no longer moving horizontally - their velocity now acquires an upward component u, the speed of the loop (Fig 5.11), in addition to the horizontal component w associated with the current (\\( I = \\lambda w \\)). The magnetic force, which is always perpendicular to the velocity, no longer points straight up, but tilts back. It is perpendicular to the net displacement of the charge (which is in the direction of v), and therefore it does no work on q. It does have a vertical component (\\( q w B \\)); indeed, the net vertical force on the charge \\( (\\lambda a) \\) in the upper segment of the loop is\n\\[F_{vert} = \\lambda a w B = I B a \\tagl{5.20}\\] (as before); but now it also has a horizontal component \\( (q u B) \\) which opposes the flow of current. Whoever is in charge of maintaining that current, therefore, must now push those charges along, against the backward component of the magnetic force.\nThe total horizontal force on the top segment is\n\\[F_{horiz} = \\lambda a u B \\tagl{5.21}\\] In a time \\( dt \\), the charges move a (horizontal) distance \\( w \\dd t \\), so the work done by this agency (presumably a battery or a generator) is\n\\[W_{battery} = \\lambda a B \\int u w \\dd t = I B a h\\] which is precisely what we naively attributed to the magnetic force in \\( \\eqref{5.19} \\). Was work done in this process? Absolutely! Who did it? The battery! What, then, was the role of the magnetic force? Well, it redirected the horizontal force of the battery into the vertical motion of the loop and the weight.\nIt may help to consider a mechanical analogy. Imagine you\u0026rsquo;re sliding a trunk up a frictionless ramp, by pushing on it horizontally with a mop (Fig 5.12). The normal force (N) does no work, because it is perpendicular to the displacement. But it does have a vertical component (which in fact is what lifts the trunk), and a (backward) horizontal component (which you have to overcome by pushing on the mop). Who is doing the work here? You are, obviously - and yet your force (which is purely horizontal) is not (at least, directly) what lifts the box. The normal force plays the same passive (but crucial) role as the magnetic force in Ex 5.3: while doing no work itself, it redirects the efforts of the active agent (you, or the battery, as the case may be), from horizontal to vertical.\nWhen charge flows over a surface, we describe it by the surface current density, K, defined as follows: Consider a \u0026ldquo;ribbon\u0026rdquo; of infinitesimal width \\( \\dd l_{\\perp} \\), running parallel to the flow (Fig 5.13). If the current in this ribbon is \\( \\dd \\vec{I} \\), the surface current density is\n\\[\\vec{K} = \\frac{\\dd \\vec{I}}{\\dd l _{\\perp}} \\tagl{5.22} \\] In words, K is the current per unit width. In particular, if the (mobile) surface charge density is \\( sigma \\) and its velocity is \\( \\vec{v} \\), then\n\\[\\vec{K} = \\sigma \\vec{v} \\tagl{5.23}\\] In general, \\( \\vec{K} \\) will vary from point to point over the surface, as \\( \\sigma \\) and/or \\( \\vec{v} \\) changes. The magnetic force on the surface current is\n\\[\\vec{F}_{mag} = \\int ( \\vec{v} \\cross \\vec{B}) \\sigma \\dd a = \\int (\\vec{K} \\cross \\vec{B}) \\dd a \\tagl{5.24}\\] Caveat: Just as E suffers a discontinuity at a surface charge, so B is discontinuous at a surface current. In \\( \\eqref{5.24} \\), you must be careful to use the average field, just as we did in Sect 2.5.3.\nWhen the flow of charge is distributed throughout a three-dimensional region, we describe it by the volume current density, J, defined as follows: consider a \u0026ldquo;tube\u0026rdquo; of infinitesimal cross section \\( \\dd a_{\\perp} \\), running parallel to the flow (Fig 5.14). If the current in this tube is \\( \\dd \\vec{I} \\), the volume current density is\n\\[\\vec{J} \\equiv \\dv{\\vec{I}}{a_{\\perp}} \\tagl{5.25}\\] In words, J is the current per unit area. If the (mobile) volume charge density is \\( \\rho \\) and the velocity is \\( \\vec{v} \\), then\n\\[\\vec{J} = \\rho \\vec{v} \\tagl{5.26}\\] The magnetic force on a volume current is therefore\n\\[\\vec{F}_{mag} = \\int (\\vec{v} \\times \\vec{B}) \\rho \\dd \\tau = \\int (\\vec{J} \\times \\vec{B} ) \\dd \\tau \\tagl{5.27}\\] Example 5.4 # Q A current I is uniformly distributed over a wire of circular cross section, with radius a (Fig 5.15). Find the volume current density J.\nA The area perpendicular to the flow is \\( \\pi a^2 \\), so\n\\[J = \\frac{I}{\\pi a^2}\\] This was trivial because the current density was uniform\nSuppose the current density in the wire is proportional to the distance from the axis,\n\\[J = k s\\] for some constant k. Find the total current in the wire.\nBecause J varies with s, we must integrate \\( \\eqref{5.25} \\). The current through the shaded patch (Fig 5.16) is \\( J \\dd a_{\\perp} \\), and \\( \\dd a_{\\perp} = s \\dd s \\dd \\phi \\) so\n\\[I = \\int (ks) (s \\dd s \\dd \\phi) = 2 \\pi k \\int _0 ^a s^2 \\dd s = \\frac{2 \\pi k a^3}{3} \\] According to Eq. 5.25, the total current crossing a surface S can be written as\n\\[I = \\int_S J \\dd a_{\\perp} = \\int_S \\vec{J} \\cdot \\dd \\vec{a} \\tagl{5.28}\\] (The dot product serves to pick out the appropriate component of \\( \\dd \\vec{a} \\)). In particular, the charge per unit time leaving a volume V is\n\\[\\oint _S \\vec{J} \\cdot \\dd \\vec{a} = \\int_V ( \\div \\vec{J}) \\dd \\tau\\] Because charge is conserved, whatever flows out through the surface must come at the expense of what remains inside:\n\\[\\int_V (\\div \\vec{J} ) \\dd \\tau = - \\dv{}{t} \\int_V \\rho \\dd \\tau = - \\int _V \\left( \\pdv{\\rho}{t} \\right)\\dd \\tau\\] (The minus sign reflects the fact that an outward flow decreases the charge left in V). Since this applies to any volume, we conclude that\n\\[\\div \\vec{J} = - \\pdv{\\rho}{t} \\tagl{5.29}\\] This is the precise mathematical statement of local charge conservation; it is called the continuity equation.\nFor future reference, let us summarize the \u0026ldquo;dictionary\u0026rdquo; we have implicitly developed for translating equations into the forms appropriate to point, line, surface, and volume currents\n\\[\\sum_{i = 1} ^n ( ) q_i \\vec{v}_i \\sim \\int_{line} () \\vec{I} \\dd l \\sim \\int_{surf} ( ) \\vec{K} \\dd a \\sim \\int_{vol} ( ) \\vec{J} \\dd \\tau \\tagl{5.30}\\] This correspondence, which is analogous to \\( q \\sim \\lambda \\dd l \\sim \\sigma \\dd a \\sim \\rho \\dd \\tau \\) for the various charge distributions, generates Eqs. 5.15, 5.24, and 5.27 from the original Lorentz force law (5.1).\n"},{"id":67,"href":"/r/notes/griffiths/ch5-2/","title":"The Biot-Savart Law","section":"Griffiths Introduction to Electrodynamics","content":" 5.2: The Biot-Savart Law # 5.2.1: Steady Currents # Stationary charges produce electric fields that are constant in time; hence the term electrostatics. Steady currents produce magnetic fields that are constant in time; the theory of steady currents is called magnetostatics.\nBy \u0026ldquo;steady current\u0026rdquo; I mean a continuous flow that has been going on forever, without change and without piling up anywhere (some people call them \u0026ldquo;stationary currents\u0026rdquo;; to my ear, that\u0026rsquo;s a contradiction in terms). Formally, electro/magnetostatics is the regime\n\\[\\pdv{\\rho}{t} = 0, \\quad \\pdv{\\vec{J}}{t} = 0 \\tagl{5.32}\\] at all places and all times. Of course, there\u0026rsquo;s no such thing in practice as a truly steady current, any more than there is a truly stationary charge. In this sense, both electrostatics and magnetostatics describe artificial worlds that exist only in textbooks. However, they represent suitable approximations as long as the actual fluctuations are remote, or gradual - in fact, for most purposes magnetostatics applies very well to household currents, which alternate 120 times per second!\nNotice that a moving point charge cannot possibly constitute a steady current. If it\u0026rsquo;s here one instant, it\u0026rsquo;s gone the next. This may seem like a minor thing to you, but it\u0026rsquo;s a major headache for me. I developed each topic in electrostatics by starting out with the simple case of a point charge at rest; then I generalized to an arbitrary charge distribution by invoking the superposition principle. This approach is not open to us in magnetostatics because a moving point charge does not produce a static field in the first place. We are forced to deal with extended current distributions right from the start, and, as a result, the arguments are bound to be more cumbersome. When a steady current flows in a wire, its magnitude I must be the same all along the line; otherwise, charge would be piling up somewhere, and it wouldn\u0026rsquo;t be a steady current. More generally, since \\( \\partial \\rho / \\partial t = 0 \\) in magnetostatics, the continuity equation (5.29) becomes\n\\[\\grad \\vec{J} = 0 \\tagl{5.33}\\] 5.2.2: The Magnetic Field of a Steady Current # The magnetic field of a steady line current is given by the Biot-Savart law:\n\\[\\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{I} \\cross \\vu{\\gr}}{\\gr ^2} \\dd l\u0026#39; = \\frac{ \\mu_0}{4 \\pi } I \\int \\frac{\\dd \\vec{l\u0026#39;} \\cross \\vu{\\gr}}{\\gr ^2} \\tagl{5.34} \\] The integration is along the current path, in the direction of the flow; \\( \\dd \\vec{l\u0026rsquo;} \\) is an element of length along the wire, and \\( \\vu{\\gr} \\), as always, is the vector from the source to the point r (Fig 5.17). The constant \\( \\mu_0 \\) is called the permeability of free space:\n\\[\\mu_0 = 4 \\pi \\times 10^{-7} \\text{N} / \\text{A}^2 \\tagl{5.35}\\] This is an exact number, not an empirical constant. It serves to define the ampereThese units are such that B itself comes out in newtons per ampere-meter (as required by the Lorentz force law), or teslas (T):\n\\[1 \\text{T} = 1 \\text{N} / (\\text{A} \\cdot \\text{m}) \\tagl{5.36}\\] As the starting point for magnetostatics, the Biot-Savart law plays a role analogous to Coulomb\u0026rsquo;s law in electrostatics. Indeed, the \\( 1/ \\gr ^2 \\) dependence is common to both laws\nExample 5.5 # Q Find the magnetic field a distance s from a long straight wire carrying a steady current I (Fig 5.18).\nA In the diagram, \\( \\dd \\vec{l\u0026rsquo;} \\cross \\gr \\) points out of the page, and has the magnitude\n\\[\\dd l\u0026#39; \\sin \\alpha = \\dd l\u0026#39; \\cos \\theta\\] Also, \\( l\u0026rsquo; = s \\tan \\theta \\), so\n\\[\\dd l\u0026#39; = \\frac{s}{ \\cos ^2 \\theta} \\dd \\theta\\] and \\( s = \\gr \\cos \\theta \\), so\n\\[\\frac{1}{\\gr^2} = \\frac{\\cos ^2 \\theta}{s^2} \\] The Biot-Savart law gives the magnetic field as\n\\[\\begin{aligned} B \u0026amp; = \\frac{\\mu_0 I}{4 \\pi} \\int _{\\theta_1}^{\\theta_2} \\left( \\frac{\\cos ^2 \\theta}{s^2} \\right) \\left( \\frac{s}{\\cos ^2 \\theta} \\right) \\cos \\theta \\dd \\theta \\\\ \u0026amp; = \\frac{\\mu_0}{4 \\pi s} \\int _{\\theta_1}^{\\theta_2} \\cos \\theta \\dd \\theta \\\\ \u0026amp; = \\frac{\\mu_0 I}{4 \\pi s} (\\sin \\theta_2 - \\sin \\theta_1) \\tagl{5.37} \\end{aligned}\\] That is the field of any straight segment of wire, in terms of the initial and final angles \\( \\theta_1 \\) and \\( \\theta_2 \\) (Fig 5.19). Of course, a finite segment by itself could never support a steady current (where would the charge go when it got to the end?), but it might be a piece of some closed circuit, and \\( \\eqref{5.37} \\) would then represent its contribution to the total field. In the case of an infinite wire, \\( \\theta_1 = - \\pi / 2 \\) and \\( \\theta_2 = \\pi / 2 \\) so we obtain\n\\[B = \\frac{\\mu_0 I}{2 \\pi s} \\tagl{5.38}\\] Notice that the field is inversely proportional to the distance from the wire - just like the electric field of an infinite line charge. In the region below the wire, B points into the page, and in general, it \u0026ldquo;circles around\u0026rdquo; the wire, in accordance with the right-hand rule\n\\[\\vec{B} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} \\tagl{5.39}\\] As an application, let\u0026rsquo;s find the force of attraction between two long, parallel wires a distance d apart, carrying currents \\( I_1 \\) and \\( I_2 \\) (Fig 5.20). The field at (2) due to (1) is\n\\[B = \\frac{\\mu_0 I_1}{2 \\pi d}\\] and it points into the page. The Lorentz force law (in the form appropriate to line currents) predicts a force directed upwards (1), of magnitude\n\\[F = I_2 \\left( \\frac{\\mu_0 I_1}{2 \\pi d} \\right) \\int \\dd l\\] The total force, not surprisingly, is infinite, but the force per unit length is\n\\[f = \\frac{ \\mu_0}{2 \\pi } \\frac{I_1 I_2}{d} \\tagl{5.40}\\] If the currents are antiparallel (one up, one down), the force is repulsive - consistent again with the qualitative observations of Sect 5.1.1.\nExample 5.6 # Q Find the magnetic field a distance z above the center of a circular loop of radius R, which carries a steady current I (Fig 5.21)\nA The field \\( \\dd \\vec{B} \\) attributable to the segment \\( \\dd \\vec{l\u0026rsquo;} \\) points as shown. As we integrate \\( \\dd \\vec{l\u0026rsquo;} \\) around the loop, \\( \\dd \\vec{B} \\) sweeps out a cone. The horizontal components cancel, and the vertical components combine, to give\n\\[B(z) = \\frac{\\mu_0 }{4 \\pi} I \\int \\frac{\\dd l\u0026#39;}{\\gr ^2} \\cos \\theta\\] (Notice that dl\u0026rsquo; and \\( \\gr \\) are perpendicular, in this case; the factor of \\( \\cos \\theta \\) projects out the vertical component.) Now, \\( \\cos \\theta \\) and \\( \\gr ^2 \\) are constants, and \\( \\int dl\u0026rsquo; \\) is simply the circumference \\( 2 \\pi R \\), so\n\\[B(z) = \\frac{\\mu_0 I}{4 \\pi} \\left( \\frac{\\cos \\theta}{\\gr ^2} \\right) 2 \\pi R = \\frac{\\mu_0 I}{2} \\frac{R^2}{(R^2 \u0026#43; z^2) ^{3/2}} \\tagl{5.41}\\] For surface and volume currents, the Biot-Savart law becomes\n\\[\\vec{B}(r) = \\frac{\\mu_0}{4 \\pi } \\int \\frac{\\vec{K}(r\u0026#39;) \\times \\vu{\\gr}}{\\gr ^2} \\dd a\u0026#39;\\] and\n\\[\\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r\u0026#39;) \\times \\vu{\\gr}}{\\gr ^2} \\dd \\tau\u0026#39; \\tagl{5.42}\\] respectively. You might be tempted to write down the corresponding formula for a moving point charge, using the \u0026ldquo;dictionary\u0026rdquo;\n\\[\\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\frac{q \\vec{v} \\times \\vu{\\gr}}{\\gr ^2} \\] but this is simply wrong. As I mentioned earlier, a point charge does not constitute a steady current, and the Biot-Savart law, which only holds for steady currents, does not correctly determine its field.\nThe superposition principle applies to magnetic fields just as it does to electric firlds: if you have a collection of source currents, the net field is the (vector) sum of the fields due to each of them taken separately.\n"},{"id":68,"href":"/r/notes/griffiths/ch5-3/","title":"The Divergence and Curl of B","section":"Griffiths Introduction to Electrodynamics","content":" 5.3: The Divergence and Curl of B # 5.3.1: Straight-Line Currents # The magnetic field of an infinite straight wire is shown in Fig 5.27 (the current is coming out of the page). At a glance, it is clear that this field has a nonzero curl (something you\u0026rsquo;ll never see in an electrostatic field); let\u0026rsquo;s calculate it.\nAccording to Eq. 5.38, the integral of B around a circular path of radius s, centered at the wire, is\n\\[\\oint \\vec{B} \\cdot \\dd l = \\oint \\frac{\\mu_0 I}{2 \\pi s} = \\frac{\\mu_0 I}{2 \\pi s } \\oint \\dd l = \\mu_0 I\\] Notice that the answer is independent of s; that\u0026rsquo;s because B decreases at the same rate as the circumference increases. In fact, it doesn\u0026rsquo;t have to be a circle; any old loop that encloses the wire would give the same answer. For if we use cylindrical coordinates \\( (s, \\phi, z) \\), with the current flowing along the z axis, \\( \\vec{B} = (\\mu_0 I / 2 \\pi s) \\vu{\\phi} \\) and \\( \\dd \\vec{l} = \\dd s , \\vu{s} + s \\dd \\phi , \\vu{\\phi} + \\dd z , \\vu{z} \\), so\n\\[\\oint \\vec{B} \\cdot \\dd l = \\frac{\\mu_0 I}{2 \\pi} \\oint \\frac{1}{s} s \\dd \\phi = \\frac{\\mu_0 I}{2 \\pi } \\int _0 ^{2 \\pi} \\dd \\phi = \\mu_0 I\\] This assumes that the loop encircles the wire exactly once; if it went around twice, then \\( \\phi \\) would go from \\( 0 \\) to \\( 4 \\pi \\), and if it didn\u0026rsquo;t enclose the wire at all, then \\( \\phi \\) would go from \\( \\phi_1 \\) to \\( \\phi_2 \\) and back again, with \\( \\int \\dd \\phi = 0 \\) (Fig 5.28).\nNow suppose we have a bundle of straight wires. Each wire that passes through our loop contributes \\( \\mu_0 I \\), and those outside contribute nothing (Fig 5.29). The line integral will then be\n\\[\\oint \\vec{B} \\cdot \\dd l = \\mu_0 I_{enc} \\tagl{5.44}\\] where \\( I_{enc} \\) stands for the total current enclosed by the integration path. If the flow of charge is represented by a volume current density J, the enclosed current is\n\\[I_{enc} = \\int \\vec{J} \\cdot \\dd \\vec{a} \\tagl{5.45}\\] with the integral taken over any surface bounded by the loop. Applying Stokes\u0026rsquo; theorem to Eq 5.44, then\n\\[\\int (\\curl \\vec{B}) \\cdot \\dd \\vec{a} = \\mu_0 \\int \\vec{J} \\cdot \\dd \\vec{a}\\] and hence\n\\[\\curl \\vec{B} = \\mu_0 \\vec{J}\\] With minimal labor, we have actually obtained the general formula for the curl of B. But our derivation is seriously flawed by the restriction to infinite straight line currents (and combinations thereof). Most current configurations cannot be constructed out of infinite straight wires, and we have no right to assume that Eq. 5.46 applies to them. So the next section is devoted to the formal derivation of the divergence and curl of B, starting from the Biot-Savart law itself.\n5.3.2: The Divergence and Curl of B # The Biot-Savart law for the general case of a volume current reads\n\\[\\vec{B}(\\vec{r}) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r\u0026#39;) \\times \\vu{\\gr}}{\\gr ^2} \\dd \\tau\u0026#39; \\tagl{5.47} \\] This formula gives the magnetic field at a point r = (x, y, z) in terms of an integral over the current distribution \\( \\vec{J} (x\u0026rsquo;, y\u0026rsquo;, z\u0026rsquo;) \\) (Fig 5.30). It is best to be absolutely explicit at this stage:\nB is a function of (x, y, z) J is a function of (x\u0026rsquo;, y\u0026rsquo;, z') \\( \\vu{\\gr} = (x - x\u0026rsquo;) \\vu{x} + (y - y\u0026rsquo;) \\vu{y} + (z - z\u0026rsquo;) \\vu{z} \\) \\( \\dd \\tau\u0026rsquo; = dx\u0026rsquo; dy\u0026rsquo; dz\u0026rsquo; \\) The integration is over the primed coordinates; the divergence and the curl of B are with respect to the unprimed coordinates.\nApplying the divergence to \\( \\eqref{5.47} \\) we obtain\n\\[\\div \\vec{B} = \\frac{\\mu_0 }{4 \\pi } \\int \\div \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right)\\dd \\tau\u0026#39; \\tagl{5.48}\\] With one of our product rules for divergences\n\\[\\div \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right) = \\frac{\\vu{\\gr}}{\\gr^2} \\cdot (\\curl \\vec{J}) - \\vec{J} \\cdot \\left( \\curl \\frac{ \\vu{\\gr}}{\\gr ^2} \\right) \\tagl{5.49}\\] But \\( \\curl \\vec{J} = 0 \\) because J doesn\u0026rsquo;t depend on the unprimed variables, while \\( \\curl ( \\vu{\\gr} / \\gr ^2) = 0 \\) (we\u0026rsquo;ve already explicitly worked that in Chapter 1), so\n\\[\\div \\vec{B} = 0 \\tagl{5.50}\\] Evidently, the divergence of the magnetic field is zero.\nApplying the curl to Eq 5.47, we obtain\n\\[\\curl \\vec{B} = \\frac{\\mu_0}{4 \\pi} \\int \\curl \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr ^2} \\right) \\dd \\tau\u0026#39; \\tagl{5.51}\\] Again, our strategy is to expand the integrand, using the appropriate product rule - in this case one of those for curls\n\\[\\curl \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right) = \\vec{J} \\left( \\div \\frac{\\vu{\\gr}}{\\gr ^2} \\right) - ( \\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} \\tagl{5.52} \\] (I have dropped terms involving derivatives of J, because J does not depend on x, y, z.) The second term integrates to zero, as we\u0026rsquo;ll see in the next paragraph. The first term involves the divergence we were at pains to calculate in Chapter 1 (Eq. 1.100):\n\\[\\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta ^2(\\gr) \\tagl{5.53}\\] Thus\n\\[\\curl \\vec{B} = \\frac{\\mu_0}{4 \\pi} \\int \\vec{J}(r\u0026#39;) 4 \\pi \\delta ^3 (\\vec{r} - \\vec{r\u0026#39;} \\dd \\tau\u0026#39; = \\mu_0 \\vec{J} (\\vec{r})\\] which confirms that Eq. 5.46 is not restricted to straight-line currents, but holds quite generally in magnetostatics.\nTo complete the argument, however, we must check that the second term in Eq 5.52 integrates to zero. Because the derivative acts only on \\( \\vu{\\gr} / \\gr ^2 \\), we can switch from \\( \\grad \\) to \\( \\grad\u0026rsquo; \\) at the cost of a minus sign\n\\[-(\\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} = (\\vec{J} \\cdot \\grad\u0026#39; ) \\frac{\\vu{\\gr}}{\\gr^2} \\tagl{5.54}\\] The x component, in particular, is\n\\[(\\vec{J} \\cdot \\grad\u0026#39;) \\left( \\frac{x - x\u0026#39; }{\\gr ^3} \\right) = \\grad\u0026#39; \\cdot \\left( \\frac{(x - x\u0026#39;)}{\\gr ^3} \\vec{J} \\right) - \\left( x - x\u0026#39; \\gr ^3 \\right)(\\grad\u0026#39; \\cdot \\vec{J})\\] Now, for steady currents the divergence of J is zero, so\n\\[\\left( - (\\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} \\right)_{x} = \\grad\u0026#39; \\cdot \\left( \\frac{(x - x\u0026#39;)}{\\gr ^3} \\vec{J} \\right)\\] and therefore this contribution to the integral can be written\n\\[\\int_{V} \\grad\u0026#39; \\cdot \\left( \\frac{(x - x\u0026#39;)}{\\gr ^3} \\vec{J} \\right) \\dd \\tau\u0026#39; = \\oint_S \\frac{(x - x\u0026#39;)}{\\gr^3} \\vec{J} \\cdot \\dd \\vec{a\u0026#39;} \\tagl{5.55}\\] (The reason for switching from \\( \\grad \\) to \\( \\grad\u0026rsquo; \\) was to allow this integration by parts).\nBut what region are we integrating over? Well, it\u0026rsquo;s the volume that appears in the Biot-Savart law (Eq. 5.47) - large enough, that is, to include all the current. You can make it bigger than that, if you like; \\( \\vec{J} = 0 \\) out there anyway, so it will add nothing to the integral. The essential point is that on the boundary the current is zero (all current is safely inside) and hence the surface integral (Eq. 5.55) vanishes.\n5.3.3: Ampere\u0026rsquo;s Law # The equation for the curl of B,\n\\[\\curl \\vec{B} = \\mu_0 \\vec{J} \\tagl{5.56}\\] is called Ampere\u0026rsquo;s Law (in differential form). It can be converted to integral form by the usual device of applying one of the fundamental theorems - in this case Stokes\u0026rsquo; theorem:\n\\[\\int (\\curl \\vec{B}) \\cdot \\dd \\vec{a} = \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 \\int \\vec{J} \\cdot \\dd \\vec{a}\\] Now, \\( \\int \\vec{J} \\cdot \\dd \\vec{a} \\) is the total current passing through the surface (Fig. 5.31), which we call \\( I_{enc} \\) (the current enclosed by the Amperian loop). Thus\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} \\tagl{5.57}\\] This is the integral version of Ampere\u0026rsquo;s law; it generalizes Eq 5.44 to arbitrary steady currents. Notice that Eq 5.57 inherits the sign ambiguity of Stokes\u0026rsquo; theorem: which way around the loop am I supposed to go? And which direction through the surface corresponds to a \u0026ldquo;positive\u0026rdquo; current? The resolution, as always, is the right-hand rule: If the fingers of your right hand indicate the direction of integration around the boundary, then your thumb defines the direction of a positive current.\nJust as the Biot-Savart law plays a role in magnetostatics that Coulomb\u0026rsquo;s law assumed in electrostatics, so Ampere\u0026rsquo;s plays the part of Gauss\u0026rsquo;s. In particular, for currents with appropriate symmetry, Ampere\u0026rsquo;s law in integral form offers a lovely and extraordinarily efficient way of calculating the magnetic field.\nExample 5.7 # Q Find the magnetic field a distance s from a long straight wire (Fig 5.32), carrying a steady current I (the same problem from the previous section, in which we used the Biot-Savart law) A We know the direction of B is \u0026ldquo;circumferential,\u0026rdquo; circling around the wire as indicated by the right-hand rule. By symmetry, the magnitude of B is constant around an Amperian loop of radius s, centered on the wire. So Ampere\u0026rsquo;s law gives\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = B \\oint dl = 2 \\pi s B = \\mu_0 I_{enc} = \\mu_0 I\\] or\n\\[B = \\frac{\\mu_0 I}{2 \\pi s} \\] This is the same answer we got before (Eq. 5.38), but it was waay easier to obtain this time.\nExample 5.8 # Q Find the magnetic field of an infinite uniform surface current \\( \\vec{K} = K \\vu{x} \\), flowing over the xy plane (Fig 5.33) A First of all, what is the direction of B? Could it have any x component? No: A glance at the Biot-Savart law (Eq. 5.42) reveals that B is perpendicular to K. Could it have a z component? No again. You could confirm this by noting that any vertical contribution from a filament at +y is canceled by the corresponding filament at - y. But there is a nicer argument: Suppose the field pointed away from the plane. By reversing the direction of the current, I could make it point toward the plane (in the Biot-Savart law, changing the sign of the current switches the sign of the field). But the z component of B cannot possibly depend on the direction of the current in the xy plane. (Think about it!) So B can only have a y component, and a quick check with your right hand should convince you that it points to the left above the plane and to the right below it.\nWith this in mind, we draw a rectangular Amperian loop as shown in Fig. 5.33, parallel to the yz plane and extending an equal distance above and below the surface. Applying Ampere\u0026rsquo;s law,\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = 2 B l = \\mu_0 I_{enc} = \\mu_0 K l\\] (One Bl comes from the top segment and the other from the bottom), so \\( B = (\\mu_0 / 2) K \\), or, more precisely\n\\[\\vec{B} = \\begin{cases} \u0026#43;(\\mu_0 / 2) K \\vu{y} \\quad \u0026amp; \\text{ for } z \u0026lt; 0 \\\\ - (\\mu_0 / 2) K \\vu{y} \\quad \u0026amp; \\text{ for } z \u0026gt; 0 \\end{cases} \\tagl{5.58}\\] Notice that the field is independent of the distance from the plane, just like the electric field of a uniform surface charge.\nExample 5.9 # Q Find the magnetic field of a very long solenoid, consisting of n closely wound turns per unit length on a cylinder of radius R, each carrying a steady current I (Fig 5.34). [The point of making the windings so close is that one can then pretend each turn is circular. If this troubles you (after all, there is a net current I in the direction of the solenoid\u0026rsquo;s axis, no matter how tight the winding), picture instead a sheet of aluminum foil wrapped around the cylinder, carrying the equivalent of a uniform surface current K = nI (Fig 5.35). Or make a double winding, going up to one end and then - always in the same sense - going back down again, thereby eliminating the net longitudinal current. But, in truth, this is all unnecessary fastidiousness, for the field inside a solenoid is huge (relatively speaking), and the field of the longitudinal current is at most a tiny refinement.\nA First of all, what is the direction of B? Could it have a radial component? No. For suppose \\( B_s \\) were positive; if we reversed the direction of the current, \\( B_s \\) would then be negative. But switching I is physically equivalent to turning the solenoid upside down, and that certainly should not alter the radial field. How about a \u0026ldquo;circumferential\u0026rdquo; component? No. For \\( B_{\\phi} \\) would be constant around an Amperian loop concentric with the solenoid (Fig. 5.36), and hence\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = B_{\\phi} (2 \\pi s) = \\mu_0 I_{enc} = 0\\] since the loop encloses no current.\nSo, the magnetic field of an infinite, closely wound solenoid runs parallel to the axis. From the right-hand rule, we expect that it points inside the solenoid and downward outside. Moreover, it certainly approaches zero as you go very far away. With this in mind, let\u0026rsquo;s apply Ampere\u0026rsquo;s law to the two rectangular loops in Fig 5.37. Loop 1 lies entirely outside the solenoid, with its sides at distances a and b from the axis:\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = [ B(a) - B(b)] L = \\mu_0 I_{enc} = 0\\] so\n\\[B(a) = B(b)\\] Evidently the field outside does not depend on the distance from the axis. But we agreed that it goes to zero for large s. It must therefore be zero everywhere! (This astonishing result can also be derived from the Biot-Savart law, of course, but it\u0026rsquo;s much more difficult. See Prob. 5.46.)\nAs for loop 2, which is half inside and half outside, Ampere\u0026rsquo;s law gives\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = B L = \\mu_0 I_{enc} = \\mu_0 n I L\\] where B is the field inside the solenoid. (The right side of the loop contributes nothing, since B = 0 out there.) Conclusion:\n\\[\\vec{B} = \\begin{cases} \\mu_0 n I \\vu{z} \u0026amp; \\quad \\text{ inside the solenoid } \\\\ 0 \u0026amp; \\quad \\text{ outside the solenoid } \\end{cases} \\tagl{5.59}\\] Notice that the field inside is uniform - it doesn\u0026rsquo;t depend on the distance from the axis. In this sense the solenoid is to magnetostatics what the parallel-plate capacitor is to electrostatics: a simple device for producing strong uniform fields.\nLike Gauss\u0026rsquo;s law, Ampere\u0026rsquo;s law is always true (for steady currents), but it is not always useful. Only when the symmetry of the problem enables you to pull B outside the integral \\( \\oint \\vec{B} \\cdot \\dd \\vec{l} \\) can you calculate the magnetic field from Ampere\u0026rsquo;s law. When it does work, it\u0026rsquo;s by far the fastest method; when it doesn\u0026rsquo;t, you have to fall back on the Biot-Savart law. The current configurations that can be handled by Ampere\u0026rsquo;s law are\nInfinite straight lines (Ex 5.7) Infinite planes (Ex 5.8) Infinite solenoids (Ex 5.9) Toroids (Ex 5.10) The last of these is a surprising and elegant application of Ampere\u0026rsquo;s law. As in Exs. 5.8 and 5.9, the hard part is figuring out the direction of the field (which we will now have done, once and for all, for each of the four geometries); the actual application of Ampere\u0026rsquo;s law takes only one line.\nExample 5.10 # Q A toroidal coil consists of a circular ring, or \u0026lsquo;donut,\u0026rsquo; around which a long wire is wrapped (Fig 5.38). The winding is uniform and tight enough so that each turn can be considered a plane closed loop. The cross-sectional shape of the coil is immaterial; I made it rectangular in Fig 5.38 for the sake of simplicity, but it could just as well be circular or even some weird asymmetrical form, as in Fig 5.39, as long as the shape remains the same all the way around the ring. In that case, it follows that the magnetic field of the toroid is circumferential at all points, both inside and outside the coil.\nA According to the Biot-Savart law, the field at r due to the current element at r\u0026rsquo; is\n\\[\\dd \\vec{B} = \\frac{\\mu_0 }{4 \\pi} \\frac{\\vec{I} \\cross \\vu{\\gr}}{\\gr ^3} \\dd l\u0026#39;\\] We may as well put r in the xz plane (Fig 5.39), so its Cartesian components are (x, 0, z), while the source coordinates are\n\\[\\vec{r\u0026#39;} = (s\u0026#39; \\cos \\phi\u0026#39;, s\u0026#39; \\sin \\phi\u0026#39;, z\u0026#39;)\\] Then \\[\\gr = (x - s\u0026#39; \\cos \\phi\u0026#39;, -s\u0026#39; \\sin \\phi\u0026#39;, z - z\u0026#39;)\\] Since the current has no \\( \\phi \\) component, \\( \\vec{I} = I_s \\vu{x} + I_z \\vu{z} \\), or (in Cartesian coordinates)\n\\[\\vec{I} = (I_s \\cos \\phi\u0026#39;, I_s \\sin \u0026#39; , I_z)\\] Accordingly,\n\\[\\vec{I} \\cross \\vec{\\gr} = \\begin{bmatrix} \\vu{x} \u0026amp; \\vu{y} \u0026amp; \\vu{z} \\\\ I_s \\cos \\phi\u0026#39; \u0026amp; I_s \\sin \\phi\u0026#39; \u0026amp; I_z \\\\ (x - s\u0026#39; \\cos \\phi\u0026#39;) \u0026amp; (- s\u0026#39; \\sin \\phi\u0026#39;) \u0026amp; (z - z\u0026#39;) \\end{bmatrix} \\\\ = [ \\sin \\phi\u0026#39; (I_s (z - z\u0026#39;) \u0026#43; s\u0026#39; I_z)] \\vu{x} \u0026#43; [I_z (x - s\u0026#39; \\cos \\phi\u0026#39;) - I_s \\cos \\phi\u0026#39; (z - z\u0026#39;)] \\vu{y} \u0026#43; [-I_s x \\sin \\phi\u0026#39;]\\vu{z}\\] But there is a symmetrically situated current element at r\u0026rsquo;\u0026rsquo;, with the same \\( s\u0026rsquo; \\) , the same \\( \\gr \\), the same \\( dl\u0026rsquo; \\), the same \\( I_s \\), and the same \\( I_z \\), but negative \\( \\phi\u0026rsquo; \\) (Fig 5.39). Because \\( \\sin \\phi\u0026rsquo; \\) changes sign, the \\( \\vu{x} \\) and \\( \\vu{z} \\) contributions from \\( \\vu{r\u0026rsquo;} \\) and \\( \\vu{r\u0026rsquo;\u0026rsquo;} \\) cancel each other, leaving only a \\( \\vu{y} \\) term. Thus the field at \\( \\vu{r} \\) is in the \\( \\vu{y} \\) direction, and in general the field points in the \\( \\vu{\\phi} \\) direction.\nNow that we know the field is circumferential, determining its magnitude is simple: just apply Ampere\u0026rsquo;s law to a circle of radius s about the axis of the toroid\n\\[B 2 \\pi s = \\mu_0 I_{enc}\\] and hence\n\\[\\vec{B}(\\vec{r}) = \\begin{cases} \\frac{\\mu_0 N I}{2 \\pi s} \\vu{\\phi} \\quad \u0026amp; \\text{ for points inside the coil} \\\\ 0 \\quad \u0026amp; \\text{ for points outside the coil} \\end{cases} \\tagl{5.60}\\] where N is the total number of turns.\n5.3.4: Comparison of Magnetostatics and Electrostatics # The divergence and curl of the electrostatic field are\n\\[\\begin{cases} \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\quad \u0026amp; \\text{(Gauss\u0026#39;s law)}\\\\ \\curl \\vec{E} = 0 \\quad \u0026amp; \\text{(no name)} \\end{cases}\\] These are Maxwell\u0026rsquo;s equations for electrostatics. Together with the boundary condition \\( \\vec{E} \\rightarrow 0 \\) far from all charges, Maxwell\u0026rsquo;s equations determine the field, if the source charge density \\( \\rho \\) is given; they contain essentially the same information as Coulomb\u0026rsquo;s law plus the principle of superposition. The divergence and curl of the magnetostatic field are\n\\[\\begin{cases} \\div \\vec{B} = 0 \\quad \u0026amp; \\text{(no name)} \\\\ \\curl \\vec{B} = \\mu_0 \\vec{J} \\quad \u0026amp; \\text{(Ampere\u0026#39;s Law)} \\end{cases}\\] These are Maxwell\u0026rsquo;s equations for magnetostatics. Again, together with the boundary condition \\( \\vec{B} \\rightarrow 0 \\) far from all currents, Maxwell\u0026rsquo;s equations determine the magnetic field; they are equivalent to the Biot-Savart law (plus superposition). Maxwell\u0026rsquo;s equations and the force law\n\\[\\vec{F} = Q(\\vec{E} \u0026#43; \\vec{v} \\cross \\vec{B})\\] constitute the most elegant formulation of electrostatics and magnetostatics.\nThe electric field diverges away from a (positive) charge; the magnetic field line curls around a current (Fig. 5.44). Electric field lines originate on positive charges and terminate on negative ones; magnetic field lines do not begin or end anywhere - to do so would require a nonzero divergence. They typically form closed loops or extend out to infinity. To put it another way, there are no point sources for B, as there are for E; there exists no magnetic analog to electric charge. This is the physical content of the statement \\( \\div \\vec{B} = 0 \\). Coulomb and others believed that magnetism was produced by magnetic charges (magnetic monopoles, as we would now call them), and in some older books you will still find references to a magnetic version of Coulomb\u0026rsquo;s law, giving the force of attraction or repulsion between them. It was Ampere who first speculated that all magnetic effects are attributable to electric charges in motion (currents). As far as we know, Ampere was right; nevertheless, it remains an open experimental question whether magnetic monopoles exist in nature (they are obviously pretty rare, or somebody would have found one), and in fact some recent elementary particle theories require them. For our purposes, though, B is divergenceless, and there are no magnetic monopoles. It takes a moving electric charge to produce a magnetic field, and it takes another moving electric charge to \u0026ldquo;feel\u0026rdquo; a magnetic field.\nTypically, electric forces are enormously larger than magnetic ones. That\u0026rsquo;s not something intrinsic to the theory; it has to do with the sizes of the fundamental constants \\( \\epsilon_0 \\) and \\( \\mu_0 \\). In general, it is only when both the source charges and the test charge are moving at velocities comparable to the speed of light that the magnetic force approaches the electric force in strength. (Problems 5.13 and 5.17 illustrate this rule.) How is it, then, that we notice magnetic effects at all? The answer is that both in the production of a magnetic field (Biot-Savart) and in its detection (Lorentz), it is the current that matters, and we can compensate for a smallish velocity by pouring huge amounts of charge down the wire. Ordinarily, this charge would simultaneously generate so large an electric force as to swamp the magnetic one. But if we arrange to keep the wire neutral, by embedding in it an equal quantity of opposite charge at rest, the electric field cancels out, leaving the magnetic field to stand alone. It sounds very elaborate, but of course this is precisely what happens in an ordinary current carrying wire.\n"},{"id":69,"href":"/r/notes/griffiths/ch5-4/","title":"Magnetic Vector Potential","section":"Griffiths Introduction to Electrodynamics","content":" 5.4: Magnetic Vector Potential # 5.4.1: The Vector Potential # Just as \\( \\curl \\vec{E} = 0 \\) permitted us to introduce a scalar potential (V) in electrostatics,\n\\[\\vec{E} = - \\grad \\vec{V}\\] so \\( \\div \\vec{B} = 0 \\) invites the introduction of a vector potential A in magnetostatics\n\\[\\vec{B} = \\curl \\vec{A} \\tagl{5.61}\\] We were allowed to define these potentials based on our extended proof of the Helmholtz theorem (back in Section 1.6). The potential formulation automatically takes care of \\( \\div \\vec{B} = 0 \\) since the divergence of a curl is always zero; there remains Ampere\u0026rsquo;s law:\n\\[\\curl \\vec{B} = \\curl (\\curl \\vec{A}) = \\grad (\\div \\vec{A}) - \\grad ^2 \\vec{A} = \\mu_0 \\vec{J} \\tagl{5.62}\\] Now, the electric potential had a built-in ambiguity: you can add to V any function whose gradient is zero (which is to say, a constant), without altering the physical quantity E. Likewise, you can add to A any function whose curl vanishes (which is to say, the gradient of any scalar), with no effect on B. We can exploit this freedom to eliminate the divergence of A:\n\\[\\div \\vec{A} = 0 \\tagl{5.63}\\] To prove that this is always possible, suppose that our original potential \\( \\vec{A_0} \\) is not divergenceless. If we add to it the gradient of \\( \\lambda \\) \\( (\\vec{A} = \\vec{A}_0 + \\grad \\lambda) \\), the new divergence is\n\\[\\div \\vec{A} = \\div \\vec{A_0} \u0026#43; \\grad ^2 \\lambda\\] We can accommodate Eq. 5.63, then, if a function \\( \\lambda \\) can be found that satisfies\n\\[\\grad ^2 \\lambda = - \\div \\vec{A_0}\\] But this is mathematically identical to Poisson\u0026rsquo;s equation\n\\[\\grad ^2 \\lambda = - \\frac{\\rho}{\\epsilon_0} \\] with \\( \\div \\vec{A}_0 \\) in place of \\( \\rho / \\epsilon_0 \\) as the \u0026ldquo;source.\u0026rdquo; And we know how to solve Poisson\u0026rsquo;s equation - that\u0026rsquo;s what electrostatics is all about. In particular, if \\( \\rho \\) goes to infinity, the solution is Eq. 2.29:\n\\[V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr} \\dd \\tau\u0026#39;\\] and by the same token, if \\( \\div \\vec{A}_0 \\) goes to zero at infinity, then\n\\[\\lambda = \\frac{1}{4 \\pi} \\int \\frac{\\div \\vec{A}_0}{\\gr } \\dd \\tau\u0026#39;\\] If \\( \\div \\vec{A}_0 \\) does not go to zero at infinity, then we\u0026rsquo;ll have to use other means to discover the appropriate \\( \\lambda \\), just as we get the electric potential by other means when the charge distribution extends to infinity. But the essential point remains: It is always possible to make the vector potential divergenceless. To put it the other way round,: the definition \\( \\vec{B} = \\curl \\vec{A} \\) specifies the curl of A, but it doesn\u0026rsquo;t say anything about the divergence - we are at liberty to pick that as we see fit, and zero is ordinarily the simplest choice.\nWith this condition on A, Ampere\u0026rsquo;s law becomes\n\\[\\grad ^2 \\vec{A} = - \\mu_0 \\vec{J} \\tagl{5.64}\\] This again is nothing but Poisson\u0026rsquo;s equation, or rather it is three of them, one for each Cartesian coordinate. In Cartesian coordinates, \\( \\grad ^2 \\vec{A} = (\\grad ^2 A_x) \\vu{x} + (\\grad ^2 A_y) \\vu{y} + (\\grad ^2 A_z) \\vu{z} \\), so 5.64 reduces to \\( \\grad ^2 A_x = - \\mu_0 J_x \\), \\( \\grad ^2 A_y = - \\mu_0 J_y \\), and \\( \\grad ^2 A_z = - \\mu_0 J_z \\). In curvilinear coordinates, the unit vectors themselves are functions of position, and must be differentiated, so it is not the case, for example, that \\( \\grad ^2 A_r = - \\mu_0 J_r \\). Remember that even if you plan to evaluate integrals such as 5.65 using curvilinear coordinates, you must first express \\( \\vec{J} \\) in terms of its Cartesian components. Assuming J goes to zero at infinity, we can read off the solution\n\\[\\vec{A} (\\vec{r}) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r\u0026#39;)}{\\gr} \\dd \\tau\u0026#39; \\tagl{5.65}\\] For line and surface currents,\n\\[\\vec{A} = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{I}}{\\gr } \\dd l\u0026#39; = \\frac{\\mu_0 \\vec{I}}{4 \\pi} \\int \\frac{1}{\\gr} \\dd \\vec{l}\u0026#39;; \\qquad \\vec{A} = \\frac{\\mu_0}{4\\pi} \\int \\frac{\\vec{K}}{\\gr} \\dd a\u0026#39; \\tagl{5.66}\\] (If the current does not go to zero at infinity, we have to find other ways to get A; some of these are explored in Exercise 5.12 and in the problems at the end of the section.)\nIt must be said that A is not as useful as V. For one thing, it\u0026rsquo;s still a vector, and although Eqs. 5.65 and 5.66 are somewhat easier to work with than the Biot-Savart law, you still have to fuss with components. It would be nice if we could get away with a scalar potential\n\\[\\vec{B} = - \\grad U\\] but this is incompatible with Ampere\u0026rsquo;s law, since the curl of a gradient is always zero. (A magnetostatic scalar potential can be used, if you stick scrupulously to simply-connected, current-free regions, but as a theoretical tool, it is of limited interest. See problem 5.29.) Moreover, since magnetic forces do no work, A does not admit a simple physical interpretation in terms of potential energy per unit charge. (In some contexts it can be interpreted as the momentum per unit charge.) Nevertheless, the vector potential has substantial theoretical importance, as we shall see in chapter 10.\nExample 5.11 # Q A spherical shell of radius R, carrying a uniform surface charge \\( \\sigma \\), is set spinning at angular velocity \\( \\omega \\). Find the vector potential it produces at r (Fig 5.45).\nA While it might seem natural to set the polar axis along \\( \\omega \\), in fact the integration is easier if we let r lie on the z axis, so that \\( \\omega \\) is tilted at an angle \\( \\psi \\). We may as well orient the x axis so that \\( \\omega \\) lies in the xz plane, as shown in Fig 5.46. According to Eq. 5.66,\n\\[\\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{K(r\u0026#39;)}}{\\gr} \\dd a\u0026#39;\\] where \\( \\vec{K} = \\sigma \\vec{v} \\), \\( \\gr = \\sqrt{R^2 + r^2 - 2 R r \\cos \\theta\u0026rsquo;} \\), and \\( \\dd a\u0026rsquo; = R^2 \\sin \\theta\u0026rsquo; \\dd \\theta \u0026rsquo; \\dd \\phi\u0026rsquo; \\ \\). Now the velocity of a point r\u0026rsquo; in a rotating rigid body is \\( \\vec{\\omega} \\cross \\vec{r\u0026rsquo;} \\); in this case,\n\\[\\begin{aligned} \\vec{v} \u0026amp; = \\vec{\\omega} \\cross \\vec{r\u0026#39;} \\\\ \u0026amp; = \\begin{vmatrix} \\vu{x} \u0026amp; \\vu{y} \u0026amp; \\vu{z} \\\\ \\omega \\sin \\psi \u0026amp; 0 \u0026amp; \\omega \\cos \\psi \\\\ R \\sin \\theta\u0026#39; \\cos \\phi\u0026#39; \u0026amp; R \\sin \\theta\u0026#39; \\sin \\phi\u0026#39; \u0026amp; R \\cos \\theta\u0026#39; \\end{vmatrix} \\\\ \u0026amp; = R \\omega [ - (\\cos \\psi \\sin \\theta\u0026#39; \\sin \\phi\u0026#39;) \\vu{x} \\\\ \u0026amp; \\qquad \u0026#43; (\\cos \\psi \\sin \\theta\u0026#39; \\cos \\phi\u0026#39; - \\sin \\psi \\cos \\theta\u0026#39;) \\vu{y} \\\\ \u0026amp; \\qquad \u0026#43; (\\sin \\psi \\sin \\theta\u0026#39; \\sin \\phi\u0026#39;) \\vu{z} ] \\end{aligned}\\] Notice that each of these terms, save one, involves either \\( \\sin \\phi\u0026rsquo; \\) or \\( \\cos \\phi\u0026rsquo; \\). Since\n\\[\\int _0 ^{2 \\pi} \\sin \\phi\u0026#39; \\dd \\phi\u0026#39; = \\int_0 ^{2\\pi} \\cos \\phi\u0026#39; \\dd \\phi\u0026#39; = 0\\] such terms do not contribute to the integral. There remains\n\\[\\vec{A}(r) = - \\frac{\\mu_0 R^3 \\sigma \\omega \\sin \\psi}{2} \\left( \\int_0 ^{\\pi} \\frac{\\cos \\theta\u0026#39; \\sin \\theta\u0026#39;}{\\sqrt{R^2 \u0026#43; r^2 - 2 R r \\cos \\theta\u0026#39;}} \\dd \\theta\u0026#39; \\right) \\vu{y}\\] Letting \\( u \\equiv \\cos \\theta\u0026rsquo; \\), the integral becomes\n\\[\\int_{-1} ^{\u0026#43;1} \\frac{u}{\\sqrt{R^2 \u0026#43; r^2 - 2 R r u}} \\dd u = \\left. - \\frac{(R^2 \u0026#43; r^2 \u0026#43; R r u)}{3 R^2 r^2} \\sqrt{R^2 \u0026#43; r^2 - 2 R r u} \\right|_{-1} ^{\u0026#43;1} \\\\ = - \\frac{1}{3 R^2 r^2} \\left[ (R^2 \u0026#43; r^2 \u0026#43; R r ) | R - r| - (R^2 \u0026#43; r^2 - Rr)(R \u0026#43; r) \\right]\\] If the point r lies inside the sphere, then \\( R \u0026gt; r \\) and this expression reduces to \\( (2r / 3R^2) \\); if r lies outside the sphere, so that \\( R \u0026lt; r \\), it reduces to \\( (2R / 3r^2) \\). Noting that \\( (\\vec{\\omega} \\cross \\vec{r}) = - \\omega r \\sin \\psi \\vu{y} \\), we have finally\n\\[\\vec{A}(r) = \\begin{cases} \\frac{\\mu_0 R \\sigma}{3} (\\vec{\\omega} \\cross \\vec{r}), \\qquad \u0026amp; \\text{ for points inside the sphere} \\\\ \\frac{\\mu_0 R^4 \\sigma}{3 r^3} (\\vec{\\omega} \\cross \\vec{r}, \\qquad \u0026amp; \\text{ for points outside the sphere} \\end{cases} \\tagl{5.68}\\] Having evaluated the integral, I revert to the \u0026ldquo;natural\u0026rdquo; coordinates of Fig. 5.45, in which \\( \\vec{\\omega} \\) coincides with the z axis and the point r is at \\( (r, \\theta, \\phi) \\):\n\\[\\vec{A}(r, \\theta, \\phi) = \\begin{cases} \\frac{\\mu_0 R \\sigma \\omega }{3} r \\sin \\theta \\vu{\\phi}, \\qquad \u0026amp; (r \\leq R) \\\\ \\frac{\\mu_0 R^4 \\omega \\sigma}{3 r^3} \\frac{\\sin \\theta}{r^2} \\vu{\\phi} , \\qquad \u0026amp; (r \\geq R) \\end{cases} \\tagl{5.69}\\] Curiously, the field inside this spherical shell is uniform:\n\\[\\vec{B} = \\curl \\vec{A} = \\frac{2 \\mu_0 R \\omega \\sigma}{3} (\\cos \\theta \\vu{r} - \\sin \\theta \\vu{\\theta}) = \\frac{2}{3} \\mu_0 R \\omega \\vu{z} = \\frac{2}{3} \\mu_0 \\sigma R \\vec{\\omega} \\tagl{5.70}\\] Example 5.12 # Q Find the vector potential of an infinite solenoid with n turns per unit length, radius R, and current I A This time we cannot use Eq 5.66, since the current itself extends to infinity. But there is a cute method that does the job. Notice that\n\\[\\oint \\vec{A} \\cdot \\dd \\vec{l} = \\int (\\curl \\vec{A}) \\cdot \\dd \\vec{a} = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\Phi \\tagl{5.71}\\] where \\( \\Phi \\) is the flux of B through the loop in question. This is reminiscent of Ampere\u0026rsquo;s law in integral form (Eq. 5.57)\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 I_{enc}\\] In fact, it\u0026rsquo;s the same equation, with \\( \\vec{B} \\rightarrow \\vec{A} \\) and \\( \\mu_0 I_{enc} \\rightarrow \\Phi \\). If symmetry permits, we can determine A from \\( \\Phi \\) in the same way we got B from \\( I_{enc} \\), in section 5.3.3. The present problem (with a uniform longitudinal magnetic field \\( \\mu_0 n I \\) inside the solenoid and no field outside) is analogous to the Ampere\u0026rsquo;s law problem of a fat wire carrying a uniformly distributed current. The vector potential is \u0026ldquo;circumferential\u0026rdquo; (it mimics the magnetic field in the analog); using a circular \u0026ldquo;Amperian loop\u0026rdquo; at radius s inside the solenoid, we have\n\\[\\oint \\vec{A} \\dd \\vec{l} = A (2 \\pi s) = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 n I (\\pi s^2)\\] so\n\\[\\vec{A} = \\frac{\\mu_0 n I}{2} s \\vu{\\phi}, \\quad \\text{ for } s \\leq R \\tagl{5.72}\\] For an Amperian loop outside the solenoid, the flux is\n\\[\\int \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 n I (\\pi R^2)\\] since the field only extends out to R. Thus\n\\[\\vec{A} = \\frac{\\mu_0 n I}{2} \\frac{R^2}{s} \\vu{\\phi} \\quad \\text{ for } s \\geq R \\tagl{5.73}\\] To check our work, we can make sure that \\( \\curl \\vec{A} = \\vec{B} \\) and \\( \\div \\vec{A} = 0 \\). Inside the solenoid,\n\\[\\begin{aligned} \\div \\vec{A} \u0026amp; = \\frac{1}{s} \\pdv{}{s} (s A_s) \u0026#43; \\frac{1}{s} \\pdv{A_\\phi}{\\phi} \u0026#43; \\pdv{A_z}{z} \\\\ \u0026amp; = \\frac{1}{s} \\pdv{}{\\phi} \\frac{\\mu_0 n I}{2} s = 0 \\\\ \\curl \\vec{A} \u0026amp; = \\left( \\frac{1}{s} \\pdv{A_z}{\\phi} - \\pdv{A_{\\phi}}{z} \\right) \\vu{s} \\\\ \u0026amp; \u0026#43; \\left( \\pdv{A_s}{z} - \\pdv{A_z}{s} \\right) \\vu{\\phi} \\\\ \u0026amp; \u0026#43; \\frac{1}{s} \\left[ \\pdv{}{s} (s A_{\\phi}) - \\pdv{A_s}{\\phi} \\right] \\vu{z} \\\\ \u0026amp; = \\frac{\\mu_0 n I}{2} \\frac{1}{s} \\left( \\pdv{}{s} s^2 \\right) \\\\ \u0026amp; = \\mu_0 n I \\end{aligned}\\] Outside the solenoid,\n\\[\\begin{aligned} \\div \\vec{A} \u0026amp; = \\frac{1}{s} \\pdv{}{s} (s A_s) \u0026#43; \\frac{1}{s} \\pdv{A_\\phi}{\\phi} \u0026#43; \\pdv{A_z}{z} \\\\ \u0026amp; = \\frac{1}{s} \\pdv{}{\\phi} \\frac{\\mu_0 n I}{2} \\frac{R^2}{s} = 0 \\\\ \\curl \\vec{A} \u0026amp; = \\left( \\frac{1}{s} \\pdv{A_z}{\\phi} - \\pdv{A_{\\phi}}{z} \\right) \\vu{s} \\\\ \u0026amp; \u0026#43; \\left( \\pdv{A_s}{z} - \\pdv{A_z}{s} \\right) \\vu{\\phi} \\\\ \u0026amp; \u0026#43; \\frac{1}{s} \\left[ \\pdv{}{s} (s A_{\\phi}) - \\pdv{A_s}{\\phi} \\right] \\vu{z} \\\\ \u0026amp; = \\frac{\\mu_0 n I}{2} \\frac{1}{s} \\left( \\pdv{}{s} R^2 \\right) \\\\ \u0026amp; = 0 \\end{aligned}\\] which is just the answer we got in section 5.3 by Biot-Savart.\nTypically, the direction of A mimics the direction of the current. For instance, both were azimuthal in Exs. 5.11 and 5.12. Indeed, if all the current flows in one direction, then Eq. 5.65 suggests that A must point that way too. Thus the potential of a finite segment of straight wire (Prob. 5.23) is in the direction of the current. Of course, if the current extends to infinity you can\u0026rsquo;t use Eq. 5.65 in the first place (see Probs. 5.26 and 5.27). Moreover, you can always add an arbitrary constant vector to A - this is analogous to changing the reference point for V, and it won\u0026rsquo;t affect the divergence or curl of A, which is all that matters (in Eq. 5.65 we have chosen the constant so that A goes to zero at infinity). In principle you could even use a vector potential that is not divergenceless, in which case all bets are off. Despite these caveats, the essential point remains: Ordinarily the direction of A will match the direction of the current.\n5.4.2: Boundary Conditions # In Chapter 2, we had a triangular diagram to summarize the relations among the three fundamental quantities in electrostatics: the charge density \\( \\rho \\), the electric field E, and the potential V. A similar figure can be constructed for magnetostatics (Fig 5.48), relating the current density J, the field B, and the potential A. There is one \u0026ldquo;missing link\u0026rdquo; in the diagram: the equation for A in terms of B. It\u0026rsquo;s unlikely you would ever need such a formula, but in case you are interested, see Probs. 5.52 and 5.53\nJust as the electric field suffers a discontinuity at a surface charge, so the magnetic field is discontinuous at a surface current. Only this time it is the tangential component that changes. For if we apply Eq. 5.50 in integral form\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{a} = 0\\] to a wafer-thin pillbox straddling the surface (Fig 5.49), we get\n\\[B_{above} ^\\perp = B_{below} ^\\perp \\tagl{5.74}\\] As for the tangential components, an Amperian loop running perpendicular to the current (Fig 5.50) yields\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = \\left( B_{above}^\\parallel - B_{below} ^{\\parallel} \\right) l = \\mu_0 I_{enc} = \\mu_0 K l \\] or\n\\[B_{above}^\\parallel - B_{below} ^{\\parallel} = \\mu_0 K \\tagl{5.75}\\] Thus the component of __B__ that is parallel to the surface but perpendicular to the current is discontinuous in the amount \\\\( \\mu_0 L \\\\). A similar Amperian loop running parallel to the current reveals that the parallel component is continuous. These results can be summarized in a single formula \\[\\vec{B}_{above} - \\vec{B}_{below} = \\mu_0 ( \\vec{K} \\cross \\vu{n} ) \\tagl{5.76}\\] where \\( \\vu{n} \\) is perpendicular to the surface, pointing \u0026ldquo;upward.\u0026rdquo;\nLike the scalar potential in electrostatics, the vector potential is continuous across any boundary:\n\\[\\vec{A}_{above} = \\vec{A}_{below} \\tagl{5.77}\\] for \\( \\div \\vec{A} = 0 \\) guarantees that the normal component is continuous, and \\( \\curl \\vec{A} = \\vec{B} \\) in the form\n\\[\\oint \\vec{A} \\cdot \\dd \\vec{l} = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\Phi\\] means that the tangential components are continuous (the flux through an Amperian loop of vanishing thickness is zero). But the derivative of A inherits the discontinuity of B:\n\\[\\pdv{A_{above}}{n} - \\pdv{A_{below}}{n} = - \\mu_0 \\vec{K} \\tagl{5.78}\\] 5.4.3: Multipole Expansion of the Vector Potential # If you want an approximate formula for the vector potential of a localized current distribution, valid at distant points, a multipole expansion is in order. Remember: the idea of a multipole expansion is to write the potential in the form of a power series in \\( 1/r \\), where r is the distance to the point in question (Fig 5.51); if r is sufficiently large, the series will be dominated by the lowest nonvanishing contribution, and the higher terms can be ignored. As we found in Section 3.4.1,\n\\[\\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r\u0026#39;}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{5.79}\\] where \\( \\alpha \\) is the angle between r and r\u0026rsquo;. Accordingly, the vector potential of a current loop can be written\n\\[\\vec{A}(r) = \\frac{\\mu_0 I}{4 \\pi} \\oint \\frac{1}{\\gr} \\dd \\vec{l\u0026#39;} = \\frac{\\mu_0 I}{4 \\pi} \\sum_{n=0} ^\\infty \\frac{1}{r^{n\u0026#43;1}} \\oint (r\u0026#39;)^n P_n (\\cos \\alpha) \\dd \\vec{l\u0026#39;} \\tagl{5.80} \\] or, more explicitly,\n\\[\\begin{aligned} \\vec{A}(r) \u0026amp; = \\frac{\\mu_0 I}{4 \\pi} \\left[ \\frac{1}{r} \\oint \\dd \\vec{l\u0026#39;} \u0026#43; \\frac{1}{r^2} \\oint r\u0026#39; \\cos \\alpha \\dd \\vec{l\u0026#39;} \\right. \\\\ \u0026amp; \\left. \\quad \u0026#43; \\frac{1}{r^3} \\oint (r\u0026#39;)^2 \\left( \\frac{3}{2} \\cos ^2 \\alpha - \\frac{1}{2} \\right) \\dd \\vec{l\u0026#39;} \u0026#43; \\ldots \\right] \\end{aligned} \\tagl{5.81}\\] As with the multipole expansion of V, we call the first term (which goes like \\( 1/r \\)) the monopole term, the second (which goes like \\( 1/r^2 \\)) dipole, the third quadrupole, and so on.\nNow, the magnetic monopole term is always zero, for the integral is just the total vector displacement around a closed loop\n\\[\\oint \\dd \\vec{l\u0026#39;} = 0 \\tagl{5.82}\\] This reflects the fact that there are no magnetic monopoles in nature (an assumption contained in Maxwell\u0026rsquo;s equation \\( \\div \\vec{B} = 0 \\), on which the entire theory of vector potential is predicated).\nIn the absence of any monopole contribution, the dominant term is the dipole (except in the rare case where it, too, vanishes):\n\\[\\vec{A}_{dip} = \\frac{\\mu_0 I}{4 \\pi r^2} \\oint r\u0026#39; \\cos \\alpha \\dd \\vec{l\u0026#39;} = \\frac{\\mu_0 I}{4 \\pi r^2} \\oint (\\vu{r} \\cdot \\vec{r\u0026#39;} ) \\dd \\vec{l\u0026#39;} \\tagl{5.83}\\] This integral can be rewritten in a more illuminating way if we invoke Eq. 1.108 with \\( \\vec{c} = \\vu{r} \\):\n\\[ \\oint (\\vu{r} \\cdot \\vec{r\u0026#39;}) \\dd \\vec{l\u0026#39;} = - \\vu{r} \\cross \\int \\dd \\vec{a\u0026#39;} \\tagl{5.84} \\] Then\n\\[\\vec{A}_{dip} (r) = \\frac{\\mu_0}{4 \\pi} \\frac{\\vec{m} \\cross \\vu{r}}{r^2} \\tagl{5.85}\\] where we define the magnetic dipole moment m:\n\\[\\vec{m} \\equiv I \\int \\dd \\vec{a} = I \\vec{a} \\tagl{5.86}\\] Here a is the \u0026ldquo;vector area\u0026rdquo; of the loop (Problem 1.62); if the loop is flat, a is the ordinary area enclosed, with the direction assigned by the usual right-hand rule (fingers in the direction of the current).\nExample 5.13 # Q Find the magnetic dipole moment of the \u0026lsquo;bookend-shaped\u0026rsquo; loop shown in Fig 5.52. All sides have length w, and it carries a current I.\nA This wire could be considered the superposition of two plane square loops (Fig 5.53). The \u0026ldquo;extra\u0026rdquo; sides (AB) cancel when the two are put together, since the currents flow in opposite directions. The net magnetic dipole moment is\n\\[\\vec{m} = I w^2 \\vu{y} \u0026#43; I w^2 \\vu{z}\\] Its magnitude is \\( \\sqrt{2} I w^2 \\) and it points along the \\( 45^{\\circ} \\) line \\( z = y \\)\nIt is clear from Eq. 5.86 that the magnetic dipole moment is independent of the choice of origin. You may remember that the electric dipole moment is independent of the origin only when the total charge vanishes (Sect. 3.4.3). Since the magnetic monopole moment is always zero, it is not really surprising that the magnetic dipole moment is always independent of origin.\nAlthough the dipole term dominates the multipole expansion (unless \\( \\vec{m} = 0 \\)) and thus offers a good approximation to the true potential, it is not ordinarily the exact potential; there will be quadrupole, octopole, and higher contributions. You might ask, is it possible to devise a current distribution whose potential is \u0026ldquo;pure\u0026rdquo; dipole - for which Eq. 5.85 is exact? Well, yes and no: like the electrical analog, it can be done, but the model is a bit contrived. To begin with, you must take an infinitesimally small loop at the origin, but then, in order to keep the dipole moment finite, you have to crank the current up to infinity, with the product \\( m = I a \\) held fixed. In practice, the dipole potential is a suitable approximation whenever the distance \\( r \\) greatly exceeds the size of the loop.\nThe magnetic field of a (perfect) dipole is easiest to calculate if we put \\( \\vec{m} \\) at the origin and let it point in the z-direction (Fig 5.54). According to Eq. 5.85, the potential at point \\( (r, \\theta, \\phi) \\) is\n\\[\\vec{A}_{dip} = \\frac{\\mu_0}{4 \\pi} \\frac{m \\sin \\theta}{r^2} \\vu{\\phi} \\tagl{5.87}\\] and hence\n\\[\\vec{B}_{dip} = \\curl \\vec{A} = \\frac{\\mu_0 m}{4 \\pi r^3} (2 \\cos \\theta \\vu{r} \u0026#43; \\sin \\theta \\vu{\\theta}) \\tagl{5.88}\\] Unsurprisingly, this is identical in structure to the field of an electric dipole (Eq. 3.103)! (Up close, however, the field of a physical magnetic dipole - a small current loop - looks quite different from the field of a physical electric dipole - plus and minus charges a short distance apart. Compare Fig 5.55 with Fig 3.37.)\n"},{"id":70,"href":"/r/notes/griffiths/ch6-1/","title":"Magnetization","section":"Griffiths Introduction to Electrodynamics","content":" 6.1: Magnetization # 6.1.1: Diamagnets, Paramagnets, Ferromagnets # If you ask the average person what \u0026ldquo;magnetism\u0026rdquo; is, you will probably be told about refrigerator decorations, compass needles, and the North Pole - none of which has any obvious connection with moving charges or current-carrying wires. Yet all magnetic phenomena are due to electric charges in motion, and in fact, if you could examine a piece of magnetic material on an atomic scale you would find tiny currents: electrons orbiting around nuclei and spinning about their axes. For macroscopic purposes, these current loops are so small that we may treat them as magnetic dipoles. Ordinarily, they cancel each other out because of the random orientation of the atoms. But when a magnetic field is applied, a net alignment of these magnetic dipoles occurs, and the medium becomes magnetically polarized, or magnetized.\nUnlike electric polarization, which is almost always in the same direction as E, some materials acquire a magnetization parallel to B (paramagnets) and some opposite to B (diamagnets). A few substances (called ferromagnets, in deference to the most common example, iron) retain their magnetization even after the external field has been removed - for these, the magnetization is not determined by the present field but by the whole magnetic \u0026ldquo;history\u0026rdquo; of the object. Permanent magnets made of iron are the most familiar examples of magnetism, but from a theoretical point of view they are the most complicated; I\u0026rsquo;ll save ferromagnetism for the end of the chapter, and begin with qualitative models of paramagnetism and diamagnetism.\n6.1.2: Torques and Forces on Magnetic Dipoles # A magnetic dipole experiences a torque in a magnetic field, just as an electric dipole does in an electric field. Let\u0026rsquo;s calculate the torque on a rectangular current loop in a uniform field B. (Since any current loop could be built up from infinitesimal rectangles, with all the \u0026ldquo;internal\u0026rdquo; sides canceling, as indicated in Fig. 6.1, there is no real loss of generality here; but if you prefer to start from scratch with an arbitrary shape, see Prob. 6.2.) Center the loop at the origin, and tilt it an angle \\( \\theta \\) from the z axis towards the y axis (Fig. 6.2). Let B point in the z direction. The forces on the two sloping sides cancel (they tend to stretch the loop, but they don\u0026rsquo;t rotate it). The forces on the \u0026ldquo;horizontal\u0026rdquo; sides are likewise equal and opposite (so the net force on the loop is zero), but they do generate a torque:\n\\[\\vec{N} = a F \\sin \\theta \\vu{x}\\] The magnitude of the force on each of these segments is\n\\[F = I b B\\] and therefore\n\\[\\vec{N} = I a b B \\sin \\theta \\vu{x} = m B \\sin \\theta \\vu{x}\\] or\n\\[\\vec{N} = \\vec{m} \\cross \\vec{B} \\tagl{6.1}\\] where \\( m = I a b \\) is the magnetic dipole moment of the loop. Equation 6.1 gives the torque on any localized current distribution, in the presence of a uniform field; in a non-uniform field it is the exact torque (about the center) for a perfect dipole of infinitesimal size.\nNotice that Eq. 6.1 is identical in form to the electrical analog, Eq. 4.4: \\( \\vec{N} = \\vec{p} \\cross \\vec{E} \\). In particular, the torque is again in such a direction as to line the dipole up parallel to the field. It is this torque that accounts for paramagnetism. Since every electron constitutes a magnetic dipole (picture it, if you wish, as a tiny spinning sphere of charge), you might expect paramagnetism to be a universal phenomenon. Actually, quantum mechanics (specifically, the Pauli exclusion principle) tends to lock the electrons within a given atom together in pairs with opposing spins, and this effectively neutralizes the torque on the combination. As a result, paramagnetism most often occurs in atoms or molecules with an odd number of electrons, where the \u0026ldquo;extra\u0026rdquo; unpaired member is subject to the magnetic torque. Even here, the alignment is far from complete, since random thermal collisions tend to destroy the order.\nIn a uniform field, the net force on a current loop is zero\n\\[\\vec{F} = I \\oint ( \\dd \\vec{l} \\cross \\vec{B}) = I \\left( \\oint \\dd \\vec{l} \\right) \\cross \\vec{B} = 0\\] the constant B comes outside the integral, and the net displacement \\( \\oint \\dd \\vec{l} \\) around a closed loop vanishes. In a nonuniform field, this is no longer the case. For example, suppose a circular wire ring of radius R, carrying a current I, is suspended above a short solenoid in the \u0026ldquo;fringing\u0026rdquo; region (Fig 6.3). Here B has a radial component, and there is a net downward force on the loop (Fig 6.4)\n\\[F = 2 \\pi I R B \\cos \\theta \\tagl{6.2}\\] For an infinitesimal loop, with dipole moment m, in a field B, the force is\n\\[\\vec{F} = \\grad (\\vec{m} \\cdot \\vec{B}) \\tagl{6.3}\\] Once again the magnetic formula is identical to its electrical \u0026ldquo;twin,\u0026rdquo; if we write the latter in the form \\( \\vec{F} = \\grad ( \\vec{p} \\cdot \\vec{E}) \\).\nIf you\u0026rsquo;re starting to get a sense of deja vu, perhaps you will have more respect for those early physicists who thought magnetic dipoles consisted of positive and negative magnetic \u0026ldquo;charges\u0026rdquo; (north and south \u0026ldquo;poles,\u0026rdquo; they called them), separated by a small distance, just like electric dipoles (Fig. 6.5(a)). They wrote down a \u0026ldquo;Coulomb\u0026rsquo;s law\u0026rdquo; for the attraction and repulsion of these poles, and developed the whole of magnetostatics in exact analogy to electrostatics. It\u0026rsquo;s not a bad model, for many purposes - it gives the correct field of a dipole (at least, away from the origin), the right torque on a dipole (at least, on a stationary dipole), and the proper force on a dipole (at least, in the absence of external currents). But it\u0026rsquo;s bad physics, because there\u0026rsquo;s no such thing as a single magnetic north pole or south pole. If you break a bar magnet in half, you don\u0026rsquo;t get a north pole in one hand and a south pole in the other; you get two complete magnets. Magnetism is not due to magnetic monopoles, but rather to moving electric charges; magnetic dipoles are tiny current loops (Fig. 6.5(c)), and it\u0026rsquo;s an extraordinary thing, really, that the formulas involving m bear any resemblance to the corresponding formulas for p. Sometimes it is easier to think in terms of the \u0026ldquo;Gilbert\u0026rdquo; model of a magnetic dipole (separated monopoles), instead of the physically correct \u0026ldquo;Ampere\u0026rdquo; model (current loop). Indeed, this picture occasionally offers a quick and clever solution to an otherwise cumbersome problem (you just copy the corresponding result from electrostatics, changing \\( \\vec{p} \\) to \\( \\vec{m} \\), \\( 1 / \\epsilon_0 \\) to \\( \\mu_0 \\), and \\( \\vec{E} \\) to \\( \\vec{B} \\)). But whenever the close-up features of the dipole come into play, the two models can yield strikingly different answers. My advice is to use the Gilbert model, if you like, to get an intuitive \u0026ldquo;feel\u0026rdquo; for a problem, but never rely on it for quantitative results.\n6.1.3: Effect of a Magnetic Field on Atomic Orbits # Electrons not only spin; they also revolve around the nucleus - for simplicity, let\u0026rsquo;s assume the orbit is a circle of radius R (Fig. 6.9). Although technically this orbital motion does not constitute a steady current, in practice the period \\( T = 2 \\pi R / v \\) is so short that unless you blink awfully fast, it\u0026rsquo;s going to look like a steady current:\n\\[I = \\frac{-e}{T} = - \\frac{ev}{2 \\pi R} \\] (The minus sign accounts for the negative charge of the electron.) Accordingly, the orbital dipole moment \\( (I \\pi R^2) \\) is\n\\[\\vec{m} = - \\frac{1}{2} e v R \\vu{z} \\tagl{6.4}\\] Like any other magnetic dipole, this one is subject to a torque \\( \\vec{m} \\cross \\vec{B} \\) when you turn on a magnetic field. But it\u0026rsquo;s a lot harder to tilt the entire orbit than it is the spin, so the orbital contribution to paramagnetism is small. There is, however, a more significant effect on the orbital motion: The electron speeds up or slows down, depending on the orientation of B. For whereas the centripetal acceleration \\( v^2 / R \\) is ordinarily sustained by electrical forces alone,\n\\[\\frac{1}{4 \\pi \\epsilon_0} \\frac{e^2}{R^2} = m_e \\frac{v^2}{R} \\tagl{6.5} \\] in the presence of a magnetic field there is an additional force, \\( -e (\\vec{v} \\cross \\vec{B}) \\). (To avoid confusion with the magnetic dipole moment m, we write the mass of the electron as \\( m_e \\).) For the sake of argument, let\u0026rsquo;s say that B is perpendicular to the plane of orbit, as shown in Fig 6.10; then\n\\[\\frac{1}{4 \\pi \\epsilon_0} \\frac{e^2}{R^2} \u0026#43; e \\overline{v} B = m_e \\frac{\\overline{v}^2}{R} \\tagl{6.6} \\] Under these conditions, the new speed \\( \\overline{v} \\) is greater than v\n\\[e \\overline{v} B = \\frac{m_e}{R} (\\overline{v} ^2 - v^2) = \\frac{m_e}{R} (\\overline{v} \u0026#43; v)(overline{v} - v)\\] or, assuming the change in \\( \\Delta v = \\overline{v} - v \\) is small,\n\\[\\Delta v = \\frac{e R B}{2 m_e} \\tagl{6.7} \\] When B is turned on, then, the electron speeds up.\nA change in orbital speed means a change in the dipole moment (Eq. 6.4):\n\\[\\Delta \\vec{m} = - \\frac{1}{2} e (\\Delta v) R \\vu{z} = - \\frac{e^2 R^2}{4 m_e} \\vec{B} \\tagl{6.8}\\] Notice that the change in m is opposite to the direction of B. (An electron circling the other way would have a dipole moment pointing upward, but such an orbit would be slowed down by the field, so the change is still opposite to B.) Ordinarily, the electron orbits are randomly oriented, and the orbital dipole moments cancel out. But in the presence of a magnetic field, each atom picks up a little \u0026ldquo;extra\u0026rdquo; dipole moment, and these increments are all anti-parallel to the field. This is the mechanism responsible for diamagnetism. It is a universal phenomenon, affecting all atoms. However, it is typically much weaker than paramagnetism, and is therefore observed mainly in atoms with even numbers of electrons, where paramagnetism is usually absent.\nIn deriving Eq. 6.8, I assumed that the orbit remains circular, with its original radius R. I cannot offer a justification for this at the present stage. If the atom is stationary while the field is turned on, then my assumption can be proved - this is not magnetostatics, however, and the details will have to await Chapter 7 (see Prob. 7.52). If the atom is moved into the field, the situation is enormously more complicated. But never mind - I\u0026rsquo;m only trying to give you a qualitative account of diamagnetism. Assume, if you prefer, that the velocity remains the same while the radius changes - the formula (Eq. 6.8) is altered (by a factor of 2), but the qualitative conclusion is unaffected. The truth is that this classical model is fundamentally flawed (diamagnetism is really a quantum phenomenon), so there\u0026rsquo;s not much point in refining the details. What is important is the empirical fact that in diamagnetic materials the induced dipole moments point opposite to the magnetic field.\n6.1.4: Magnetization # In the presence of a magnetic field, matter becomes magnetized; that is, upon microscopic examination, it will be found to contain many tiny dipoles, with a net alignment along some direction. We have discussed two mechanisms that account for this magnetic polarization: (1) paramagnetism (the dipoles associated with the spins of unpaired electrons experience a torque tending to line them up parallel to the field) and (2) diamagnetism (the orbital speed of the electrons is altered in such a way as to change the orbital dipole moment in a direction opposite to the field). Whatever the cause, we describe the state of magnetic polarization by the vector quantity\n\\[\\vec{M} \\equiv \\text{ magnetic dipole moment per unit volume } \\tagl{6.9}\\] M is called the magnetization; it plays a role analogous to the polarization P in electrostatics. In the following section, we will not worry about how the magnetization got there - it could be paramagnetism, diamagnetism, or even ferromagnetism - we shall take M as a given, and calculate the field this magnetization itself produces.\nIncidentally, it may have surprised you to learn that materials other than the famous ferromagnetic trio (iron, nickel, and cobalt) are affected by a magnetic field at all. You cannot, of course, pick up a piece of wood or aluminum with a magnet. The reason is that diamagnetism and paramagnetism are extremely weak: It takes a delicate experiment and a powerful magnet to detect them at all. If you were to suspend a piece of paramagnetic material above a solenoid, as in Fig. 6.3, the induced magnetization would be upward, and hence the force downward. By contrast, the magnetization of a diamagnetic object would be downward and the force upward. In general, when a sample is placed in a region of nonuniform field, the paramagnet is attracted into the field, whereas the diamagnet is repelled away. But the actual forces are pitifully weak - in a typical experimental arrangement the force on a comparable sample of iron would be \\( 10^4 \\) or \\( 10^5 \\) times as great. That\u0026rsquo;s why it was reasonable for us to calculate the field inside a piece of copper wire, say, in Chapter 5, without worrying about the effects of magnetization.\n"},{"id":71,"href":"/r/notes/griffiths/ch6-2/","title":"The Field of a Magnetized Object","section":"Griffiths Introduction to Electrodynamics","content":" 6.2: The Field of a Magnetized Object # 6.2.1: Bound Currents # Suppose we have a piece of magnetized material; the magnetic dipole moment per unit volume, M, is given. What field does this object produce? Well, the vector potential of a single dipole m is given by Eq. 5.85\n\\[\\vec{A}_{dip} (r) = \\frac{\\mu_0}{4 \\pi} \\frac{\\vec{m} \\cross \\vu{r}}{r^2}\\] In the magnetized object, each volume element \\( \\dd \\tau\u0026rsquo; \\) carries a dipole moment \\( M \\dd \\tau\u0026rsquo; \\), so the total vector potential is (Fig 6.11)\n\\[\\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{M}(r\u0026#39;) \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau\u0026#39; \\tagl{6.11}\\] That does it, in principle. But, as in the electrical case, the integral can be cast in a more illuminating form by exploiting the identity\n\\[\\grad \u0026#39; \\frac{1}{\\gr} = \\frac{\\vu{\\gr}}{\\gr^2} \\] With this,\n\\[\\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\left[ \\vec{M}(r\u0026#39;) \\cross \\left( \\grad \u0026#39; \\frac{1}{\\gr} \\right) \\right] \\dd \\tau\u0026#39;\\] Integrating by parts gives\n\\[\\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\left( \\int \\frac{1}{\\gr} [ \\grad \u0026#39; \\cross \\vec{M}(r\u0026#39;) ] \\dd \\tau\u0026#39; - \\int \\grad\u0026#39; \\cross \\left[ \\frac{\\vec{M}(r\u0026#39;)}{\\gr} \\right] \\dd \\tau\u0026#39; \\right)\\] Problem 1.61 invites us to express the latter as a surface integral \\[\\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{1}{\\gr} [ \\grad\u0026#39; \\cross \\vec{M}(r\u0026#39;)] \\dd \\tau\u0026#39; \u0026#43; \\frac{\\mu_0}{4 \\pi} \\oint \\frac{1}{\\gr} [ \\vec{M}(r\u0026#39;) \\cross \\dd \\vec{a\u0026#39;} ] \\tagl{6.12}\\] The first term looks just like the potential of a volume current\n\\[\\vec{J}_b = \\curl \\vec{M} \\tagl{6.13}\\] while the second looks like the potential of a surface current\n\\[\\vec{K_b} = \\vec{M} \\cross \\vu{n} \\tagl{6.14}\\] where \\( \\vu{n} \\) is the normal unit vector. With these definitions,\n\\[\\vec{A}(r) = \\frac{\\mu_0}{4\\pi} \\int_V \\frac{\\vec{J_b}(r\u0026#39;)}{\\gr} \\dd \\tau\u0026#39; \u0026#43; \\frac{\\mu_0}{4 \\pi} \\oint _S \\frac{\\vec{K_b}(r\u0026#39;)}{\\gr} \\dd a\u0026#39; \\tagl{6.15}\\] What this means is that the potential (and hence also the field) of a magnetized object is the same as would be produced by a volume current \\( \\vec{J_b} = \\curl \\vec{M} \\) throughout the material, plus a surface current \\( \\vec{K_b} = \\vec{M} \\cross \\vu{n} \\), on the boundary. Instead of integrating the contributions of all the infinitesimal dipoles, using Eq. 6.11, we first determine the bound currents, and then find the field they produce, in the same way we would calculate the field of any other volume and surface currents. Notice the striking parallel with the electrical case: there the field of a polarized object was the same as that of a bound volume charge \\( \\rho_b = - \\div \\vec{P} \\) plus a bound surface charge \\( \\sigma_b = \\vec{P} \\cdot \\vu{n} \\).\nExample 6.1 # Q Find the magnetic field of a uniformly magnetized sphere A Choosing the z axis along the direction of M (Fig 6.12), we have\n\\[\\vec{J}_b = \\curl \\vec{M} = 0, \\qquad \\vec{K}_b = \\vec{M} \\cross \\vu{n} = M \\sin \\theta \\vu{\\phi}\\] With no bound current density, we can replace our magnetized sphere by a spinning surface current density. Conveniently enough, \\\\( \\vec{M} \\cross \\vu{n} \\\\) looks a whole lot like the current density of a rotating spherical shell of uniform surface charge \\\\( \\sigma \\\\) \\[\\vec{K} = \\sigma \\vec{v} = \\sigma \\omega R \\sin \\theta \\vu{\\phi}\\] which we already worked out in Exercise 5.11. So, identifying \\( \\sigma R \\vec{\\omega} \\rightarrow \\vec{M} \\), we can write down\n\\[\\vec{B} = \\frac{2}{3} \\mu_0 \\vec{M} \\tagl{6.16}\\] inside the sphere, and outside the sphere the field is that of a perfect dipole\n\\[\\vec{m} = \\frac{4}{3} \\pi R^3 \\vec{M}\\] \\[\\rightarrow \\quad \\vec{B} = \\frac{4 \\mu_0 R^3}{9 r^3} (2 \\cos \\theta \\vu{r} \u0026#43; \\sin \\theta \\vu{\\theta} )\\] Notice that the internal field is uniform, like the electric field inside a uniformly polarized sphere (Eq. 4.14), although the actual formulas for the two cases are curiously different ( \\( \\frac{2}{3} \\) in place of \\( -\\frac{1}{3} \\)). The external fields are also analogous: pure dipole in both instances.\n6.2.2: Physical Interpretation of Bound Currents # In the last section, we found that the field of a magnetized object is identical to the field that would be produced by a certain distribution of \u0026ldquo;bound\u0026rdquo; currents, \\( \\vec{J}_b \\) and \\( \\vec{K}_b \\). I want to show you how these bound currents arise physically. This will be a heuristic argument - the rigorous derivation has already been given. Figure 6.15 depicts a thin slab of uniformly magnetized material, with the dipoles represented by tiny current loops. Notice that all the \u0026ldquo;internal\u0026rdquo; currents cancel: every time there is one going to the right, a contiguous one is going to the left. However, at the edge there is no adjacent loop to do the canceling. The whole thing, then, is equivalent to a single ribbon of current I flowing around the boundary (Fig. 6.16).\nWhat is this current, in terms of M? Say that each of the tiny loops has area a and thickness t (Fig 6.17). In terms of the magnetization, its dipole moment is \\( m = M a t \\). In terms of the circulating current I, however, \\( m = I a \\). Therefore \\( I = M t \\), so the surface current is \\( K_b = I / t = M \\). Using the outward-drawn unit vector \\( \\vu{n} \\) (Fig 6.16), the direction of \\( \\vec{K_b} \\) is conveniently indicated by the cross product:\n\\[\\vec{K_b} = \\vec{M} \\cross \\vu{n}\\] (This expression also records the fact that there is no current on the top or bottom surface of the slab; here M is parallel to \\( \\vu{n} \\), so the cross product vanishes.)\nThis bound surface current is exactly what we obtained in Sect. 6.2.1. It is a peculiar kind of current, in the sense that no single charge makes the whole trip - on the contrary, each charge moves only in a tiny little loop within a single atom. Nevertheless, the net effect is a macroscopic current flowing over the surface of the magnetized object. We call it a \u0026ldquo;bound\u0026rdquo; current to remind ourselves that every charge is attached to a particular atom, but it\u0026rsquo;s a perfectly genuine current, and it produces a magnetic field in the same way any other current does.\nWhen the magnetization is nonuniform, the internal currents no longer cancel. Figure 6.18(a) shows two adjacent chunks of magnetized material, with a larger arrow on the one to the right suggesting greater magnetization at that point. On the surface where they join, there is a net current in the x direction, given by\n\\[I_x = [M_z (y \u0026#43; \\dd y) - M_z (y)] \\dd z = \\pdv{M_z}{y} \\dd y \\dd z\\] The corresponding volume current density is therefore\n\\[(J_b)_x = \\pdv{M_z}{y}\\] By the same token, a nonuniform magnetization in the y direction would contribute an amount\n\\[(J_b)_x = \\pdv{M_z}{y} - \\pdv{M_y}{z}\\] In general, then\n\\[\\vec{J_b} = \\curl \\vec{M}\\] consistent, again, with the result of Section 6.2.1.\nIncidentally, like any other steady current, \\( \\vec{J_b} \\) should obey the conservation law 5.33\n\\[\\div \\vec{J_b} = 0\\] Does it? Yes, for the divergence of a curl is always zero.\n6.2.3: The Magnetic Field Inside Matter # Like the electric field, the actual microscopic magnetic field inside matter fluctuates wildly from point to point and instant to instant. When we speak of \u0026ldquo;the\u0026rdquo; magnetic field in matter, we mean the macroscopic field: the average over regions large enough to contain many atoms. (The magnetization M is \u0026ldquo;smoothed out\u0026rdquo; in the same sense.) It is this macroscopic field that one obtains when the methods of Sect. 6.2.1 are applied to points inside magnetized material, as you can prove for yourself in the following problem.\nProblem 6.11 # Q In Sect 6.2.1, we began with the potential of a perfect dipole (Eq. 6.10), whereas in fact we are dealing with physical dipoles. Show, by the method of Section 4.2.3, that we nonetheless get the correct macroscopic field. A As in Section 4.2.3, we want the average of\n\\[\\vec{B} = \\vec{B} _{out} \u0026#43; \\vec{B}_{in}\\] where \\( \\vec{B} _ {out} \\) is due to molecules outside a small sphere around point P, and \\( \\vec{B} _ {in} \\) is due to molecules inside the sphere. The average of \\( \\vec{B} _ {out} \\) is the same as the field at the center, and for this it is OK to use Equation 6.10, since the center is \u0026ldquo;far\u0026rdquo; from all the molecules in question:\n\\[\\vec{A}_{out} = \\frac{\\mu_0}{4 \\pi} \\int_{outside} \\frac{\\vec{M} \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau \\] The average of \\( \\vec{B} _ {in} \\) is \\( \\frac{\\mu_0}{4 \\pi} \\frac{2 \\vec{m}}{R^3} \\) - Equation 5.93 - where \\( \\vec{m} = \\frac{4}{3} \\pi R^3 \\vec{M} \\). Thus the average \\( \\vec{B} _ {in} \\) is \\( 2 \\mu_0 \\vec{M}/3 \\). But what is left out of the integral \\( \\vec{A} _ {out} \\) is the contribution of a uniformly magnetized sphere, to wit \\( 2 \\mu_0 \\vec{M} / 3 \\) (Equation 6.16), and this is precisely what \\( \\vec{B}_{in} \\) puts back in. So we\u0026rsquo;ll get the correct macroscopic field using Equation 6.10.\n"},{"id":72,"href":"/r/notes/griffiths/ch6-3/","title":"The Auxiliary Field H","section":"Griffiths Introduction to Electrodynamics","content":" 6.3: The Auxiliary Field H # 6.3.1: Ampere\u0026rsquo;s Law in Magnetized Materials # In Section 6.2, we found that the effect of magnetization is to establish bound currents \\( \\vec{J_b} = \\curl \\vec{M} \\) within the material and \\( \\vec{K_b} = \\vec{M} \\cross \\vu{n} \\) on the surface. The field due to magnetization of the medium is just the field produced by these bound currents. We are now ready to put everything together: the field attributable to bound currents, plus the field due to everything else - which I will call the free current. The free current might flow through wires embedded in the magnetized substance or, if the latter is a conductor, through the material itself. In any event, the total current can be written as\n\\[\\vec{J} = \\vec{J_b} \u0026#43; \\vec{J_f} \\tagl{6.17}\\] There is no new physics in Eq. 6.17; it is simply a convenience to separate the current into these two parts, because they got there by quite different means: the free current is there because somebody hooked up a wire to a battery - it involves actual transport of charge; the bound current is there because of magnetization - it results from the conspiracy of many aligned atomic dipoles.\nIn view of Eqs. 6.13 and 6.17, Ampere\u0026rsquo;s law can be written\n\\[\\frac{1}{\\mu_0} (\\curl \\vec{B}) = \\vec{J} = \\vec{J_b} \u0026#43; \\vec{J_f} = \\vec{J_f} \u0026#43; (\\curl \\vec{M})\\] or, collecting together the two curls:\n\\[\\curl \\left( \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\right) = \\vec{J_f}\\] The quantity in parentheses is designated by the letter H:\n\\[\\vec{H} \\equiv \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\tagl{6.18}\\] In terms of H, then, Ampere\u0026rsquo;s law reads\n\\[\\curl \\vec{H} = \\vec{J_f} \\tagl{6.19}\\] or, in integral form,\n\\[\\oint \\vec{H} \\cdot \\dd \\vec{l} = I _{f, enc} \\tagl{6.20}\\] where \\( I_{f, enc} \\) is the total free current passing through the Amperian loop.\nH plays a role in magnetostatics analogous to D in electroostatics: Just as D allowed us to write Gauss\u0026rsquo;s law in terms of the free charge alone, H permits us to express Ampere\u0026rsquo;s law in terms of the free current alone - and free current is what we control directly. Bound current, like bound charge, comes along for the ride - the material gets magnetized, and this results in bound currents; we cannot turn them on or off independently, as we can the free currents. In applying Eq. 6.20, all we need to worry about is the free current, which we know about because we put it there. In particular, when symmetry permits, we can calculate H immediately from Eq. 6.20 by the usual Ampere\u0026rsquo;s law methods. (For example, problems 6.7 and 6.8 can be done in one line by noting that \\( \\vec{H} = 0 \\).)\nExample 6.2 # Q A long copper rod of radius \\( R \\) carries a uniformly distributed (free) current \\( I \\) (Fig 6.19). Find H inside and outside the rod. A Copper is weakly diamagnetic, so the dipoles will line up opposite to the field. This results in a bound current running antiparallel to \\( I \\), within the wire, and parallel to \\( I \\) along the surface (Fig 6.20). Just how great these bound currents will be we are not yet in a position to say - but in order to calculate H it is sufficient to realize that all the currents are longitudinal, so \\( \\vec{B} \\), \\( \\vec{M} \\), and therefore also \\( \\vec{H} \\), are circumferential. Applying Eq. 6.20 to an Amperian loop of radius \\( s \u0026lt; R \\),\n\\[H (2 \\pi s) = I_{f, enc} = I \\frac{\\pi s^2}{\\pi R^2} \\] so, inside the wire,\n\\[\\vec{H} = \\frac{I}{2 \\pi R^2} s \\vu{\\phi} \\quad (s \\leq R) \\tagl{6.21} \\] Outside the wire,\n\\[\\vec{H} = \\frac{I}{2 \\pi s} \\vu{\\phi} \\quad (s \\geq R) \\tagl{6.22}\\] Outside of the wire (as always in empty space) \\( \\vec{M} = 0 \\), so\n\\[\\vec{B} = \\mu_0 \\vec{H} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} \\quad (s \\geq R)\\] the same as for a nonmagnetized wire (Ex. 5.7). Within the wire, we are not yet quite ready to determine B, since we have no way of knowing M. (In practice, the magnetization of copper is so small that for most purposes we can ignore it altogether.)\nAs it turns, out, H is a more useful quantity than D. In the laboratory, you will frequently hear people talking about H (more often even than B), but you will never hear anyone speak of D (only E). The reason is this: to build an electromagnet you run a certain (free) current through a coil. The current is the thing you read on the dial, and this determines H (or at any rate, the line integral of H); B depends on the specific materials you used and even, if iron is present, on the history of your magnet. On the other hand, if you want to set up an electric field, you do not plaster a known free charge on the plates of a parallel plate capacitor; rather, you connect them to a battery of known voltage. It\u0026rsquo;s the potential difference you read on your dial, and that determines E (or rather, the line integral of E); D depends on the details of the dielectric you\u0026rsquo;re using. If it were easy to measure charge, and hard to measure potential, then you\u0026rsquo;d find experimentalists talking about D instead of E. So the relative familiarity of H as contrasted with D derives from purely practical considerations; theoretically, they\u0026rsquo;re on an equal footing.\nMany authors call H, not B, the \u0026ldquo;magnetic field.\u0026rdquo; Then they have to invent a new word for B: the \u0026ldquo;flux density,\u0026rdquo; or magnetic \u0026ldquo;induction\u0026rdquo; (an absurd choice, since that term already has at least two other meanings in electrodynamics). Anyway, B is indisputably the fundamental quantity, so I shall continue to call it the \u0026ldquo;magnetic field,\u0026rdquo; as everyone does in the spoken language. H has no sensible name: just call it \u0026ldquo;H.\u0026rdquo;\n6.3.2: A Deceptive Parallel # Equation 6.19 looks just like Ampere\u0026rsquo;s original law, except that the total current is replaced by the free current, and \\( \\vec{B} \\) is replaced by \\( \\mu_0 \\vec{H} \\). As in the case of D, however, I must warn you against reading too much into this correspondence. It does not say that \\( \\mu_0 \\vec{H} \\) is \u0026ldquo;just like \\( \\vec{B} \\), only its source is \\( \\vec{J_f} \\) instead of \\( \\vec{J} \\).\u0026rdquo; For the curl alone does not determine a vector field - you must also know the divergence. And whereas \\( \\div \\vec{B} = 0 \\), the divergence of H is not, in general, zero. In fact, from Eq. 6.18,\n\\[\\div \\vec{H} = - \\div \\vec{M} \\tagl{6.23}\\] Only when the divergence of M vanishes is the parallel between \\( \\vec{B} \\) and \\( \\mu_0 \\vec{H} \\) faithful.\nIf you think I\u0026rsquo;m being pedantic, consider the example of a bar magnet - a short cylinder of iron that carries a permanent uniform magnetization M parallel to its axis. In this case there is no free current anywhere, and a naive application of Eq. 6.20 might lead you to suppose that \\( \\vec{H} = 0 \\), and hence that \\( \\vec{B} = \\mu_0 M \\) inside the magnet and \\( \\vec{B} = 0 \\) outside, which is nonsense. It is quite true that the curl of H vanishes everywhere, but the divergence does not (check top and bottom surfaces of the magnet!). Advice: When you are asked to find B or H in a problem involving magnetic materials, first look for symmetry. If the problem exhibits cylindrical, plane, solenoidal, or toroidal symmetry, then you can get H directly from Eq. 6.20 by the usual Ampere\u0026rsquo;s law methods. (Evidently, in such cases \\( \\div \\vec{M} \\) is automatically zero, since the free current alone determines the answer.) If the requisite symmetry is absent, you\u0026rsquo;ll have to think of another approach, and in particular you must not assume that H is zero just because there is no free current in sight.\n6.3.3: Boundary Conditions # The magnetostatic boundary conditions of Section 5.4.2 can be rewritten in terms of H and the free current. From Eq. 6.23 it follows that\n\\[H_{above} ^\\perp - H_{below} ^\\perp = -(M_{above} ^\\perp - M_{below} ^\\perp) \\tagl{6.24}\\] while Eq. 6.19 says\n\\[\\vec{H}_{above} ^\\parallel - \\vec{H}_{below} ^\\parallel = \\vec{K}_f \\cross \\vu{n} \\tagl{6.25}\\] In the presence of materials, these are sometimes more useful than the corresponding boundary conditions on B (Eqs. 5.74 and 5.76)\n\\[B_{above} ^\\perp - B_{below} ^\\perp = 0 \\tagl{6.26}\\] and\n\\[\\vec{B}_{above} ^\\parallel - \\vec{B}_{below} ^\\parallel = \\mu_0 (\\vec{K} \\cross \\vu{n} ) \\tagl{6.27}\\] "},{"id":73,"href":"/r/notes/griffiths/ch6-4/","title":"Linear and Nonlinear Media","section":"Griffiths Introduction to Electrodynamics","content":" 6.4: Linear and Nonlinear Media # 6.4.1: Magnetic Susceptibility and Permeability # In paramagnetic and diamagnetic materials, the magnetization is sustained by the field; when B is removed, M disappears. In fact, for most substances the magnetization is proportional to the field, provided the field is not too strong. For notational consistency with the electrical case (Eq. 4.30), I should express the proportionality thus\n\\[\\vec{M} = \\frac{1}{\\mu_0} \\chi_m \\vec{B} \\tagl{6.28} \\] but custom dictates that it be written in terms of H, instead of B:\n\\[\\vec{M} = \\chi_m \\vec{H}\\] The constant of proportionality \\( \\chi_m \\) is called the magnetic susceptibility; it is a dimensionless quantity that varies from one substance to another - positive for paramagnets and negative for diamagnets. Typical values are around \\( 10^{-5} \\).\nMaterials that obey Eq. 6.29 are called linear media. In view of Eq. 6.18,\n\\[\\vec{B} = \\mu_0 (\\vec{H} \u0026#43; \\vec{M}) = \\mu_0 (1 \u0026#43; \\chi_m) \\vec{H} \\tagl{6.30}\\] for linear media. Thus B is also proportional to H:\n\\[\\vec{B} = \\mu \\vec{H} \\tagl{6.31}\\] where\n\\[\\mu \\equiv \\mu_0 (1 \u0026#43; \\chi_m) \\tagl{6.32}\\] \\( \\mu \\) is called the permeability of the material. In a vacuum, where there is no matter to magnetize, the susceptibility \\( \\chi_m \\) vanishes, and the permeability is \\( \\mu_0 \\). That\u0026rsquo;s why \\( \\mu_0 \\) is called the permeability of free space.\nExample 6.3 # Q An infinite solenoid (n turns per unit length, current I), is filled with linear material of susceptibility \\( \\chi_m \\). Find the magnetic field inside the solenoid.\nA Since B is due in part to the bound currents (which we don\u0026rsquo;t yet know), we cannot compute it directly. However, this is one of those symmetrical cases in which we can get H from the free current alone, using Ampere\u0026rsquo;s law in the form of Eq. 6.20\n\\[\\vec{H} = n I \\vu{z}\\] According to Eq. 6.31, then\n\\[\\vec{B} = \\mu_0(1 \u0026#43; \\chi_m) n I \\vu{z}\\] If the medium is paramagnetic, the field is slightly enhanced; if it\u0026rsquo;s diamagnetic, the field is somewhat reduced. This reflects the fact that the bound surface current\n\\[\\vec{K}_b = \\vec{M} \\cross \\vu{n} = \\chi_m (\\vec{H} \\cross \\vu{n} ) = \\chi_m n I \\vu{\\phi}\\] is in the same direction as I, in the former case (\\( \\chi_m \u0026gt; 0 \\)), and opposite in the latter (\\( \\chi_m \u0026lt; 0 \\)).\nYou might suppose that linear media escape the defect in the parallel between B and H: since M and H are now proportional to B, does it not follow that their divergence, like B\u0026rsquo;s, must always vanish? Unfortunately, it does not; at the boundary between two materials of different permeability, the divergence of M can actually be infinite. For instance, at the end of a cylinder of linear paramagnetic material, M is zero on one side but not on the other. For the \u0026ldquo;Gaussian pillbox\u0026rdquo; shown in Fig 6.23, \\( \\oint \\vec{M} \\cdot \\dd \\vec{a} \\neq 0 \\), and hence, by the divergence theorem, \\( \\div \\vec{M} \\) cannot vanish everywhere within it.\nIncidentally, the volume bound current density in a homogeneous linear material is proportional to the free current density:\n\\[\\vec{J}_b = \\curl \\vec{M} = \\curl (\\chi_m \\vec{H}) = \\chi_m \\vec{J_f} \\tagl{6.33}\\] In particular, unless free current actually flows through the material, all bound current will be at the surface.\n6.4.2: Ferromagnetism # In a linear medium, the alignment of atomic dipoles is maintained by a magnetic field imposed from the outside. Ferromagnets - which are emphatically not linear - require no external fields to sustain magnetization; the alignment is \u0026ldquo;frozen in.\u0026rdquo; Like paramagnetism, ferromagnetism involves the magnetic dipoles associated with the spins of unpaired electrons. The new feature, which makes ferromagnetism so different from paramagnetism, is the interaction between nearby dipoles: In a ferromagnet, each dipole \u0026ldquo;likes\u0026rdquo; to point in the same direction as its neighbors. The reason for this preference is essentially quantum mechanical, and I won\u0026rsquo;t try to explain it here; it is enough to know that the correlation is so strong as to align virtually 100% of the unpaired electron spins. If you could somehow magnify a piece of iron and \u0026ldquo;see\u0026rdquo; the individual dipoles as tiny arrows, it would look something like Fig. 6.25, with all the spins pointing the same way.\nBut if that is true, why isn\u0026rsquo;t every wrench and nail a powerful magnet? The answer is that the alignment occurs in relatively small patches, called domains. Each domain contains billions of dipoles, all lined up (these domains are actually visible under a microscope, given suitable etching techniques - see Fig 6.26), but the domains themselves are randomly oriented. The household wrench contains an enormous number of domains, and their magnetic fields cancel, so the wrench as a whole is not magnetized. (Actually, the orientation of domains is not completely random; within a given crystal, there may be some preferential alignment along the crystal axes. But there will be just as many domains pointing one way as the other, so there is still no large-scale magnetization. Moreover, the crystals themselves are randomly oriented within any sizable chunk of metal.)\nHow, then, would you produce a permanent magnet, such as they sell in toy stores? If you put a piece of iron into a strong magnetic field, the torque \\( \\vec{N} = \\vec{m} \\cross \\vec{B} \\) tends to align the dipoles parallel to the field. Since they like to stay parallel to their neighbors, most of the dipoles will resist this torque. However, at the boundary between two domains, there are competing neighbors, and the torque will throw its weight on the side of the domain most nearly parallel to the field; this domain will win some converts, at the expense of the less favorably oriented one. The net effect of the magnetic field, then, is to move the domain boundaries. Domains parallel to the field grow, and the others shrink. If the field is strong enough, one domain takes over entirely, and the iron is said to be saturated.\nIt turns out that this process (the shifting of domain boundaries in response to an external field) is not entirely reversible: When the field is switched off, there will be some return to randomly oriented domains, but it is far from complete; there remains a preponderance of domains in the original direction. You now have a permanent magnet.\nA simple way to accomplish this, in practice, is to wrap a coil of wire around the object to be magnetized (Fig. 6.27). Run a current I through the coil; this provides the external magnetic field (pointing to the left in the diagram). As you increase the current, the field increases, the domain boundaries move, and the magnetization grows. Eventually, you reach the saturation point, with all the dipoles aligned, and a further increase in current has no effect on M (Fig. 6.28, point b).\nNow suppose you reduce the current. Instead of retracing the path back to \\( \\vec{M} = 0 \\), there is only a partial return to randomly oriented domains; M decreases, but even with the current off there is some residual magnetization (point c). The wrench is now a permanent magnet. If you want to eliminate the remaining magnetization, you\u0026rsquo;ll have to run a current backwards through the coil (a negative I). Now the external field points to the right, and as you increase I (negatively), M drops down to zero (point d). If you turn I still higher, you soon reach saturation in the other direction-all the dipoles now pointing to the right (e). At this stage, switching off the current will leave the wrench with a permanent magnetization to the right (point f). To complete the story, turn I on again in the positive sense: M returns to zero (point g), and eventually to the forward saturation point (b).\nThe path we have traced out is called a hysteresis loop. Notice that the magnetization of the wrench depends not only on the applied field (that is, on/), but also on its previous magnetic \u0026ldquo;history.\u0026rdquo; For instance, at three different times in our experiment the current was zero (a, c, and f), yet the magnetization was different for each of them. Actually, it is customary to draw hysteresis loops as plots of B against H, rather than M against I. (If our coil is approximated by a long solenoid, with n turns per unit length, then \\( H = nI \\), so H and I are proportional. Meanwhile, \\( \\vec{B} = \\mu_0 (\\vec{H} + \\vec{M}) \\), but in practice M is huge compared to H, so to all intents and purposes B is proportional to M.)\nTo make the units consistent (teslas), I have plotted \\( (\\mu_0 H) \\) horizontally (Fig. 6.29); notice, however, that the vertical scale is \\( 10^4 \\) times greater than the horizontal one. Roughly speaking, \\( \\mu_0 \\vec{H} \\) is the field our coil would have produced in the absence of any iron; B is what we actually got, and compared to \\( \\mu_0 \\vec{H} \\), it is gigantic. A little current goes a long way, when you have ferromagnetic materials around. That\u0026rsquo;s why anyone who wants to make a powerful electromagnet will wrap the coil around an iron core. It doesn\u0026rsquo;t take much of an external field to move the domain boundaries, and when you do that, you have all the dipoles in the iron working with you.\nOne final point about ferromagnetism: It all follows, remember, from the fact that the dipoles within a given domain line up parallel to one another. Random thermal motions compete with this ordering, but as long as the temperature doesn\u0026rsquo;t get too high, they cannot budge the dipoles out of line. It\u0026rsquo;s not surprising, though, that very high temperatures do destroy the alignment. What is surprising is that this occurs at a precise temperature (\\( 770^\\circ C \\), for iron). Below this temperature (called the Curie point), iron is ferromagnetic; above, it is paramagnetic. The Curie point is rather like the boiling point or the freezing point in that there is no gradual transition from ferro- to para-magnetic behavior, any more than there is between water and ice. These abrupt changes in the properties of a substance, occurring at sharply defined temperatures, are known in statistical mechanics as phase transitions.\n"},{"id":74,"href":"/r/notes/griffiths/ch7-1/","title":"Electromotive Force","section":"Griffiths Introduction to Electrodynamics","content":" 7.1 Electromotive Force # 7.1.1: Ohm\u0026rsquo;s Law # To make a current flow, you have to push on the charges. How fast they move, in response to a given push, depends on the nature of the material. For most sub- stances, the current density \\( \\vec{J} \\) is proportional to the force per unit charge, \\( \\vec{f} \\):\n\\[\\vec{J} = \\sigma \\vec{f} \\tagl{7.1}\\] The proportionality factor \\( \\sigma \\) (not to be confused with surface charge) is an empirical constant that varies from one material to another; it\u0026rsquo;s called the conductivity of the medium. Actually, the handbooks usually list the reciprocal of \\( \\sigma \\), called the resistivity: \\( \\rho = 1 / \\sigma \\) (not to be confused with charge density - I\u0026rsquo;m sorry, but we\u0026rsquo;re running out of Greek letters, and this is the standard notation). Some typical values are listed in Table 7.1. Notice that even insulators conduct slightly, though the conductivity of a metal is astronomically greater; in fact, for most purposes metals can be regarded as perfect conductors, with \\( \\sigma = \\infty \\) , while for insulators we can pretend \\( \\sigma = 0 \\).\nIn principle, the force that drives the charges to produce the current could be anything - chemical, gravitational, or trained ants with tiny harnesses. For our purposes, though, it\u0026rsquo;s usually an electromagnetic force that does the job. In this case \\( \\eqref{7.1} \\) becomes\n\\[\\vec{J} = \\sigma (\\vec{E} \u0026#43; \\vec{v} \\cross \\vec{B}) \\tagl{7.2}\\] Ordinarily, the velocity of the charges is sufficiently small that the second term can be ignored:\n\\[\\vec{J} = \\sigma \\vec{E} \\tagl{7.3}\\] (However, in plasmas, for instance, the magnetic contribution to \\( \\vec{f} \\) can be significant.) Equation 7.3 is called Ohm\u0026rsquo;s law, thought the physics behind it is really contained in Eq. 7.1, of which 7.3 is just a special case.\nRemember back in chapter 2 when we talked about conductors and determined that \\( \\vec{E} = 0 \\) inside a conductor? Well, that was the case for stationary charges \\( ( \\vec{J} = 0) \\). Moreover, for perfect conductors \\( \\vec{E} = \\vec{J} / \\sigma = 0 \\) even if current is flowing. In practice, metals are such good conductors that the electric field required to drive currents in them is negligible. Thus we routinely treat the connecting wires in electric circuits (for example) as equipotentials. Resistors, by contrast, are made of poorly conducting materials.\nExample 7.1 # Q A cylindrical resistor of cross-sectional area \\( A \\) and length \\( L \\) is made from material with conductivity \\( \\sigma \\) . (See Fig. 7.1; as indicated, the cross section need not be circular, but I do assume it is the same all the way down.) If we stipulate that the potential is constant over each end, and the potential difference between the ends is \\( V \\), what current flows?\nA As it turns out, the electric field is uniform within the wire (I\u0026rsquo;ll prove this in a moment). It follows from Eq. 7.3 that the current density is also uniform, so\n\\[I = J A = \\sigma E A = \\frac{\\sigma A}{L} V\\] Example 7.2 # Q Two long coaxial metal cylinders (radii \\( a \\) and \\( b \\)) are separated by material of conductivity \\( \\sigma \\) (Fig. 7.2). If they are maintained at a potential difference \\( V \\), what current flows from one to the other, in a length \\( L \\)?\nA The field between the cylinders is\n\\[\\vec{E} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s}\\] where \\( \\lambda \\) is the charge per unit length on the inner cylinder. The current is therefore\n\\[I = \\int \\vec{J} \\cdot \\dd \\vec{a} = \\sigma \\int \\vec{E} \\cdot \\dd \\vec{a} = \\frac{\\sigma}{\\epsilon_0} \\lambda L\\] (The integral is over any surface enclosing the inner cylinder.) Meanwhile, the potential difference between the cylinders is\n\\[V = - \\int_b ^a \\vec{E} \\cdot \\dd \\vec{l} = \\frac{\\lambda}{2 \\pi \\epsilon_0} \\ln \\left( \\frac{b}{a} \\right)\\] so\n\\[I = \\frac{2 \\pi \\sigma L}{\\ln (b / a)} V\\] As these examples illustrate, the total current flowing from one electrode to the other is proportional to the potential difference between them:\n\\[V = I R \\tagl{7.4}\\] This, of course, is the more familiar version of Ohm\u0026rsquo;s law. The constant of proportionality \\( R \\) is called the resistance; it\u0026rsquo;s a function of the geometry of the arrangement and the conductivity of the medium between the electrodes. (In Ex. 7.1, \\( R = L / \\sigma A \\); in Ex. 7.2, \\( R = \\ln (b /a ) / 2 \\pi \\sigma L \\)) Resistance is measured in ohms (\\( \\Omega \\)): an ohm is a volt per ampere. Notice that the proportionality between \\( V \\) and \\( I \\) is a direct consequence of Eq. 7.3: if you want to double \\( V \\), you simply double the charge on the electrodes - that doubles \\( \\vec{E} \\), which (for an ohmic material) doubles \\( \\vec{J} \\), which doubles \\( \\vec{I} \\).\nFor steady currents and uniform conductivity,\n\\[\\div \\vec{E} = \\frac{1}{\\sigma} \\div \\vec{J} = 0 \\tagl{7.5}\\] and therefore the charge density is zero; any unbalanced charge resides on the surface. (We proved this long ago, for the case of stationary charges, using the fact that \\( \\vec{E} = 0 \\); evidently, it is still true when the charges are allowed to move.) It follows, in particular, that Laplace\u0026rsquo;s equation holds within a homogeneous ohmic material carrying a steady current, so all the tools and tricks of Chapter 3 are available for calculating the potential.\nExample 7.3 # Q I asserted that the field in exercise 7.1 is uniform. Let\u0026rsquo;s prove it A Within the cylinder \\( V \\) obeys Laplace\u0026rsquo;s equation. What are the boundary conditions? At the left end the potential is constant - we may as well set it equal to zero. At the right end the potential is likewise constant - call it \\( V_0 \\). On the cylindrical surface, \\( \\vec{J} \\cdot \\vu{n} = 0 \\), or else charge would be leaking out into the surrounding space (which we take to be nonconducting). Therefore \\( \\vec{E} \\cdot \\vu{n} = 0 \\) , and hence \\( \\pdv{V}{n} = 0 \\) . With \\( V \\) or its normal derivative specified on all surfaces, the potential is uniquely determined (Prob. 3.5). But it\u0026rsquo;s easy to guess one potential that obeys Laplace\u0026rsquo;s equation and fits these boundary conditions:\n\\[V(z) = \\frac{V_0 z}{L} \\] where \\( z \\) is measured along the axis. The uniqueness theorem guarantees that this is the solution. The corresponding field is\n\\[\\vec{E} = - \\grad V = - \\frac{V_0}{L} \\vu{z}\\] which is indeed uniform.\nContrast the enormously more difficult problem that arises if the conducting material is removed, leaving only a metal plate at either end (Fig 7.3). Evidently in the present case charge arranges itself over the surface of the wire in just such a way as to produce a nice uniform field within.\nI don\u0026rsquo;t suppose there is any formula in physics more familiar than Ohm\u0026rsquo;s law, and yet it\u0026rsquo;s not really a true law, in the sense of Coulomb\u0026rsquo;s or Ampere\u0026rsquo;s; rather, it is a \u0026ldquo;rule of thumb\u0026rdquo; that applies pretty well to many substances. You\u0026rsquo;re not going to win a Nobel prize for finding an exception. In fact, when you stop to think about it, it\u0026rsquo;s a little surprising that Ohm\u0026rsquo;s law ever holds. After all, a given field \\( \\vec{E} \\) produces a force \\( q \\vec{E} \\) (on a charge \\( q \\) ), and according to Newton\u0026rsquo;s second law, the charge will accelerate. But if the charges are accelerating, why doesn\u0026rsquo;t the current increase with time, growing larger and larger the longer you leave the field on? Ohm\u0026rsquo;s law implies, on the contrary, that a constant field produces a constant current, which suggests a constant velocity. Isn\u0026rsquo;t that a contradiction to Newton\u0026rsquo;s law?\nNo, for we are forgetting the frequent collisions electrons make as they pass down the wire. It\u0026rsquo;s a little like this: Suppose you\u0026rsquo;re driving down a street with a stop sign at every intersection, so that, although you accelerate constantly in between, you are obliged to start all over again with each new block. Your average speed is then a constant, in spite of the fact that (save for the periodic abrupt stops) you are always accelerating. If the length of a block is \\( \\lambda \\) and your acceleration is \\( a \\) , the time it takes to go a block is\n\\[t = \\sqrt{ \\frac{2 \\lambda}{a} }\\] and hence your average velocity is\n\\[v_{ave} = \\frac{1}{2} a t = \\sqrt{\\frac{\\lambda a}{2} }\\] But wait! That\u0026rsquo;s no good either! It says that the velocity is proportional to the square root of the acceleration, and therefore that the current should be proportional to the square root of the field! There\u0026rsquo;s another twist to the story: In practice, the charges are already moving very fast because of their thermal energy. But the thermal velocities \\( v_{th} \\) have random directions, and average to zero. The drift velocity we are concerned with is a tiny extra bit (Prob. 5.20). So the time between collisions is actually much shorter than we supposed; if we assume for the sake of argument that all charges travel the same distance \\( \\lambda \\) between collisions, then\n\\[t = \\frac{\\lambda}{v_{th}} \\] and therefore\n\\[v_{ave} = \\frac{1}{2} a t = \\frac{a \\lambda}{2 v_{th}} \\] If there are \\( n \\) molecules per unit volume, and \\( f \\) free electrons per molecule, each with charge \\( q \\) and mass \\( m \\), the current density is\n\\[\\vec{J} = n f q \\vec{v}_{ave} = \\frac{n f q \\lambda}{2 v_{th}} \\frac{\\vec{F}}{m} = \\left( \\frac{n f \\lambda q^2}{2 m v_{th}} \\right) \\vec{E} \\tagl{7.6}\\] We can\u0026rsquo;t claim that Eq. 7.6 is an accurate formula for the conductivity, but it does have all the basic ingredients, and it correctly predicts that conductivity is proportional to the density of the moving charges, and (ordinarily), decreases with increasing temperature.\nAs a result of all the collisions, the work done by the electrical force is converted into heat in the resistor. Since the work done per unit charge is \\( V \\) and the charge flowing per unit time is \\( I \\) , the power delivered is\n\\[P = V I = I^2 R \\tagl{7.7}\\] This is the Joule heating law. With \\( I \\) in amperes and \\( R \\) in ohms, \\( P \\) comes out in watts (joules per second).\n7.1.2: Electromotive Force # If you think about a typical electric circuit - a battery hooked up to a light bulb, say (Fig. 7.7 ) - a perplexing question arises: In practice, the current is the same all the way around the loop; why is this the case, when the only obvious driving force is inside the battery? Off hand, you might expect a large current in the battery and none at all in the lamp. Who\u0026rsquo;s doing the pushing, in the rest of the circuit, and how does it happen that this push is exactly right to produce the same current in each segment? What\u0026rsquo;s more, given that the charges in a typical wire move (literally) at a snail\u0026rsquo;s pace (see Prob. 5.20), why doesn\u0026rsquo;t it take half an hour for the current to reach the light bulb? How do all the charges know to start moving at the same instant?\nAnswer: If the current were not the same all the way around (for instance, during the first split second after the switch is closed), then charge would be piling up somewhere, and - here\u0026rsquo;s the crucial point - the electric field of this accumulating charge is in such a direction as to even out the flow. Suppose, for instance, that the current into the bend in Fig. 7.8 is greater than the current out. Then charge piles up at the \u0026ldquo;knee,\u0026rdquo; and this produces a field aiming away from the kink. This field opposes the current flowing in (slowing it down) and promotes the current flowing out (speeding it up) until these currents are equal, at which point there is no further accumulation of charge, and equilibrium is established. It\u0026rsquo;s a beautiful system, automatically self-correcting to keep the current uniform, and it does it all so quickly that, in practice, you can safely assume the current is the same all around the circuit, even in systems that oscillate at radio frequencies.\nThere are really two forces involved in driving current around a circuit: the source, \\( \\vec{f}_S \\) , which is ordinarily confined to one portion of the loop (a battery, say), and an electrostatic force, which serves to smooth out the flow and communicate the influence of the source to distant parts of the circuit:\n\\[\\vec{f} = \\vec{f}_S \u0026#43; \\vec{E} \\tagl{7.8}\\] The physical agency responsible for \\( \\vec{f}_S \\) can be many different things: in a battery it\u0026rsquo;s a chemical force; in a piezoelectric crystal mechanical pressure is converted into an electrical impulse; in a thermocouple it\u0026rsquo;s a temperature gradient that does the job; in a photoelectric cell it\u0026rsquo;s light; and in a Van de Graaff generator the electrons are literally loaded onto a conveyor belt and swept along. Whatever the mechanism, its net effect is determined by the line integral off around the circuit:\n\\[\\mathcal{E} = \\oint \\vec{f} \\cdot \\dd \\vec{l} = \\oint \\vec{f}_S \\cdot \\dd \\vec{l} \\tagl{7.9}\\] ( Because \\( \\oint \\vec{E} \\cdot \\dd \\vec{l} = 0 \\) for electrostatic fields, it doesn\u0026rsquo;t matter whether you use \\( \\vec{f} \\) or \\( \\vec{f}_S \\)). \\( \\mathcal{E} \\) is called the electromotive force, or emf, of the circuit. It\u0026rsquo;s a lousy term, since this is not a force at all - it\u0026rsquo;s the integral of a force per unit charge. Some people would prefer the word electromotance, but emf is so established that I think we\u0026rsquo;d better stick with it.\nWithin an ideal source of emf (a resistanceless battery, for instance), the net force on the charges is zero (Eq. 7.1 with \\( \\sigma = \\infty \\) ), so \\( \\vec{E} = - \\vec{f}_S \\). The potential difference between the terminals (a and b) is therefore\n\\[V = - \\int _a ^b \\vec{E} \\cdot \\dd \\vec{l} = \\int_a ^b \\vec{f}_S \\cdot \\dd \\vec{l} = \\oint \\vec{f}_S \\cdot \\dd \\vec{l} = \\mathcal{E} \\tagl{7.10}\\] (we can extend the integral to the entire loop because \\( \\vec{f}_s = 0 \\) outside the source). The function of a battery, then, is to establish and maintain a voltage difference equal to the electromotive force (a 6V battery, for example, holds the positive terminal 6V above the negative terminal). The resulting electrostatic field drives current around the rest of the circuit (notice, however, that inside the battery \\( \\vec{f}_S \\) drives current in the direction opposite to \\( \\vec{E} \\) ).\nBecause it\u0026rsquo;s the line integral of \\( \\vec{f}_S \\), \\( \\mathcal{E} \\) can be interpreted as the work done per unit charge, by the source - indeed, in some books the electromotive force is defined this way. However, as you\u0026rsquo;ll soon see, there is some subtlety involved in this interpretation, so I prefer Eq. 7.9.\n7.1.3: Motional emf # In the last section, I listed several possible sources of electromotive force, batteries being the most familiar. But I did not mention the commonest one of all: the generator. Generators exploit motional emfs, which arise when you move a wire through a magnetic field. Figure 7.10 suggests a primitive model for a generator. In the shaded region there is a uniform magnetic field \\( \\vec{B} \\) , pointing into the page, and the resistor \\( R \\) represents whatever it is (maybe a light bulb or a toaster) we\u0026rsquo;re trying to drive current through. If the entire loop is pulled to the right with speed \\( v \\) , the charges in segment ab experience a magnetic force whose vertical component \\( q v B \\) drives current around the loop, in the clockwise direction. The emf is\n\\[\\mathcal{E} = \\oint \\vec{f}_{mag} \\cdot \\dd \\vec{l} = v B h \\tagl{7.11}\\] Notice that the integral you perform to calculate \\( \\mathcal{E} \\) (Eq. 7.9 or 7.11) is carried out at one instance in time - take a \u0026ldquo;snapshot\u0026rdquo; of the loop, if you like, and work from that. Thus \\( \\dd \\vec{l} \\), for the segment ab in Fig 7.10, points straight up, even though the loop is moving to the right. You can\u0026rsquo;t quarrel with this - it\u0026rsquo;s simply the way emf is defined - but it\u0026rsquo;s important to be clear about it.\nIn particular, although the magnetic force is responsible for establishing the emf, it is not doing any work - magnetic forces never do work. Who, then, is supplying the energy that heats the resistor? Answer: The person who\u0026rsquo;s pulling on the loop. With the current flowing, the free charges in segment ab have a vertical velocity (call it \\( \\vec{u} \\) ) in addition to the horizontal velocity \\( \\vec{v} \\) they inherit from the motion of the loop. Accordingly, the magnetic force has a component \\( q u \\vec{B} \\) to the left. To counteract this, the person pulling on the wire must exert a force per unit charge\n\\[f_{pull} = u B\\] to the right (Fig 7.11). This force is transmitted to the charge by the structure of the wire.\nMeanwhile, the particle is usually moving in the direction of the resultant velocity \\( \\vec{w} \\), and the distance it goes is \\( (h / \\cos \\theta) \\). The work done per unit charge is therefore\n\\[\\int f_{pull} \\cdot \\dd \\vec{l} = (u B) \\left( \\frac{h}{\\cos \\theta} \\right)\\sin \\theta = v B h = \\mathcal{E}\\] (\\( \\sin \\theta \\) coming from the dot product). As it turns out, then, the work done per unit charge is exactly equal to the emf, though the integrals are taken along entirely different paths (Fig. 7.12), and completely different forces are involved. To calculate the emf, you integrate around the loop at one instant, but to calculate the work done you follow a charge in its journey around the loop; \\( f_{pull} \\) contributes nothing to the emf, because it is perpendicular to the wire, whereas \\( f_{mag} \\) contributes nothing to work because it is perpendicular to the motion of the charge.\nThere is a particularly nice way of expressing the emf generated in a moving loop. Let \\( \\Phi \\) be the flux of \\( \\vec{B} \\) through the loop:\n\\[\\Phi \\equiv \\int \\vec{B} \\cdot \\dd \\vec{a} \\tagl{7.12}\\] For the rectangular loop in Fig 7.10,\n\\[\\Phi = B h x\\] As the loop moves, the flux decreases:\n\\[\\dv{\\Phi}{t} = B h \\dv{x}{t} = - B h v\\] (The minus sign accounts for the fact that dx/dt is negative.) But this is precisely the emf (Eq. 7.11); evidently the emf generated in the loop is minus the rate of change of flux through the loop:\n\\[\\mathcal{E} = - \\dv{\\Phi}{t} \\tagl{7.13}\\] This is the flux rule for motional emf.\nApart from its delightful simplicity, the flux rule has the virtue of applying to non-rectangular loops moving in arbitrary directions through nonuniform magnetic fields; in fact, the loop need not even maintain a fixed shape.\nProof. Figure 7.13 shows a loop of wire at time \\( t \\) , and also a short time \\( \\dd t \\) later. Suppose we compute the flux at time \\( t \\) , using surface \\( S \\) , and the flux at time \\( t + \\dd t \\) , using the surface consisting of \\( S \\) plus the \u0026ldquo;ribbon\u0026rdquo; that connects the new position of the loop to the old. The change in flux, then, is\n\\[\\dd \\Phi = \\Phi(t \u0026#43; \\dd t) - \\Phi(t) = \\Phi_{ribbon} = \\int_{ribbon} \\vec{B} \\cdot \\dd \\vec{a}\\] Focus your attention on point \\\\( P \\\\): in time \\\\( \\dd t \\\\) it moves to \\\\( P' \\\\). Let \\\\( \\vec{v} \\\\) be the velocity of the wire, and \\\\( \\vec{u} \\\\) the velocity of a charge down the wire; \\\\( \\vec{w} = \\vec{v} + \\vec{u} \\\\) is the resultant velocity of a charge at \\\\( P \\\\). The infinitesimal element of area on the ribbon can be written as \\[\\dd \\vec{a} = ( \\vec{v} \\cross \\dd \\vec{l} ) \\dd t\\] (see inset in Fig 7.13). Therefore\n\\[\\dv{\\Phi}{t} = \\oint \\vec{B} \\cdot ( \\vec{v} \\cross \\dd \\vec{l})\\] Since \\( \\vec{w} = ( \\vec{v} + \\vec{u}) \\) and \\( \\vec{u} \\) is parallel to \\( \\dd \\vec{l} \\), we can just as well write\n\\[\\dv{\\Phi}{t} = \\oint \\vec{B} \\cdot ( \\vec{w} \\cross \\dd \\vec{l})\\] Now, the scalar triple product can be rewritten:\n\\[\\vec{B} \\cdot ( \\vec{w} \\cross \\dd \\vec{l}) = - ( \\vec{w} \\cross \\vec{B}) \\cdot \\dd \\vec{l}\\] so\n\\[\\dv{\\Phi}{t} = - \\oint ( \\vec{w} \\cross \\vec{B}) \\cdot \\dd \\vec{l}\\] But \\( (\\vec{w} \\cross \\vec{B}) \\) is the magnetic force per unit charge, \\( \\vec{f}_{mag} \\), so\n\\[\\dv{\\Phi}{t} = - \\oint \\vec{f}_{mag} \\cdot \\dd \\vec{l}\\] and the integral of \\( \\vec{f}_{mag} \\) is the emf:\n\\[\\mathcal{E} = - \\dv{\\Phi}{t}\\] There is a sign ambiguity in the definition of emf (Eq. 7.9): Which way around the loop are you supposed to integrate? There is a compensatory ambiguity in the definition of flux (Eq. 7.12): Which is the positive direction for \\( \\dd \\vec{a} \\) ? In applying the flux rule, sign consistency is governed (as always) by your right hand: If your fingers define the positive direction around the loop, then your thumb indicates the direction of \\( \\dd \\vec{a} \\). Should the emf come out negative, it means the current will flow in in the negative direction around the circuit.\nThe flux rule is a nifty short-cut for calculating motional emfs. It does not contain any new physics - just the Lorentz force law. But it can lead to error or ambiguity if you\u0026rsquo;re not careful. The flux rule assumes you have a single wire loop - it can move, rotate, stretch, or distort (continuously), but beware of switches, sliding contacts, or extended conductors allowing a variety of current paths. A standard \u0026ldquo;flux rule paradox\u0026rdquo; involves the circuit in Figure 7.14. When the switch is thrown (from a to b) the flux through the circuit doubles, but there\u0026rsquo;s no motional emf (no conductor moving through a magnetic field), and the ammeter (A) records no current.\nExample 7.4 # Q A metal disk of radius \\( a \\) rotates with angular velocity \\( \\omega \\) about a vertical axis, through a uniform field \\( \\vec{B} \\), pointing up. A circuit is made by connecting one end of a resistor to the axle and the other end to a sliding contact, which touches the outer edge of the disk (Fig 7.15). Find the current in the resistor\nA The speed of a point on the disk at a distance \\( s \\) from the axis is \\( v = \\omega s \\), so the force per unit charge is \\( \\vec{f}_{mag} = \\vec{v} \\cross \\vec{B} = \\omega s B \\vu{s} \\). The emf is therefore\n\\[\\mathcal{E} = \\int _0 ^a f_{mag} \\dd s = \\omega B \\int_0 ^a s \\dd s = \\frac{\\omega B a^2}{2}\\] and the current is\n\\[I = \\frac{\\mathcal{E}}{R} = \\frac{\\omega B a^2}{2 R} \\] Example 7.4 (the Faraday disk, or Faraday dynamo) involves a motional emf that you can\u0026rsquo;t calculate (at least, not directly) from the flux rule. The flux rule assumes the current flows along a well-defined path, whereas in this example the current spreads out over the whole disk. It\u0026rsquo;s not even clear what the \u0026ldquo;flux through the circuit\u0026rdquo; would mean in this context.\nEven more tricky is the case of eddy currents. Take a chunk of aluminum (say), and shake it around in a nonuniform magnetic field. Currents will be generated in the material, and you will feel a kind of \u0026ldquo;viscous drag\u0026rdquo; - as though you were pulling the block through molasses (this is the force I called \\( \\vec{f}_{pull} \\) in the discussion of motional emf). Eddy currents are notoriously difficult to calculate, but easy and dramatic to demonstrate. You may have witnessed the classic experiment in which an aluminum disk mounted as a pendulum on a horizontal axis swings down and passes between the poles of a magnet (Fig. 7.16a). When it enters the field region it suddenly slows way down. To confirm that eddy currents are responsible, one repeats the demonstration using a disk that has many slots cut in it, to prevent the flow of large-scale currents (Fig. 7.16b). This time the disk swings freely, unimpeded by the field.\n"},{"id":75,"href":"/r/notes/griffiths/ch7-2/","title":"Electromagnetic Induction","section":"Griffiths Introduction to Electrodynamics","content":" 7.2: Electromagnetic Induction # 7.2.1: Faraday\u0026rsquo;s Law # In 1831 Michael Faraday reported on a series of experiments, including three that (with some violence to history) can be characterized as follows:\nExperiment 1:. He pulled a loop of wire to the right through a magnetic field (Fig 7.21a). A current flowed in the loop.\nExperiment 2: He moved the magnet to the left, holding the loop still (Fig 7.21b). Again, a current flowed in the loop.\nExperiment 3: With both the loop and the magnet at rest (Fig 7.21c), he changed the strength of the field (he used an electromagnet, and varied the current in the coil). Once again, current flowed in the loop.\nThe first experiment, of course, is a straightforward case of motional emf; according to the flux rule:\n\\[\\mathcal{E} = - \\dv{\\Phi}{t}\\] I don\u0026rsquo;t think it will surprise you to learn that exactly the same emf arises in Experiment 2 - all that really matters is the relative motion of the magnet and the loop. Indeed, in the light of special relativity it has to be so. But Faraday knew nothing of relativity, and in classical electrodynamics this simple reciprocity is a remarkable coincidence. For if the loop moves, it\u0026rsquo;s a magnetic force that sets up the emf, but if the loop is stationary, the force cannot be magnetic - stationary charges experience no magnetic forces. In that case, what is responsible? What sort of field exerts a force on charges at rest? Well, electric fields do, of course, but in this case there doesn\u0026rsquo;t seem to be any electric field in sight.\nFaraday had an ingenious inspiration:\n\\[\\textbf{A changing magnetic field induces an electric field}\\] It is this induced electric field that accounts for the emf in Experiment 2. Indeed, if (as Faraday found empirically) the emf is again equal to the rate of change of the flux,\n\\[\\mathcal{E} = \\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{\\Phi}{t} \\tagl{7.14}\\] then \\( \\vec{E} \\) is related to the change in \\( \\vec{B} \\) by the equation\n\\[\\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\int \\pdv{\\vec{B}}{t} \\cdot \\dd \\vec{a} \\tagl{7.15}\\] This is Faraday\u0026rsquo;s law, in integral form. We can convert it to differential form by applying Stokes\u0026rsquo; theorem:\n\\[\\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\tagl{7.16}\\] Note that Faraday\u0026rsquo;s law reduces to the old rule \\( \\oint \\vec{E} \\cdot \\dd \\vec{l} = 0 \\) (or, in differential form, \\( \\curl \\vec{E} = 0 \\)) in the static case (constant \\( \\vec{B} \\)), as, of course, it should.\nIn Experiment 3, the magnetic field changes for entirely different reasons, but according to Faraday\u0026rsquo;s law an electric field will again be induced, giving rise to an emf \\( - d \\Phi / dt \\). Indeed, one can subsume all three cases (and for that matter any combination of them) into a kind of universal flux rule:\nWhenever (and for whatever reason) the magnetic flux through a loop changes, an emf\n\\[\\mathcal{E} = - \\pdv{\\Phi}{t} \\tagl{7.17}\\] will appear in the loop.\nMany people call this \u0026ldquo;Faraday\u0026rsquo;s law.\u0026rdquo; Maybe I\u0026rsquo;m overly fastidious, but I find this confusing. There are really two totally different mechanisms underlying Eq. 7.17, and to identify them both as \u0026ldquo;Faraday\u0026rsquo;s law\u0026rdquo; is a little like saying that because identical twins look alike we ought to call them by the same name. In Faraday\u0026rsquo;s first experiment, it\u0026rsquo;s the Lorentz force law at work; the emf is magnetic. But in the other two it\u0026rsquo;s an electric field (induced by the changing magnetic field) that does the job. Viewed in this light, it is quite astonishing that all three processes yield the same formula for the emf. In fact, it was precisely this \u0026ldquo;coincidence\u0026rdquo; that led Einstein to the special theory of relativity - he sought a deeper understanding of what is, in classical electrodynamics, a peculiar accident. But that\u0026rsquo;s a story for chapter 12. In the meantime, I shall reserve the term \u0026ldquo;Faraday\u0026rsquo;s law\u0026rdquo; for electric fields induced by changing magnetic fields, and I do not regard Experiment 1 as an instance of Faraday\u0026rsquo;s law.\nExample 7.5 # Q A long cylindrical magnet of length \\( L \\) and radius \\( a \\) carries a uniform magnetization \\( \\vec{M} \\) parallel to its axis. It passes at constant velocity \\( \\vec{v} \\) through a circular wire ring of slightly larger diameter (Fig. 7.22). Graph the emf induced in the ring, as a function of time.\nA The magnetic field is the same as that of a long solenoid with surface current \\( \\vec{K}_b = M \\vu{\\phi} \\) . So the field inside is \\( \\vec{B} = \\mu_0 \\vec{M} \\) , except near the ends, where it starts to spread out. The flux through the ring is zero when the magnet is far away; it builds up to a maximum of \\( \\mu_0 M \\pi a^2 \\) as the leading end passes through; and it drops back to zero as the trailing end emerges (Fig. 7.23a). The emf is (minus) the derivative of \\( \\Phi \\) with respect to time, so it consists of two spikes, as shown in Fig. 7.23b.\nKeeping track of the signs in Faraday\u0026rsquo;s law can be a real headache. For instance, in Ex. 7.5 we would like to know which way around the ring the induced current flows. In principle, the right-hand rule does the job (we called \\( \\Phi \\) positive to the left, in Fig. 7.22, so the positive direction for current in the ring is counter-clockwise, as viewed from the left; since the first spike in Fig. 7.23b is negative, the first current pulse flows clockwise, and the second counterclockwise). But there\u0026rsquo;s a handy rule, called Lenz\u0026rsquo;s law, whose sole purpose is to help you get the directions right:\nNature abhors a change in flux The induced current will flow in such a direction that the flux it produces tends to cancel the change. (As the front end of the magnet in Ex. 7.5 enters the ring, the flux increases, so the current in the ring must generate a field to the right - it therefore flows clockwise.) Notice that it is the change in flux, not the flux itself, that nature abhors (when the tail end of the magnet exits the ring, the flux drops, so the induced current flows counterclockwise, in an effort to restore it). Faraday induction is a kind of \u0026ldquo;inertial\u0026rdquo; phenomenon: A conducting loop \u0026ldquo;likes\u0026rdquo; to maintain a constant flux through it; if you try to change the flux, the loop responds by sending a current around in such a direction as to frustrate your efforts. (It doesn\u0026rsquo;t succeed completely; the flux produced by the induced current is typically only a tiny fraction of the original. All Lenz\u0026rsquo;s law tells you is the direction of the flow.)\nExample 7.6 # Q The \u0026lsquo;jumping ring\u0026rsquo; demonstration. If you wind a solenoidal coil around an iron core (the iron is there to beef up the magnetic field), place a metal ring on top, and plug it in, the ring will jump several feet in the air (Fig 7.24). Why?\nA Before you turn on the current, the flux through the ring was zero. Afterward a flux appeared (upward in the diagram), and the emf generated in the ring led to a current (in the ring) which, according to Lenz\u0026rsquo;s law, was in such a direction that its field tended to cancel this new flux. This means that the current in the loop is opposite to the current in the solenoid. As opposite currents repel (as we saw in our Biot-Savart calculations in the last chapter), the ring flies off. 7.2.2: The Induced Electric Field # Faraday\u0026rsquo;s law generalizes the electrostatic rule \\( \\curl \\vec{E} = 0 \\) to the time-dependent regime. The divergence of \\( \\vec{E} \\) is still given by Gauss\u0026rsquo;s law (\\( \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\) ). If \\( \\vec{E} \\) is a pure Faraday field (due exclusively to a changing \\( \\vec{B} \\) , with \\( \\rho = 0 \\) ), then\n\\[\\div \\vec{E} = 0 \\qquad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t}\\] This is mathematically identical to magnetostatics\n\\[\\div \\vec{B} = 0 \\qquad \\curl \\vec{B} = \\mu_0 \\vec{J}\\] Conclusion: Faraday-induced electric fields are determined by \\( -(\\partial \\vec{B} / \\partial t) \\) in exactly the same way as magnetostatic fields are determined by \\( \\mu_0 \\vec{J} \\). The analog to Biot-Savart is\n\\[\\vec{E} = - \\frac{1}{4 \\pi} \\int \\frac{(\\partial \\vec{B} / \\partial t) \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau = - \\frac{1}{4 \\pi} \\pdv{}{t} \\int \\frac{\\vec{B} \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau \\tagl{7.18}\\] and if symmetry permits, we can use all the tricks associated with Ampere\u0026rsquo;s law in integral form (\\( \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} \\)), only now it\u0026rsquo;s Faraday\u0026rsquo;s law in integral form:\n\\[\\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{\\Phi}{t} \\tagl{7.19}\\] The rate of change of (magnetic) flux through the Amperian loop plays the role formerly assigned to \\( \\mu_0 I_{enc} \\).\nExample 7.7 # Q A uniform magnetic field \\( \\vec{B}(t) \\), pointing straight up, fills the shaded circular region of Fig. 7.25. If \\( \\vec{B} \\) is changing with time, what is the induced electric field?\nA \\( \\vec{E} \\) points in the circumferential direction, just like the magnetic field inside a long straight wire carrying a uniform current density. Draw an Amperian loop of radius \\( s \\), and apply Faraday\u0026rsquo;s law:\n\\[\\oint \\vec{E} \\cdot \\dd \\vec{l} = E ( 2 \\pi s) = - \\dv{\\Phi}{t} = - \\dv{}{t} \\left( \\pi s^2 B(t) \\right) = - \\pi s^2 \\dv{B}{t}\\] Therefore\n\\[\\vec{E} = - \\frac{s}{2} \\dv{B}{t} \\vu{\\phi}\\] If \\( \\vec{B} \\) is increasing, \\( \\vec{E} \\) runs clockwise, as viewed from above.\nExample 7.8 # Q A line charge \\( \\lambda \\) is glued to the rim of a wheel of radius \\( b \\), which is then suspended horizontally, as shown in Fig 7.26, so that it is free to rotate (the spokes are made of some nonconducting material - wood, maybe). In the central region, out to radius \\( a \\) , there is a uniform magnetic field \\( \\vec{B}_0 \\), pointing up. Now someone turns the field off. What happens? A The changing magnetic field will induce an electric field, curling around the axis of the wheel. This electric field exerts a force on the charges at the rim, and the wheel starts to turn. According to Lenz\u0026rsquo;s law, it will rotate in such a direction that its field tends to restore the upward flux. The motion, then, is counterclockwise, as viewed frrom above. Faraday\u0026rsquo;s law, applied to the loop at radius \\( b \\), says\n\\[\\oint \\vec{E} \\cdot \\dd \\vec{l} = E ( 2 \\pi b) = - \\dv{\\Phi}{t} = - \\pi ^2 \\dv{B}{t}\\] or\n\\[\\vec{E} = - \\frac{a^2}{2b} \\dv{B}{t} \\vu{\\phi}\\] The torque on a segment of length \\( \\dd \\vec{l} \\) is \\( (\\vec{r} \\cross \\vec{F}) \\) , or \\( b \\lambda E \\dd l \\) . The total torque on the wheel is therefore\n\\[N = b \\lambda \\left( - \\dv{\\Phi}{t} = - \\pi ^2 \\dv{B}{t} \\right) \\oint \\dd l = - b \\lambda \\pi a^2 \\dv{B}{t}\\] and the angular momentum imparted to the wheel is\n\\[\\int N \\dd t = - \\lambda \\pi a^2 b \\int_{B_0} ^0 \\dd B = \\lambda \\pi a^2 b B_0\\] It doesn\u0026rsquo;t matter how quickly or slowly you tum off the field; the resulting angular velocity of the wheel is the same regardless. (If you find yourself wondering where the angular momentum came from, you\u0026rsquo;re getting ahead of the story! Wait for the next chapter.)\nNote that it\u0026rsquo;s the electric field that did the rotating. To convince you of this, I deliberately set things up so that the magnetic field is zero at the location of the charge. The experimenter may tell you she never put in any electric field - all she did was switch off the magnetic field. But when she did that, an electric field automatically appeared, and it\u0026rsquo;s this electric field that turned the wheel.\nI must warn you, now, of a small fraud that tarnishes many applications of Faraday\u0026rsquo;s law: Electromagnetic induction, of course, occurs only when the magnetic fields are changing, and yet we would like to use the apparatus of magnetostatics (Ampere\u0026rsquo;s law, the Biot-Savart law, and the rest) to calculate those magnetic fields. Technically, any result derived in this way is only approximately correct. But in practice the error is usually negligible, unless the field fluctuates extremely rapidly, or you are interested in points very far from the source. Even the case of a wire snipped by a pair of scissors (Prob. 7.18) is static enough for Ampere\u0026rsquo;s law to apply. This regime, in which magnetostatic rules can be used to calculate the magnetic field on the right hand side of Faraday\u0026rsquo;s law, is called quasistatic. Generally speaking, it is only when we come to electromagnetic waves and radiation that we must worry seriously about the breakdown of magnetostatics itself.\nExample 7.9 # Q An infinitely long straight wire carries a slowly varying current \\( I(t) \\). Determine the induced electric field, as a function of the distance \\( s \\) from the wire\nA In the quasistatic approximation, the magnetic field is \\( (\\mu_0 I / 2 \\pi s) \\) and it circles around the wire. Like the B-field of a solenoid, E here runs parallel to the axis. For the rectangular \u0026ldquo;Amperian loop\u0026rdquo; in Fig 7.27, Faraday\u0026rsquo;s law gives:\n\\[\\begin{aligned} \\oint \\vec{E} \\cdot \\dd \\vec{l} \u0026amp; = E (s_0) l - E(s) l \\\\ \u0026amp; = - \\dv{}{t} \\int \\vec{B} \\cdot \\dd \\vec{a} \\\\ \u0026amp; = - \\frac{\\mu_0 l }{2\\pi} \\dv{I}{t} \\int _{s_0} ^s \\frac{1}{s} \\dd s\u0026#39; \\\\ \u0026amp; = - \\frac{\\mu_0 l}{2 \\pi} \\dv{I}{t} ( \\ln \\, s - \\ln \\, s_0 ) \\end{aligned}\\] Thus\n\\[\\vec{E}(s) = \\left( \\frac{\\mu_0}{2\\pi} \\dv{I}{t} \\ln \\, s \u0026#43; K \\right) \\vu{z} \\tagl{7.20}\\] where \\( K \\) is a constant (that is to say, it is independent of \\( s \\) - it might still be a function of \\( t \\) ). The actual value of \\( K \\) depends on the whole history of the function \\( I(t) \\) - we\u0026rsquo;ll see some examples in Chapter 10.\nEquation 7.20 has the particular implication that \\( E \\) blows up as \\( s \\) goes to infinity. That can\u0026rsquo;t be true\u0026hellip; What\u0026rsquo;s gone wrong? Answer: we have overstepped the limits of the quasistatic approximation. As we shall see in Chapter 9, electromagnetic \u0026ldquo;news\u0026rdquo; travels at the speed of light, and at large distances B depends not on the current now, but on the current as it was at some earlier time (indeed, a whole range of earlier times, since different points on the wire are different distances away). If \\( \\tau \\) is the time it takes \\( I \\) to change substantially, then the quasistatic approximation should hold only for\n\\[s \\ll c \\tau \\tagl{7.21}\\] and hence Eq. 7.20 simply does not apply, at extremely large \\( s \\).\n7.2.3: Inductance # Suppose you have two loops of wire, at rest (Fig 7.30). If you run a steady current \\( I_1 \\) around loop 1, it produces a magnetic field \\( \\vec{B}_1 \\) . Some of the field lines pass through loop 2; let \\( \\Phi_2 \\) be the flux of \\( \\vec{B}_1 \\) through 2. You might have a tough time actually calculating \\( \\vec{B_1} \\), but a glance at the Biot-Savart law,\n\\[\\vec{B}_1 = \\frac{\\mu_0}{4 \\pi} I_1 \\oint \\frac{\\dd \\vec{l}_1 \\cross \\vu{\\gr}}{\\gr^2} \\] reveals one significant fact about this field: It is proportional to the current \\( I_1 \\) . Therefore, so too is the flux through loop 2:\n\\[\\Phi_2 = \\int \\vec{B}_1 \\cdot \\dd \\vec{a}_2\\] Thus\n\\[\\Phi_2 = M_{21} I_1 \\tagl{7.22}\\] where \\( M_{21} \\) is the constant of proportionality; it is known as the mutual inductance of the two loops.\nThere is a cute formula for the mutual inductance, which you can derive by expressing the flux in terms of the vector potential, and invoking Stokes\u0026rsquo; theorem:\n\\[\\Phi_2 = \\int \\vec{B}_1 \\cdot \\dd \\vec{a}_2 = \\int (\\curl \\vec{A}_1) \\cdot \\dd \\vec{a}_2 = \\oint \\vec{A}_1 \\cdot \\dd \\vec{l}_2\\] Now, according to Eq. 5.66,\n\\[\\vec{A}_1 = \\frac{\\mu_0 I_1}{4 \\pi} \\oint \\frac{\\dd \\vec{l}_1}{\\gr} \\] and hence\n\\[\\Phi_2 = \\frac{\\mu_0 I_1}{4 \\pi} \\oint \\left( \\oint \\frac{\\dd \\vec{l}_1}{\\gr} \\right) \\cdot \\dd \\vec{l}_2\\] Evidently\n\\[M_{21} = \\frac{\\mu_0}{4 \\pi} \\oint \\oint \\frac{\\dd \\vec{l}_1 \\cdot \\dd \\vec{l}_2}{\\gr} \\tagl{7.23}\\] This is the Neumann formula; it involves a double line integral - one integration around loop 1, the other around loop 2 (Fig 7.31). It\u0026rsquo;s not very useful for practical calculations, but it does reveal two important things about the mutual inductance:\n\\( M_{21} \\) is a purely geometrical quantity, having to do with the sizes, shapes, and relative positions of the two loops. The integral in Eq. 7.23 is unchanged if we switch the roles of loops 1 and 2; it follows that \\[M_{21} = M_{12} \\tagl{7.24}\\] This is an astonishing conclusion: Whatever the shapes and positions of the loops, the flux through 2 when we run a current I around 1 is identical to the flux through 1 when we send the same current I around 2. We may as well drop the subscripts and call them both M.\nExample 7.10 # Q A short solenoid (length \\( l \\) and radius \\( a \\), with \\( n_1 \\) turns per unit length) lies on the axis of a very long solenoid (radius \\( b \\) , \\( n_2 \\) turns per unit length) as shown in Fig 7.32. Current \\( I \\) flows in the short solenoid. What is the flux through the long solenoid?\nA Since the inner solenoid is short, it has a very complicated field; moreover, it puts a different flux through each turn of the outer solenoid. It would be a miserable task to compute the total flux this way. However, if we exploit the equality of the mutual inductances, the problem becomes very easy. Just look at the reverse situation: run the current \\( I \\) through the outer solenoid, and calculate the flux through the inner one. The field inside the long solenoid is constant\n\\[B = \\mu_0 n_2 I\\] so the flux through a single loop of the short solenoid is\n\\[B \\pi a^2 = \\mu_0 n_2 I \\pi a^2\\] There are \\( n_1 l \\) turns in all, so the total flux through the inner solenoid is\n\\[\\Phi = \\mu_0 \\pi a^2 n_1 n_2 I\\] This is also the flux a current \\( I \\) in the short solenoid would put through the long one, which is what we set out to find. Incidentally, the mutual inductance, in this case, is\n\\[M = \\mu_0 \\pi a^2 n_1 n_2 l\\] Suppose now, that you vary the current in loop 1. The flux through loop 2 will vary accordingly, and Faraday\u0026rsquo;s law says this changing flux will induce an emf in loop 2:\n\\[\\mathcal{E}_2 = - \\dv{\\Phi_2}{t} = - M \\dv{I_1}{t} \\tagl{7.25}\\] (In quoting Eq. 7.22 - which was based on the Biot-Savart law - I am tacitly assuming that the currents change slowly enough for the system to be considered quasistatic.) What a remarkable thing: Every time you change the current in loop 1, and induced current flows in loop 2 - even though there are no wires connecting them!\nCome to think of it, a changing current not only induces an emf in any nearby loops, it also induces an emf in the source loop itself (Fig 7.33). Once again, the field (and therefore the flux) is proportional to the current\n\\[\\Phi = L I \\tagl{7.26}\\] The constant of proportionality \\( L \\) is called the self inductance (or simply the inductance) of the loop. As with \\( M \\), it depends on the geometry (side and shape ) of the loop. If the current changes, the emf induced in the loop is\n\\[\\mathcal{E} = - L \\dv{I}{t} \\tagl{7.27}\\] Inductance is measured in henries (H); a henry is a volt-second per ampere.\nExample 7.11 # Q Find the self inductance of a toroidal coil with rectangular cross-section (inner radius \\( a \\), outer radius \\( b \\), height \\( h \\)), that carries a total of \\( N \\) turns. A The magnetic field inside of a toroid is\n\\[B = \\frac{\\mu_0 N I}{2 \\pi s} \\] The flux through a single turn (Fig 7.34) is \\[\\int \\vec B \\cdot \\dd \\vec a = \\frac{\\mu_0 N I}{2 \\pi} h \\int_a ^b \\frac{1}{s} = \\frac{\\mu_0 N I h}{2 \\pi} \\ln \\frac{b}{a} \\] The total flux is \\( N \\) times this, so the self-inductance (Eq. 7.26) is\n\\[L = \\frac{\\mu_0 N^2 h}{2 \\pi} \\ln \\left( \\frac{b}{a} \\right) \\tagl{7.28}\\] Inductance (like capacitance) is an intrinsically positive quantity. Lenz\u0026rsquo;s law, which is enforced by the minus sign in Eq. 7.27, dictates that the emf is in such a direction as to oppose any change in current. For this reason, it is called a back emf. Whenever you try to alter the current in a wire, you must fight against this back emf. Inductance plays somewhat the same role in electric currents that mass plays in mechanical systems: The greater \\( L \\), the harder it is to change the current, just as the larger the mass, the harder it is to change an object\u0026rsquo;s velocity.\nExample 7.12 # Q Suppose a current \\( I \\) is flowing around a loop, when someone suddenly cuts the wire. The current drops \u0026ldquo;instantaneously\u0026rdquo; to zero. This generates a whopping back emf, for although \\( I \\) may be small, \\( d I / d t \\) is enormous. (That\u0026rsquo;s why you sometimes draw a spark when you unplug an iron or toaster - electromagnetic induction is desperately trying to keep the current going, even if it has to jump the gap in the circuit.) Nothing so dramatic occurs when you plug in a toaster or iron. In this case induction opposes the sudden increase in current, prescribing instead a smooth and continuous buildup. Suppose, for instance, that a battery (which supplies a constant emf \\( \\mathcal{E}_0 \\) ) is connected to a circuit of resistance \\( R \\) and inductance \\( L \\) (Fig. 7.35). What current flows?\nA The total emf in this circuit is \\( \\mathcal{E}_0 \\) from the battery plus \\( -L (\\dv{I}{t}) \\) from the inductance. Ohm\u0026rsquo;s law says\n\\[\\mathcal{E}_0 - L \\dv{I}{t} = I R\\] This is a first-order differential equation for \\( I \\) as a function of time. The general solution is\n\\[I(t) = \\frac{\\mathcal{E}_0}{R} \u0026#43; k e^{-(R/L) t}\\] where \\( k \\) is a constant to be determined by the initial conditions. In particular, if you close the switch at time \\( t = 0 \\), so \\( I(0) = 0 \\), then \\( k = - \\mathcal{E}_0 / R \\), and\n\\[I(t) = \\frac{\\mathcal{E}_0}{R} \\left( 1 - e^{-(R/L) t} \\right) \\tagl{7.29}\\] This function is plotted in Fig 7.36. Had there been no inductance in the circuit, the current would have jumped immediately to \\( \\mathcal{E}_0 / R \\). In practice, every circuit has some self-inductance, and the current approaches \\( \\mathcal{E}_0 / R \\) asymptotically. The quantity \\( \\tau = L / R \\) is the time constant for an LR circuit; it tells you how long the current takes to reach a substantial fraction \\( (1 - 1/e) \\) of its final value.\n7.2.4: Energy in Magnetic Fields # It takes a certain amount of energy to start a current flowing in a circuit. I\u0026rsquo;m not talking about the energy delivered to the resistors and converted into heat - that is irretrievably lost, as far as the circuit is concerned, and can be large or small, depending on how long you let the current run. What I am concerned with, rather, is the work you must do against the back emf to get the current going. This is fixed amount, and it is recoverable: you get it back when the current is turned off. In the meantime, it represents energy latent in the circuit; as we\u0026rsquo;ll see in a moment, it can be regarded as energy stored in the magnetic field.\nThe work done on a unit charge, against the back emf, in one trip around the circuit is \\( - \\mathcal{E} \\) (the minus sign records the fact that this is the work done by you against the emf, not the work done by the emf). The amount of charge per unit time passing down the wire is I. So the total work done per unit time is\n\\[\\dv{W}{t} = - \\mathcal{E}I = L I \\dv{I}{t}\\] If we start with zero current and build it up to a final value I, the work done (integrating the last equation over time) is\n\\[W = \\frac{1}{2} L I^2 \\tagl{7.30}\\] So, this is the energy stored in an inductor, or in any loop that has an inductance \\( L \\). It does not depend on how long we take to crank up the current, only on the geometry of the loop (in the form of \\( L \\) ) and the final current \\( I \\).\nThis is only really sensible for a system of conducting loops, but we can be a bit more general. We can express \\( W \\) by recalling that the flux \\( \\Phi \\) through a loop (which is \\( LI \\) ) is\n\\[\\Phi = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\int (\\curl \\vec{A}) \\cdot \\dd \\vec{a} = \\oint \\vec{A} \\cdot \\dd \\vec{l}\\] where the line integral is around the perimeter of the loop. So, we have\n\\[LI = \\oint \\vec{A} \\cdot \\dd \\vec{l}\\] and therefore\n\\[W = \\frac{1}{2} I \\oint \\vec{A} \\cdot \\dd \\vec{l} = \\frac{1}{2} \\oint (\\vec{A} \\cdot \\vec{I}) \\dd l \\tagl{7.31}\\] We can pretty obviously generalize this to volume currents\n\\[W = \\frac{1}{2} \\int _V (\\vec{A} \\cdot \\vec{J}) \\dd \\tau \\tagl{7.32}\\] But we can do one better, expressing \\( W \\) entirely in terms of the magnetic field: \\( \\curl \\vec{B} = \\mu_0 \\vec{J} \\) lets us eliminate the current density from the picture\n\\[W = \\frac{1}{2 \\mu_0} \\int \\vec{A} \\cdot (\\curl \\vec{B}) \\dd \\tau \\tagl{7.33}\\] Integration by parts gets us to slap the derivative from B to A\n\\[\\div (\\vec{A} \\cross \\vec{B}) = \\vec{B} \\cdot (\\curl \\vec{A}) - \\vec{A} \\cdot (\\curl \\vec{B})\\] so\n\\[\\vec{A} \\cdot (\\curl \\vec{B}) = \\vec{B} \\cdot \\vec{B} - \\div (\\vec{A} \\cross \\vec{B}\\] Consequently\n\\[W = \\frac{1}{2\\mu_0} \\left( \\int B^2 \\dd \\tau - \\int \\div (\\vec{A} \\cross \\vec{B} \\dd \\tau \\right) \\\\ = \\frac{1}{2\\mu_0} \\left( \\int _V B^2 \\dd \\tau - \\oint_S (\\vec{A} \\cross \\vec{B} ) \\cdot \\dd \\vec{a} \\right) \\tagl{7.34}\\] Now, the integration in Eq. 7.32 is to be taken over the entire volume occupied by the current. But any region larger than this will do just as well, for \\( \\vec{J} \\) is zero out there anyway. In Eq. 7.34, the larger the region we pick the greater is the contribution from the volume integral, and therefore the smaller is that of the surface integral (this makes sense: as the surface gets farther from the current, both A and B decrease). In particular, if we agree to integrate over all space, then the surface integral goes to zero, and we are left with\n\\[W = \\frac{1}{2 \\mu_0} \\int _{\\text{all space}} B^2 \\dd \\tau \\tagl{7.35}\\] In view of this result, we say the energy is \u0026ldquo;stored in the magnetic field,\u0026rdquo; in the amount \\( (B^2 / 2 \\mu_0) \\) per unit volume. This is a nice way to think of it, though someone looking at Eq. 7.32 might prefer to say that the energy is stored in the current distribution, in the amount \\( \\frac{1}{2} (\\vec{A} \\cdot \\vec{J}) \\) per unit volume. The distinction is one of bookkeeping; the important quantity is the total energy \\( W \\) , and we need not worry about where (if anywhere) the energy is \u0026ldquo;located.\u0026rdquo;\nYou might find it strange that it takes energy to set up a magnetic field - after all, magnetic fields themselves do no work. The point is that producing a magnetic field, where previously there was none, requires changing the field, and a changing B-field, according to Faraday, induces an electric field. The latter, of course, can do work. In the beginning, there is no \\( \\vec{E} \\) , and at the end there is no \\( \\vec{E} \\) ; but in between, while \\( \\vec{B} \\) is building up, there is an \\( \\vec{E} \\) , and it is against this that the work is done. (You see why I could not calculate the energy stored in a magnetostatic field back in Chapter 5.) In the light of this, it is extraordinary how similar the magnetic energy formulas are to their electrostatic counterparts:\nExample 7.13 # Q A long coaxial cable carries current \\( I \\) (the current flows down the surface of the inner cylinder, radius \\( a \\) , and back along the outer cylinder, radius \\( b \\) ) as shown in Fig 7.40. Find the magnetic energy stored in a section of length \\( l \\)\nA Ampere\u0026rsquo;s law will tell us that B between the surfaces is\n\\[\\vec{B} = \\frac{\\mu_0 I}{2 \\pi s } \\vu{\\phi}\\] and outside the cable, the field is zero. Eq. 7.35 gives us the volume energy density\n\\[\\frac{1}{2 \\mu_0} \\left( \\frac{\\mu_0 I}{2 \\pi s} \\right)^2 = \\frac{\\mu_0 I^2}{8 \\pi^2 s^2} \\] The energy in a shell of length \\( l \\), radius \\( s \\) and thickness \\( \\dd s \\) is\n\\[\\left( \\frac{\\mu_0 I^2}{8 \\pi ^2 s^2} \\right)2 \\pi l s \\, \\dd s = \\frac{\\mu_0 I^2 l}{4 \\pi} \\left( \\frac{\\dd s}{s} \\right)\\] Integrating from \\( a \\) to \\( b \\) , we have\n\\[W = \\frac{\\mu_0 I^2 l}{4 \\pi} \\ln \\left( \\frac{b}{a} \\right)\\] Incidentally, this suggests a very simple way to calculate the self-inductance of the cable. According to Eq. 7.30, the energy can also be written as \\( \\frac{1}{2} L I^2 \\). Comparing the two expressions,\n\\[L = \\frac{\\mu_0 l}{2 \\pi} \\ln \\left( \\frac{b}{a} \\right)\\] This method of calculating the self-inductance is especially useful when the current is not confined to a single path, but spreads over some surface or volume, so that different parts of the current enclose different amounts of flux. In such cases, it can be very tricky to get the inductance directly from Eq. 7.26, and it is best to let 7.30 define L\n"},{"id":76,"href":"/r/notes/griffiths/ch7-3/","title":"Maxwell's Equations","section":"Griffiths Introduction to Electrodynamics","content":" 7.3: Maxwell\u0026rsquo;s Equations # 7.3.1: Electrodynamics Before Maxwell # So far, we have encountered the following laws, specifying the divergence and curl of electric and magnetic fields\n\\[\\begin{aligned} (\\text{i}) \u0026amp; \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho \\quad \\text{(Gauss\u0026#39;s law)} \\\\ (\\text{ii}) \u0026amp; \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng\u0026#39;s Law)} \\\\ (\\text{iii}) \u0026amp; \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday\u0026#39;s Law}) \\\\ (\\text{iv}) \u0026amp; \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} \\quad \\text{(Ampere\u0026#39;s Law)} \\end{aligned}\\] These equations represent the state of electromagnetic theory in the mid-nineteenth century, when Maxwell began his work. They were not written in so compact a form, in those days, but their physical content was familiar. Now, it happens that there is a fatal inconsistency in these formulas. It has to do with the old rule that divergence of curl is always zero. If you apply the divergence to number (iii), everything works out:\n\\[\\div (\\curl \\vec{E}) = \\div \\left( - \\pdv{\\vec{B}}{t} \\right) = - \\pdv{}{t} (\\div \\vec{B})\\] The left side is zero because divergence of curl is zero; the right side is zero by virtue of equation (ii). But when you do the same thing to number (iv), you get into trouble:\n\\[\\div (\\curl \\vec{B}) = \\mu_0 (\\div \\vec{J}) \\tagl{7.36}\\] the left side must be zero, but the right side, in general, is not. For steady currents, the divergence of J is zero, but when we go beyond magnetostatics Ampere\u0026rsquo;s law cannot be right.\nThere\u0026rsquo;s another way to see that Ampere\u0026rsquo;s law is bound to fail for non-steady currents. Suppose we\u0026rsquo;re in the process of charging up a capacitor (Fig. 7.43). In integral form, Ampere\u0026rsquo;s law reads\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc}\\] I want to apply it to the Amperian loop shown in the diagram. How do I deter- mine \\( I_{enc} \\) ? Well, it\u0026rsquo;s the total current passing through the loop, or, more precisely, the current piercing a surface that has the loop for its boundary. In this case, the simplest surface lies in the plane of the loop - the wire punctures this surface, so \\( I_{enc} = I \\) . Fine - but what if I draw instead the balloon-shaped surface in Fig. 7.43? No current passes through this surface, and I conclude that \\( I_{enc} = 0 \\) ! We never had this problem in magnetostatics because the conflict arises only when charge is piling up somewhere (in this case, on the capacitor plates). But for nonsteady currents (such as this one) \u0026ldquo;the current enclosed by the loop\u0026rdquo; is an ill-defined notion; it depends entirely on what surface you use. (If this seems pedantic to you - \u0026ldquo;obviously one should use the plane surface\u0026rdquo; - remember that the Amperian loop could be some contorted shape that doesn\u0026rsquo;t even lie in a plane.)\nOf course, we had no right to expect Ampere\u0026rsquo;s law to hold outside of magnetostatics; after all, we derived it from the Biot-Savart law. However, in Maxwell\u0026rsquo;s time there was no experimental reason to doubt that Ampere\u0026rsquo;s law was of wider validity. The flaw was a purely theoretical one, and Maxwell fixed it by purely theoretical arguments.\n7.3.2: How Maxwell Fixed Ampere\u0026rsquo;s Law # The problem is on the right side of Eq. 7.36, which should be zero, but isn\u0026rsquo;t. Applying the continuity equation (5.29) and Gauss\u0026rsquo;s law, the offending term can be rewritten:\n\\[\\div \\vec{J} = - \\pdv{\\rho}{t} = - \\pdv{}{t} ( \\epsilon_0 \\div \\vec{E}) = - \\div \\left( \\epsilon_0 \\pdv{\\vec{E}}{t} \\right)\\] If we were to combine \\( \\epsilon_0 (\\partial \\vec{E} / \\partial t) \\) with \\( \\vec{J} \\), in Ampere\u0026rsquo;s law, it would be just right to kill off the extra divergence:\n\\[\\curl \\vec{B} = \\mu_0 \\vec{J} \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\tagl{7.37}\\] (Maxwell himself had other reasons for wanting to add this quantity to Ampere\u0026rsquo;s law. To him, the rescue of the continuity equation was a happy dividend rather than a primary motive. But today we recognize this argument as a far more compelling one than Maxwell\u0026rsquo;s, which was based on a now-discredited model of the ether.)\nSuch a modification changes nothing, as far as magnetostatics is concerned: when \\( \\vec{E} \\) is constant, we still have \\( \\curl \\vec{B} = \\mu_0 \\vec{J} \\) . In fact, Maxwell\u0026rsquo;s term is hard to detect in ordinary electromagnetic experiments, where it must compete for attention with \\( \\vec{J} \\) - that\u0026rsquo;s why Faraday and the others never discovered it in the laboratory. However, it plays a crucial role in the propagation of electromagnetic waves, as we\u0026rsquo;ll see in Chapter 9.\nApart from curing the defect in Ampere\u0026rsquo;s law, Maxwell\u0026rsquo;s term has a certain aesthetic appeal: Just as a changing magnetic field induces an electric field (Faraday\u0026rsquo;s law), so\n\\[\\textbf{A changing electric field induces a magnetic field}\\] Of course, theoretical convenience and aesthetic consistency are only suggestive - there might, after all, be other ways to doctor up Ampere\u0026rsquo;s law. The real confirmation of Maxwell\u0026rsquo;s theory came in 1888 with Hertz\u0026rsquo;s experiments on electromagnetic waves.\nMaxwell called his extra term the displacement current:\n\\[\\vec{J}_d = \\epsilon_0 \\pdv{\\vec{E}}{t} \\tagl{7.38}\\] (It\u0026rsquo;s a misleading name; \\( \\epsilon_0 (\\partial \\vec{E} / \\partial t) \\) has nothing to do with current, except that it adds to \\( \\vec{J} \\) in Ampere\u0026rsquo;s law.) Let\u0026rsquo;s see now how displacement current resolves the paradox of the charging capacitor (Fig. 7.43). If the capacitor plates are very close together (I didn\u0026rsquo;t draw them that way, but the calculation is simpler if you assume this), then the electric field between them is\n\\[E = \\frac{1}{\\epsilon_0} \\sigma = \\frac{1}{\\epsilon_0 } \\frac{Q}{A} \\] where Q is the charge on the plate and \\( A \\) is its area. Thus, between the plates\n\\[\\pdv{E}{t} = \\frac{1}{\\epsilon_0 A} \\dv{Q}{t} = \\frac{1}{\\epsilon_0 A} I\\] Now, Eq. 7.37 reads, in integral form\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} \u0026#43; \\mu_0 \\epsilon_0 \\int \\left( \\pdv{\\vec{E}}{t} \\right) \\cdot \\dd \\vec{a} \\tagl{7.39}\\] If we choose the flat surface, then \\( E = 0 \\) and \\( I_{enc} = I \\) . If, on the other hand, we use the balloon-shaped surface, then \\( I_{enc} = 0 \\) , but \\( \\int (\\partial \\vec{E} / \\partial t) \\cdot \\dd \\vec{a} = I / \\epsilon_0 \\) . So we get the same answer for either surface, though in the first case it comes from the conduction current, and in the second from the displacement current.\nExample 7.14 # Q Imagine two concentric metal spherical shells (Fig. 7.44). The inner one (radius \\( a \\) ) carries a charge \\( Q(t) \\) , and the outer one (radius \\( b \\) ) an opposite charge \\( -Q(t) \\) . The space between them is filled with Ohmic material of conductivity \\( \\sigma \\) , so a radial current flows\n\\[ \\vec J = \\sigma \\vec E = \\sigma \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vu{r}; \\quad I = - \\dot{Q} = \\int \\vec J \\cdot \\dd \\vec a = \\frac{\\sigma Q}{\\epsilon_0} \\] This configuration is spherically symmetrical, so the magnetic field has to be zero (the only direction it could possibly point is radial, and \\( \\div \\vec B = 0 \\rightarrow \\oint \\vec B \\cdot \\dd \\vec a = B(4 \\pi r^2) = 0 \\), so \\( B = 0 \\)). What? I thought currents produce magnetic fields! Isn\u0026rsquo;t that what Biot-Savart and Ampere taught us? How can there be a \\( \\vec J \\) with no accompanying \\( \\vec B \\)?\nA This is not a static configuration! \\( Q, \\vec E, \\vec J \\) are all functions of time; Ampere and Biot-Savart do not apply. The displacement current\n\\[J_d = \\epsilon_0 \\pdv{\\vec E}{t} = \\frac{1}{4 \\pi} \\frac{\\dot{Q}}{r^2} \\vu{r} = - \\sigma \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r}\\] exactly cancels the conduction current (in Eq. 7.37), and the magnetic field (determined by \\( \\div \\vec{B} = 0 \\) and \\( \\curl \\vec{B} = 0 \\) is indeed zero.\n7.3.3 Maxwell\u0026rsquo;s Equations # In the last section we put the finishing touches on Maxwell\u0026rsquo;s equations:\nMaxwell\u0026rsquo;s Equations\n\\[\\begin{aligned} (\\text{i}) \u0026amp; \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho \\quad \\text{(Gauss\u0026#39;s law)} \\\\ (\\text{ii}) \u0026amp; \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng\u0026#39;s Law)} \\\\ (\\text{iii}) \u0026amp; \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday\u0026#39;s Law}) \\\\ (\\text{iv}) \u0026amp; \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\quad \\text{(Ampere\u0026#39;s Law)} \\end{aligned} \\tagl{7.40}\\] Together, with the force law,\n\\[\\vec{F} q (\\vec{E} \u0026#43; \\vec{v} \\cross \\vec{B}) \\tagl{7.41}\\] they summarize the entire theoretical content of classical electrodynamics (save for some special properties of matter, which we encountered in Chapters 4 and 6). Even the continuity equation,\n\\[\\div \\vec{J} = - \\pdv{\\rho}{t} \\tagl{7.42}\\] which is the mathematical expression of conservation of charge, can be derived from Maxwell\u0026rsquo;s equations by applying the divergence to number (iv).\nI have written Maxwell\u0026rsquo;s equations in the traditional way, which emphasizes that they specify the divergence and curl of \\( \\vec{E} \\) and \\( \\vec{B} \\) . In this form, they reinforce the notion that electric fields can be produced either by charges (\\( \\rho \\) ) or by changing magnetic fields (\\( \\partial \\vec{B} / \\partial t \\) ), and magnetic fields can be produced either by currents (\\( \\vec{J} \\) ) or by changing electric fields (\\( \\partial \\vec{E} / \\partial t \\) ). Actually, this is misleading, because \\( \\partial \\vec{B} / \\partial t \\) and \\( \\partial \\vec{E} / \\partial t \\) are themselves due to charges and currents. I think it is logically preferable to write\n\\[\\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho\\] \\[\\div \\vec{B} = 0\\] \\[\\curl \\vec{E} \u0026#43; \\pdv{\\vec{B}}{t} = 0\\] \\[\\curl \\vec{B} - \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} = \\mu_0 \\vec{J}\\] with the fields (\\( \\vec{E} \\) and \\( \\vec{B} \\) ) on the left and the sources (\\( \\rho \\) and \\( \\vec{J} \\) ) on the right. This notation emphasizes that all electromagnetic fields are ultimately attributable to charges and currents. Maxwell\u0026rsquo;s equations tell you how charges produce fields; reciprocally, the force law tells you how fields affect charges.\n7.3.4: Magnetic Charge # There is a pleasing symmetry to Maxwell\u0026rsquo;s equations; it is particularly striking in free space, where \\( \\rho \\) and \\( \\vec J \\) vanish\n\\[\\div \\vec E = 0 \\qquad \\curl \\vec E = - \\pdv{\\vec B}{t} \\\\ \\div \\vec B = 0 \\qquad \\curl \\vec B = - \\mu_0 \\epsilon_0 \\pdv{\\vec E}{t}\\] If you replace \\( \\vec E \\) by \\( \\vec B \\) and \\( \\vec B \\) by \\( - \\mu_0 \\epsilon_0 \\vec E \\), the first pair of equations turns into the second, and vice versa. This symmetry between \\( \\vec E \\) and \\( \\vec B \\) is spoiled, though, by the charge term in Gauss\u0026rsquo;s law and the current term in Ampere\u0026rsquo;s law. You can\u0026rsquo;t help wondering why the corresponding quantities are \u0026ldquo;missing\u0026rdquo; from \\( \\div \\vec B = 0 \\) and \\( \\curl \\vec E = - \\partial \\vec B / \\partial t \\) . What if we had\n\\[\\begin{aligned} (\\text{i}) \u0026amp; \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho_e \\\\ (\\text{ii}) \u0026amp; \\quad \\div \\vec{B} = \\mu_0 \\rho_m \\\\ (\\text{iii}) \u0026amp; \\quad \\curl \\vec{E} = - \\mu_0 \\vec{J}_m - \\pdv{\\vec{B}}{t} \\\\ (\\text{iv}) \u0026amp; \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\end{aligned} \\tagl{7.44}\\] Then \\( \\rho_m \\) would represent the density of magnetic \u0026ldquo;charge\u0026rdquo;, and \\( \\rho_e \\) the density of electric charge; \\( \\vec J_m \\) would be the current of magnetic charge, and \\( \\vec J_e \\) the current of electric charge. Both charges would be conserved:\n\\[\\div \\vec{J}_m = - \\pdv{\\rho_m}{t} \\quad \\div \\vec{J}_e = - \\pdv{\\rho_e}{t} \\tagl{7.45}\\] The former follows by application of the divergence to (iii), the latter by taking the divergence of (iv).\nIn a sense, Maxwell\u0026rsquo;s equations beg for magnetic charge to exist - it would fit in so nicely. And yet, in spite of a diligent search, no one has ever found any. As far as we know, \\( \\rho_m \\) is zero everywhere, and so is \\( \\vec J_m \\) ; \\( \\vec B \\) is not on equal footing with \\( \\vec E \\) : there exist stationary sources for \\( \\vec E \\) (electric charges) but none for \\( \\vec B \\) . (This is reflected in the fact that magnetic multipole expansions have no monopole term, and magnetic dipoles consist of current loops, not separated north and south \u0026ldquo;poles.\u0026rdquo;) In quantum electrodynamics, by the way, it\u0026rsquo;s a more than merely aesthetic shame that magnetic charge does not seem to exist: Dirac showed that the existence of magnetic charge would explain why electric charge is quantized.\n7.3.5: Maxwell\u0026rsquo;s Equations in Matter # Maxwell\u0026rsquo;s equations in the form 7.40 are complete and correct as they stand. However, when you are working with materials that are subject to electric and magnetic polarization there is a more convenient way to write them. For inside polarized matter there will be accumulations of \u0026ldquo;bound\u0026rdquo; charge and current, over which you exert no direct control. It would be nice to reformulate Maxwell\u0026rsquo;s equations so as to make explicit reference only to the \u0026ldquo;free\u0026rdquo; charges and currents.\nWe have already learned, from the static case, that an electric polarization \\( \\vec{P} \\) produces a bound charge density\n\\[\\rho_b = - \\div \\vec{P} \\tagl{7.47}\\] (Eq. 4.12). Likewise, a magnetic polarization (or \u0026ldquo;magnetization\u0026rdquo;) \\( \\vec{M} \\) results in a bound current\n\\[\\vec{J}_b = \\curl \\vec{M} \\tagl{7.48}\\] (Eq. 6.13). There\u0026rsquo;s just one new feature to consider in the nonstatic case: Any change in the electric polarization involves a flow of (bound) charge (call it \\( \\vec{J}_p \\) ), which must be included in the total current. For suppose we examine a tiny chunk of polarized material (Fig. 7.47). The polarization introduces a charge density \\( \\sigma_b = P \\) at one end and \\( - \\sigma_b \\) at the other (Eq. 4.11). If \\( P \\) now increases a bit, the charge on each end increases accordingly, giving a net current\n\\[\\dd I = \\pdv{\\sigma_b}{t} \\dd a_{\\perp} = \\pdv{P}{t} \\dd a_{\\perp}\\] The current density, therefore, is\n\\[\\vec{J}_p = \\pdv{\\vec{P}}{t} \\tagl{7.49}\\] This polarization current has nothing to do with the bound current \\( \\vec{J}_b \\) . The latter is associated with magnetization of the material and involves the spin and orbital motion of electrons; \\( \\vec{J}_p \\) by contrast, is the result of the linear motion of charge when the electric polarization changes. If \\( P \\) points to the right, and is increasing, then each plus charge moves a bit to the right and each minus charge to the left; the cumulative effect is the polarization current \\( \\vec{J}_p \\) . We ought to check that Eq. 7.49 is consistent with the continuity equation:\n\\[\\div \\vec{J}_p = \\div \\pdv{\\vec{P}}{t} = \\pdv{}{t} ( \\div \\vec{P}) = - \\pdv{\\rho_b}{t}\\] Yes: the continuity equation is satisfied: in fact \\( \\vec{J}_p \\) is essential to ensure the conservation of bound charge. (Incidentally, a changing magnetization does not lead to any analogous accumulation of charge or current. The bound current \\( \\vec{J}_b = \\curl \\vec{M} \\) varies in response to \\( \\vec{M} \\) , to be sure, but that\u0026rsquo;s about it.)\nIn view of all this, the total charge density can be separated into two parts:\n\\[\\rho = \\rho_f \u0026#43; \\rho_b = \\rho_f - \\div \\vec{P} \\tagl{7.50}\\] and the current density into three parts\n\\[\\vec{J} = \\vec{J}_f \u0026#43; \\vec{J}_b \u0026#43; \\vec{J}_p = \\vec{J}_f \u0026#43; \\curl \\vec{M} \u0026#43; \\pdv{\\vec{P}}{t} \\tagl{7.51}\\] Gauss\u0026rsquo;s law can now be written as\n\\[\\div \\vec{E} = \\frac{1}{\\epsilon_0} (\\rho _f - \\div \\vec{P})\\] or\n\\[\\div \\vec{D} = \\rho_f \\tagl{7.52}\\] where, as in the static case\n\\[\\vec{D} = \\epsilon_0 \\vec{E} \u0026#43; \\vec{P} \\tagl{7.53}\\] Meanwhile, Ampere\u0026rsquo;s law (with Maxwell\u0026rsquo;s term) becomes\n\\[\\curl \\vec{B} = \\mu_0 \\left( \\vec{J}_f \u0026#43; \\curl \\vec{M} \u0026#43; \\pdv{\\vec{P}}{t} \\right) \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t}\\] or\n\\[\\curl \\vec{H} = \\vec{J}_f \u0026#43; \\pdv{\\vec{D}}{t} \\tagl{7.54}\\] where, as before\n\\[\\vec{H} \\equiv \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\tagl{7.55}\\] Faraday\u0026rsquo;s law and \\( \\div \\vec{B} = 0 \\) are not affected by our separation of charge and current into free and bound parts, since they do not involve \\( \\rho \\) or \\( \\vec{J} \\) .\nIn terms of free charges and currents, then, Maxwell\u0026rsquo;s equations read\nMaxwell\u0026rsquo;s Equations in Matter\n\\[\\begin{aligned} (\\text{i}) \u0026amp; \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss\u0026#39;s law)} \\\\ (\\text{ii}) \u0026amp; \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng\u0026#39;s Law)} \\\\ (\\text{iii}) \u0026amp; \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday\u0026#39;s Law}) \\\\ (\\text{iv}) \u0026amp; \\quad \\curl \\vec{H} = \\vec{J}_f \u0026#43; \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere\u0026#39;s Law)} \\end{aligned} \\tagl{7.56} \\] Some people regard these as the \u0026ldquo;true\u0026rdquo; Maxwell\u0026rsquo;s equations, but please understand that they are in no way more \u0026ldquo;general\u0026rdquo; than Eq. 7.40; they simply reflect a convenient division of charge and current into free and nonfree parts. And they have the disadvantage of hybrid notation, since they contain both \\( \\vec{E} \\) and \\( \\vec{D} \\) , both \\( \\vec{B} \\) and \\( \\vec{H} \\) . They must be supplemented, therefore, by appropriate constitutive relations, giving \\( \\vec{D} \\) and \\( \\vec{H} \\) in terms of \\( \\vec{E} \\) and \\( \\vec{B} \\) . These depend on the nature of the material; for linear media\n\\[\\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\qquad \\text{ and } \\qquad \\vec{M} = \\chi_m \\vec{H} \\tagl{7.57}\\] so\n\\[\\vec{D} = \\epsilon \\vec{E} \\qquad \\text{ and } \\qquad \\vec{H} = \\frac{1}{\\mu} \\vec{B} \\tagl{7.58}\\] where \\( \\epsilon = \\epsilon_0 (1 + \\chi_e) \\) and \\( \\mu = \\mu_0 (1 + \\chi_m) \\). Incidentally, you\u0026rsquo;ll remember that \\( \\vec{D} \\) is called the electric \u0026ldquo;displacement\u0026rdquo;; that\u0026rsquo;s why the second term in the Ampere/Maxwell equation came to be called the displacement current. In this context\n\\[\\vec{J}_d \\equiv \\pdv{\\vec{D}}{t} \\tagl{7.59}\\] 7.3.6: Boundary Conditions # In general, the fields \\( \\vec{E}, \\vec{B}, \\vec{D}, \\) and \\( \\vec{H} \\) will be discontinuous at a boundary between two different media, or at a surface that carries a surface charge density \\( \\sigma \\) or a current density \\( \\vec{K} \\). The explicity form of these discontinuities can be deduced from Maxwell\u0026rsquo;s equations in their integral form\n\\[\\begin{aligned} (1) \\quad \u0026amp; \\oint_S \\vec{D} \\cdot \\dd \\vec{a} = Q_{f, enc} \\\\ (2) \\quad \u0026amp; \\oint _S \\vec{B} \\cdot \\dd \\vec{a} = 0 \\\\ (3) \\quad \u0026amp; \\oint _P \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{}{t} \\int _S \\vec{B} \\cdot \\dd \\vec{a} \\\\ (4) \\quad \u0026amp; \\oint _P \\vec{H} \\cdot \\dd \\vec{l} = I_{f, enc} \u0026#43; \\dv{}{t} \\int_S \\vec{D} \\cdot \\dd \\vec a \\end{aligned}\\] Applying (1) to a tiny, wafer-thin Gaussian pillbox extending just slightly into the material on either side of the boundary (Fig 7.48), we obtain\n\\[\\vec{D}_1 \\cdot \\vec{a} - \\vec{D}_2 \\cdot \\vec a = \\sigma_f a\\] (The positive direction for \\( \\vec a \\) is from 2 toward 1. The edge of the wafer contributes nothing in the limit as the thickness goes to zero; nor does any volume charge density.) Thus, the component of \\( \\vec D \\) that is perpendicular to the interface is discontinuous in the amount\n\\[D_1 ^\\perp - D_2 ^\\perp = \\sigma_f \\tagl{7.60}\\] Identical reasoning, applied to equation (2) yields\n\\[B_1 ^\\perp - B_2 ^\\perp = 0 \\tagl{7.61}\\] Turning to (3), a very thin Amperian loop straddling the surface gives\n\\[\\vec{E_1} \\cdot \\vec{l} - \\vec{E_2} \\cdot \\vec{l} = - \\dv{}{t} \\int_S \\vec{B} \\cdot \\dd \\vec a \\] But in the limit as the width of the loop goes to zero, the flux vanishes. (I have already dropped the contribution of the two ends to \\( \\oint \\vec{E} \\cdot \\dd \\vec l \\), on the same grounds)\nTherefore,\n\\[\\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\tagl{7.62}\\] That is, the components of \\( \\vec E \\) parallel to the interface are continuous across the boundary. By the same token, (4) implies\n\\[\\vec{H}_1 \\cdot \\vec l - \\vec{H}_2 \\cdot \\vec l = I_{f, enc}\\] where \\( I_{f, enc} \\) is the free current passing through the Amperian loop. No volume current density will contribute (in the limit of infinitesimal width), but a surface current can. In fact, if \\( \\vu{n} \\) is a unit vector perpendicular to the interface (pointing from 2 toward 1), so that \\( (\\vu{n} \\cross \\vec l ) \\) is normal to the Amperian loop (Fig 7.49), the\n\\[I_{f, enc} = \\vec{K_f} \\cdot (\\vu n \\cdot \\vec l) = (\\vec{K_f} \\cross \\vu n) \\cdot \\vec l\\] and hence \\[\\vec{H}_1 ^\\parallel - \\vec{H}_2 ^\\parallel = \\vec{K}_f \\cross \\vu n \\tagl{7.63}\\] So the parallel components of \\( \\vec H \\) are discontinuous by an amount proportional to the free surface current density.\nEquations 7.60-63 are the general boundary conditions for electrodynamics. In the case of linear media, they can be expressed in terms of \\( \\vec E \\) and \\( \\vec B \\) alone\n\\[\\begin{aligned} (1) \\quad \u0026amp; \\epsilon_1 E_1 ^\\perp - \\epsilon_2 E_2 ^\\perp = \\sigma_f \\\\ (2) \\quad \u0026amp; \\vec{B}_1 ^\\perp - \\vec{B}_2 ^\\perp = 0 \\\\ (3) \\quad \u0026amp; \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\\\ (4) \\quad \u0026amp; \\frac{1}{\\mu_1} \\vec{B}_1 ^\\parallel - \\frac{1}{\\mu_2} \\vec{B}_2 ^\\parallel = \\vec{K}_f \\cross \\vu{n} \\end{aligned} \\tagl{7.64}\\] In particular, if there is no free charge or free current at the interface, then\n\\[\\begin{aligned} (1) \\quad \u0026amp; \\epsilon_1 E_1 ^\\perp - \\epsilon_2 E_2 ^\\perp = 0 \\\\ (2) \\quad \u0026amp; \\vec{B}_1 ^\\perp - \\vec{B}_2 ^\\perp = 0 \\\\ (3) \\quad \u0026amp; \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\\\ (4) \\quad \u0026amp; \\frac{1}{\\mu_1} \\vec{B}_1 ^\\parallel - \\frac{1}{\\mu_2} \\vec{B}_2 ^\\parallel = 0 \\end{aligned} \\tagl{7.65}\\] These equations provide the basis for the theory of reflection and refraction.\n"},{"id":77,"href":"/r/notes/griffiths/ch8-0/","title":"Phys 544 Introduction","section":"Griffiths Introduction to Electrodynamics","content":" 8.0: Phys 544 Introduction # Author Notes # From here on out, the text will be based on the lecture content, not on the Griffiths textbook. The class will follow the 4th edition pretty closely, but there will definitely be some differences. Hope I don\u0026rsquo;t get too much wrong!\nCourse Structure # We\u0026rsquo;ve got some real homework this time!\n33% Homework, more or less weekly 33% Midterm. 1 hour, closed-book 33% Final + presentation. Topic should be related to EM radiation applications. It\u0026rsquo;ll be a ~10 minute presentation and a 4-page paper. For homework, either email it by Thursday 5pm, or preferably just hand it in on Wednesday evening during class.\n"},{"id":78,"href":"/r/notes/griffiths/ch8-1/","title":"Charge and Energy","section":"Griffiths Introduction to Electrodynamics","content":" 8.1: Charge and Energy # As far as particles go, we\u0026rsquo;re familiar with the momentum and energy of charges. With fields, we have a similar situation. The fields determine what the particles do, and together with the energy/momentum of the particles themselves, the energy/momentum of the fields form a set of conservation laws\nConservation of Charge # We know that charge is conserved. You can\u0026rsquo;t create or destroy charge by itself - you must compensate by destroying or creating some other charge. It\u0026rsquo;s captured in the equation of continuity, which relates charge to the divergence of current. In laymen\u0026rsquo;s terms, it says \u0026ldquo;if a charge was here, it must still be here or it must flow away.\u0026rdquo; You can relate the amount of charge that\u0026rsquo;s missing from a given region of space to the amount of charge that flows away from that element of space. This is a statement of conservation of local charge, not just global charge, which is a much stronger statement.\nSay we have some charge distribution \\( \\rho(\\vec{r}, t) \\) over a region S, and we have some current defined over the boundary \\( \\vec{J}(\\vec{r}, t) \\). The relation between \\( \\rho \\) and \\( \\vec{J} \\) gives the equation of continuity. The change of charge within the entire volume must be compensated for by an amount of charge flowing in/out of the system:\n\\[\\dv{Q}{t} = \\dv{}{t} \\int_{V} \\rho \\dd \\tau = - \\oint \\vec{J} \\cdot \\dd a\\] We get a negative sign because by convention the normal to the surface points outward. We can take the time integral inside, and use Gauss\u0026rsquo;s theorem (divergence theorem) on the right-side\n\\[\\int_V \\pdv{\\rho}{t} \\dd \\tau = - \\int_V (\\div \\vec{J}) \\dd \\tau\\] Since we chose an arbitrary volume, it must be the case that the integrands are equal, over all space.\n\\[\\pdv{\\rho}{t} = - \\div \\vec{J} \\tagl{8.1}\\] Let\u0026rsquo;s check that this is consistent with our Maxwell equations. Ampere law:\n\\[\\curl \\vec{B} = \\mu_0 \\vec{J} \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t}\\] Take divergence of both sides\n\\[\\div( \\curl \\vec{B} ) = \\mu_0 \\div( \\vec{J} ) \u0026#43; \\mu_0 \\epsilon_0 \\div \\left( \\pdv{\\vec{E}}{t} \\right) \\\\ 0 = \\mu_0 \\div( \\vec{J} ) \u0026#43; \\mu_0 \\epsilon_0 \\pdv{}{t} \\left( \\div \\vec{E} \\right) \\\\ 0 = \\mu_0 \\div( \\vec{J} ) \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\rho}{t} \\frac{1}{\\epsilon_0} \\\\ \\rightarrow \\pdv{\\rho}{t} \u0026#43; \\div \\vec{J} = 0\\] So we\u0026rsquo;re back to the continuity equation!\nConservation of Energy and Poynting\u0026rsquo;s Theorem # The energy conservation we\u0026rsquo;ll find is a combination of the energy stored in / done on the charges and the energy stored in the field. The new concept is the Poynting vector, describing the spatial flow of energy.\nThe Maxwell\u0026rsquo;s equations tell us how to get from distributions of charges and currents to fields \\( \\vec{E} \\) and \\( \\vec{B} \\). The given distributions must satisfy the continuity relation, of course. Let\u0026rsquo;s take a look at the amount of work that\u0026rsquo;s done on the source charges, and use the result to connect to \u0026ldquo;where\u0026rdquo; the energy is, in some volume.\n\\[\\vec{F} = q [ \\vec{E} \u0026#43; \\vec{v} \\cross \\vec{B} ]\\] \\[\\dd W = \\vec{F} \\cdot \\dd \\vec{l} = q ( \\vec{E} \u0026#43; \\vec{v} \\cross \\vec{B} ) \\cdot \\vec{v} \\dd t\\] We want to generalize to a charge distribution \\( \\dd q = \\rho \\dd \\tau \\)\n\\[\\dv{W}{t} = \\int_V \\vec{E} \\cdot [ (\\rho \\dd \\tau) \\vec{v} ] = \\int_V (\\vec{E} \\cdot \\vec{J}) \\dd \\tau\\] Can we get an expression for \\( \\dd W \\) in terms of the fields only? Let\u0026rsquo;s try and eliminate \\( \\vec{F} \\) in terms of \\( \\vec{E} \\) and \\( \\vec{B} \\) using Maxwell\u0026rsquo;s equations.\n\\[\\vec{J} = \\frac{\\curl \\vec{B}}{\\mu_0} - \\epsilon_0 \\pdv{\\vec{E}}{t}\\] \\[\\dv{W}{t} = \\int_V \\left( \\frac{\\vec{E} \\cdot (\\curl \\vec{B}) }{\\mu_0} - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau\\] Using vector identity\n\\[\\curl (\\vec{A} \\cross \\vec{B}) = \\vec{B} \\cdot (\\curl \\vec{A}) - \\vec{A} \\cdot (\\curl \\vec{B})\\] \\[\\dv{W}{t} = \\int_V \\left( \\frac{1}{\\mu_0} \\left( \\vec{B} \\cdot (\\curl \\vec{E}) - \\div (\\vec{E} \\cross \\vec{B}) \\right) - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau \\\\ = \\int_V \\left( - \\frac{\\vec{B}}{\\mu_0} \\cdot \\pdv{\\vec{B}}{t} - \\div \\vec{S} - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau\\] where \\( \\vec{S} \\) is the so-called Poynting vector defined as\n\\[\\vec{S} = \\frac{\\vec{E} \\cross \\vec{B}}{\\mu_0}\\] Let\u0026rsquo;s take a look at the quantity \\( \\dv{}{t} (\\vec{E} ^2) \\)\n\\[\\dv{}{t} \\left( \\vec{E}^2 \\right) = \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \u0026#43; \\pdv{\\vec{E}}{t} \\cdot \\vec E \\quad \\rightarrow \\quad \\vec E \\cdot \\pdv{\\vec E}{t} = \\frac{1}{2} \\dv{}{t} \\left( \\vec E ^2 \\right)\\] \\[\\dv{W}{t} = \\int_V \\left( - \\frac{1}{2 \\mu_0} \\pdv{}{t} \\left( \\vec{B} ^2 \\right) - \\frac{\\epsilon_0}{2} \\pdv{}{t} \\left( \\vec E ^2 \\right) - \\div \\vec{S} \\right) \\dd \\tau \\\\ = - \\dv{}{t} \\left[ \\int_V \\frac{\\vec B ^2}{2 \\mu_0} \\dd \\tau \u0026#43; \\int_V \\frac{\\epsilon_0 \\vec E ^2}{2} \\dd \\tau \\right] - \\int_V \\div \\vec{S} \\dd \\tau\\] The integrands in the middle are just the energy densities of the electric and magnetic fields\n\\[\\frac{\\vec B ^2}{2 \\mu_0} = u_m \\qquad \\frac{\\epsilon_0 \\vec E ^2}{2} = u_e \\qquad u_{em} \\equiv u_e \u0026#43; u_m = u\\] \\[\\dv{W}{t} = - \\dv{}{t} \\int_V u_{em} \\dd \\tau - \\oint _S \\vec{S} \\cdot \\dd \\vec a \\tagl{8.2}\\] So what does this tell us? The left hand side reads \u0026ldquo;the change in energy in a region V\u0026rdquo;. The first part of the r.h.s is the change in the local energy stored in the E and B fields, and the second part is an acknowledgment that our region is part of a larger space, and energy may be flowing in/out of the region. The flow of energy is therefore given by the Poynting vector, so we call \\( \\eqref{8.1} \\) \u0026ldquo;Poynting\u0026rsquo;s Theorem\u0026rdquo;\nIn a charge-free region:\n\\[0 = - \\dv{}{t} \\int_V u_{em} \\dd \\tau - \\int_V ( \\div \\vec S ) \\cdot \\dd \\tau \\\\ 0 = - \\int_V \\pdv{u_{em}}{t} \\dd \\tau - \\int_V ( \\div \\vec S) \\dd \\tau\\] Since this is true for any arbitrary V, we have\n\\[\\rightarrow \\pdv{u_{em}}{t} \u0026#43; \\div \\vec{S} = 0 \\quad \\text{(in charge-free region)}\\] Let\u0026rsquo;s look at an example of the application of this statement of conservation of energy.\nProblem 8.2 # Q Consider the charging capacitor from problem 7.34. (a) Find the electric and magnetic fields in the gap, as functions of the distance s from the axis at time t (assume the charge is zero at t = 0). (b) Find the energy density \\( u_{em} \\) and the Poynting vector \\( \\vec S \\) in the gap. Note especially the direction of \\( \\vec S \\). Check that \\( \\eqref{8.2} \\) is satisfied. (c) Determine the total energy in the gap, as a function of time. Calculate the total power flowing into the gap, by integrating the Poynting vector over the appropriate surface. Check that the power input is equal to the rate of increase of energy in the gap. A (a) From the ch. 7 problems, we know\n\\[\\vec B = \\frac{\\mu_0 I}{2 \\pi a^2} s \\hat{\\phi}\\] Treating the gap as a parallel-plate capacitor, the electric field in the gap is\n\\[\\vec E = \\frac{\\sigma}{\\epsilon_0} \\hat{z} = \\frac{I t}{\\epsilon_0 \\pi a^2} \\hat{z}\\] (b)\n\\[u_{em} = \\frac{\\vec B ^2}{2 \\mu_0} \u0026#43; \\frac{\\epsilon_0 ^2 \\vec E ^2}{2} \\\\ = \\frac{1}{2} \\epsilon_0 \\frac{ I^2 t^2}{\\epsilon_0 \\pi^2 a^4} \u0026#43; \\frac{\\mu_0 ^2 I^2}{2 \\mu_0 4 \\pi ^2 a^4} s^2 \\\\ u_{em} = \\frac{1}{2} \\frac{I^2}{\\pi ^2 a^4} \\left[ \\frac{t^2}{\\epsilon_0} \u0026#43; \\frac{\\mu_0 s^2}{4} \\right]\\] \\[\\vec S = \\frac{1}{\\mu_0} (\\vec E \\cross \\vec B)\\] First, let\u0026rsquo;s check what the direction of \\( \\vec S \\) is. We know that \\( \\vec B \\) is azimuthal and \\( \\vec E \\) is axial, so \\( \\vec S \\) must point radially inward. Working through the expression, we get\n\\[\\vec S = - \\frac{1}{\\epsilon_0} \\frac{I^2 s t}{2 \\pi^2 a^4} \\hat{s}\\] Checking Poynting\u0026rsquo;s theorem\u0026hellip;\n\\[\\pdv{u_{em}}{t} = \\frac{I^2 t}{\\pi^2 a^4 \\epsilon_0} \\] \\[\\div \\vec S = \\frac{1}{s} \\pdv{}{s} \\left( s S_s \\right) = \\frac{I^2 t}{\\pi^2 a^4 \\epsilon_0} \\quad \\checkmark\\] (c) The power flowing into the gap: we add a negative sign, since conventionally the normal direction is outwards, and the poynting vector points inwards.\n\\[- \\frac{1}{\\mu_0} \\int (\\vec E \\cross \\vec B) \\cdot \\dd \\vec a = \u0026#43; \\frac{1}{\\epsilon_0} \\frac{I^2 s t}{2 \\pi^2 a^4} \\cross 2 \\pi a w \\\\ = \\frac{1}{\\epsilon_0} \\frac{I^2 t w}{\\pi a^2}\\] The rate of increase of energy in the gap would be the time integral of the volume integral of the energy density \\( u_{em} \\) in the gap. Again, the area of the cylinder is \\( \\pi a^2 w \\)\n\\[\\dv{}{t} \\left[ \\int u_{em} \\dd \\tau \\right] = \\dv{}{t} \\frac{I^2}{2 \\pi^2 a^4} \\frac{t^2}{\\epsilon_0} \\cdot \\pi a^2 w \\\\ = \\frac{I^2 t}{\\epsilon_0 \\pi a^2 } = - \\oint \\vec S \\cdot \\dd \\vec a \\quad \\checkmark\\] "},{"id":79,"href":"/r/notes/griffiths/ch8-2/","title":"Momentum","section":"Griffiths Introduction to Electrodynamics","content":" 8.2: Momentum # In this chapter we talk about:\nElectromagnetic momentum Maxwell stress tensor Conservation of electromagnetic momentum Angular momentum in EM fields 8.2.1: Electromagnetic Momentum # As it turns out, if you disregard the momentum associated with electromagnetic fields, Newton\u0026rsquo;s laws appear not to work out! Consider a basic system in cartesian coordinates of two moving point charges:\nWhat happens between the two charges? Well, the magnetic field of \\( q_1 \\) points into the page at \\( q_2 \\), so the magnetic force on \\( q_2 \\) is to the right, and the magnetic field of \\( q_2 \\) is out of the page at \\( q_1 \\) , so the magnetic force on \\( q_1 \\) is upward. The net electric force between the two charges is repulsive and opposite, but the magnetic forces aren\u0026rsquo;t, so the electromagnetic force on \\( q_1 \\) on \\( q_2 \\) is equal but not opposite to the force of \\( q_2 \\) on \\( q_1 \\), in violation of Newton\u0026rsquo;s third law! We\u0026rsquo;ve got a problem, and we\u0026rsquo;re going to solve it by invoking the momentum of the EM field.\n8.2.2 The Maxwell Stress Tensor # The way to recover conservation of momentum proceeds the same way we recovered the conservation of energy via the Poynting vector. Starting with the basic Coulomb/Lorentz laws, we\u0026rsquo;ll write down an expression for the electromagnetic force on charges in a volume. We\u0026rsquo;re going to integrate that over all space, which can have any distribution of charge, and relate that expression for force to an expression which only involves the field. In the interest of brevity, we\u0026rsquo;ll skip around a bit and leave the full derivations for the real textbook.\nSuppose we have a volume \\( V \\) containing some distribution of charge, current, and electromagnetic fields. The total force on that volume is\n\\[\\vec{F} = \\int_V ( \\vec E \u0026#43; \\vec v \\cross \\vec B) \\rho \\dd \\tau \\qquad (\\dd q = \\rho \\dd \\tau) \\\\ = \\int _V (\\rho \\vec E \u0026#43; \\vec J \\cross \\vec B ) \\dd \\tau \\qquad (\\vec J = \\rho \\vec v)\\] Again, the goal is to replace anything that looks like a source in favor of fields, using Maxwell\u0026rsquo;s equations. It\u0026rsquo;s handy to define the force per unit volume \\( f \\):\n\\[\\vec f \\equiv \\rho \\vec E \u0026#43; \\vec J \\cross \\vec B\\] \\[\\rho = \\epsilon_0 ( \\div \\vec E ) \\quad \\text{(Gauss\u0026#39; Law)} \\\\ \\vec J = \\frac{1}{\\mu_0} \\curl \\vec B - \\epsilon_0 \\pdv{\\vec E}{t}\\] \\[\\rightarrow \\vec f = \\epsilon_0 ( \\div \\vec E) \\vec E \u0026#43; \\left( \\frac{\\curl \\vec B}{\\mu_0} - \\epsilon_0 \\pdv{\\vec E}{t} \\right) \\cross \\vec B\\] Skipping through a few steps, we cut to the chase. Similar to the derivation of the Poynting theorem, also using the other two Maxwell equations we haven\u0026rsquo;t yet, we get\n\\[\\vec f = \\div \\overline{\\vec T} - \\epsilon_0 \\mu_0 \\pdv{\\vec S}{t}\\] \\[\\overline{\\vec T} \\equiv \\begin{pmatrix} T_{xx} \u0026amp; T_{xy} \u0026amp; T_{xz} \\\\ T_{yx} \u0026amp; T_{yy} \u0026amp; T_{yz} \\\\ T_{zx} \u0026amp; T_{zy} \u0026amp; T_{zz} \\end{pmatrix} \\\\ T_{ij} \\equiv \\epsilon_0 \\left( E_i E_j - \\frac{1}{2} \\delta_{ij} E^2 \\right) \u0026#43; \\frac{1}{\\mu_0} \\left(B_i B_j - \\frac{1}{2} \\delta_{ij} B^2 \\right)\\] where \\( \\vec S \\) is the Poynting vector and \\( \\overline{\\vec T} \\) is the so-called \u0026ldquo;Maxwell stress tensor.\u0026rdquo; To keep in mind what kind of units we\u0026rsquo;re talking about here, \\( \\vec f \\) has units force per unit volume, and the divergence will strip one spatial dimension, so the Maxwell stress tensor will have units of stress (force per unit area).\nThe tensor has diagonal \u0026ldquo;pressure\u0026rdquo; terms and off-diagonal \u0026ldquo;shear\u0026rdquo; terms. For \u0026ldquo;pressure\u0026rdquo; forces, the force and area are in the same direction, and in the \u0026ldquo;shear\u0026rdquo; case the force and area are orthogonal.\nThe divergence term \\( \\div \\overline{\\vec T} \\) is itself a vector\n\\[\\div \\overline{\\vec T} = \\left( \\vu{i} \\pdv{}{x} \u0026#43; \\vu j \\pdv{}{y} \u0026#43; \\vu k \\pdv{}{z} \\right) \\cdot \\overline{\\vec T} \\\\ = \\epsilon_0 \\left[ (\\div \\vec E) E_j \u0026#43; (\\vec E \\cdot \\grad) E_j - \\frac{1}{2} \\grad _j E^2 \\right] \\\\ \u0026#43; \\frac{1}{\\mu_0} \\left[ ( \\div \\vec B) B_j \u0026#43; (\\vec B \\cdot \\grad) B_j - \\frac{1}{2} \\grad _j B^2 \\right]\\] As we do the volume integral to go from \\( \\vec f \\) to \\( \\vec F \\)\n\\[\\vec F = \\int _V \\left(\\div\\overline{\\vec T} - \\epsilon_0 \\mu_0 \\pdv{\\vec S}{t} \\right) \\dd \\tau \\\\ = \\oint \\overline{\\vec{T}} \\cdot \\dd \\vec a - \\epsilon_0 \\mu_0 \\pdv{}{t} \\int \\vec{S} \\dd \\tau\\] 8.2.3 Conservation of Electromagnetic Momentum # \\[\\vec F = \\dv{\\vec{p}_{mech}}{t} = - \\epsilon_0 \\mu_0 \\dv{}{t} \\int_V \\vec S \\dd \\tau \u0026#43; \\oint \\overline{\\vec T} \\cdot \\dd \\vec a\\] The first term on the right is related to the momentum stored in the electromagnetic field. The second term is the rate at which momentum flows across the surface, and we describe the left-hand-side as the rate of change of the momentum of charges within the volume.\nWe identify another useful term as the first integrand on the right:\n\\[\\vec g \\equiv \\epsilon_0 \\mu_0 \\vec S = \\epsilon_0 (\\vec E \\cross B) \\quad \\text{(momentum density in EM fields)}\\] which is the momentum density within the fields. Just as a note, the signs here are swapped from the Poynting theorem - the Maxwell stress tensor is defined such that momentum flowing into the region corresponds with increasing \\( \\overline{\\vec T} \\), and vice-versa, opposite the case we had with \\( \\vec S \\).\nIn a charge-free region,\n\\[- \\dv{}{t} \\int_V \\vec g \\dd \\tau \u0026#43; \\oint_S \\overline{\\vec T} \\cdot \\dd \\vec a = 0\\] and since the above is true for all regions \\( V \\), we have our familiar continuity-type equation\n\\[- \\pdv{\\vec g}{t} \u0026#43; \\div \\overline{\\vec T} = 0\\] Example: Problem 8.7 # Q Consider an infinite parallel-plate capacitor, with the lower plate (at \\( z = -d/2 \\)) carrying surface charge density \\( -\\sigma \\), and the upper plate (at \\( z = +d/2 \\)) carrying charge density \\( + \\sigma \\).\n(a) Determine all elements of the stress tensor, in the region between the plates.\n(b) Use \\( \\vec F = \\oint_S \\overline{\\vec T} \\cdot \\dd \\vec a \\) at the boundary to determine the electromagnetic force per unit area on the top plate.\n(c) What is the electromagnetic momentum per unit area, per unit time, crossing the xy plane (or any other plane parallel to that one, between the plates)?\nA (a) Lucky for us, the magnetic field between the plates is\n\\[\\vec B = 0\\] The electric field is very simple\n\\[\\vec E = \\frac{\\sigma}{\\epsilon_0} ( - \\vu z)\\] So we can already tell that the off-diagonal terms will be zero, since they all contain two factors of \\( E_i E_j \\), one of which will be zero. The \\( B \\) terms on the diagonal will be zero, and\n\\[\\overline{\\vec T} = \\frac{\\sigma^2}{2 \\epsilon_0} \\begin{pmatrix} -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}\\] (b) The force on the top plate will be\n\\[\\vec F / A = \\frac{1}{A} \\oint _S \\overline{\\vec T} \\cdot \\dd \\vec a \\\\ = \\frac{1}{A} \\frac{\\sigma ^2}{2 \\epsilon_0} \\begin{pmatrix} -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -A \\end{pmatrix} \\\\ = - \\frac{\\sigma^2}{2 \\epsilon_0} \\vu z\\] (c) This is just to show us that the momentum flow through the interior of the capacitor is the same as whatever force is pulling the top and bottom plates apart.\n\\[- T_{zz} = - \\sigma^2 / 2 \\epsilon_0 \\] is the momentum in the z direction crossing a surface perpendicular to z, per unit area, per unit time.\n8.2.4 Angular Momentum in EM Fields # As a reminder, we associate with the electromagnetic fields an energy density\n\\[u_{em} = \\frac{1}{2} \\epsilon_0 E^2 \u0026#43; \\frac{\\mu_0}{2} B^2\\] and a momentum density\n\\[\\vec g = \\epsilon_0 ( \\vec E \\cross \\vec B)\\] For that matter, we define angular momentum in the normal fashion\n\\[\\vec l = \\vec r \\cross \\vec g = \\epsilon_0 [ \\vec r \\cross ( \\vec E \\cross \\vec B) ]\\] where the presence of \\( \\vec r \\) means it\u0026rsquo;s defined about some point or axis.\nExample 8.4 # Q Imagine a very long solenoid with radius \\( R \\), \\( n \\) turns per unit length, and current \\( I \\). Coaxial with the solenoid are two long cylindrical (non-conducting) shells of length \\( l \\) - one inside the solenoid at radius \\( a \\) carries a charge \\( +Q \\) distributed uniformly over its surface; the other outside the solenoid at radius \\( b \\) carries charge \\( -Q \\) . When the current in the solenoid is gradually reduced to nothing, the cylinders begin to rotate. Question: where does the angular momentum come from?\nA We\u0026rsquo;ll start by writing down the initial angular momentum of the system, then see what happens when we start to change the current. It is assumed that the solenoid is very long compared with the charged cylinders, and that the length of each charged cylinder is much much greater than its radius.\nThe initial \\( \\vec E \\) is given by looking at a Gaussian cylinder between the two charged cylinders\n\\[E 2 \\pi s l = \\frac{Q}{\\epsilon_0}\\] \\[\\rightarrow \\vec E = \\frac{Q}{2 \\pi \\epsilon_0 l} \\frac{1}{s} \\vu s \\qquad a \u0026lt; s \u0026lt; b\\] The initial \\( \\vec B \\) is just that of the solenoid, namely\n\\[\\vec B = \\begin{cases} \\mu_0 n I \\vu z \u0026amp; \\qquad 0 \u0026lt; s \u0026lt; R \\\\ 0, \u0026amp; \\qquad s \u0026gt; R \\end{cases}\\] The linear momentum density we defined earlier is\n\\[\\vec g = \\epsilon_0 \\mu_0 \\vec S = \\epsilon_0 ( \\vec E \\cross \\vec B) = \\frac{Q n I \\mu_0 ^2}{2 \\pi s} (- \\vu \\phi) \\] And the angular momentum density with respect to the cylindrical axis is\n\\[\\vec l = \\vec s \\cross \\vec g\\] And it will point in the \\( - \\vu z \\) direction\n\\[\\vec L = \\int _V \\vec l \\dd \\tau = \\int_V \\vec s \\cross \\epsilon_0 \\frac{Q}{2 \\pi \\epsilon_0 l} \\frac{1}{s} \\vu s \\cross \\mu_0 I n \\vu z \\dd \\tau\\] \\[= - \\frac{\\mu_0 n I Q }{2 \\pi l} \\int _V \\vu s \\cross \\vu \\phi \\dd \\tau = - \\frac{\\mu_0 n I Q}{2 \\pi l} \\vu z \\pi (R^2 - a^2 ) l = - \\frac{\\mu_n n I Q}{2} (R^2 - a^2) \\vu z\\] So that\u0026rsquo;s the initial angular momentum. Of course, nothing is moving, so this is just the momentum stored in the EM fields. Assuming nothing like friction complicates the situation, any angular momentum lost by the EM fields will be transferred to the cylinders as rotation.\nAs we crank down the current, we know that a time variation in the magnetic field implies an induced EMF \\( \\mathcal{E} = - \\dv{\\Phi}{t} \\). \\( \\vec E \\) experienced at radii \\( a \\) and \\( b \\) are now different. Use Faraday\n\\[E_a 2 \\pi a = - \\mu_0 n \\dv{I}{t} \\pi a^2 \\rightarrow \\vec E_a = - \\frac{\\mu_0 n a}{2} \\dv{I}{t} \\vu \\phi\\] This field is in the direction to torque cylinder \\( +Q \\)\n\\[\\vec{\\Gamma_a} = \\vec s \\cross (Q \\vec{E_a}) = - \\frac{\\mu_0 n a}{2} \\dv{I}{t} Q a \\vu \\phi\\] \\[= - \\frac{\\mu_0 n a^2}{2} Q \\dv{I}{t} \\vu z\\] Integrating torque over time, we get the changed angular momentum of the cylinder\n\\[\\vec{L_a} = \\int_0 ^t \\tau \\dd t = - \\frac{\\mu_0 n a^2}{2} Q \\int_0 ^\\infty \\dv{I}{t} \\dd t \\vu z = - \\frac{\\mu_0 n a^2}{2} Q \\vu z \\int_0 ^\\infty \\dd I\\] \\[= \\frac{\\mu_0 n I}{2} a^2 \\vu z\\] What happens to the outer cylinder? Something very similar, but when we calculated from Faraday\u0026rsquo;s law our loop radius is \\( b \\), and the area of flux is \\( \\pi R^2 \\)\n\\[\\vec E_b = - \\frac{\\mu_0 n R^2}{2a} \\dv{I}{t} \\vu \\phi\\] \\[\\vec{F_b} = - Q \\vec{E}_b\\] \\[\\vec {\\Gamma _b} = b \\vu s \\cross \\vec{F_b}\\] \\[\\rightarrow \\vec{L}_b = \\frac{\\mu_0 n R^2 Q}{2} \\int_0 ^\\infty \\dv{I}{t} \\dd t \\vu z = - \\frac{\\mu_0 n R^2 Q I}{2} \\vu z\\] So the total angular momentum once the current is finally turned down is\n\\[\\vec{L_a} \u0026#43; \\vec{L_b} = - \\frac{\\mu_0 n I Q}{2} (R^2 - a^2) \\vu z\\] which is exactly the angular momentum that was stored in the fields, so we\u0026rsquo;ve successfully conserved angular momentum :)\n"},{"id":80,"href":"/r/notes/UWAA560/90-student-lectures/","title":"Student Lectures","section":"Plasma Diagnostics","content":" \\[\\] Particle Diagnostics: Neutron diagnostics (Aria Johansen) # Start from the basics: what is a neutron? It\u0026rsquo;s a spin \\( 1/2 \\) baryon which interacts with all forces. Primarily decays into usually an electron and an antielectron neutron as well as a proton. Another decay process with 1/1000 of intensity decays to gamma. Mass is \\( 939.6 MeV/c^2 \\), very comparable to proton.\nWhere do neutrons come from? They decay quite rapidly - 15 minute lifetime. # One of the ways you can make a neutron is through inverse beta decay - a rare process in which an electron antineutrino collides with a proton creating a neutron and positron Spallation from atmospheric muons - creates a background of neutrinos Neutron emission - larger unstable nuclei can produce free daughter neutrons (spontaneous fission, induced fission, photoneutron, spontaneous neutron emission, beta-delayed neutron emission) Nuclear fusion - depending on the size of the nuclei fused, neutrons of a specific energy are emitted. This is the focus of this talk and the focus of neutron diagnostics in fusion plasmas. In a fusion reaction, neutrons come from the parent nuclei, although not every fusion reaction creates a free daughter neutron. D+T fusion in particular produces a 14.1 MeV daughter neutron and neutral \\( 4He \\). D+D produces a daughter neutron of 2.45 MeV and 3He about 50% of the time.\nNeutron spectroscopy # You can count the neutrons that come from a fusion reaction, and you can also look at their spectra. One of the best ways to look at the energy of the neutrons is to measure the response in a scintillator which releases a proportional number of photons\n\\[E_p = E_n \\sin ^2 \\theta\\] Neutron energy classification # The general strategy for detecting neutrons is to collide them with a target nucleus, to produce a charged particle or electromagnetic interaction that can be detected. These processes are\nrecoil nucleus: scintillating materials for fast neutron spectroscopy proton: proportional counters with high pressure He fill gas alpha particle: 6Li and B Fission fragments: 233U, 235U, 239P are the target nuclei Neutron energy dictates a specific detector\u0026rsquo;s desirability Two major categories of techniques for measuring fast neutrons:\nfast neutron spectroscopy cool fast neutrons through mediation before counting Neutrons of different energies are detected in different ways. Fast neutrons are our main concern (1-20MeV), as they are the products of nuclear reactions.\nIn the scintillator, an incident neutron causes an emission of photons from the scintillating material, which make their way to the photocathode. The photocathode feeds into the photomultiplier which feeds a substantial electric pulse. Measurement is done in a time window. The scintillator material might have a characteristic time of some tens of nanoseconds. A few common types used for fast neutron counting and spectroscopy are\nLiquid organic scintillators Plastic scintillators 3He scintillators Liquid organics # Pros Can distinguish neutrons from x-rays by pulse shape Robust to radiation Cost effective for really big detectors Cons Slower response time (100s of ns) makes high count rate applications unreliable Liquid needs a container Oxygen impurity removal 3He Scintillator # Pros Several nanosecond decay time Robust to radiation Cons Rare element using 3He Hard to contain as it outgasses readily Under pressure (up to 150 atm) Liquid Ni cold Poor pulse height resolution Cross section for faster neutrons becomes small with increasing energy Plastic Scintillator # Pros Fast pulse response Maintains its own shape Easy to assemble Great cross section with neutrons because of high hydrogen content and fast neutron energy range considerations Decay time is generally 1-4 ns Cons No pulse shape discrimination Degrades from radiation exposure Photodetectors used for neutron detection # Scintillating materials make a range of photon wavelengths and numbers specific to the material and incident radiation known as a Compton spectrum\nPairing scintillators with photomultipliers requires that they have matching geometry, similar maximum emission (scintillator) and peak (PM) wavelengths\nPhotomultipliers have different time responses dictating their pulse resolution in time, and thus reliability with high neutron incidence. Increasing distance from source reduces incidence, increasing reliability, however more detectors need to be used for reliable counting measures.\nPhotomultiplier Tubes # Pros Large range of types and characteristics available Slightly higher gain and robustness to temp than a SiPM Generally cheaper than SiPM Cons Fragile and light sensitive, even when not in operation Requires stable high voltage supply Active area per dead time is generally greater than SiPMs Some types require magnetic shielding Can have smaller active area, but even with this has larger profile than SiPM Useful PMT Terminology:\nDynodes: produce a measurable current at anode of the photomultiplier. Usually a conductor with insulated coating. Gain: Total amplification of electron across PM, \\( G = \\delta ^n \\) Secondary emission factor: gain of each electrode \\( \\delta = K V_d \\) Linearity: when each stage in PM is proportional to initial cathode current Dead time (Pulse width): the finite time required by the detector to process an event Extendable (paralyzable) - detector becomes insensitive to events Non-extendable (non-paralyzable) - detector is still sensitive to events Pulse pile up: Non-extendable dead time is a distortion of signal caused by two events with overlapping dead time Silicon Photomultipliers (SiPMs) # Pros Small active area Insensitive to magnetic fields Good photo detection efficiency Cons False positives (afterpulse/crosstalk) Finite dynamic range (quasi-analog) Excess voltage dependent noise Strong temperature dependence Activation Detectors # Using the daughter particles of neutron collision in a scintillating detector is the strategy of activation detectors. Here, neutrons collide with an arsenic loaded epoxy making gamma of 304, 280, and 24 keV energies. Active detectors have a pulse or current signal produced by a neutron. As you increase neutron energy, cross-section goes way down.\nNeutron Counting # Discarding neutron energy information allows for easier counting. We \u0026ldquo;thermalize\u0026rdquo; the neutrons in order to make their cross-section better for counting them up. Proportional tubes used for slow neutron detection. Fill tube with say 3He and \\( BF_3 \\). Charged daughter particles created from the neutron interacting with the fill gas are accelerated to either anode or cathode. The electrons created from the ionizing event collect on the anode producing a readable signal. The applied voltage amplifies the number of ion-electron pairs created in a cascade. To prevent runaway events, a quenching gas is added.\nCHERS (Charge exchange recombination spectroscopy) # Charge exchange process in which an electron from a (usually) neutral atom or molecule (B on LHS) loses an electron to an ion (A on LHS)\n\\[A^\u0026#43; \u0026#43; B \\rightarrow A^\\star \u0026#43; B^\u0026#43;\\] As a result of this exchange, the ion has become neutralized without losing (much) momentum. Within the context of plasma, injecting a neutral species the same as the plasma ion species prevents plasma contamination.\nFor an electron associated with an injected neutral beam of hydrogen, the fully stripped impurity ion has a more attractive electric potential than the hydrogen\u0026rsquo;s singly charged nucleus. Still, the electron needs sufficient energy to cross the saddle point to become associated with the impurity ion, that is, its binding energy is approximately the size of the saddle.\nHow it works: A neutral beam of (usually) hydrogen atoms is injected into a stream of plasma. The neutral beam becomes ionized, while impurity ions gain an electron that then radiates giving us temperature, density, and flow velocity for this otherwise unreadable impurity.\nChord-integrated diagnostic, difficult to distinguish impurity spectra from background bremsstrahlung. Possible coincident lines. Interference of the impurity having a natural single electron population. Imparting the impurities with an electron, they then travel to the core of the plasma or elsewhere where they dissipate radiation without being observed by the diagnostic. Absolute beam intensity, and accurate photon yield cross sections, and spectrometer calibration is required for density measurements (not as good as bremsstrahlung measurements for density measurements)\nUsing flow velocity to measure E-field\nUsing the force balance equation, the electric field can be determined from measuring the flow velocity (for tokamak):\n\\[E_r = u_\\phi B_\\theta - u_\\theta B_\\theta \u0026#43; \\frac{\\grad P_i}{Z e n_i}\\] where subscript \\( i \\) indicates the CHERSing ion. This information can be used to evaluate the \\( E \\cross B \\) flow shear which is directly related to the stability of the plasma.\nGood for measure the temp and flow velocity of impurities, but challenging for use as impurity density measurement\nRequires a way to inject a neutral beam into a plasma, and multiple viewing angles to see flow velocity.\nQuestions:\nThe scintillator needs to be very well isolated from external EM radiation, and needs to be highly internally reflective. Do we also need to worry about excluding charged particles? Answer: By far the biggest concern is isolating our scintillator from EM radiation. The background radiation is the biggest source of noise in the scintillator signal, so we first need to spend a lot of effort making sure that the detector is completely light-tight. If you can leave the detector collecting overnight with extremely low photo background, then you can start worrying about charged particles, etc. Ex Situ: ESCA/XPS, Raman Microscopy, Profilometry (Reed Thompson) # x-ray photoelectron spectroscopy. Ex situ basically just means off-site, so off site investigation of surface composition and structure\nElectron spectroscopy for chemical analysis (ESCA) # Most favorable due to its theoretical strength, sample versatility, and high information content. Similar to auger electron spectroscopy and high-resolution electron energy loss spectroscopy (HREELS). Based on the photoelectric effect: when a photon collides with an electron, all of the energy of the photon is absorbed by the electron and the electron is emitted.\nSimplified ESCA experimental setup:\nMaterial sample is placed in a vacuum and x-rays are directed onto the sample. Atoms in the sample are emitted by core electrons. Photoelectrons are collected and kinetic energy is measured. The magnitude of the kinetic energy contains the binding energy of the electron. Each element has different binding energies for their electrons, so parent element can be identified. Relative contribution of different energies can be used to determine the concentration of an element within the sample and morphology of the surface. When a photon and atom collide, there are three possible events that occur:\nThe photon passes through the atom with no interaction The photon will lose a portion of its energy due to a collision with an orbital electron (Compton scattering) The photon will transfer all of its energy to the orbital electron and eject the electron Ejections are the important case for ESCA. Photon energy must be sufficient to excite the bound electron. Once electrons start to emit, the number of electrons emitted is a function of intensity of x-ray source and population of the emitting species. Any extra energy over the limit necessary to excite the electron will be added to the electron\u0026rsquo;s energy in a linear fashion (standard photoelectric effect)\n\\[E_B = h \\nu - K E\\] Whe the sample is conductive, then the sample and spectrometer must be grounded together so that they are at the same Fermi level. For insulating materials, reducing space charge effects can\u0026rsquo;t be done with grounding. Extra electrons must be added to the environment. The energy of the flooding electrons is added to the work function.\nPeak width and IMFP (inelastic mean free path): Peak width is equal to planck constant over the lifetime of the vacant hole left by the photoelectron. Peak width are larger for inner electrons and for larger atoms.\nIMFP: ESCA is surface analysis, but x-rays can penetrate deep into the sample. Photoelectrons liberated deep within the sample will not make it back to the surface, so ESCA has a limited effective depth..\nInelastic scattering tail: Inelastic electrons will have a higher binding energy.\nAuger electron emission peaks: Auger peaks can make it to spectrometer. Small compared to signal, and can be differentiated from the photoelectric peaks by switching x-ray sources.\nDegree of vacuum needed depends on sample being examined. Moderate vacuum is fine for most (\\( 10^{-6} \\) torr), but for reactive samples higher vacuum is needed.\nX-ray source: Monochromator limits x-rays projected onto the source to a limited frequency band. Do not want to expose sample directly to x-ray source to avoid noise from Bremsstrahlung electrons.\nElectron analyzer: hemispherical electron frequency analyzer splits electrons by energy. Positive and negatively charged hemisphere. The collection lens needs to collect as many electrons as possible to increase signal and protect sample from degradation.\nRaman Microscopy # Need to look this up on my own\nProfilometry # Method of 3D imaging. Fourier transform profilometry (FTP) works by projecting a uniform fringe pattern over an object and then using an image acquisition sensor to record the deformed fringe pattern. The shape of the object can be found by calculating the Fourier transform, filter the spatial frequency, and then calculate the inverse Fourier transform. The height of the object is given by the phase modulation.\nQuasi-sine projection and pi-phase shifting technique: Take two pictures of the fringe deformation. Between shots move the grating half a period. This changes the intensity distribution on the object and this method increases the surface slope that can be measured by threefold.\nTwo dimension fourier transform profilometry: This method helps measure the general shape of a coarse object that has small randomly distributed structures over its surface. The frequency variations of the small structures are attenuated when they are very far away from the carrier frequency of the light. Using a phase and phase unwrapping algorithms will return a 2D continuous function.\nTime-delay and integration cameras: TDI cameras are used in FTP to take a 360 degree image of the object and uses a single strip of structured light to find the height distribution on the objects surface.\nReflectivity should not affect the phase modulation, so you get an accurate measurement even if you don\u0026rsquo;t have a sample of uniform reflectivity.\nThis is used to determine the surface profile of the cornea when designing contact lenses.\nQuestions:\nImportance of spectral range of x-ray source? Source must be as monochromatic as possible. Quartz focusing crystal is used to achieve. Is the intensity of the transition peak proportional to the abundance of each species? Answer: yes, the cross-section for the absorption is nearly identical for each of the species and the x-ray source frequency is chosen to avoid resonances. The relative abundance of each atomic species appears readily in the intensity ratio (but only for well-calibrated detector!). Since they make use of effects in bound electrons, when are XPS/Raman microscopy applicable to plasma experiments? Answer: Ex situ (off-site) measurements are used to investigate samples after the experiment. Examples like titanium oxidized by an O plasma, divertor components, etc. Infrared Imaging and Two Color Pyrometry - Brett Biggs # Motivations for optical methods: Many of the methods of plasma diagnostics provide precise data on the composition of the plasma at the cost of directly influencing the state of the plasma\nOften sensors cannot withstand exposure to the extreme temperature Noise can be introduced to the system that is difficult to filter - need to isolate sensors electrically or extra post-processing Many sensors are required to adequately determine the temperature profile to a sufficient resolution (often interfering with the data you\u0026rsquo;re trying to collect) - perturbative not great Optical method features:\nNon-perturbing methods allow various plasma parameters to be directly measured without influencing the plasma. IR imaging and two color pyroscopy allow for measurement of the plasma facing components Qualitative observations that can predict failure modes Can be used to diagnose failures in fluid systems - leakage can be readily detected via thermal imaging. Correlation in thermal models to anchor structural models - helpful for determining stresses from CTE, material limits, fatigue, etc..\nWhy do we care about the temperature in EP devices (such as Hall thrusters)? We care about the life-limiting factors\ncathode insert life is generally a function of temperature the anode is greatly impacted by erosion that will degrade thruster performance over time (and cause significant mission impact) magnetization will drop as the temperature of the magnet coils increase. Hall thrusters rely upon this magnetic field, will see a significant decrease in efficiency as the magnet coil temperatures rise IR imaging theory # What are we measuring? All objects at a temp greater than absolute zero emits photons in the IR range that can be detected.\nParticle motion in warm objects result in charge acceleration or dipole oscillation which produce EM radiation In general, the IR emissions from 3 sources will be detected produced by the object due to its temp - some is lost by finite medium transmissibility reflected by the target, sourced by other objects sourced from the medium between the target and camera - vacuum system helps Camera composition:\nLens - focuses infrared light from target Detector - converts to electric impulse Processing electronics User interface Common detectors\nShort wavelength (SWIR): 0.9-1.7 \\( \\mu m \\) InGaAs Medium wavelength Indium Antimonide Long wavelength SLS HgCdTe Microbolometer In the study of plasma carrying devices, it is best to use long wavelength detectors for signal clarity - minimize interference between the PFCs and plasma\nCooled Detectors # Utilize single photon counting detectors to create the requisite signal for processing. Higher sensitivity than un-cooled detectors Cryogenic levels (-200 deg C) must be maintained to avoid blinding the sensor Self irradiance from unit can blind the sensors, making measurements useless Costly to procure and maintain Often significant downtime for maintenance Relatively low noise equivalent differential temperature (NEDT) Lower NEDT corresponds to improved resolution Can often resolve 10 mK Operating Principles Based on quantum principles. Photoelectric effect raises electrons from valence band to conduction band This changes the conductivity of the material The resulting photocurrent is proportional to the intensity It takes a minimum amount of energy for the photons impacting the detector to elevate the electron from the valence band Peak operating temperature (for camera) higher for shorter wavelength cameras Un-cooled detectors # Photon \u0026ldquo;masses\u0026rdquo; required to create signal for imaging electronics Impingement upon resistor laid over large silicon element creates required signal Relies upon bulk change (i.e. temperature) for signal generation Higher NEDT than cooled cameras - less sensitive, 30-200mK resolution Often provide enough insight into plasma systems to be more useful than cooled systems Theory # IR imaging is based off classical equations of heat transfer and photo detection. Materials with non-zero emissivity produce infrared radiation that can be detected by an IR camera. Takes advantage of radiative exchange\n\\[M_{bb} = \\epsilon(T) \\sigma T^4\\] Need to define an emissivity to define the emission from a real target. Not constant for any material. In real applications, the rise in temp of the PFCs will alter the emissivity. In applications like EP thrusters, the shift can be measured and is usually small.\nThe Stefan-Boltzmann equation makes a crucial assumption - that the emissivity over the whole spectral domain (\\( \\lambda \\)) is taken into account. In IR cameras, only a short section of the spectral domain is detected, limiting the effectiveness of the Stefan Boltzmann equation. Instead, the classical definition of the emissivity must be used \\[\\epsilon(\\lambda, T) = \\frac{M_\\lambda(\\lambda, T)}{M_T(\\lambda, T)}\\] If we examine the surface of interest in order to determine the temperature, the radiation flux from a surface can be defined as\n\\[\\dd Q = L \\cos \\theta \\dd S \\dd \\Omega\\] If we make assumption that surface is Lambertian, radiance is identical in all spatial direction, the photonic flux density is just \\( \\sin ^2 \\theta \\Omega \\). If we integrate the equations and alter the radiative power equation for a non-blackbody, we find\n\\[Q = M_\\lambda (T) s = \\overline{\\epsilon} \\sigma T^4 S\\] While true that this is a simplified case, it\u0026rsquo;s not uncommon in aerospace grade materials. For example, ceramics found in Hall thrusters often are near Lambertian, and can utilized these assumptions.\nOver a short range of wavelengths, we can integrate the assumed blackbody photon flux over the spectral domain of interest and multiply by the average emissivity to generate a true photon flux for our measurements\n\\[M_{\\Delta \\lambda} (T) = \\overline{\\epsilon}(T) \\int _{\\Delta \\lambda} M_\\lambda ^{bb} (\\lambda, T) \\dd \\lambda\\] In the case of average emissivity being independent of wavelength, we have what\u0026rsquo;s called a gray body and we can simplify the calculations.\nIR imaging assumptions # Assume emissivity is known Assume a Lambertian material is being used (simplifying assumption) Non-Lambertian materials don\u0026rsquo;t prevent measurements from being made, but researcher needs to be wary of curvature Imaging device uses wavelength filter to only take in a certain band of spectral range Limitations # Emissivity changes with temperature Can be mitigated by thorough understanding of material Emissivity can change with erosion Viewing window must be designed to not block IR radiaiton normal glass not compatible, materials such as sapphire glass must be used what other problems will we have even with \u0026ldquo;compatible\u0026rdquo; mediums? finite transmission coefficient solution: fiber optic to bypass viewing windows Measurement is only as good as your view poor experiment design may prevent critical measurements Limited ability to detect wavelengths (single input signal) Emissivity can change dramatically with angle of emission - limits measurements of 3D extended bodies where we do not have a normal view of the surface. Really want to be looking at flat features for qualitative assessment. Depending on material properties, increasing angle of incidence may increase or decrease emissivity. Emissivity of materials is highly wavelength dependent - depending on range you\u0026rsquo;re looking at Two Color Pyrometry # Need to know the emissivity of the PFCs limit the application of IR imaging. In general, the emissivity of the PFCs will change over time. To overcome limitation, principle is similar to that of single wavelength IR imaging. Two separate signals are ratio\u0026rsquo;d together.\nPlanck\u0026rsquo;s law describes the relationship between radiative intensity and wavelength/temperature. Gives \\( u(\\lambda, T) \\). Simplifying Planck\u0026rsquo;s law allows us to simplify the model. Assume \\( \\exp(- C_2/T \\lambda) \\gg 1 \\). In the wavelengths and temps we are interested in, there is generally less than 1 degree difference in model using this simplification.\nTwo color pyrometry allows us to understand absolute temperature without complete knowledge of the emissivity through all space and time\n\\[R_{12} = \\sigma_{12} \\frac{\\epsilon_1 (T)}{\\epsilon_2(T)} \\left( \\frac{\\lambda_2}{\\lambda_1} \\right) ^2 \\exp \\left( \\frac{C_2}{T} \\left( \\frac{1}{\\lambda_2} - \\frac{1}{\\lambda_1} \\right) \\right)\\] where \\( \\sigma_{12} \\) is a constant associated with assuming IR emission assuming black body source. It is assumed that the materials will act as a gray-body\nEmissivity is not a function of wavelength Deviations from this assumption will increase error Inverting gives a ratio temperature \\( T_r \\)\n\\[T_r = \\left( \\frac{ \\ln | R_{12} | - \\ln \\left| \\sigma_{12} \\left( \\frac{\\lambda_2}{\\lambda_1} \\right) ^5 \\right|}{C_2 \\left( \\frac{1}{\\lambda_2} - \\frac{1}{\\lambda_1} \\right)} \\right)^{-1}\\] What this indicates is that a temperature can be extrapolated solely from the irradiance of the material being targeted while analyzed to the wavelengths of interest, independent of emissivity. Based on the ratio temperature, some might be wondering how accurate the ratio temperature is. Luckily, it\u0026rsquo;s simple to extrapolate:\n\\[T = \\left( \\frac{1}{T_R} - \\frac{\\ln \\left| \\frac{\\epsilon_1}{\\epsilon_2} \\right|}{C_2 \\left( \\frac{1}{\\lambda_2} - \\frac{1}{\\lambda_1} \\right)} \\right) ^{-1}\\] and the temperature error is\n\\[\\Delta T = \\left| \\frac{ \\ln \\frac{\\epsilon_1}{\\epsilon_2}}{C_2 \\left( \\frac{1}{\\lambda_2}- \\frac{1}{\\lambda_1}\\right)} \\right|\\] Error will stem from the difference in wavelengths being measured. Enter multi-color pyrometry. By taking up to 4 colors allows for higher accuracy measurements to be taken. Higher levels of complexity to resolve additional input signals\nleast squares methods common in determining temperature Error varies by the precise method being used Diminishing returns as the number of wavelengths being analyzed increases, 2 color is common, most don\u0026rsquo;t go past 4 color Questions:\nEmissivity measurement error bars are pretty wide (10 percent). What limits this measurement? Answer: Usual way you measure this is by putting your material in an oven and measuring emission with thermocouples, which can be difficult for an entire setup. Additionally, emissivity can change dramatically with angle of incidence, wavelength, etc. So a better way to reduce this error is by computing emissivity ratios by measuring multiple wavelengths (multi-color pyrometry). Coded Aperture Imaging - Matt Russell # Technique for imaging high energy x-rays and gamma rays.\nMotivated by the lack of a suitable image forming device for x-ray astronomy. By the 1960\u0026rsquo;s, a pinhole camera was the device by which x-rays were imaged. The pinhole camera projects a 3D image into a 2D plane, with angular resolution determined by the length ratios between the image and source. For a fixed source, this leads to a trade-off between good angular resolution and signal to noise ratio.\nAbles and Dicke addressed the problem by introducing a large number of pinholes. The field of view can remain constant while increasing the SNR by having a larger open aperture area - scatter-hole camera.\nLinear imaging system # Linear imaging systems - to first order, let\u0026rsquo;s consider the imaging system as a linear function from the sensor plane to the detector plane.\n\\( E(x, y) = S[I(x, y)] \\)\nif the emitting system can be decomposed into a series of point sources\n\\[I(x_0, y_0) = \\iint_{-\\infty}^{\\infty} I(x, y) \\delta( x - x_0) \\delta (y - y_0) \\dd x \\dd y\\] \\[ E_0 (x, y) = S\\left[ \\iint _{- \\infty} ^{\\infty} I(x, y) \\delta (x - x_0) \\delta (y - y_0) \\dd x \\dd y \\right]\\] Rather than measuring the complete linear response, we define an instrument response function (IRF) which measures how the instrument independently responds to each point. We make an assumption that the instrument does indeed respond independently at each point. We also make an isoplantic assumption, assuming that the mapping does not vary over space. This turns the instrument response function into a point source function.\nNoise in the system # Vignetting Blurring Distortions Diffraction Saturation Add these into a background noise term \\( N \\)\n\\[E = I \\cdot PSF \u0026#43; N\\] \\[E(x, y) = \\frac{1}{M^2} \\iint _{- \\infty} ^\\infty I (\\frac{x}{M} , \\frac{y}{M}) PSF(x - x_0, y-y_0) \\dd x_0 \\dd y_0\\] So, what now? We can remove the correlation between the imaging system and the point source function by taking a Fourier transform. But doing so leads to the Fourier transform of the PSF in the denominator, which contains very small terms that increase the noise. Instead, we can introduce a new re-construction\n\\[\\hat {O} = P \\cdot G \\\\ = RO \\cdot (A \\cdot G) \u0026#43; N \\cdot G\\] If we can construct a post-processing correlation function \\( G \\) such that the correlation between \\( A \\) and \\( G \\) is approximately a delta function, then we can remove the correlation without major introduction of noise.\n\\[A \\cdot G \\approx \\delta\\] The simplest answer is \\( G = A \\). This results in autocorrelation and a postprocessing array that is identical to the aperture. This is simple, but there are some problems\nNonflat sidelobes High-frequency fluctuations Lack of signal-to-noise improvement For a coded aperture, we can have a very large (tens of thousands) of pinholes. This means an exact theory for the response of the system would require the response of tens of thousands of pinholes. Mechanically impossible, so use statistics instead.\nQuestions:\nHow far apart are the spacings in the coded aperture in real devices? I assume they have to be much larger than the diffraction range for x-rays (tens of nm). Answer: The imaging grids are usually on the order of a few centimeters, so for a MURA grid with tens of thousands of apertures you\u0026rsquo;d get grid spacing in the 10-100 micrometer range. Laser-Induced Fluorescence and TALIF (Two-Photon Absorption) - Aqil Khairi # Motivation # Many passive diagnostic methods measure the spectroscopy of plasma self-emission. Now, we induce the plasma excitations with a laser. Bound electrons undergo similar excitation and decay to lower states, and emit radiation. The detected radiation is measured and analysed to obtain certain characteristics of the plasma. The technique is known as Laser-Induced Fluorescence (LIF).\nThis can offer a very localized measurement through the crossing of the exciting and viewing beams. Species-specific diagnostic; one can tune the laser energy for the desired transition. LIF has far reaching applications; tumor diagnosis, detection of biomolecules, palentology, and density and velocity measurements of neutral hydrogen in fusion experiments. Neutrals and impurities concentrate in the plasma edge, which is a region of interest especially with regards to PMI at the wall.\nPhysical Theory - LIF # Decay from the excited state is on the order of 10ns, so LIF signal depends on the population of excited species. Considered non-perturbing due to short excitation lifetime.\nCan stay within this regime by pulsing the laser quickly or maintaining low intensities.\nRelationship becomes nonlinear when intensity is so high that ground state ion population is depleted.\nFluorescence from ground state is preferred due to its greater population compared to existing excited states. This helps to avoid saturation.\nPhysical Theory - TALIF # Transmitting UV radiation through windows can be problematic. Fortunately, we can use photons of lower energy and combine them to reach transition energy. Because the fluorescence energy is lower than the excitation, it can be transmitted back through the windows.\nIn contrast to LIF, the signal is proportional to the square of the laser intensity. This increases the localization of the measurement at the focal point of the beam. Emission as a function of laser wavelength can be used to determine temperature, neutral density, and flow.\nCounter-propagation of beams can eliminate Doppler effects. A photon is supplied by each beam, and their Doppler shifts along the beam are equal and opposite.\nPhysical Theory - Broadening Effects # Familiar techniques are used to extract temperatures from the observed spectra. Random thermal motion of the particles imparts Doppler shifts to the signal, resulting in Doppler broadening of the line radiation. Before calculating Doppler broadening, you need to take into account a few other effects\nSaturation broadening occurs when the number of excitable particles in the emission volume is exceeded. Not an issue for hydrogen due to rapid decay, but more so in heavier calibration species. One way to expand the linear region is to increase the volume of interest, trade-off in localization.\nIsotopic effects: Energy of atomic states is proportional to the reduced mass (nucleus and electron mass) resulting in a spectral shift. For small atoms, the shift is large and clearly distinguishes isotopes. For higher-mass species, the shift due to isotopic effects is small, but can contribute some artificial broadening.\nStark broadening: Usually negligible for weak or short-lived electric fields.\nZeeman splitting: in regions of high magnetic field\nLaser linewidth: can also broaden the signal if it is too large, so the laser system is monitored.\nPhysical Theory - Signal Dependency # For TALIF, relationship between emission signal and laser intensity\n\\[S(\\lambda) = \\frac{\\Delta \\Omega}{4 \\pi} n(v) I^2 \\sigma \\alpha G\\] where\n\\( \\Delta \\Omega \\) is solid angle of emission collection \\( n(v) \\) is velocity space density \\( \\sigma \\) is absorption cross section for transition \\( \\alpha \\) is branching ratio and spectral efficiency of detector \\( G \\) is PMT gain The wavelength of the laser is scanned to determine the spectral width of the emission.\nOmitting the constants and putting in terms of linewidths,\n\\[S(\\nu) \\sim \\frac{I^2 n_0}{\\sqrt{\\Delta \\nu _d ^2 \u0026#43; 2 \\Delta \\nu_l ^2}}\\] Method # Development of high energy, narrow spectrum lasers has allowed evolution of LIF to TALIF.\nDye lasers utilize an organic liquid lasing medium that allows for a wider range of possible wavelengths. Highly tunable, frequency doubling or tripling is possible. Pump laser: Provides input energy to the tunable laser. Narrow spectrum to match absorption of lasing medium Diode lasers also useful: ubiquitous, can be pumped with current. Use known pressure of a neutral gas for absolute calibration, e.g. krypton. Krypton is a particularly good choice for calibration because it has a transition very similar in energy to hydrogen and deuterium\nApplications # HIT-SI3 Experiment # TALIF used for absolute density and temperature measurements of deuterium neutrals Frequency tripled dye laser and frequency doubled Nd:YAG laser Confocal configuration: Location of measurement controlled by moving focusing lens PMT collects emission through a band-pass filter. Time resolution achieved through PMT gating. Temp, density by fitting measured intensity as laser frequency is scanned. Measuring a shift in the centroid of the broadened distribution indicates bulk flow. MadHex Helicon Plasma Facility # LIF setup to diagnose a helicon plasma at Wisconsin using tunable diode laser Ion energy distribution function obtained along laser path Doppler shift and broadening gives ion velocity and temperature. Experimental setup showing alignment can look like this:\nITER Divertor # Planned LIF configuration for helium density Characterization of helium ash removal Use Nd:YAG optically pumped dye laser. Questions:\nYou mentioned TALIF measurements of neutral hydrogen in HIT-SI3. Do these LIF measurements have an upper temperature limit once hydrogen is completely ionized, limiting us to early start-up times, or is there still enough neutral hydrogen to fluoresce at operating temperature? Answer: There are still neutrals at the outer edge, but the density decreases as you go deeper into the plasma. In HIT-SI3, they are particularly interested in re-use of hydrogen by liberating neutrals from the wall. In that experiment the focusing lens targets an area very close to the wall in order use TALIF to monitor neutral density/temperature near the wall. VISAR - Sungyoung Ha # Fundamentals of VISAR # VISAR - Velocity Interferometer System for Any Reflector. It is a laser interferometer system for measuring the velocity of a fast-moving surface. Typically used to measure shock propagation or speed of projectiles.\nBackground of VISAR # In a Michelson interferometer setup, light from a monochromatic source is split by a beam splitter and travels through two path lengths. If a mirror is moved a distance \\( d \\) and \\( m \\) fringes appear/disappear at the center, then \\( \\lambda = 2 d / m \\). How can we use this to measure the speed of a surface? Use the material surface as a mirror for the Michelson interferometer, and count the number of fringes that pass a fixed point.\nRequires a good surface finish for coherent reflection Low range of speed measurement (up to a few \\( mm / \\mu s \\)) Another option is to reflect a laser source off the moving surface, causing doppler shifting depending on the velocity. Measuring the doppler shift (knowing the path initial difference) can deduce the velocity. Still requires a very good surface finish, diffusive light will lead to a large intensity difference between legs of the interferometer.\nLimitations of Michelson interferometer # In practice, the input light is not perfectly collimated and is distributed over an angular range. As mirror separation is increased, the spot size (radius from the center to the first minima) decreases. This means that the detector needs to be smaller to properly detect fringes, but this reduces signal strength. Overcome this by a WAMI (Wide Angle Michelson Interferometer) setup. WAMI # Insert an etalon in one leg of a Michelson interferometer, allows the formation of a virtual mirror closer than the actual mirror. By tuning the apparent position to match the mirror position in the other leg, the detector can see the same image while having a path length difference. The fringe count depends on the amount of the doppler shifted wavelength.\nBuilding on the strengths of WAMI, the VISAR adds:\nBIM (Beam Intensity Monitor) - normalizes the intensity variation of the input (reflected light) WP (Wave Plate) - Shifts the phase ofo one polarization by 1/8 wavelengths (total of 1/4 wavelength) PBS (Polarizing Beamsplitter) - Splits the output beam Strengths of VISAR # Overcome intensity changes that affect resolution through both design and BIM Does not require the surface to be specially treated for reflectivity. Good sensitivity and range Having two polarized signals with 90 degree phase difference prevents sensitivity loss near the peaks, and allows distinction between positive and negative movement. Push-Pull VISAR # A push-pull VISAR setup adds a second detector, utilizing the light lost through the beam splitter.\nStronger signal strength allows better noise performance, especially in the presence of weak or largely incoherent light. Requires additional calibration and balancing. Mach-Zehnder Interferometer version # Exactly the same as Michelson VISAR, but instead of reflecting through etalon in one leg, the etalon is just inserted into one leg of the Mach-Zehnder interferometer.\nInstead of polarizing components, a streak camera is used to obtain a spatial and temporal fringe pattern.\nComponents of VISAR # Etalon # Fabry-Perot interferometer Two partially reflective mirrors separated by a gap. Typically, if the gap is adjustable, it is called an interferometer, and if it is fixed it is called an etalon Often used to create interference patterns, filter through certain wavelengths, etc. Beam Splitter # Split beams through half-silvered mirrors of equivalent coating Polarizing beam splitters can use birefringent materials, elongated nanoparticles, doped and stretched polymer sheets, Fresnel reflection, or wire gratings to achieve polarization. Waveplate # Birefringent (n depending on polarization) material delays a certain polarization state, shifting the phase. Typically made of quartz. Shift depends on material, wavelength, thickness, polarization. Calibrate based on the wavelength of laser you are using. In VISAR setup this can be an issue, since the source is doppler-shifted.\nData interpretation # Returning to the conventional VISAR setup with two detectors split by 90 degree phase, the normalized components can be expressed as\n\\[D_x = \\frac{D_1}{D_{BIM}} = x_0 \u0026#43; A_x (t ) \\cos \\phi (t) \\\\ D_y = \\frac{D_2}{D_{BIM}} = y_0 \u0026#43; A_y (t ) \\sin (\\phi (t) - \\epsilon)\\] Because the two polarization states can have different coupling constants throughout the interferometer, we have \\( x_0 \\) and \\( y_0 \\). \\( \\epsilon \\) is the phase shift error, or the error from not having an ideal 90 degree phase shift between the two polarization components, including the doppler shift and other various calibration issues.\nFor \\( \\phi \\) the phase associated with a fringe, we can express it in an elliptic form:\n\\[\\tan \\phi(t) = \\tan \\epsilon \u0026#43; \\frac{ D_y (t) - y_0 }{D_x(t) - x_0 A_y(t)} \\frac{A_x(t)}{A_y(t)} \\sec \\epsilon\\] Because we don\u0026rsquo;t know all of these parameters, but only have a vague understanding of the possible range, we need to perform a fitting of these factors before we can determine velocity data from phase data (fit the ellipses). Usually use iterative numerical schemes to converge to a fitting.\nPhase to Velocity # Once we have the ellipse fitting to interpret the phase data, we need to convert it into velocity data.\nFringe count is just the phase diff divided by \\( 2 \\pi \\) (number of fringes).\n\\[F(t) = \\frac{\\phi(t) - \\phi(t_0)}{2 \\pi}\\] Use a VISAR approximation: that the timescale of the event is larger than the delay. Then the approximation can be used to relate the phase and velocity\n\\[v(t) \\approx v_0 \u0026#43; \\frac{\\lambda_0}{2 (1 \u0026#43; \\delta) \\tau_0} F(t) \u0026#43; O(\\tau_0 ^2)\\] Corrections and Errors # Window corrections (including shocked windows). Typically the surface must be observed through a window. Dispersive media (etalon, beamsplitters, etc) Fringe uncertainty and ambiguity VISAR approximation Diffusive reflections Multiphase interference Applications # Most applications involve high-energy studies.\nPressure and shock response of materials (Hugoniot) Equation of state experiments Measure projectile velocity ICF: Lasers impinging against the wall of a hohlraum generate shockwaves that compress the deuterium fuel capsule. Questions:\nWhat wavelength range is typically used for VISAR? It would seem that there is a trade-off between the velocity resolution (wants longer wavelength) and error correction (wants shorter wavelength). Answer: VISAR setups usually use lasers like Nd:YAG in the visible range. They are readily available and have good signal/noise characteristics for VISAR. Pulsed Polarimetry - Arvindh Sharma # What is it # Pulsed polarimetry is\nA LiDAR like approach to measure the magnetic field and electron density simultaneously. A non-perturbative diagnostic best suited for plasmas with high \\( n_e \\) Uses a combination of Thomson scattering and Faraday rotation: when light passes through a plasma, there is some rotation of the polarization LiDAR Thomson scattering + polarimetry (measuring the handedness of the polarization of laser light) = pulsed polarimetry Used for measuring the parallel component of the magnetic field in a plasma Principles of Operation # Three main steps in determining \\( n_e \\) and \\( B_\\parallel \\)\nOptical scattering of laser beam by electrons in the plasma (back-scattering). Provides position and number density of the scattering particles Rotation of the polarization azimuth of the probe beam due to \\( B_\\parallel \\). Measurement of time-of-flight and polarization azimuth of the returned laser pulse. The difference in polarization angles when comparing with the reference beam provides information about \\( B_\\parallel \\). Refresher on LiDAR (Light Detection and Ranging) # LiDAR uses laser pulses to detect positions of objects remotely It is an active system that supplies energy in the form of laser light The reflected light is collected, and the waveform provides useful information about the objects because reflections from more distant objects take longer to return. For example, in a plasma, the time difference between peaks would indicate length scales of density variations. The amplitude of the peaks also gives interesting information. A density profile can be constructed from the temporal differences in peaks. Refresher on polarization # Polarization describes the plane in which EM radiation oscillates. An EM wave is polarized when the plane of fluctuation of the electric field is well-defined (the plane of fluctuation is perpendicular to the direction of propagation) Many natural sources of light are unpolarized Laser light is also unpolarized: need polarizers to make it polarized. There are 3 types of polarization: Linear polarization: Electric field oscillates in one plane. Could be the vector sum of linearly polarized components with no phase difference. Circular polarization: Special case of elliptic polarization where the phase difference is \\( \\pi/2 \\) and the constituent amplitudes are equal. Comes in two flavors: right-handed and left-handed. Elliptic polarization: Produced by the vector sum of E field components that have different amplitudes and possess arbitrary phase difference. Generic case of polarization. Optical scattering in pulsed polarimetry # Laser pulses are directed through the plasma. Electrons produce backscatter radiation through Thomson scattering Thomson scattering is elastic absorption and radiation of EM waves by charged particles. The particles are affected by E and B fields through Lorentz force and oscillate in sync with radiation. The acceleration of the charged particle produces radiation at the same frequency as incoming EM wave. Emitted radiation travels in all directions In a range of solid angles around \\( \\vec k \\), the scattering preserves polarization LiDAR techniques along with Thomson scattering provide information \u0026ldquo;local\u0026rdquo; to the scattering region Measuring of \\( T_e \\), \\( n_e \\), and \\( \\langle v_e \\rangle \\) from Thomson scattering # Spectrally-resolved scattering intensity is proportional to the electron number density. Gaussian broadening provides information about the electron temperature and mean velocity. Faraday Effect # Superposition of left- and right-handed circular polarizations in phas is linearly-polarized light In the presence of a magnetic field along \\( \\vec k \\), the right-hand wave propagates faster than the left-hand wave, introducing a phase difference proportional to the index of refraction difference. The rotation is proportional to the product of \\( n_e B_\\parallel \\). Also proportional to \\( \\lambda ^2 \\). Putting it all together # The plasma backscattering location is given by \\( x = ct / 2 \\) where \\( t \\) is the time of flight. The total rotation of polarization azimuth will be twice \\( \\alpha(I) \\) since the light beam travels twice through the plasma: \\( \\alpha(I, T) = 2.63 \\cdot 10^{13} \\lambda ^2 \\int 0 ^l (n_e B\\parallel) [s, t(s)] \\dd s \\). Differentiating \\( \\alpha \\) with respect to path length \\( n_e B_\\parallel (l) = \\left.\\frac{1.9 \\cdot 10^{12}}{\\lambda ^2} \\pdv{\\alpha_{PP(s)}}{s} \\right|_l \\) In practice, \\( \\alpha_{PP} \\) is plotted as a time series at discrete intervals of \\( \\Delta T = 2 \\Delta L / c \\). Considerations - Laser Pulse Length # The spatial resolution depends on the laser pulse length and detection system time scales. If detector response time is negligible, then \\( \\Delta L \\) (spatial resolution possible) is \\[\\Delta L = c/2 \\sqrt{ [ \\tau_{det} ^2 \u0026#43; \\tau_{digitizer} ^2 \u0026#43; (L _{pulse} / c)^2]}\\] The scattering volume is given by \\( \\Delta V = \\pi r_{beam} ^2 \\Delta L \\) There is a potential trade-off here: To get good spatial resolution, we need \\( \\Delta L \\) to be as small as possible But a short laser pulse might not carry enough energy to induce detectable backscatter or affect SNR Lowering \\( \\Delta L \\) by lowering detector response time adversely affects SNR, since SNR scales roughly as inverse square root of detector band-width. Considerations - Laser Wavelength # Recall that Faraday effect has a square dependence on wavelength of rotation. The wavelength needs to be chosen to get detectable resolution of \\( \\alpha(l, T) \\) for smaller path lengths \\( l \\). At the same time, the total rotation through the entire plasma width must not exceed detection range. Considerations - Time Scales # Pulsed polarimetry measures the rotation azimuth on the way to the backscattering location, and on the way back to the detector after scattering For this to be true, the plasma conditions need to be \u0026ldquo;quasi-static\u0026rdquo; during the laser transit time: must be the same on the first pass as the second pass. In practice this is a very fast time scale and easily satisfied for most plasmas. Other Considerations # The Cotton Mouton effect can be significant in plasmas with strong perpendicular B component. The polarization acquires an ellipticity that increases with distance and is quadratic with the intensity of perpendicular B component. This effect is due to difference in refractive indices of O- and X-waves. Implementation # There are 4 major components:\nLaser source with a spatially narrow, polarized pulse at a chosen wavelength A polarization-preserving collection optic to collimate backscatter from remote plasma locations within a certain solid angle. A directional coupler to redirect laser pulse from source along target optic axis. Don\u0026rsquo;t want the laser to be looking directly into backscattered light. A polarimeter to detect polarization azimuth and intensity of collected light. Components: Polarizers # Polarizers are used to polarize light in preferred directions There are 3 main types: reflective, dichroic, and birefringent Reflective: Surface with a preferential electron direction of motion. Absorb light and reflect light with a well-defined polarization. wire-grid polarizers, Brewster angle polarizers Dichroic: Absorb specific polarization. Birefringent: Different polarizations have different refractive indices. Components: Polarimeter # Polarimeters measure the Faraday rotation and/or electron density In amplitude polarimetry, a beam splitter lets original polarization pass and reflects orthogonal component. The Faraday rotation induces the orthogonal component. The reflected beam is combined with a frequency-shifted reference beam. The amplitude of the heterodyne beat is proportional to the Faraday rotation. Applications (proposed) # FRX-L plasma Experiment at LANL Uncompressed FRC 50 GW Nd:YAG laser suggested \\( L_{pulse}/c = 20 ps \\) 10 pulses at 100mJ/pulse FRCHX plasma Experiment at Kirkland AFB device Compressed FRC 100 GW tripled Ti:sapphire laser \\( L_{pulse}/c = 1 ps \\) ITER Challenging because of low electron density and magnetic field A far infrared laser source needed Pulsed polarimetry could provide real-time control to prevent MHD instabilities In-use application: University of Washington Pulsed Polarimeter was modified to sense magnetic field using a streak camera and fiber optic PP (Smith and Weber, 2016).\nAdvantages # Measures local magnetic field using a remote, non-perturbative method Spatial resolution of millimeters on high energy density plasmas Can simultaneously measure \\( T_e \\), \\( n_e \\), and \\( B \\) along line of sight. Resilient to refractive effects and short measurement duration, so can be used to control instabilities. Summary # Pulsed polarimetry is a LiDAR-like technique that incorporates Thomson scattering with Faraday rotation It is proposed to be a peerless technique for remotely sensing magnetic field in plasmas With the right choice of optical components and energy sources, it can aid in real-time control of experiments to address instabilities. Technology is still nascent - lots of potential for future applications. Questions:\nHow are we able to get spatially-resolved information about the parallel magnetic field strength when the Faraday rotation is going to be line-integrated along the line of sight? A: The rotation is cumulative over the path of travel, so we get a cumulative distribution over the path. If we can compute the local \\( n_e \\) along the path, we can differentiate to recover the local magnetic field. LIDAR Incoherent Thomson Scattering - Josh Perry # Pulsed Thomson scattering is one of the essential elements of pulsed polarimetry. Used to determine temperature and density of electrons. There are limitations to other methods of getting these diagnostics:\nLangmuir probes: perturbative interferometry: line-integrated Spectroscopy of line radiation: indirect Thomson scattering is direct, weakly-perturbing, and localized\nOverview # Light from a laser source scatters off electrons in the plasma. Temperature is determined from the doppler broadening of scattered light. Density is determined from the absolute intensity of scattered light. Measurements are localized to the intersection of laser and viewing beams\nScattering Theory # For \\( h \\lambda \\ll m_e V_e \\), quantum effects can be ignored. This is the difference between Compton scattering (quantum) and Thomson scattering (classical). The electron oscillates in the wave fields, giving off radiation from its acceleration. When the Debye length is much greater than \\( \\lambda \\), incoherent scattering.\nDoppler shifting of scattered radiation is only in the \\( \\vec k \\) direction. At high temperatures (1keV), relativistic effects shift towards blue, making temperature seem higher.\nDensity determined from scattering cross-section\n\\[\\pdv{P_s}{l} = P_i n_e \\frac{8}{3} \\pi r_e ^2\\] The cross-section is small, and for high spatial resolution the scattering volume is small, so very high intensity laser required. Calibration performed using a neutral gas.\nNoise Sources # External sources: any stray light can be an important noise source, since the scattered intensity is low. Blocking sightlines, covering windows, and using viewing dump can help reduce external noise. Stray laser radiation: self-absorption of laser source. Narrow bandwidth makes it easy to identify in spectrum. Use beam dump to reduce noise. Line radiation from plasma. Avoid coincident lines by adjusting laser frequency if using tunable laser. Bremsstrahlung: most unavoidable noise source \\[\\frac{S}{N} = \\left[ \\frac{h c}{C 2 \\sin (\\theta / 2) \\sin \\theta} \\right]^{1/2} r_2 ^2 N_i \\left[ \\frac{\\omega_S L}{d D \\Delta t} \\right]^{1/2} \\frac{T_e ^{1/4}}{Z_{eff}^{1/2}} Q^{1/2}\\] For polarized laser, polarimetry can be used to reduce noise.\nTypes of Thomson Scattering Setup # Perpendicular Thomson Scattering # Simplest and most common TS configuration. Viewing and laser beams are orthogonal, scattering volume limited to a relatively small region TV Thomson Scattering # Use a group of lenses to gather scattered light from multiple locations along one or more laser beams. Forms a 2D image at the spectrometer detector. High complexity due to many optical paths.\nLIDAR Configuration # Laser and detector optics are located on same port, measuring backscattered radiation. The time of flight depends on the plasma depth, allowing for spatial resolution along flight path. Requires extremely short laser pulse. LIDAR systems used in large tokamak experiments (ITER, JET).\nQuestions:\nDo you know what limits the repeatability of LIDAR TS? 0.5 Hz doesn\u0026rsquo;t give you very many data points in a single shot. In very high-intensity lasers, what tends to limit the repeatability is the process of the lasing medium de-exciting back to the ground state. This takes much longer than any other process in the collection setup. Coherent Thomson Scattering - Simon Fraser # When Thomson scattering occurs in the limit that \\( k \\lambda_D \\ll 1 \\), collective effects become dominant and we all the phenomenon coherent Thomson scattering (CTS). CTS diagnostic is a non-perturbative measurement that provides information on the electron density, electron temperature, ion temperature, and flow velocity. CTS has very good spatial and temporal resolution and has been used in systems ranging from Hall thrusters to fusion confinement experiments.\nWhat is Thomson scattering? # Thomson scattering is elastic scattering of EM radiation by a free charged particle. It is also the low energy limit of Compton scattering. Ignores relativistic effects, applicable for \\( T_e \u0026lt; 1 keV \\)\nThe process goes like this\nEM radiation approaches a free particle The particle moves in the electric and magnetic fields of the EM wave The movement of the free particle creates an EM wave Scattered in all directions; direction of interest determined by line of observation What is coherent (collective) Thomson scattering? # Scattering calculations in a plasma involve electric field contributions from many electrons. In incoherent Thomson scattering, the collective effects of the plasma can be ignored and all contributions are assumed to be uncorrelated. Coherent Thomson scattering is the case where the contributions from separate particles are correlated. Recall that electrons shield each other on the order of the Debye length. If \\( k \\lambda_D \\gg 1 \\) then the phase difference between the scattering resulting from an electron or its shielding cloud is large. The scattered radiation is then incoherent. If \\( k \\lambda_D \\ll 1 \\) then the phase difference is small, and the scattering off of an electron is balanced by the absence of scattering from its shielding cloud. Scattering from ions is negligible (factor of \\( m_e/m_i \\)). Recall that \\( k \\lambda_D \\ll 1 \\) is equivalent to \\( 2 \\pi \\lambda_D \\ll \\lambda \\). Sometimes given in terms of a scattering parameter \\( \\alpha = 1/k \\lambda_D \\). Use as a plasma diagnostic # Scattered field from a single electron \\[E_s(\\vec x, t) = \\frac{r_e}{R} \\vec \\Pi \\cdot \\vec E_i\\] with classical electron radius \\( r_e \\), tensor polarization operator \\( \\vec \\Pi \\), and distance to observer \\( R = x - \\vu s \\cdot \\vec r \\) where \\( x \\) is position of observation. From a population of electrons,\n\\[E_s(\\nu _s) = \\frac{r_e e^{i \\vec k _s \\cdot \\vec x}}{x} \\iint F_e \u0026#39; \\kappa \u0026#39; \\vec \\Pi \u0026#39; \\cdot \\vec E_i e^{i (\\omega _s t\u0026#39; - k_s \\vec r \u0026#39;)} \\dd t\u0026#39; \\dd r\u0026#39; \\dd \\nu \u0026#39;\\] with Klimontovich point distribution function \\( F_e \\). For monochromatic incident light, the total scattered power is\n\\[P_S = \\frac{P_0 r_0 ^2 L n_e \\dd \\Omega}{2 \\pi} \\left| \\vu s \\cross ( \\vu s \\cross \\vu E_i) \\right|^2 \\int \\dd \\omega_s S(k_\\alpha, \\omega)\\] with \\( r_0 = e^2 / m_e c^2 \\) and \\( |\\vu s \\cross (\\vu s \\cross \\vu E_i) = 1 - \\sin ^2 \\theta \\cos ^2 \\beta \\) where \\( \\theta \\) is detection angle and \\( \\beta \\) is the angle of the incident polarization vector relative to the scattering plane and \\( L \\) is the length of the Thomson scattering volume. Notably, incident polarization perpendicular to the scattering plane gives the strongest signal.\nCollective ion-acoustic features can be observed, as long as \\( Z T_e / T_i \u0026gt; k_a ^2 \\lambda_D ^2 \\). This means that the electrons follow the motion of the ions. In this regime the scattering occurs from co- and counterpropagating ion-acoustic waves.\nThe wavelength separation between the ion-acoustic features is\n\\[\\Delta \\lambda = \\lambda_0 \\frac{4}{c} \\sin \\frac{\\theta}{2} \\sqrt{ \\frac{k_B T_e}{M} \\left( \\frac{Z}{1 \u0026#43; k^2 \\lambda_D ^2} \u0026#43; \\frac{3 T_i}{T_e} \\right)}\\] \\( \\Delta \\lambda / \\lambda_0 \\) is often very small.\nDoppler shift # The Doppler shift of the scattered radiation contains information about the bulk velocity.\n\\[\\Delta \\lambda = \\vec k \\cdot \\vec v \\lambda_0 ^2 2 \\pi c\\] Note that charges parallel to the incident light get the maximum shift.\nDiagnostic calculations # Temperature of electrons and ions can be derived by fitting the observed spectrum to the appropriate form factor.\nElectron density can be determined through the total scattered power, the frequency of the electron plasma wave, or two-color Thomson scattering. The two-color approach relies on the difference between the ion acoustic frequency at large and small k-vector values. The small k-vector fluctuations provide electron temperature while large k-vector fluctuations provide the density. This can be done with a single laser and different scattering angles.\nNote that \\( k \\) increases with both frequency and angle of incidence.\nDetection angle \\( \\theta \\) considerations:\n90 degress is convenient: reduces stray light and gives best measurement localization greater than 90 degrees is backscattering, less than 90 degrees is forward scattering Detection angle affects whether the scattering observed is coherent or incoherent; behavior is more collective at small \\( \\theta \\). Laser considerations:\nLaser frequency must be greater than the plasma frequency to avoid heating Laser pulse energy should also be tuned to avoid heating Scattering signal is most intense for a laser polarized perpendicular to the plane of the laser beam and observation direction Shorter pulses increase SNR and temporal resolution Wave resonances need to be avoided to avoid heating Sources of error\nPlasma self-emission Stray light Rayleigh scattering background Broadening effects A more powerful laser increases signal-to-noise ratio Use in experiments # Diagnosing Hall Thruster instabilities (PRAXIS)\nPropulsion analysis experiment via infrared scattering Uses 10.6 \\( \\mu m \\) \\( CO_2 \\) laser split into two beams, primary and local oscillator Local oscillator is a reference beam to get phase and amplitude information Local oscillator gets mixed with scattering signal Very highly localized measurement determined by interference between the two beams ASDEX tokamak\nUses CTS to measure ion velocity, ion temp, and ion composition Here, magnetic vector perpendicular to the wave vector, so can observe the effects of ion Bernstein wave Uses 2ms pulses Questions:\nYou mentioned experiments using both continuous and pulsed laser systems. Did you come across any configurations that used LiDAR-like ranging schemes for spatial resolution? Answer: Did not come across any configurations that used ranging schemes to spatially resolve. The time scale over which ion-acoustic affects occur may be too long compared to the time of flight to obtain meaningfully resolved ion temperature/density measurements. "},{"id":81,"href":"/r/notes/griffiths/ch9-1/","title":"Electromagnetic Waves in One Dimension","section":"Griffiths Introduction to Electrodynamics","content":" 9.1: Electromagnetic Waves in One Dimension # 9.1.1 THE WAVE EQUATION # Before we start writing down how electromagnetic energy propagates in space as radiation, we\u0026rsquo;ll dust off our mathematical background on waves. Thanks to Fourier analysis, we know that sinusoidal waves form a complete basis for all periodic functions, so we restrict our investigation to sinusoidal variations.\nSuppose some quantity \u0026ldquo;\\( f \\)\u0026rdquo; is propagating with speed \\( v \\) in the \\( +z \\) direction, and we focus on sinusoidal \u0026ldquo;disturbances.\u0026rdquo; At \\( t = 0 \\),\n\\[f = f_m \\cos k z \\qquad k = \\frac{2 \\pi}{\\lambda}\\] And at any time \\( t = t \\)\n\\[f = f_m \\cos [ k ( z - vt) ] \\] \\[= a_m \\cos ( kz - \\omega t) \\qquad \\omega \\equiv v k\\] And if we look at a particular position \\( z = 0 \\) (or other) and watch \\( f \\) as a function of time, we\u0026rsquo;ll see the same sinusoidal behavior\n\\[f(z = 0, t) = f_m ( \\omega t)\\] Generalizing to the 3-D case, instead of a wavenumber we have a wave vector \\( \\vec k = k_x \\vu x + k_y \\vu y + k_z \\vu z \\)\n\\[f = f_m \\cos (\\vec k \\cdot \\vec r - \\omega t \u0026#43; \\delta)\\] Any wave that looks like this satisfies the wave equation\n\\[\\frac{\\partial ^2 a}{\\partial z^2} = \\frac{1}{v} \\frac{\\partial ^2 a}{\\partial t^2} \\qquad v = \\frac{\\omega}{k} \\quad \\text{(in 1-D)}\\] "},{"id":82,"href":"/r/notes/UWAA560/91-zeeman-spectroscopy/","title":"My class lecture","section":"Plasma Diagnostics","content":" \\[\\] Zeeman Spectroscopy # Motivation # In high-energy-density plasmas, the current density distribution is strongly tied to the magnetic field distribution, and in some regions measurement of the B-field is the only way to obtain current density measurements. The magnetic field also determines resistivity, plasma dynamics, and energy balance.\nThe diagnostic methods to measure localized magnetic fields in plasma make use of the Zeeman effect, the Stark effect, Faraday rotation, or Faraday\u0026rsquo;s law. Each method has benefits/drawbacks that need to be taken into account. Some provide chord-integrated measurements, some spatially resolved. Physical probes can perturb the plasma you are trying to measure, or be destroyed in a plasma-facing environment. Zeeman spectroscopy is a non-perturbative, passive diagnostic. The extent of spatial and temporal resolution and error is highly dependent on the experimental setup.\nZeeman Splitting # The Zeeman effect is the effect of a magnetic field on the states of a bound electron. Discovered by Pieter Zeeman in 1896 when there was very little understanding of quantum mechanics, it was the subject of the 1902 Nobel prize. It was discovered that spectral lines seemed to split in a magnetic field, and the satisfactory explanation had to wait until the development quantum mechanics. A bound electron has two magnetic moments, one associated with the orbital motion\n\\[\\vec \\mu_l = - \\frac{e}{2 m c} \\vec L\\] and the magnetic moment due to the spin\n\\[\\vec \\mu_s = - \\frac{e}{m c} \\vec S\\] the classical Hamiltonian of this magnetic moment interacting with an external magnetic field \\( \\vec B \\) is\n\\[H_{Zeeman} = - (\\vec \\mu_l \u0026#43; \\vec \\mu _s) \\cdot \\vec B \\\\ = \\frac{e}{2 m c} (\\vec L \u0026#43; 2 \\vec S) \\cdot \\vec B \\\\ = \\frac{e}{2 m c} (L_z \u0026#43; 2 S_z) B\\] where \\( \\hat \\mu \\) is the magnetic moment of the electron, \\( m_e \\) is the electron mass, \\( c \\) is the speed of light, \\( \\hat L \\) is the angular momentum operator, \\( \\hat S \\) is the spin operator, and \\( \\vec B \\) is the externally-applied magnetic field we wish to measure. By convention, we choose the z direction to align with \\( \\vec B \\). This term is treated as an addition to the \u0026ldquo;un-perturbed\u0026rdquo; Hamiltonian\n\\[H = H_0 \u0026#43; H_{fine-structure} \u0026#43; H_{Zeeman}\\] Things would be rather simple without incorporating the fine structure term, but alas we cannot discount it entirely. In the fine structure term, there is something like an internal magnetic field in the atom which is responsible for the spin-orbit coupling term. We can treat separately the cases where the external magnetic field is either much smaller or much larger than this internal magnetic field. In the weak case, we treat \\( H_{Zeeman} \\) as a perturbation atop the result of the fine-structure states. In the strong case, the Zeeman effect is assumed to be much stronger than the spin-orbit coupling, so we then take the fine structure Hamiltonian as a perturbation atop \\( H^{(0)} + H_{Zeeman} \\). For the fields which we wish to measure in plasma diagnostics, it is the \u0026ldquo;weak\u0026rdquo; Zeeman effect which generally applies. In the \u0026ldquo;strong\u0026rdquo; Zeeman effect, known as the Paschen-Back effect, the spin-orbit coupling is broken by the externally applied field, and the line splitting reverts to the normal (total spin 0) Zeeman effect.\nFor the weak effect, we calculate the expectation values of the perturbing Hamiltonian, which will add to the known energy levels:\n\\[E_{Zeeman} = \\frac{e B}{2 m c} \\langle \\psi | L_z \u0026#43; 2 S_z | \\psi \\rangle\\] As it turns out, this product is completely diagonalizable and the energy level splitting is given by:\n\\[\\langle \\psi | L_z \u0026#43; 2 S_z | \\psi \\rangle = \\frac{g_J m_j}{\\hbar}\\] \\[E_{Zeeman} = g_J \\mu_B m_j B\\] where \\( g_J \\) is the Land g-factor \\[g_J = 1 \u0026#43; \\frac{ j (j\u0026#43;1) \u0026#43; s (s\u0026#43;1) - l (l\u0026#43;1)}{2 j (j\u0026#43;1)}\\] and \\( \\mu_B = \\frac{e \\hbar}{2 m_e c} \\) is the Bohr magneton. Two features are of key importance: First, the energy level splitting is proportional to \\( B \\), inviting a measurement of the field by a direct observation of the level splitting. Unfortunately, for most applications the Doppler and Stark broadening are an order of magnitude greater than the Zeeman splitting. Fortunately, conservation of angular momentum gives us another useful property: the polarization of the emitted radiation is dependent on \\( \\Delta m \\) (the projection of the total angular momentum in the direction of \\( \\vec B \\)). Emission is only possible for transitions for which \\( \\Delta m = [-1, 0, 1] \\). The components for which \\( \\Delta m = 0 \\) are called the \\( \\pi \\) components, and the components for which \\( \\Delta m = \\pm 1 \\) are the \\( sigma \\) components. When emission is viewed parallel to \\( \\vec B \\), only the \\( \\sigma \\) components are observable and the emission is circularly polarized, right handed direction for \\( \\Delta m = +1 \\) and circularly polarized, left handed for \\( \\Delta m = -1 \\). When viewed perpendicular to \\( \\vec B \\), the emission is linearly polarized, with \\( \\pi \\) components polarized parallel to \\( \\vec B \\) and the \\( \\sigma \\) components polarized perpendicular to \\( \\vec B \\). Since the Doppler and other broadening effects do not change the polarization of the light, these properties allow for resolution of the Zeeman splitting in the presence of substantial broadening using polarization discrimination techniques.\n[https://doi.org/10.1029/1999RG900011] Low-density magnetic field measurement # Early (1989) spectroscopic measurement of the magnetic field in a low-density (\\( \\sim 10^{15} cm^{-3} \\)) plasma was carried out by directly measuring the Zeeman splitting in a high-power diode plasma [PhysRevA.39.5856]. At low density high temperature, the Doppler broadening is the main broadening effect. The Zeeman splitting can overcome the Doppler broadening by observing the splitting of long-wavelength emissions (the Zeeman splitting is proportional to \\( \\lambda _0 \\)) from heavier ions, in this case Ba-II.\nMeasurements of magnetic field in Z-pinches # In a Z-pinch configuration, the magnetic field has a well-defined (azimuthal) direction, and polarization techniques can be used to resolve the Zeeman splitting even when smeared by Doppler and pressure broadening. With a magnetic field of known direction, we have a choice between measuring emissions parallel to the field direction (perpendicular to the axis of symmetry) or perpendicular to the field direction (\u0026ldquo;end-on\u0026rdquo; measurements). In the cylindrical geometry, this decision amounts to choosing between chord-integrated and radially-resolved measurements.\nEnd-on observations # Observing the splitting of line emission end-on allows for radially-resolved measurements:\n[doi:10.1063/5.0009432] The polarization of Zeeman-split emission perpendicular to the magnetic field is linearly polarized, and both \\( \\pi \\) and \\( \\sigma \\) components are visible. This was the method used in one of the first measurements of the B-field in a Z-pinch experiment [doi:10.1063/1.872637]. The \\( \\sigma \\) and \\( \\pi \\) components are orthogonally polarized, so they can be distinguished by the use of linear polarizers. The \\( \\sigma \\) components have the same polarization, so they cannot be separated from each other by polarizers. A fit of the broadened line can be obtained for each polarization direction, assuming identical broadening parameters (temperature, pressure, etc.). The difference between the spectral linewidths is proportional to the magnitude of the magnetic field. In general, the linewidth difference is very small, and the diagnostic requires a very high signal/noise ratio. In this experiment, such a signal/noise ratio required averaging over many discharges, so this measurement is highly dependent on shot reproducibility. Stark and Doppler broadening also limit the applicability of the method; at small radius and high pressure, the linewidth difference becomes too small to determine.\n[doi:10.1063/5.0009432] Vertical observations (Two-polarizations method) # For a line of sight parallel to the magnetic field, only the circularly polarized \\( \\sigma^+ \\) and \\( \\sigma ^- \\) components are visible. By combining a quarter-wave plate and a linear polarizer, we can obtain separate broadened lines for the \\( \\sigma^+ \\) and \\( \\sigma^- \\) components. The Doppler and Stark broadenings are polarization independent so identical fitting parameters are used for each line. Calculating the separation between the centroids gives a more reliable measurement, less sensitive to noise. Of the Zeeman splitting diagnostic methods, this has the highest sensitivity to B since it relies on the line positions rather than their shapes.\n[doi:10.1063/5.0009432] In a cylindrical geometry like a z-pinch, at first glance it appears that we can only obtain a single measurement along the chord at the outer-most radius of the plasma extent. At other positions, the magnetic field is not parallel to the line of sight, so the emission is elliptically polarized. This has the effect of lowering the perceived Zeeman shift, since the transverse components appear as un-shifted lines. We could perform an Abel inversion of the chord-integrated measurements to recover the radial magnetic field distribution, but in most imploding plasma experiments there is a lack of sufficient azimuthal symmetry and Abel inversion gives misleading results.\nThere is a significant radial temperature gradient in a z-pinch. For the most part, impurities burn through at well-defined temperatures, resulting in a radial distribution of impurity species. In shells closer to the symmetry axis, lower ionization states become ionized. This allows us to select emission lines specific to the various impurity species, and \u0026ldquo;see through\u0026rdquo; the outer shells by observing emission lines not present in the outer shells. If the radial distribution of impurity species is known, the radial distribution of the magnetic field can be resolved without the need for Abel inversion.\nNeutral beam injector # Zeeman spectroscopy can be achieved with the help of neutral beam injection. This is particularly useful in a configuration in which large populations of impurity species are highly undesirable, or when large intrinsic radial electric fields preclude the use of the motional Stark effect (MSE). In a neutral beam injection diagnostic, a beam of neutral atoms (\\( \\sim 30keV \\) beam energy) is injected into the plasma. The beam becomes excited by collisions with the plasma, emitting characteristic line radiation which is split by the magnetic field. Because the beam is neutrally charged, the beamline is not altered by the presence of the strong magnetic field in the plasma. The beam continues through the plasma to a beam dump opposite the injector. The optical setup is arranged vertically, perpendicular to the beam injection, in order to obtain radially-resolved measurements with maximum sensitivity to the longitudinal magnetic field.\nConsiderations for the neutral beam injector:\nThe beam should be of a high enough energy to penetrate well into the plasma. The beam penetration determines the radial extent of the measurement. Extremely low intrinsic ion temperatures are preferable. Doppler broadening is the main obstacle to overcome in resolving the Zeeman splitting, and beam temperature is a major contributor to the linewidth. Sources of intrinsic ion temperatures of \\( \\sim 0.1 eV \\) have been demonstrated in use. The beam current must strike a balance between signal strength and perturbation of the plasma. An extremely low beam current will not produce sufficient line emission, but an excessive beam current has the potential to disrupt the system under study. A tightly focused beam improves the volumetric resolution. [doi:10.1063/1.1526928]\nWith a sufficiently high signal/noise ratio, the direction and magnitude of the magnetic field can be determined. For example, in the DIII-D diagnostic configuration shown here, the ratio of circular to linear polarization in the \\( \\sigma^- \\) line is sufficient to determine the direction of the magnetic field.\nPros:\nHigh spatial resolution Applicable even when direction of magnetic field is not known Cons:\nNeutral beam injector itself is a substantial addition to the experimental setup. Beam injector requires its own set of diagnostics. Can be perturbative, depending on beam strength Pellet injection and laser blow-off are alternate methods of injecting impurity species. Pellet injectors are common in tokamaks for fuel injection during operation. They can also be used to inject pellets of impurity species for the purpose of spectroscopy. A pellet is launched into the plasma, where it vaporizes. The ablation cloud emits line radiation, which can then be analyzed for Zeeman splitting. Used in TFTR and others. In a laser blow-off setup, ions can be injected at the plasma edge by preparing a thin film on a glass plate, then blasting it off into the plasma chamber with a laser. Used in Wendelstein 7-X and others.\nRemote sensing of solar magnetic field # For extraterrestrial plasmas, the only diagnostics available are spectroscopic. Investigation of the solar magnetic field lead to the early development of polarimetric techniques for the measurement of Zeeman splitting. The Zeeman effect provides the most common and useful diagnostic of solar magnetic fields [https://doi.org/10.1029/1999RG900011]. The spectral shapes of emission and absorption lines are altered by the solar magnetic field, and circular and linear polarization are imparted according to the direction of the field. Analysis of the absorption and emission spectra requires assumptions about the structure of the solar atmosphere. Neutral iron (Fe I) is a particularly good source of data about sunspots, as ferromagnetic atoms are especially Zeeman-sensitive and the long-wavelength 630.2 nm absorption lines enhance the splitting effect. By decomposing the polarization of absorbed emission, the direction and strength of the solar magnetic field can be recovered. This is how it was discovered that sunspots are regions of high magnetic flux; weak magnetic fields away from sunspot, then suddenly when you look at the spot the lines split.\n"},{"id":83,"href":"/r/notes/griffiths/ch9-2/","title":"Wave Equation for E and B","section":"Griffiths Introduction to Electrodynamics","content":" 9.2.1 Wave Equation for E and B # Recall Maxwell\u0026rsquo;s equations\n\\[\\begin{aligned} (\\text{i}) \u0026amp; \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss\u0026#39;s law)} \\\\ (\\text{ii}) \u0026amp; \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng\u0026#39;s Law)} \\\\ (\\text{iii}) \u0026amp; \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday\u0026#39;s Law}) \\\\ (\\text{iv}) \u0026amp; \\quad \\curl \\vec{H} = \\vec{J}_f \u0026#43; \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere\u0026#39;s Law)} \\end{aligned}\\] To get to a wave equation from these, we start with a curl of curl and use a standard vector identity\n\\[\\curl (\\curl \\vec E) = \\grad ( \\div \\vec E) - \\grad ^2 \\vec E\\] Use Faraday\u0026rsquo;s law on the left hand side (and move the spatial derivative through the temporal one), and on the right hand side use Gauss\u0026rsquo; law to re-write the divergence\n\\[\\rightarrow - \\pdv{}{t} ( \\curl \\vec B) = \\frac{1}{\\epsilon_0} \\grad \\rho - \\grad ^2 \\vec E\\] \\[\\rightarrow - \\pdv{}{t} \\left( \\mu_0 \\vec J \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\right) = - \\mu_0 \\pdv{\\vec J}{t} - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t ^2} = \\frac{\\grad \\rho}{\\epsilon_0} - \\grad ^2 \\vec E\\] \\[\\rightarrow \\grad ^2 \\vec E - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t^2} = \\frac{1}{\\epsilon_0} \\grad \\rho \u0026#43; \\mu_0 \\pdv{\\vec{J}}{t} \\quad \\text{\u0026#34;non-homogeneous\u0026#34; wave equation for E}\\] In a vacuum, \\( \\rho = 0 \\) and \\( \\vec J = 0 \\) so\n\\[\\grad ^2 \\vec E - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t^2} = 0\\] which is just a standard 3D wave equation. We can identify the speed of propagation based on the constant of proportionality \\( v = \\frac{1}{\\sqrt{\\mu_0 \\epsilon_0}} = c \\)\nWe could also have started with \\( \\curl (\\curl \\vec B) \\) to obtain\n\\[\\grad ^2 \\vec B - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec B}{\\partial t^2} = - \\mu_0 (\\curl \\vec J) \\quad \\text{\u0026#34;non-homogeneous\u0026#34; wave equation for B}\\] So, in vacuum you get the exact same wave equation\n\\[\\grad ^2 \\vec B - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec B}{\\partial t^2} = 0\\] So, both \\( \\vec E \\) and \\( \\vec B \\) must satisfy these wave equations. We know that the wave equations admit certain sets of solutions, but that\u0026rsquo;s not the entire story. We\u0026rsquo;ll see additional constraints on solutions to \\( \\vec E \\) and \\( \\vec B \\) due to the fact that the waves need to satisfy all of the Maxwell equations, so \\( \\vec E \\) and \\( \\vec B \\) are very intimately linked.\n9.2.2: Monochromatic Plane Waves # Consider monochromatic sine waves (plane waves) in a single direction, so that all variation happens in the z-direction. Again, using the superposition principle we\u0026rsquo;ll be able to build up more complicated solutions.\n\\[\\vec E = \\vec E_0 \\cos (k z - \\omega t \u0026#43; \\delta) \\qquad \\vec B = \\vec B_0 \\cos (k z - \\omega t \u0026#43; \\delta)\\] \\[\\vec E_0 = E_{0, x} \\vu x \u0026#43; E_{0, y} \\vu y \u0026#43; E_{0, z} \\vu z\\] Let\u0026rsquo;s apply Gauss\u0026rsquo; law (in vacuum)\n\\[\\div \\vec E = 0\\] \\[\\div (\\vec E_0 \\cos (k z - \\omega t \u0026#43; \\delta) ) = 0\\] \\[\\rightarrow \\pdv{E_{0, x}}{x} \\cos (k z - \\omega t \u0026#43; \\delta ) \u0026#43; \\pdv{E_{0, y}}{y} \\cos (kz - \\omega t \u0026#43; \\delta)\\] \\[\u0026#43; \\pdv{E_{0, z}}{z} \\cos (kz - \\omega t \u0026#43; \\delta) \u0026#43; E_{0,z}(- k \\sin(kt - \\omega t \u0026#43; \\delta)) = 0\\] We\u0026rsquo;ve decided that there is no variation in the x- and y-directions, so only the final term survives, and must be equal to zero\n\\[E_{0, z} (- k \\sin(kt - \\omega t \u0026#43; \\delta)) = 0 \\quad \\rightarrow \\quad E_{0,z} = 0\\] Which is to say that EM plane waves are \u0026ldquo;transverse\u0026rdquo; waves.\nLet\u0026rsquo;s also use Faraday\u0026rsquo;s law to relate \\( \\vec E \\) and \\( \\vec B \\) :\n\\[\\curl \\vec E = - \\pdv{B}{t}\\] \\[\\curl \\vec E = \\begin{pmatrix} \\vu x \u0026amp; \\vu y \u0026amp; \\vu z \\\\ \\pdv{}{x} \u0026amp; \\pdv{}{y} \u0026amp; \\pdv{}{z} \\\\ E_{0, x} \\cos (k z - \\omega t) \u0026amp; E_{0, y} \\cos (k z - \\omega t) \u0026amp; 0 \\end{pmatrix}\\] \\[= \\vu x ( E_{0, y} k \\sin(kz - \\omega t)) - \\vu y (E_{0, x} k \\sin(k z - \\omega t))\\] \\[- \\pdv{B}{t} = - \\vu x \\pdv{}{t} B_{0, x} \\cos (kz - \\omega t) - \\vu y \\pdv{}{t}B_{0, y} \\cos (kz - \\omega t)\\] \\[= - \\vu x B_{0, x} \\omega \\sin (k z - \\omega t) - \\vu y B_{0, y} \\omega \\sin (kz - \\omega t)\\] Matching up components and canceling the sine functions,\n\\[\\rightarrow k E_{0, y} = - \\omega B_{0, x} \\qquad k E_{0, x} = \\omega B_{0, y}\\] So every time you have an \\( \\vec E \\) field, you will have a \\( \\vec B \\) field in an orthogonal direction - they are mutually orthogonal - and they are in phase, since the proportionality factors are real.\n9.2.3: Energy and Momentum in Electromagnetic Waves # To repeat, for monochromatic plane waves propagating in the z-direction,\n\\[\\vec E = \\vec{E_0} \\cos ( k z - \\omega t \u0026#43; \\delta) = E_0 \\cos (k z - \\omega t \u0026#43; \\delta) \\vu x\\] \\[\\vec B = B_0 \\cos (kz - \\omega t \u0026#43; \\delta) \\vu y\\] and\n\\[E_0 / B_0 = c\\] What does the energy density due to these fields look like?\n\\[u = \\frac{1}{2} \\epsilon_0 E^2 \u0026#43; \\frac{1}{2\\mu_0} B^2 = \\epsilon_0 E^2 = \\epsilon_0 E_0 \\cos ^2 (kz - \\omega t \u0026#43; \\delta)\\] The electric and magnetic contributions are equal. The resulting Poynting vector is\n\\[\\vec S = \\frac{1}{\\mu_0} \\vec E \\cross \\vec B = \\frac{E_0 B_0 }{\\mu_0} \\cos ^2 (kz - \\omega t \u0026#43; \\delta ) \\vu z\\] \\[= \\frac{E_0 ^2}{\\mu_0 c} \\cos ^2 (kz - \\omega t \u0026#43; \\delta) \\vu z = c \\epsilon_0 E_0 ^2 \\cos ^2 (kz - \\omega t \u0026#43; \\delta) \\vu z\\] \\[\\vec S = c u \\vu z\\] So the Poynting vector points in the direction of propagation, and it has amplitude \\( c u \\). What about the momentum density?\n\\[\\vec g = \\mu_0 \\epsilon_0 \\vec S = \\epsilon_0 (\\vec E \\cross \\vec B) = \\frac{1}{c^2} cu \\vu z = \\frac{u}{c} \\vu z\\] The rate of oscillation of these waves is typically very high, so we are mostly interested in the average of the oscillatory behavior over a period. Recall that the time-average of \\( \\cos ^2 \\) over a cycle is \\( \\frac{1}{2} \\), so\n\\[\\langle u \\rangle = \\frac{1}{2} \\epsilon_0 E_0 ^2\\] \\[\\langle \\vec S \\rangle = \\frac{1}{2} c \\epsilon_0 E_0 ^2 \\vu z\\] \\[\\langle \\vec g \\rangle = \\frac{1}{2c} \\epsilon_0 E_0 ^2 \\vu z\\] Another useful quantity we usually throw around is the Root Mean Square (RMS) value of the field\n\\[\\sqrt{\\langle E_0 ^2 \\cos ^2 (kz - \\omega t \u0026#43; \\delta) \\rangle } = \\sqrt{\\frac{E_0^2}{2}} = \\frac{E_0}{\\sqrt{2}} \\approx 0.7 E_0\\] The \u0026ldquo;intensity\u0026rdquo; of the electromagnetic wave is defined as its power per unit area, or energy per unit area per unit time\n\\[I = \\frac{\\text{power}}{\\text{area}} = \\frac{\\text{energy}}{\\text{area}\\cdot\\text{time}} \\] \\[= \\langle | \\vec S | \\rangle = \\langle | cu \\vu z | \\rangle = c \\langle u \\rangle = \\frac{1}{2} c \\epsilon_0 E_0 ^2\\] If the light hits the surface of a perfect absorber, it will transfer its momentum to the surface. In a time \\( \\Delta t \\) the momentum transfer will be\n\\[\\Delta \\vec p = \\langle \\vec g \\rangle A c \\Delta t\\] so the radiation pressure (average force per unit area) is\n\\[P = \\frac{1}{A} \\frac{\\Delta p}{\\Delta t} = \\frac{1}{2} \\epsilon_0 E_0 ^2 = \\frac{I}{c}\\] Of course, when falling on a perfect reflector, the radiation pressure is twice as big, since the resulting momentum of the reflected light switches direction instead of being absorbed.\nExample Problem 9.10 # Q The intensity of sunlight hitting the earth is about 1300 \\( W/m^2 \\). If sunlight strikes a perfect absorber, what pressure does it exert? How about a perfect reflector? What fraction of atmospheric pressure does this amount to? A The radiation pressure for a perfect absorber is\n\\[P = \\frac{I}{c} = \\frac{1300}{3 \\cdot 10^8} \\approx 4.3 \\cdot 10^{-6} N/m^2\\] The atmospheric pressure on earth\u0026rsquo;s surface is about \\( 10^5 N \\), so\n\\[\\frac{\\text{Radiation pressure}}{\\text{Atmos pressure}} \\approx 10^{-11}\\] The resulting radiation pressure is tiny compared with the normal pressures we\u0026rsquo;re used to, but in space where atmospheric pressure is absent the result can be significant, and laser beams on individual atoms with tiny masses can slow and trap individual particles via radiation pressure.\n"},{"id":84,"href":"/r/notes/griffiths/ch9-3/","title":"Electromagnetic Waves in Matter","section":"Griffiths Introduction to Electrodynamics","content":" 9.3: Electromagnetic Waves in Matter # 9.3.1: Propagation in Linear Media and Non-conductors # Maxwell\u0026rsquo;s equations in linear media are\n\\[\\begin{aligned} (\\text{i}) \u0026amp; \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss\u0026#39;s law)} \\\\ (\\text{ii}) \u0026amp; \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng\u0026#39;s Law)} \\\\ (\\text{iii}) \u0026amp; \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday\u0026#39;s Law}) \\\\ (\\text{iv}) \u0026amp; \\quad \\curl \\vec{H} = \\vec{J}_f \u0026#43; \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere\u0026#39;s Law)} \\end{aligned}\\] In linear media, our constitutive relations are\n\\[\\vec D = \\epsilon_0 \\vec E \u0026#43; \\vec P = \\epsilon \\vec E\\] and\n\\[\\vec H = \\frac{\\vec B}{\\mu_0} - \\vec M = \\frac{\\vec B}{\\mu}\\] In charge-free region, the Maxwell equations in linear media look a lot like those in vacuum\n\\[\\rho_f = 0 \\qquad \\vec J_f = 0\\] \\[\\rightarrow \\begin{aligned} (\\text{i}) \u0026amp; \\quad \\div \\vec E = 0 \\\\ (\\text{ii}) \u0026amp; \\div \\vec B = 0 \\\\ (\\text{iii}) \u0026amp; \\curl \\vec E = - \\pdv{\\vec B}{t} \\\\ (\\text{iv}) \u0026amp; \\curl \\vec B = \\mu \\epsilon \\pdv{\\vec E}{t} \\end{aligned}\\] So that\u0026rsquo;s nearly identical, except where we had \\( \\epsilon_0 \\) now we have \\( \\epsilon \\), and where we had \\( \\mu_0 \\) now we have \\( \\mu \\), so we can make these same substitutions in the solutions. The resulting wave equations in linear matter are\n\\[\\nabla ^2 \\vec E = \\frac{1}{v^2} \\frac{\\partial ^2 \\vec E}{\\partial t^2} \\qquad \\nabla ^2 \\vec B = \\frac{1}{v^2} \\frac{\\partial ^2 \\vec B}{\\partial t^2} \\] where now the speed is\n\\[v = \\frac{1}{\\sqrt{\\mu \\epsilon}} = \\text{speed of EM wave in linear medium} = \\frac{c}{n}\\] where\n\\[n \\equiv \\sqrt{\\frac{\\mu}{\\mu_0} \\frac{\\epsilon}{\\epsilon_0}} = \\text{ index of refraction }\\] 9.3.2: Reflection and Transmission at Normal Incidence # On of the most simple interesting situations that can arise when the index of refraction changes is when light crosses a sudden interface, i.e., what happens when light passes from one transparent medium into another? As in the case of waves on a string, we expect to get a reflected wave and a transmitted wave. Suppose we have waves incident on the boundary (in the x-y plane) between two media, call the media \u0026ldquo;1\u0026rdquo; and \u0026ldquo;2\u0026rdquo; with indices of refraction \\( n_1 \\) and \\( n_2 \\). The z-axis is normal to the boundary.\nLet\u0026rsquo;s write our incident wave \\( \\vec{E_I} \\) in so-called \u0026ldquo;phasor notation\u0026rdquo; (just complex exponential notation)\n\\[\\vec{E_I} (\\vec r, t) = \\vec{E_{0, I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)}\\] where the actual wave itself is the real part of the complex exponential. We define the magnetic field in the same way\n\\[\\vec{B_I} (\\vec r, t) = \\vec{B_{0, I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} = \\frac{1}{v_1} ( \\vec{k_I} \\cross \\vec{E_I})\\] We write down similar expressions\n\\[\\vec{E_{R}}, \\quad \\vec{B_{R}}\\] for the reflected wave, and\n\\[\\vec{E_{T}}, \\quad \\vec{B_{T}}\\] for the transmitted wave. At the \\( z = 0 \\) plane\n\\[\\vec{E_{0,I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} \u0026#43; \\vec{E_{0,R}} e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)} = \\vec{E_{0,T}} e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)}\\] That\u0026rsquo;s true for all \\( x, y \\) on the interface and for all time, so this immediately implies that \\( \\omega \\) has to be the same for each of the waves (we already implicitly assumed this in the notation). So the \\( \\omega t \\) terms drop out of all three terms, and we can focus on the wavenumbers. It has to be the case that\n\\[\\vec{k_I} \\cdot \\vec r = \\vec{k_R} \\cdot \\vec r = \\vec{k_T} \\cdot \\vec r\\] \\[\\rightarrow k_{I, x}x \u0026#43; k_{I, y} y = k_{R, x}x \u0026#43; k_{R, y} y = k_{T, x}x \u0026#43; k_{T, y} y\\] where so far there is no restriction on \\( k_z \\). We can simply orient our \\( x-z \\) axes such that \\( \\vec{k_I} \\) lies in the \\( x-z \\) plane. This means that \\( \\vec{k_R} \\) and \\( \\vec{k_T} \\) will also lie in the plane, and\n\\[k_{I, x} = k_{I} \\sin \\theta_I = k_R \\sin \\theta_R \\quad \\rightarrow \\quad \\sin \\theta_I = \\sin \\theta_R\\] \\[\\rightarrow \\quad \\theta_I = \\theta_R\\] \\[k_{I} \\sin \\theta_I = k_T \\sin \\theta_T \\quad \\rightarrow \\frac{n_1}{n_2} \\sin \\theta_I = \\sin \\theta_T\\] \\[\\rightarrow \\frac{\\sin \\theta_T}{\\sin \\theta_I} = \\frac{n_1}{n_2}\\] 9.3.3 Reflection and Transmission at Oblique Incidence # In the more general case where the incoming wave hits the boundary at some angle \\( \\theta_I \\). Suppose that a monochromatic plane wave\n\\[\\vec{E_I}(\\vec r, t) = \\vec{E_0}_I e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_I}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_I \\cross \\vec{E_I})\\] approaches from the left. We\u0026rsquo;ll get a reflected wave\n\\[\\vec{E_R}(\\vec r, t) = \\vec{E_0}_R e^{i(\\vec{k_R} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_R}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_R \\cross \\vec{E_T})\\] and a transmitted wave\n\\[\\vec{E_T}(\\vec r, t) = \\vec{E_0}_T e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_T}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_T \\cross \\vec{E_T})\\] All three waves have the same frequency \\( \\omega \\). The three wave numbers are related by\n\\[k_I v_1 = k_R v_1 = k_T v_2 = \\omega \\qquad \\rightarrow \\qquad k_I = k_R = \\frac{v_2}{v_1} k_T = \\frac{n_1}{n_2} k_T\\] The combined fields in medium 1, \\( \\vec{E_I} + \\vec{E_R} \\) and \\( \\vec{B_I} + \\vec{B_R} \\) must be joined to the fields in medium 2 using the boundary conditions we get from Maxwell\u0026rsquo;s equations. All of the boundary conditions share the generic structure\n\\[()e^{i(\\vec{k}_I \\cdot \\vec r - \\omega t)} \u0026#43; ()e^{i(\\vec{k}_R \\cdot \\vec r - \\omega t)} = ()e^{i(\\vec{k}_T \\cdot \\vec r - \\omega t)} \\qquad \\text{ at } z = 0\\] For now the important thing to notice is that the x, y, and t dependence is confined to the exponents. Because the boundary conditions must hold at all points on the plane, and for all times, these exponential factors must be equal at the boundary.\n\\[\\vec{k_I} \\cdot r = \\vec{k_R} \\cdot r = \\vec{k_T} \\cdot r\\] or\n\\[x(k_I)_x \u0026#43; y(k_I)_y = x(k_R)_x \u0026#43; y(k_R)_y = x(k_T)_x \u0026#43; y(k_T)_y \\] for all x and y, which can only be true if all both components are separately equal. So we may as well orient our axes so that \\( \\vec{k_I} \\) lies in the \\( x-z \\) plane - our boundary condition ensures that if we do that, \\( \\vec{k_R} \\) and \\( \\vec{k_T} \\) will also lie in the plane.\n!!! theorem \u0026ldquo;The incident, reflected, and transmitted wave vectors form a plane (called the plane of incidence), which also includes the normal to the surface.\u0026rdquo;\nWe can also say that\n\\[k_I \\sin \\theta_I = k_R \\sin \\theta_R = k_T \\sin \\theta_T\\] where \\( \\theta_I \\) is the angle of incidence, \\( \\theta_R \\) is the angle of reflection, and \\( \\theta_T \\) is the angle of transmission, or more commonly the \u0026ldquo;angle of refraction,\u0026rdquo; all of them measured with respect to the normal.\n!!! theorem \u0026ldquo;The angle of incidence is equal to the angle of refraction \\( \\theta_I = \\theta_R \\)\u0026rdquo;\nAnd as for the transmitted angle\n!!! theorem \u0026ldquo;The law of refraction: \\( \\frac{\\sin \\theta_T}{\\sin \\theta_I} = \\frac{n_1}{n_2} \\)\u0026rdquo;\nSo our exponential factors are dealt with, and we can move on to the Maxwell boundary conditions\n\\[(i) \\quad \\epsilon_0 \\left( \\vec{E}_{0,I} \u0026#43; \\vec{E}_{0,R} \\right)_z = \\epsilon_2 \\left( \\vec{E}_{0,T} \\right) _z \\\\ (ii) \\quad \\left( \\vec{B}_{0,I} \u0026#43; \\vec{B}_{0,R} \\right) _z = \\left( \\vec{B}_{0,T} \\right)_z \\\\ (iii) \\quad \\left( \\vec{E}_{0,I} \u0026#43; \\vec{E}_{0,R} \\right)_{x,y} = \\left( \\vec{E}_{0,T} \\right)_{x,y} \\\\ (iv) \\quad \\frac{1}{\\mu_1} \\left( \\vec{B}_{0,I} \u0026#43; \\vec{B}_{0,R} \\right) _{x,y} = \\frac{1}{\\mu_2} \\left( \\vec{B}_{0,T} \\right)_{x,y}\\] where \\( \\vec{B}_0 = \\frac{1}{v} \\vu k \\cross \\vec{E}_0 \\) in each case.\nIf we now suppose the plane-polarized case, in which the polarization of the incident light is parallel to the plane of incidence, it follows that the reflected and transmitted waves are also polarized in this plane. Then (i) reads\n\\[ \\epsilon_i \\left( - \\vec{E}_{0,I} \\sin \\theta_I \u0026#43; \\vec{E}_{0,R} \\sin \\theta_R \\right) = \\epsilon_2 \\left( - \\vec{E}_{0,T} \\sin \\theta_T \\right)\\] and (iv) says\n\\[\\frac{1}{\\mu_1 v_1} \\left( \\vec{E}_{0,I} - \\vec{E}_{0,R} \\right) = \\frac{1}{\\mu_2 v_2} \\vec{E}_{0,T}\\] We can reduce these down to\n\\[\\vec{E}_{0,I} - \\vec{E}_{0,R} = \\beta \\vec{E}_{0,T} \\qquad \\text{ and } \\qquad \\vec{E}_{0,I} \u0026#43; \\vec{E}_{0,R} = \\alpha \\vec{E}_{0,T}\\] where \\( \\beta \\) is defined as\n\\[\\beta \\equiv \\frac{\\mu_1 v_1}{\\mu_2 v_2} = \\frac{\\mu_1 n_2}{\\mu_2 n_1}\\] and \\( \\alpha \\) is\n\\[\\alpha \\equiv \\frac{\\cos \\theta_T}{\\cos \\theta_I}\\] Solving for the reflected and transmitted amplitudes, we obtain\n\\[ \\vec{E}_{0,R} = \\left( \\frac{\\alpha - \\beta}{\\alpha \u0026#43; \\beta} \\right) \\vec{E}_{0,I} \\qquad \\vec{E}_{0,T} = \\left( \\frac{2}{\\alpha \u0026#43; \\beta } \\right) \\vec{E}_{0,I}\\] These are the Fresnel\u0026rsquo;s equations for the case of polarization in the plane of incidence. Note that the transmitted wave is always in phase with the incident one; the reflected wave is either in phase (\u0026ldquo;right side up\u0026rdquo;) if \\( \\alpha \u0026gt; \\beta \\), or \\( \\pi \\) out of phase (\u0026ldquo;upside down\u0026rdquo;) if \\( \\alpha \u0026lt; \\beta \\)\nOf note is the interesting incident angle \\( \\theta_B \\) where the reflected wave is completely extinguished. That happens when \\( \\alpha = \\beta \\), or\n\\[\\sin^2 \\theta_B = \\frac{1 - \\beta^2}{(n_1 / n_2)^2 - \\beta^2}\\] In a typical case \\( \\mu_1 \\approx \\mu_2 \\) and \\( \\beta \\approx n_2 / n_1 \\) so \\( \\sin^2 \\theta_B \\approx \\beta^2 / (1 + \\beta^2) \\) or\n\\[\\tan \\theta_B \\approx \\frac{n_2}{n_1}\\] For s-polarized fields (i.e., the electric field is polarized perpendicular to the plane of incidence),\n\\[\\vec{E}_{0,R} = \\left( \\frac{1 - \\alpha \\beta}{1 \u0026#43; \\alpha \\beta} \\right) \\vec{E}_{0,I} \\qquad \\vec{E}_{0,T} = \\frac{2}{1 \u0026#43; \\alpha \\beta} \\vec{E}_{0,I}\\] To finish things up, let\u0026rsquo;s look at the intensity of the reflected and transmitted waves, since that\u0026rsquo;s what we\u0026rsquo;re generally measuring directly. The intensity depends on the electric field magnitude and the index of refraction\n\\[I = \\frac{1}{2} c \\epsilon_0 E_{0} ^2 \\qquad \\text{(in vacuum)}\\] \\[ = \\frac{1}{2} v \\epsilon E_{0} ^2 = \\frac{1}{2} c n \\epsilon_0 E_0 ^2 \\qquad \\text{(in linear media)}\\] We get reflection and transmission coefficients defined as:\n\\[R = \\frac{I_R}{I_I} \\qquad T = \\frac{I_T}{I_I}\\] Plugging in our previous expressions for the incoming and reflected fields,\n\\[R = \\frac{\\frac{1}{2} c n_1 \\epsilon_0}{\\frac{1}{2} c n_1 \\epsilon_0} \\left( \\frac{1 - \\beta}{1 \u0026#43; \\beta} \\right) ^2 = \\left( \\frac{n_1 - n_2}{n_1 \u0026#43; n_2} \\right)\\] and the transmitted field,\n\\[T = \\frac{\\frac{1}{2} c n_2 \\epsilon_0}{\\frac{1}{2} c n_1 \\epsilon_0} \\left( \\frac{2}{1 \u0026#43; \\frac{n_2}{n_1} } \\right) ^2 = \\frac{n_2}{n_1} \\frac{4 n_1 ^2}{(n_1 \u0026#43; n_2)^2} = \\frac{4 n_1 n_2}{(n_1 \u0026#43; n_2)^2}\\] We should check that \\( R + T = 1 \\), since we haven\u0026rsquo;t got any absorption in our scenario:\n\\[R \u0026#43; T = \\left( \\frac{n_1 - n_2}{n_1 \u0026#43; n_2} \\right)^2 \u0026#43; \\frac{4 n_1 n_2}{(n_1 \u0026#43; n_2)^2} = 1\\] "},{"id":85,"href":"/r/notes/griffiths/ch9-4/","title":"Electromagnetic Waves in Conductors","section":"Griffiths Introduction to Electrodynamics","content":" 9.4.1 Electromagnetic Waves in Conductors # When we formulated our description of how electromagnetic waves move through linear media, we relied on the stipulation that the free charge density \\( \\rho_f \\) and free current density \\( \\vec J _f \\) were zero. In the case of conductors, we\u0026rsquo;re going to need to re-visit that assumption.\nAs a refresher, when we last took a good look at conductors we had just moved beyond the electrostatic picture. In statics, we could assume that the field within a conductor is always zero, because any field would eventually (quickly) cause the free changes to arrange in such a way as to cancel the field. Now, we want to see how the conductor responds when we have a varying electric field.\nSo, what do we already know about electric fields in matter?\n\\[\\div \\vec E = \\frac{\\rho_f}{\\epsilon} \\qquad \\text{(Gauss\u0026#39; Law in media)}\\] \\[\\div \\vec J _f \u0026#43; \\pdv{\\rho_f}{t} = 0 \\qquad \\text{(Continuity)}\\] \\[\\vec J_f = \\sigma \\vec E \\qquad \\text{(Ohm\u0026#39;s Law)}\\] \\[\\rightarrow \\div \\left( \\frac{\\vec J _f}{\\sigma} \\right) = \\frac{\\rho_f}{\\epsilon}\\] \\[\\rightarrow \\frac{1}{\\sigma} \\left( - \\pdv{\\rho_f}{t} \\right) = \\frac{\\rho_f}{\\epsilon}\\] \\[\\rightarrow \\pdv{\\rho_f}{t} = - \\frac{\\sigma}{\\epsilon} \\rho_f\\] We see differential equations like this all the time - it\u0026rsquo;s the equation for an exponential decay. The decay timescale of the free charge density is\n\\[ \\tau_{1/e} = \\frac{\\epsilon}{\\sigma} \\] Applying to the situation of a very good conductor where \\( \\sigma \\) is very large, e.g. copper, the time constant is around \\( 10^{-19} \\) seconds. This means that if a little bit of free charge has accumulated at any time, then from that point charge will flow to suppress that field on the timescale of \\( 10^{-19} \\) seconds. So, for the vast majority of radiation (wave period up to \\( \\approx 10^{-18} \\)), the time constant is fast enough that the conductor is effectively impermeable, and all our previous descriptions of conductors are still sound. Namely, the Maxwell equations look like\n\\[\\div \\vec E = 0\\] \\[\\div \\vec B = 0\\] \\[\\curl \\vec E = - \\pdv{\\vec B}{t}\\] \\[\\curl \\vec B = \\mu \\epsilon \\pdv{\\vec E}{t} \u0026#43; \\mu \\sigma \\vec E \\qquad (\\sigma \\vec E = \\vec J)\\] As before, to get our wave equations, apply \\( \\curl (\\curl \\vec E) \\) and \\( \\curl (\\curl \\vec B) \\)\n\\[\\grad (\\div \\vec E) - \\laplacian \\vec E = - \\curl (\\pdv{\\vec B}{t}) = - \\pdv{}{t} (\\curl \\vec B)\\] \\[\\rightarrow - \\laplacian \\vec E = - \\mu \\epsilon \\frac{\\partial^2 \\vec E}{\\partial t^2} - \\mu \\sigma \\pdv{\\vec E}{t}\\] \\[\\rightarrow \\laplacian \\vec E = \\mu \\epsilon \\frac{\\partial ^2 \\vec E}{\\partial t^2} \u0026#43; \\mu \\sigma \\pdv{\\vec E}{t}\\] As \\( \\sigma \\rightarrow 0 \\), we recover the wave equation for linear media as we\u0026rsquo;d expect. We now have a \u0026ldquo;lossy\u0026rdquo; wave equation, since there is attenuation of the wave over time.\nFrom \\( \\curl ( \\curl \\vec B) \\) we get\n\\[\\laplacian \\vec B = \\mu \\epsilon \\frac{\\partial^2 \\vec B}{\\partial t^2} \u0026#43; \\mu \\sigma \\pdv{\\vec B}{t}\\] Recall that for \\( \\sigma = 0 \\) the wave equation supports waves of the form \\( e ^{i(\\vec k \\cdot \\vec r - \\omega t)} \\). For propagation along the z-axis, \\( e^{i(kz - \\omega t)} \\). To solve the lossy version, we\u0026rsquo;ll look for solutions by introducing an imaginary component to \\( k \\) in order to satisfy the damping term.\n\\[\\rightarrow \\tilde{k} = k \u0026#43; i\\kappa\\] For the one-dimensional case, let\u0026rsquo;s plug our new wave solution in\n\\[\\laplacian \\vec E = \\mu \\epsilon \\frac{\\partial ^2 \\vec E}{\\partial t^2} \u0026#43; \\mu \\sigma \\pdv{\\vec E}{t}\\] \\[\\rightarrow - \\tilde{k}^2 = \\mu \\epsilon (- \\omega^2) \u0026#43; \\mu \\sigma (-i \\omega)\\] \\[\\rightarrow k^2 - \\kappa ^2 \u0026#43; 2 i k \\kappa = \\mu \\epsilon \\omega^2 \u0026#43; i \\mu \\sigma \\omega\\] \\[\\rightarrow k^2 - \\kappa ^2 = \\mu \\epsilon \\omega \\qquad 2 k \\kappa = \\mu \\sigma \\omega\\] \\[\\rightarrow k = \\frac{ \\mu \\sigma \\omega}{2 \\kappa} \\qquad \\left( \\frac{\\mu \\sigma \\omega}{2 \\kappa} \\right)^2 - \\kappa ^2 = \\mu \\epsilon \\omega^2\\] \\[\\rightarrow \\kappa ^4 \u0026#43; \\mu \\epsilon \\omega^2 \\kappa^2 - \\left( \\frac{\\mu \\sigma \\omega}{2} \\right)^2 = 0\\] We just write down the solution to the quadratic equation in \\( \\kappa \\)\n\\[\\kappa = \\sqrt{\\frac{- \\mu \\epsilon \\omega^2 \\pm \\sqrt{(\\mu \\epsilon \\omega^2)^2 \u0026#43; (\\mu \\sigma \\omega)^2}}{2}}\\] Because we enforce that \\( \\kappa \\) is real, and all of the parameters within the square root are positive, we need to choose the \\( + \\) sign\n\\[\\kappa = \\sqrt{\\frac{\\sqrt{(\\mu \\epsilon \\omega^2)^2 \u0026#43; (\\mu \\sigma \\omega)^2} - \\mu \\epsilon \\omega^2 }{2}}\\] \\[\\rightarrow \\kappa = \\sqrt{\\frac{\\mu \\epsilon \\omega^2}{2}} \\sqrt{ \\sqrt{ 1 \u0026#43; \\left( \\frac{\\sigma}{\\epsilon \\omega} \\right)^2 } - 1 }\\] \\[\\rightarrow \\kappa = \\omega \\sqrt{ \\frac{\\epsilon \\mu}{2} } \\left[ \\sqrt{ 1 \u0026#43; \\left( \\frac{\\sigma}{\\epsilon \\omega} \\right)^2 } - 1 \\right] ^{1/2}\\] Plugging \\( \\kappa \\) back in to our expression for \\( k \\) and doing some algebraic manipulation,\n\\[k = \\omega \\sqrt{ \\frac{\\epsilon \\mu}{2}} \\left[ \\sqrt{1 \u0026#43; \\left( \\frac{\\sigma}{\\epsilon \\omega} \\right) ^2 } \u0026#43; 1 \\right]^{1/2}\\] So the fields look like\n\\[\\vec E ( z, t) = \\vec{E_0} e^{- \\kappa z} e^{i(kz - \\omega t)}\\] \\[\\vec B ( z, t) = \\vec{B_0} e^{- \\kappa z} e^{i(kz - \\omega t)}\\] As a check, if \\( \\sigma \\rightarrow 0 \\), \\( \\kappa \\rightarrow 0 \\), \\( k \\rightarrow \\omega \\sqrt{ \\frac{\\epsilon \\mu}{2}} \\sqrt{2} = \\omega \\sqrt{ \\epsilon \\mu} = \\omega / v \\) and we are back to the linear medium we started with. In the case of a really good conductor (where \\(1 / \\omega \\gg \\epsilon / \\sigma \\)), both \\( k \\) and \\( \\kappa \\) will converge\n\\[k \\approx \\kappa \\rightarrow \\omega \\sqrt{\\frac{\\epsilon \\mu}{2}} \\left[ \\frac{\\sigma}{\\epsilon \\omega} \\right] ^{1/2} \\\\ = \\sqrt{ \\omega \\frac{\\epsilon \\mu}{2} \\frac{\\sigma}{\\epsilon}} \\\\ = \\sqrt{ \\frac{ \\omega \\mu \\sigma}{2}}\\] For a good conductor, both \\( \\vec E \\) and \\( \\vec B \\) get damped over a distance scale given by \\( 1/\\kappa \\), which can be quite short. Since \\( k ~ \\kappa \\) we can relate this \u0026ldquo;skin depth\u0026rdquo; to the wavelength\n\\[\\lambda = \\frac{2\\pi}{k} \\rightarrow d = \\frac{1}{\\kappa} = \\frac{\\lambda}{2 \\pi}\\] Before, we had a real proportionality factor between \\( \\vec{E_0} \\) and \\( \\vec{B_0} \\). If we go through the same motions here with our new lossy solution,\n\\[\\curl \\vec E = - \\pdv{\\vec B}{t} \\rightarrow \\vec B _0 = \\frac{\\tilde{k}}{\\omega} \\vec E _0 \\] Because \\( \\tilde{k} \\) is a complex number, this ratio will also be a complex number. For our phasor expressions for the fields, this amounts to a phase shift between the two.\n!!! info \u0026ldquo;In conductors, there is a phase difference between the \\( \\vec E \\) and \\( \\vec B \\) fields. For a good conductor, the phase shift is \\( 45 ^\\circ \\)\u0026rdquo;\n9.4.2: Reflection at a Conducting Surface # In this situation, we consider a wave traveling through a linear medium (or vacuum) impinging on a conducting surface at a normal to the surface. We can go ahead and write down the general boundary conditions:\n\\[\\begin{aligned} (i) \u0026amp; \\epsilon_1 E_{1}^\\perp - \\epsilon_2 E_2 ^\\perp = \\sigma_f \\\\ (ii) \u0026amp; B_{1}^\\perp - B_2 ^\\perp = 0 \\\\ (iii) \u0026amp; E_{1}^\\parallel - E_2 ^\\parallel = 0 \\\\ (iv) \u0026amp; \\frac{1}{\\mu_1}B_{1}^\\parallel - \\frac{1}{\\mu_2} B_2 ^\\parallel = \\vec{k_f} \\cross \\vu n \\end{aligned}\\] For a conductor obeying Ohm\u0026rsquo;s law \\( \\vec{J_f} = \\sigma \\vec E \\). So, since \\( \\vec E \\) is finite, we will have a finite \\( \\vec{J}_f \\), so we have \\( \\vec{k}_f \\) must go to zero, which takes care of the right hand side of equation (iv). Since we know that \\( \\vec{E} \\) and \\( \\vec{B} \\) are transverse to the wave,\n\\[E_1 ^\\perp = E_2 ^\\perp = 0 \\rightarrow \\sigma_f = 0\\] Putting all of these together, we have the boundary conditions for a conducting-linear interface:\n\\[\\begin{aligned} (i) \u0026amp; E_{1}^\\perp - E_2 ^\\perp = 0 \\\\ (ii) \u0026amp; B_{1}^\\perp - B_2 ^\\perp = 0 \\\\ (iii) \u0026amp; E_{1}^\\parallel - E_2 ^\\parallel = 0 \\\\ (iv) \u0026amp; \\frac{1}{\\mu_1}B_{1}^\\parallel - \\frac{1}{\\mu_2} B_2 ^\\parallel = 0 \\end{aligned}\\] For an incident wave that looks like \\( \\tilde{\\vec{E}}I = \\tilde{E}{0, I} e^{i (k_1 z - \\omega t)} \\vu x \\) and \\( \\tilde{\\vec{B_I}} = \\frac{k_1}{\\omega} \\tilde{E}_{0,I} e^{i (k_1 z - \\omega t)} \\vu y \\), we will have:\na reflected wave\n\\[\\tilde{\\vec{E_R}} = \\tilde{E}_{0, I} e^{i (-k_1 z - \\omega t)} \\vu x \\\\ \\tilde{\\vec{B_R}} = -\\frac{k_1}{\\omega} \\tilde{E}_{0,I} e^{i (-k_1 z - \\omega t)} \\vu y\\] and a transmitted wave\n\\[\\tilde{\\vec{E_T}} = \\tilde{E}_{0, T} e^{i (-k_1 z - \\omega t)} \\vu x \\\\ \\tilde{\\vec{B_T}} = -\\frac{k_2}{\\omega} \\tilde{E}_{0,T} e^{i (k_2 z - \\omega t)} \\vu y\\] Apply boundary condition at \\( z = 0 \\) and set the time phase factors equal to each other (as they must be)\n\\[\\tilde{E}_{0,I} \u0026#43; \\tilde{E}_{0,R} = \\tilde{E}_{0,T}\\] \\[\\frac{1}{\\mu_1 v_1} \\tilde{E}_{0,I} - \\frac{1}{\\mu_1 v_1} \\tilde{E}_{0,R} = \\frac{1}{\\mu_2} \\frac{\\tilde{k_2}}{\\omega} \\tilde{E}_{0,T}\\] \\[\\tilde{E}_{0,I} - \\tilde{E}_{0,R} = \\tilde{\\beta} \\tilde{E}_{0,T} \\qquad \\tilde{\\beta} = \\frac{\\mu_1 v_1}{\\mu_2 \\omega} \\tilde{k_2}\\] Manipulating these together,\n\\[\\tilde{E}_{0,T} = \\left( \\frac{2}{1 \u0026#43; \\tilde{\\beta}} \\right) \\tilde{E}_{0,I}\\] \\[\\tilde{E}_{0,R} = \\left( \\frac{1 - \\tilde{\\beta}}{1 \u0026#43; \\tilde{\\beta}} \\right) \\tilde{E}_{0,I}\\] For a good conductor (where \\( \\sigma / \\epsilon \\omega \\gg 1 \\)), \\( \\tilde{\\beta} \\) will approach\n\\[\\tilde{\\beta} \\rightarrow \\frac{\\mu_1 v_1}{\\mu_2 \\omega}\\sqrt{ \\frac{ \\omega \\sigma \\mu_2}{2} } (1 \u0026#43; i)\\] For \\( \\sigma \\rightarrow \\infty \\) (a perfect conductor),\n\\[\\tilde{E}_{0,R} \\rightarrow - \\tilde{E}_{0,I}\\] \\[\\tilde{E}_{0,T} \\rightarrow 0\\] Which is to say, a perfect conductor is a perfect reflector with a \\( 180^\\circ \\) phase change at the interface. The obvious applications are silvered mirrors and fully metal mirrors.\n9.4.3: The Frequency Dependence of Permittivity # For electromagnetic radiation, dispersion is a measure of the frequency response of a propagating wave on the permittivity, permeability, and conductivity of the medium it is propagating through.\n\\[\\text{Permittivity:} \\quad \\epsilon \\rightarrow \\epsilon(\\omega)\\] The majority of radiation that we interact with is not monochromatic, and every real radiation source has some non-zero linewidth. The spectrum of frequencies contained in the radiation will broaden and \u0026ldquo;disperse\u0026rdquo; depending on the dispersion relation of the material the wave propagates through. This makes it very important to consider the dispersion properties of whatever material a wave is propagating through, no matter what the source is.\nAs usual, we\u0026rsquo;ll start with the simplest case: the linear relation\n\\[\\text{Linear dispersion in free space: } \\quad \\omega = ck\\] Actually, many real media are have very nearly a linear dispersion relationship. Since the wave velocity depends on \\( \\epsilon \\) and \\( \\mu \\), we now have a velocity which is a function of frequency. In general, there are two velocities that we care about: the wave/phase velocity \\( v = \\omega / k \\) and the group velocity \\( v_g = \\dv{\\omega}{k} \\). We can think of the phase velocity as the velocity at which each individual sinusoidal component of a wave packet travels, while the group velocity defines the speed of the overall packet/envelope.\nWhat gives rise to the real relationship? Well, as a wave propagates through some medium, the atoms that make up the medium will have their own resonances depending on their state. In general, any mildly complex medium will have many many resonances, and the shape of the permittivity curve will be defined by the full composition of all of the resonances and can be quite complicated.\nBecause the mass of the electron is so much smaller than the mass of the nucleus, the electron responds as if it were tied to a central potential like a spring with damping.\n\\[m \\frac{d^2 x}{dt^2} \u0026#43; m \\gamma \\dv{x}{t} \u0026#43; m \\omega_0 ^2 x = q E_0 \\cos (\\omega t)\\] where \\( \\gamma \\) is a damping term. There are many potential mechanisms by which the system can lose energy, and we lump them all together into the simple damping term.\nSetting up shop back in our phasor space,\n\\[x \\equiv \\text{Re}\\left[ \\tilde{x} \\right]\\] \\[\\frac{d^2 \\tilde{x}}{dt^2} \u0026#43; \\gamma \\dv{\\tilde{x}}{t} \u0026#43; m \\omega_0 ^2 \\tilde{x} = \\frac{q E_0}{m} e^{-i \\omega t}\\] Plugging in the form for \\( \\tilde{x} \\) we know we\u0026rsquo;re going to get (a response with the same frequency as the driving field)\n\\[\\tilde{x} = \\tilde{x}_0 e^{-i \\omega t}\\] \\[ \\left[ - \\omega^2 - i \\gamma \\omega \u0026#43; \\omega_0 ^2 \\right] \\tilde{x} e^{-i \\omega t} = \\frac{q E_0}{m} e^{-i \\omega t}\\] \\[\\rightarrow \\tilde{x}_0 = \\frac{q E_0 / m}{(\\omega_0 ^2 - \\omega ^2 ) - i \\gamma \\omega } \\qquad x_0 = \\text{Re}[\\tilde{x}_0]\\] What\u0026rsquo;s the dipole moment of the system of the electron moving up and down with the wave? We will see that the dipole moment can be connected to the permittivity of the system through the polarization, and from there we\u0026rsquo;ll get our dispersion relation.\n\\[\\begin{aligned} \\tilde{p}(t) \u0026amp; = \u0026amp; q \\tilde{x}(t) \\\\ \u0026amp; = \u0026amp; \\frac{q^2 / m}{(\\omega_0 ^2 - \\omega ^2 ) - i \\gamma \\omega} E_0 e^{-i \\omega t} \\end{aligned}\\] If we assume we have N molecules per unit volume and multiple resonances \\( j : \\omega_j, \\gamma _j \\), and oscillator \u0026ldquo;strength\u0026rdquo; \\( f_j \\) (where we lump together the oscillation response of the resonance), the polarization P is\n\\[\\tilde{\\vec{P}}(t) = \\frac{N q^2}{m} \\left( \\sum_j \\frac{f_i}{(\\omega_j ^2 - \\omega ^2) - i \\gamma_j \\omega } \\right) \\tilde{\\vec{E}}\\] Recall the relation between polarization and susceptibility\n\\[\\vec{P} = \\epsilon_0 \\chi_e \\vec{E}\\] \\[\\vec{D} = \\epsilon \\vec{E} \\\\ = \\epsilon_0 (1 \u0026#43; \\chi_e) \\vec{E} \\\\ = \\epsilon_0 \\vec{E} \u0026#43; \\vec{P}\\] And we defined the relative permittivity\n\\[\\tilde{\\epsilon}_r = \\frac{\\tilde{\\epsilon}}{\\epsilon_0} \\\\ = 1 \u0026#43; \\tilde{\\chi}_e \\\\ = 1 \u0026#43; \\frac{N q^2}{m \\epsilon_0} \\left( \\sum_j \\frac{f_i}{(\\omega_j ^2 - \\omega ^2) - i \\gamma_j \\omega }\\right) \\\\ = n^2 \\text{ (for non-magnetic media)}\\] So that\u0026rsquo;s how the electron response to a wave can be connected with the index of refraction, but we\u0026rsquo;ve got a complex permittivity \u0026amp; dielectric constant. What does the generalized wave equation tell us about how this changes our solutions?\n\\[\\laplacian \\tilde{\\vec{E}} = \\tilde{\\epsilon} \\mu_0 \\frac{\\partial ^2 \\vec{E}}{\\partial t^2}\\] Again, insert a plane wave solution\n\\[\\tilde{\\vec{E}} = \\tilde{\\vec{E}}_0 e^{i(\\tilde{k} z - \\omega t)}\\] \\[\\rightarrow \\quad - \\tilde{k}^2 = \\tilde{\\epsilon} \\mu_0 (- \\omega ^2) \\] \\[\\rightarrow \\tilde{k}^2 = \\tilde{\\epsilon} \\mu_0 \\omega^2\\] and\n\\[\\tilde{k} = \\sqrt{\\tilde{\\epsilon} \\mu_0} \\omega = k \u0026#43; i \\kappa\\] When \\( n^2 - 1 \\) is small compared to 1, then we can expand \\( n \\) as\n\\[n = \\tilde{n} = 1 \u0026#43; \\frac{Nq^2}{2m\\epsilon_0} \\sum_j \\frac{f_j}{(\\omega _j ^2 - \\omega_0 ^2 ) - i \\gamma_j \\omega}\\] \\[\\tilde{\\vec{E}} = \\tilde{\\vec{E}}_0 e^{- \\kappa z} e^{i(kz - \\omega t)}\\] \u0026ldquo;\\( n^2 - 1 \\) is small compared to 1\u0026rdquo; just means the larger term on the RHS is small compared to 1, and this is true for many gaseous systems resulting in an index of refraction close to 1. We often compare the frequency response to the so-called absorption coefficient \\( \\alpha = 2 \\kappa \\), because the intensity is proportional to \\( E^2 \\) which goes as \\( e^{-2 \\kappa z} \\) so the characteristic width of the distribution is \\( 2 \\kappa \\). For gases (diffuse media),\n\\[\\alpha = 2 \\kappa \\approx \\frac{N q^2 \\omega^2}{m \\epsilon_0 c} \\sum_j \\frac{ f_j \\gamma _j}{(\\omega_j ^2 - \\omega ^2 ) ^2 \u0026#43; \\gamma _j ^2 \\omega ^2}\\] "},{"id":86,"href":"/r/notes/griffiths/ch9-5/","title":"Guided Waves","section":"Griffiths Introduction to Electrodynamics","content":" 9.5.1 Guided Waves # Moving beyond plane waves with infinite extent, now we consider waves confined to the interior to some sort of pipe, or wave guide. To make things simple, the overall geometry of the pipe should be large compared with the wavelength, and we\u0026rsquo;ll assume it\u0026rsquo;s made of a perfect conductor so there\u0026rsquo;s no loss (perfect reflection).\nMonochromatic plane wave solutions will look like\n\\[\\vec E(x, y, z, t) = \\vec{E_0} (x, y) e^{i(kz - \\omega t)}\\] \\[\\vec B(x, y, z, t) = \\vec{B_0} (x, y) e^{i(kz - \\omega t)}\\] The (source-free) boundary conditions within the waveguide are\n\\[\\begin{aligned} E^\\parallel \u0026amp; = \u0026amp; 0 \\\\ B_\\perp \u0026amp; = \u0026amp; 0 \\\\ \\div \\vec E \u0026amp; = \u0026amp; 0 \\\\ \\curl \\vec E \u0026amp; = \u0026amp; - \\pdv{\\vec{B}}{t} \\\\ \\div \\vec B \u0026amp; = \u0026amp; 0 \\\\ \\curl \\vec B \u0026amp; = \u0026amp; \\frac{1}{c^2} \\pdv{\\vec E}{t} \\end{aligned}\\] So it turns out that within a waveguide we aren\u0026rsquo;t necessarily limited to transverse solutions only. We include the longitudinal components of the fields when plugging in the coordinates into our boundary conditions and generic solutions:\n\\[\\vec E_0 = E_x \\vu x \u0026#43; E_y \\vu y \u0026#43; E_z \\vu z\\] \\[\\vec B_0 = B_x \\vu x \u0026#43; B_y \\vu y \u0026#43; B_z \\vu z\\] Using the Maxwell equations and boundary conditions, eventually we can get independent expressions for the fields\n\\[\\left[ \\frac{\\partial^2}{\\partial x^2} \u0026#43; \\frac{\\partial^2}{\\partial y^2} \u0026#43; \\left( \\frac{\\omega}{c} \\right) ^2 - k^2 \\right] E_z = 0\\] \\[\\left[ \\frac{\\partial^2}{\\partial x^2} \u0026#43; \\frac{\\partial^2}{\\partial y^2} \u0026#43; \\left( \\frac{\\omega}{c} \\right) ^2 - k^2 \\right] B_z = 0\\] For trivial solutions, we can come up with separate classes of solutions: If \\( E_z = 0 \\), we call the solutions transverse electric (TE) waves. If \\( B_z = 0 \\) they are called transverse magnetic (TM) waves. And if both are zero, we call them TEM waves, but it turns out that TEM waves can\u0026rsquo;t occur in a hollow wave guide, since\n\\[E_z = 0 \\rightarrow \\pdv{E_x}{x} \u0026#43; \\pdv{E_y}{y} = 0\\] and\n\\[B_z = 0 \\rightarrow \\pdv{E_y}{x} - \\pdv{E_x}{y} = 0\\] imply that \\( \\vec E_0 \\) can be written as the gradient of a scalar potential that satisfies Laplace\u0026rsquo;s equation. But the boundary condition on \\( \\vec E \\) requires that hte surface is an equipotential, so the only available potential is a constant and the field is zero everywhere, so no wave exists at all.\n9.5.2 Rectangular Wave Guide # Let\u0026rsquo;s look at a rectangular wave guide with height \\( a \\) and width \\( b \\). As we saw, there are two chief classes of solutions, TE (\\( E_z = 0 \\)) and TM (\\( B_z = 0 \\)). Let\u0026rsquo;s specifically take a look at the TM solutions (the process is very similar for TE waves, except that the boundary conditions are flipped).\nTM solutions:\n\\[B_z = 0 \\text{ everywhere }\\] \\[\\left[ \\frac{\\partial^2}{\\partial x^2} \u0026#43; \\frac{\\partial^2}{\\partial y^2} \u0026#43; \\left( \\frac{\\omega}{c} \\right) ^2 - k^2 \\right] E_z = 0\\] We\u0026rsquo;ve got a relatively simple situation here (really just a 2D Laplacian and constants), so start with the method of separation of variables.\n\\[E_z(x, y) = X(x) Y(y)\\] So plugging in:\n\\[Y \\dv{^2 X}{x^2} \u0026#43; X \\dv{^2 Y}{y^2} \u0026#43; \\left[(\\omega / x)^2 - k^2 \\right]XY = 0\\] Separate out the X and Y terms\n\\[\\frac{1}{X} \\dv{^2X}{x^2} = -k_x ^2\\] \\[\\frac{1}{Y} \\dv{^2Y}{y^2} = -k_y ^2\\] where\n\\[-k_x ^2 -k_y ^2 \u0026#43; (\\omega / c)^2 - k^2 = 0\\] We already know the general solutions\n\\[X(x) = A \\sin (k_x x) \u0026#43; B \\cos (k_x x)\\] The boundary conditions require that \\( E_z = 0 \\) at \\( x = 0 \\) and at \\( x = a \\), so\n\\[X(0) = B \\cos k_x (0) = B = 0 \\\\ \\rightarrow X(a) = A \\sin k_x a = 0 \\\\ \\rightarrow k_x a = m \\pi \\\\ \\rightarrow k_x = \\frac{m \\pi}{a}\\] Similarly,\n\\[k_y = \\frac{n \\pi}{b} \\\\ Y(y) = A \\sin k_y y\\] So the TM modes are\n\\[TM_{mn}: \\quad E_z (x, y) = E_0 \\sin \\left( \\frac{m \\pi x}{a} \\right) \\sin \\left( \\frac{n \\pi y}{b} \\right)\\] Note that if \\( m = 0 \\) or \\( n = 0 \\), then \\( E_z \\) becomes zero immediately, so the \u0026ldquo;bottom-most\u0026rdquo; term is \\( m = n = 1 \\). How do \\( m \\) and \\( n \\) relate to the wavenumber and frequency of the modes? We got our separable wavenumbers from\n\\[k_x ^2 \u0026#43; k_y ^2 \u0026#43; k^2 = \\left( \\frac{\\omega}{c} \\right) ^2 \\\\ \\rightarrow \\left( \\frac{m \\pi}{a} \\right) ^2 \u0026#43; \\left( \\frac{n \\pi}{b} \\right) ^2 \u0026#43; k^2 = \\left( \\frac{\\omega}{c} \\right) ^2 \\\\ \\rightarrow \\omega = c \\sqrt{ \\left( \\frac{m \\pi}{a} \\right) ^2 \u0026#43; \\left( \\frac{n \\pi}{b} \\right) ^2 \u0026#43; k^2 }\\] This is a sort of weird dispersion relation. We get a minimum \\( \\omega \\) as \\( k \\rightarrow 0 \\), since \\( k^2 \u0026lt; 0 \\) would give an attenuated wave which we don\u0026rsquo;t care about, so there is a cutoff frequency\n\\[\\omega_{cutoff} = \\omega_{k \\rightarrow 0} \\rightarrow c \\sqrt{ \\pi^2 (\\frac{1}{a^2} \u0026#43; \\frac{1}{b^2})}\\] This means that TM modes can not propagate with frequencies below \\( c \\sqrt{ \\pi^2 (\\frac{1}{a^2} + \\frac{1}{b^2})} \\).\nOn the flip-side, what do the group and phase velocities look like?\n\\[k = \\sqrt{ \\left( \\frac{\\omega}{c} \\right) ^2 - \\left( \\frac{m \\pi}{a} \\right) ^2 - \\left( \\frac{n \\pi}{b} \\right) ^2 } \\\\ = \\sqrt{ \\left( \\frac{\\omega}{c} \\right) ^2 - \\left( \\frac{\\omega_{mn}}{c} \\right) ^2} \\\\ \\omega_{mn} = c\\sqrt{\\left( \\frac{m \\pi}{a} \\right) ^2 \u0026#43; \\left( \\frac{n \\pi}{b} \\right) ^2 } \\\\ \\rightarrow k = \\frac{\\sqrt{\\omega^2 - \\omega_{mn}^2}}{c} \\\\ \\rightarrow k = \\frac{\\omega \\sqrt{1 - \\omega_{mn}^2 / \\omega^2}}{c} \\\\ \\rightarrow \\frac{\\omega}{k} = v_p = \\frac{c}{\\sqrt{1 - \\omega_{mn}^2 / \\omega^2}}\\] \\[v_g = \\dv{\\omega}{k} \\\\ = \\frac{1}{\\dv{k}{\\omega}} = \\frac{1}{\\frac{1}{c} \\frac{2 \\omega}{2 \\sqrt{\\omega^2 - \\omega_{mn}^2}}} \\\\ = \\frac{c \\sqrt{\\omega^2 - \\omega_{mn}^2}}{\\omega} \\\\ v_g = c \\sqrt{1 - \\frac{\\omega_{mn}^2}{\\omega^2}}\\] What about the \\( TE_{mn} \\) modes?\n\\[B_z(x, y) = X(x) Y(y)\\] \\[\\frac{1}{x} \\dv{^2 X}{x^2} = - k_x ^2 \\\\ \\rightarrow X(x) = A \\sin k_x x \u0026#43; B \\cos k_x x\\] Now, the boundary conditions say that\n\\[B^ \\perp = 0\\] \\[\\pdv{B_x}{x} = 0 \\text{ at } x = 0 \\text{ and } x = a\\] \\[\\rightarrow A k_x \\cos k_x x - B k_x \\sin k_x x = 0 \\\\ \\rightarrow A = 0\\] Plugging in \\( x = a \\),\n\\[Bk_x \\sin k_x a = 0 \\\\ \\rightarrow k_x a = m \\pi\\] \\[\\rightarrow B_z(x, y) = B_0 \\cos \\left( \\frac{m \\pi x}{a} \\right) \\cos \\left( \\frac{n \\pi y}{b} \\right)\\] \\[k = \\sqrt{(\\omega / c)^2 - \\pi ^2[(m / a)^2 \u0026#43; (n / b)^2] }\\] If\n\\[\\omega \u0026lt; c \\pi \\sqrt{(m / a)^2 \u0026#43; (n / b)^2} \\equiv \\omega_{c, mn}\\] then the wave number is imaginary, and instead of a traveling wave we have exponentially attenuated fields, so this is our cutoff frequency for TE waves. Given the convention of always associating the first index with the larger dimension (assume \\( a \\geq b \\)), the lowest frequency for a given wave guide is\n\\[\\omega_{1,0} = c \\pi / a\\] Example 9.1 # Q Suppose we have a rectangular waveguide of dimensions 1cm by 2cm.\n(a) What is the lowest mode? Find the cutoff frequency.\n(b) If the waveguide were filled with lossless plastic with \\( \\epsilon_r = 2\\), how would the cutoff frequency change?\nA (a) What is the lowest allowed mode? We should check the lowest TE and TM modes to find out!\nThe lowest TM mode is\n\\[\\omega_{c, TM} = c \\sqrt{\\pi^2 \\left( \\frac{1}{a^2} \u0026#43; \\frac{1}{b^2} \\right) }\\] and the lowest TE mode is\n\\[\\omega_{1, 0} = c \\frac{\\pi}{2 \\text{cm}} \\\\ \\rightarrow f_{co} = \\frac{c}{4 \\text{cm}} = \\frac{3 \\times 10^{10} \\text{cm/s}}{4} = 7.5 \\text{GHz}\\] It\u0026rsquo;s worth noting that the lowest TM mode will always be higher, since it requires both \\( m \\geq 1 \\) and \\( n \\geq 1 \\), so the lowest frequency will be the TE mode.\n(b) For vacuum,\n\\[\\frac{1}{c^2} = \\mu_0 \\epsilon_0\\] In plastic,\n\\[v = \\frac{c}{\\sqrt{\\epsilon / \\epsilon_0}} = \\frac{c}{\\sqrt{2}}\\\\ \\rightarrow f_{co} = \\frac{7.5 \\text{GHz}}{\\sqrt{2}} \\approx 5.4\\text{GHz}\\] !!! question \u0026quot;\u0026quot;\n"},{"id":87,"href":"/r/notes/UWAA545/formulary/","title":"Formulary","section":"Computational Methods For Plasmas","content":" \\( \\) Formulary # Kinetic Description\n\\[\\dv{\\vec v}{t} = \\frac{q_i}{m_i} (\\vec E \u0026#43; \\vec v_i \\cross \\vec B) \u0026#43; \\sum_{j \\neq i} \\left[ \\left. \\dv{\\vec v_{ij}}{t} \\right|_{coll} \\delta(\\vec r_i - \\vec r_j) \\right]\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\frac{1}{c^2} \\pdv{\\vec E}{t} = \\curl \\vec B - \\mu_0 \\sum_i q_i \\vec v_i \\delta (\\vec r - \\vec r_i)\\] \\[\\div \\vec{B} = 0\\] \\[\\div \\vec E = \\frac{1}{\\epsilon_0} \\sum_i q_i \\delta (\\vec r - \\vec r_i)\\] Klimontovich equation:\n\\[\\dv{N}{t} = 0 = \\pdv{N}{t} \u0026#43; \\pdv{}{q_i} \\cdot (\\dot{q_i} N) \\\\ N \\equiv \\sum_i \\delta (p - p_i) \\delta(q - q_i)\\] Plasma Fluid Description\nBoltzmann Equation\n\\[\\pdv{f_\\alpha}{t} \u0026#43; \\vec v \\cdot \\pdv{f_\\alpha}{t} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} = \\left. \\pdv{f_\\alpha}{t} \\right|_{coll} = \\sum_{\\beta \\neq \\alpha} C_{\\alpha \\beta}\\] Maxwellian distribution:\n\\[f_\\alpha (\\vec v) = n_\\alpha \\left( \\frac{m_\\alpha}{2 \\pi T} \\right)^{3/2} e^{- \\frac{m_\\alpha(\\vec v - \\vec v_\\alpha)^2}{2T}}\\] Moments of fluid model (moments of distribution \\( \\rightarrow \\) moments of Boltzmann equation:\n\\[\\text{Continuity:} \\qquad n_\\alpha = \\int f_\\alpha \\dd \\vec v \\\\ \\quad \\rightarrow \\pdv{n_\\alpha}{t} \u0026#43; \\div (n_\\alpha \\vec v_\\alpha) = 0\\] \\[\\text{Momentum:} \\qquad n_\\alpha \\vec v_\\alpha = \\int \\vec v f_\\alpha \\dd v \\\\ \\quad \\rightarrow \\quad \\pdv{}{t} (n_\\alpha \\vec v_\\alpha ) \u0026#43; \\div (n_\\alpha \\vec v_\\alpha \\vec v_\\alpha) \u0026#43; \\frac{1}{m_\\alpha} \\div \\vec P_\\alpha - \\frac{q_\\alpha}{m_\\alpha} n_\\alpha ( \\vec E \u0026#43; \\vec v _\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v\\] \\[\\rightarrow \\rho_\\alpha \\left(\\pdv{\\vec v_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad \\vec v_\\alpha \\right) \u0026#43; \\grad \\vec P_\\alpha \u0026#43; \\div \\vec \\Pi_\\alpha - q_\\alpha n_\\alpha (\\vec E \u0026#43; \\vec v_\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\vec R_{\\alpha \\beta}\\] \\[\\text{Energy:} \\qquad \\int \\vec v \\vec v \\pdv{f_\\alpha}{t} \\dd \\vec v = \\pdv{}{t} \\int \\vec v \\vec v f_\\alpha \\dd \\vec v = \\pdv{}{t} \\vec E_\\alpha / m_\\alpha \\rightarrow \\pdv{}{t} \\vec P_\\alpha\\] \\[\\rightarrow \\quad \\frac{3}{2} n_\\alpha \\left( \\pdv{T_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad T_\\alpha \\right) \u0026#43; P_\\alpha \\div \\vec v_\\alpha \u0026#43; \\vec \\Pi_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha \u0026#43; \\div \\vec h_\\alpha = \\sum_{\\beta \\neq \\alpha} Q_{\\alpha \\beta}\\] Closure relations\n\\[\\vec h_\\alpha = - \\kappa \\grad T_\\alpha\\] \\[\\overline \\Pi_ \\alpha = \\nu \\grad \\vec v_\\alpha\\] Ideal MHD # Ideal MHD\nContinuity:\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0\\] Momentum:\n\\[\\rho \\left( \\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad p - \\vec j \\cross \\vec B = 0\\] Generalized Ohm\u0026rsquo;s Law\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = \\frac{1}{Zen}\\cancel{(\\vec j \\cross \\vec B - \\grad p_e)} = 0\\] Energy\n\\[\\dv{}{t} \\left( \\frac{p}{\\rho^\\gamma} \\right) = 0\\] \u0026ldquo;Lawson Criterion\u0026rdquo;\n\\[n \\tau_E \u0026gt; 10^4 s/m^3\\] Conservation Law Form of Ideal MHD\nContinuity:\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0\\] Momentum:\n\\[\\pdv{(\\rho \\vec v)}{t} \u0026#43; \\div \\left[ \\rho \\vec v \\vec v - \\frac{\\vec B \\vec B}{\\mu_0} \u0026#43; \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\overline{I} \\right] = 0\\] Energy:\n\\[\\pdv{\\epsilon}{t} \u0026#43; \\div \\left[ \\left( \\epsilon \u0026#43; p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\vec v - (\\vec B \\cdot \\vec v) \\frac{\\vec B}{\\mu_0} \\right] = 0\\] \\[\\pdv{\\vec B}{t} \u0026#43; \\div ( \\vec v \\vec B - \\vec B \\vec v) = 0\\] where\n\\[\\epsilon = \\frac{1}{\\gamma - 1} p \u0026#43; \\frac{1}{2} \\rho v^2 \u0026#43; \\frac{B^2}{2\\mu_0}\\] Static Equilibrium:\n\\[\\vec j \\cross \\vec B = \\grad p\\] \\[\\frac{B^2}{\\mu_0} \\vec K = \\grad_\\perp (p \u0026#43; \\frac{B^2}{2 \\mu_0})\\] \\[\\vec K \\equiv \\frac{\\vec B}{|B|} \\cdot \\grad \\frac{ \\vec B}{|B|}\\] Conservation of flux:\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = 0\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\rightarrow \\dv{}{t} \\left( \\frac{\\vec B}{\\rho} \\right) = \\frac{\\vec B}{\\rho} \\cdot \\grad \\vec v\\] 1D Equilibria # \\( \\theta \\)-pinch\n\\[B_\\theta = 0\\] \\[j_\\theta B_z = \\dv{p}{r}\\] \\[j_\\theta = - \\frac{1}{\\mu_0} \\dv{B_z}{r}\\] \\[\\rightarrow p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} = \\frac{B_0 ^2}{2 \\mu_0}\\] \\[\\langle \\beta \\rangle = \\frac{2}{a^2} \\int_0 ^a \\frac{r p}{B_0 ^2 / 2 \\mu_0} \\dd r\\] \\[q = \\infty\\] \\[W = \\frac{\\mu_0 r}{B_z ^2} \\dv{}{r} \\left( p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} \\right) = 0\\] Z-pinch\n\\[B_z =0\\] \\[\\grad p = \\dv{p}{r} = - j_z B_\\theta\\] \\[- \\dv{}{r} \\left( p \u0026#43; \\frac{B_\\theta ^2}{2 \\mu_0} \\right) = \\frac{B_\\theta ^2}{\\mu_0 r}\\] \\[\\langle \\beta \\rangle = \\frac{2 \\mu_0}{B_0 ^2 \\pi a^2} \\int _0 ^a 2 \\pi r p \\dd r = 1 \\quad \\text{ if } \\quad p(a) = 0\\] \\[q = S = 0\\] \\[W = 1\\] Screw pinch\n\\[\\dv{}{r} \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) = - \\frac{B_\\theta ^2}{\\mu_0 r}\\] \\[\\beta_t = \\frac{2 \\mu_0}{B_z (a) ^2} \\left( \\frac{1}{\\pi a^2} \\int_0 ^a 2 \\pi r p \\dd r \\right)\\] \\[\\beta_p = \\left( 1 - \\frac{\\alpha_t}{\\beta _t} \\right)^{-1} \\qquad \\alpha_t \\equiv \\frac{2}{a^2} \\int_0 ^a \\left(1 - \\frac{B_z ^2}{B_0 ^2} \\right) r \\dd r\\] \\[q = \\frac{2 \\pi r B_z}{L B_\\theta}\\] \\[q_a = \\frac{4 \\pi ^2 a^2 B_0}{\\mu_0 I_a}\\] \\[S = \\frac{r}{q} \\dv{q}{r}\\] \\[W = - \\frac{B_\\theta ^2}{B_\\theta ^2 \u0026#43; B_z ^2}\\] Stability\nShear:\n\\[S = 2 \\frac{ dq / q}{dV / V} = 2 \\frac{d \\ln q}{d \\ln V}\\] \\[q = \\frac{\\text{\\# long windings}}{\\text{\\# short windings}} = \\dv{\\psi_t}{\\psi_p}\\] Shear for toroid\n\\[q = \\frac{r B_\\phi}{R B_\\theta}\\] Shear for cylinder\n\\[q = 2 \\pi \\frac{r B_z}{L B_\\theta}\\] Well\n\\[W = \\frac{ d \\langle p \u0026#43; B^2 / 2 \\mu_0 \\rangle / \\langle B^2 / 2 \\mu_0 \\rangle}{dV / V}\\] For stabilization, \\( B^2 / 2 \\mu_0 \\) should increase faster than \\( p \\) decreases\n2D Equilibria # Grad-Shafranov Equation: Static toroidal equilibrium\n\\[\\grad p = \\vec j_\\theta \\cross \\vec B_\\phi \u0026#43; \\vec j_\\phi \\cross \\vec B_\\theta\\] \\[A_\\phi = \\frac{\\phi}{R} \\vu \\phi\\] \\[\\phi = \\frac{\\phi_p}{2 \\pi}\\] \\[\\vec B_\\theta = - \\frac{\\vu R}{R} \\pdv{\\psi}{z} \u0026#43; \\frac{ \\vu z}{R} \\pdv{\\psi}{R} = \\frac{ \\grad \\psi}{R} \\cross \\vu \\phi\\] \\[F \\equiv R B_\\phi\\] \\[\\Delta ^\\star \\equiv R \\pdv{}{R} \\frac{1}{R} \\pdv{}{R} \u0026#43; \\pdv{^2}{z^2}\\] \\[\\Delta ^\\star \\psi = \\pdv{^2 \\psi}{z^2} \u0026#43; \\pdv{^2 \\psi}{R^2} - \\frac{1}{R} \\pdv{\\psi}{R}\\] \\[\\vec j_\\phi = - \\frac{1}{\\mu_0 R} \\Delta ^\\star \\psi \\vu \\phi\\] \\[\\vec j_\\theta = \\frac{1}{\\mu_0 R} \\grad (F) \\cross \\vu \\phi\\] \\[R^2 \\mu_0 \\pdv{p}{\\psi} = - \\Delta ^\\star \\psi - F \\pdv{F}{\\psi}\\] \\[q(\\psi) = \\frac{F(\\psi)}{2 \\pi} \\oint_{p} \\frac{r \\dd \\theta}{R^2 B_\\theta}\\] Limits:\n\\[\\text{Force-free:} \\qquad \\vec j \\parallel \\vec B\\] \\[\\rightarrow \\Delta ^\\star \\psi \u0026#43; F F\u0026#39; = 0\\] \\[\\text{Connected $\\theta$ pinch:} \\qquad FF\u0026#39; \\gg \\Delta ^\\star \\psi\\] \\[\\rightarrow \\grad p \\approx \\vec j_\\theta \\cross \\vec B_\\phi\\] \\[\\text{Connected Z-pinch:} \\qquad FF\u0026#39; \\ll \\Delta ^\\star \\phi\\] \\[\\rightarrow \\grad p \\approx j_\\phi \\cross B_\\theta\\] MHD Stability # Linear stability\n\\[\\pdv{\\rho_1}{t} = - \\vec v_1 \\grad \\rho_0 - \\rho_0 \\div \\vec v_1\\] \\[\\pdv{\\vec B_1}{t} = \\curl ( \\vec v_1 \\cross \\vec B_0)\\] \\[\\rho_0 \\pdv{\\vec v_1}{t} = - \\grad p_1 \u0026#43; \\vec j_0 \\cross \\vec B_1 - \\vec j_1 \\cross \\vec B_0\\] \\[\\pdv{p_1}{t} = - \\vec v_1 \\cdot \\grad p_0 - \\gamma p_0 \\div \\vec v_1\\] For linear perturbation \\( \\vec \\xi = \\int_0 ^t \\vec v_1 \\dd t \\) the momentum equation becomes\n\\[\\rho_0 \\pdv{^2 \\xi}{t^2} = \\vec F(\\xi)\\] where\n\\[F(\\xi) = \\grad (\\xi \\cdot \\grad p_0 \u0026#43; \\gamma p_0 \\div \\xi) \u0026#43; \\frac{1}{\\mu_0} \\left[ ( \\curl \\vec B_0) \\cross \\curl (\\xi \\cross \\vec B_0) \u0026#43; \\curl \\curl (\\xi \\cross \\vec B_0) \\cross \\vec B_0 \\right]\\] Eigenvalues of \\( \\frac{1}{\\rho_0} \\vec F (\\xi) = \\omega^2 \\xi \\) are real and ordered. Only need to check \\( n=0 \\) to determine stability/instability of configuration.\n\\( \\delta W \\) Approach\n\\( \\delta W = \\) change in potential energy due to a displacement \\( \\xi \\)\n\\[\\delta W \u0026lt; 0 \\rightarrow \\text{instability}\\] \\[\\delta W = - \\frac{1}{2} \\int \\xi \\cdot F(\\xi) \\dd V = \\delta W_F \u0026#43; \\delta W_S \u0026#43; \\delta W_V\\] Surface term:\n\\[\\delta W_s = \\frac{1}{2} \\oint \\dd S (\\vu n \\cdot \\xi) ^2 \\left( \\vu n \\cdot \\grad p_0 \u0026#43; \\left[ \\vu n \\cdot \\grad \\frac{B_0 ^2}{2 \\mu_0} \\right]_{jump} \\right)\\] Vacuum term:\n\\[\\delta W_V = \\int_{vac} \\dd V \\frac{B_1 ^2}{\\mu_0}\\] Plasma (free) term:\n\\[\\delta W_F = \\frac{1}{2} \\int \\dd V \\frac{ |B_{1, \\perp}|^2}{\\mu_0} \\quad \\leftarrow \\text{Shear Alfven} \\\\ \u0026#43; \\mu_0 \\left| \\frac{B_{1, \\parallel}}{\\mu_0} - \\frac{B_0 \\xi \\cdot \\grad p_0}{B_0} ^2 \\right|^2 \\quad \\leftarrow \\text{Fast magnetosonic} \\\\ \u0026#43; \\Gamma p_0 |\\div \\xi|^2 \\quad \\leftarrow \\text{Acoustic}\\\\ \u0026#43; \\frac{\\vec j_0 \\cdot \\vec B_0}{B_0 ^2} (\\vec B_0 \\cross \\vec \\xi) \\cdot \\vec B_1 \\quad \\leftarrow \\text{Current-driven (kink)} \\\\ - 2 ( \\vec \\xi \\cdot \\grad p_0)(\\vec \\xi \\cdot \\vec \\kappa) \\quad \\leftarrow \\text{pressure-driven (interchange/balooning)}\\] Shear Alfven, fast magnetosonic, and acoustic modes are stabilizing. Current-driven and pressure-driven modes can lead to instability.\n"},{"id":88,"href":"/r/notes/UWAA558/formulary/","title":"Formulary","section":"MHD Theory","content":" \\( \\) Formulary # Kinetic Description\n\\[\\dv{\\vec v}{t} = \\frac{q_i}{m_i} (\\vec E \u0026#43; \\vec v_i \\cross \\vec B) \u0026#43; \\sum_{j \\neq i} \\left[ \\left. \\dv{\\vec v_{ij}}{t} \\right|_{coll} \\delta(\\vec r_i - \\vec r_j) \\right]\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\frac{1}{c^2} \\pdv{\\vec E}{t} = \\curl \\vec B - \\mu_0 \\sum_i q_i \\vec v_i \\delta (\\vec r - \\vec r_i)\\] \\[\\div \\vec{B} = 0\\] \\[\\div \\vec E = \\frac{1}{\\epsilon_0} \\sum_i q_i \\delta (\\vec r - \\vec r_i)\\] Klimontovich equation:\n\\[\\dv{N}{t} = 0 = \\pdv{N}{t} \u0026#43; \\pdv{}{q_i} \\cdot (\\dot{q_i} N) \\\\ N \\equiv \\sum_i \\delta (p - p_i) \\delta(q - q_i)\\] Plasma Fluid Description\nBoltzmann Equation\n\\[\\pdv{f_\\alpha}{t} \u0026#43; \\vec v \\cdot \\pdv{f_\\alpha}{t} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} = \\left. \\pdv{f_\\alpha}{t} \\right|_{coll} = \\sum_{\\beta \\neq \\alpha} C_{\\alpha \\beta}\\] Maxwellian distribution:\n\\[f_\\alpha (\\vec v) = n_\\alpha \\left( \\frac{m_\\alpha}{2 \\pi T} \\right)^{3/2} e^{- \\frac{m_\\alpha(\\vec v - \\vec v_\\alpha)^2}{2T}}\\] Moments of fluid model (moments of distribution \\( \\rightarrow \\) moments of Boltzmann equation:\n\\[\\text{Continuity:} \\qquad n_\\alpha = \\int f_\\alpha \\dd \\vec v \\\\ \\quad \\rightarrow \\pdv{n_\\alpha}{t} \u0026#43; \\div (n_\\alpha \\vec v_\\alpha) = 0\\] \\[\\text{Momentum:} \\qquad n_\\alpha \\vec v_\\alpha = \\int \\vec v f_\\alpha \\dd v \\\\ \\quad \\rightarrow \\quad \\pdv{}{t} (n_\\alpha \\vec v_\\alpha ) \u0026#43; \\div (n_\\alpha \\vec v_\\alpha \\vec v_\\alpha) \u0026#43; \\frac{1}{m_\\alpha} \\div \\vec P_\\alpha - \\frac{q_\\alpha}{m_\\alpha} n_\\alpha ( \\vec E \u0026#43; \\vec v _\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v\\] \\[\\rightarrow \\rho_\\alpha \\left(\\pdv{\\vec v_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad \\vec v_\\alpha \\right) \u0026#43; \\grad \\vec P_\\alpha \u0026#43; \\div \\vec \\Pi_\\alpha - q_\\alpha n_\\alpha (\\vec E \u0026#43; \\vec v_\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\vec R_{\\alpha \\beta}\\] \\[\\text{Energy:} \\qquad \\int \\vec v \\vec v \\pdv{f_\\alpha}{t} \\dd \\vec v = \\pdv{}{t} \\int \\vec v \\vec v f_\\alpha \\dd \\vec v = \\pdv{}{t} \\vec E_\\alpha / m_\\alpha \\rightarrow \\pdv{}{t} \\vec P_\\alpha\\] \\[\\rightarrow \\quad \\frac{3}{2} n_\\alpha \\left( \\pdv{T_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad T_\\alpha \\right) \u0026#43; P_\\alpha \\div \\vec v_\\alpha \u0026#43; \\vec \\Pi_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha \u0026#43; \\div \\vec h_\\alpha = \\sum_{\\beta \\neq \\alpha} Q_{\\alpha \\beta}\\] Closure relations\n\\[\\vec h_\\alpha = - \\kappa \\grad T_\\alpha\\] \\[\\overline \\Pi_ \\alpha = \\nu \\grad \\vec v_\\alpha\\] Ideal MHD # Ideal MHD\nContinuity:\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0\\] Momentum:\n\\[\\rho \\left( \\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad p - \\vec j \\cross \\vec B = 0\\] Generalized Ohm\u0026rsquo;s Law\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = \\frac{1}{Zen}\\cancel{(\\vec j \\cross \\vec B - \\grad p_e)} = 0\\] Energy\n\\[\\dv{}{t} \\left( \\frac{p}{\\rho^\\gamma} \\right) = 0\\] \u0026ldquo;Lawson Criterion\u0026rdquo;\n\\[n \\tau_E \u0026gt; 10^4 s/m^3\\] Conservation Law Form of Ideal MHD\nContinuity:\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0\\] Momentum:\n\\[\\pdv{(\\rho \\vec v)}{t} \u0026#43; \\div \\left[ \\rho \\vec v \\vec v - \\frac{\\vec B \\vec B}{\\mu_0} \u0026#43; \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\overline{I} \\right] = 0\\] Energy:\n\\[\\pdv{\\epsilon}{t} \u0026#43; \\div \\left[ \\left( \\epsilon \u0026#43; p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\vec v - (\\vec B \\cdot \\vec v) \\frac{\\vec B}{\\mu_0} \\right] = 0\\] \\[\\pdv{\\vec B}{t} \u0026#43; \\div ( \\vec v \\vec B - \\vec B \\vec v) = 0\\] where\n\\[\\epsilon = \\frac{1}{\\gamma - 1} p \u0026#43; \\frac{1}{2} \\rho v^2 \u0026#43; \\frac{B^2}{2\\mu_0}\\] Static Equilibrium:\n\\[\\vec j \\cross \\vec B = \\grad p\\] \\[\\frac{B^2}{\\mu_0} \\vec K = \\grad_\\perp (p \u0026#43; \\frac{B^2}{2 \\mu_0})\\] \\[\\vec K \\equiv \\frac{\\vec B}{|B|} \\cdot \\grad \\frac{ \\vec B}{|B|}\\] Conservation of flux:\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = 0\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\rightarrow \\dv{}{t} \\left( \\frac{\\vec B}{\\rho} \\right) = \\frac{\\vec B}{\\rho} \\cdot \\grad \\vec v\\] 1D Equilibria # \\( \\theta \\)-pinch\n\\[B_\\theta = 0\\] \\[j_\\theta B_z = \\dv{p}{r}\\] \\[j_\\theta = - \\frac{1}{\\mu_0} \\dv{B_z}{r}\\] \\[\\rightarrow p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} = \\frac{B_0 ^2}{2 \\mu_0}\\] \\[\\langle \\beta \\rangle = \\frac{2}{a^2} \\int_0 ^a \\frac{r p}{B_0 ^2 / 2 \\mu_0} \\dd r\\] \\[q = \\infty\\] \\[W = \\frac{\\mu_0 r}{B_z ^2} \\dv{}{r} \\left( p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} \\right) = 0\\] Z-pinch\n\\[B_z =0\\] \\[\\grad p = \\dv{p}{r} = - j_z B_\\theta\\] \\[- \\dv{}{r} \\left( p \u0026#43; \\frac{B_\\theta ^2}{2 \\mu_0} \\right) = \\frac{B_\\theta ^2}{\\mu_0 r}\\] \\[\\langle \\beta \\rangle = \\frac{2 \\mu_0}{B_0 ^2 \\pi a^2} \\int _0 ^a 2 \\pi r p \\dd r = 1 \\quad \\text{ if } \\quad p(a) = 0\\] \\[q = S = 0\\] \\[W = 1\\] Screw pinch\n\\[\\dv{}{r} \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) = - \\frac{B_\\theta ^2}{\\mu_0 r}\\] \\[\\beta_t = \\frac{2 \\mu_0}{B_z (a) ^2} \\left( \\frac{1}{\\pi a^2} \\int_0 ^a 2 \\pi r p \\dd r \\right)\\] \\[\\beta_p = \\left( 1 - \\frac{\\alpha_t}{\\beta _t} \\right)^{-1} \\qquad \\alpha_t \\equiv \\frac{2}{a^2} \\int_0 ^a \\left(1 - \\frac{B_z ^2}{B_0 ^2} \\right) r \\dd r\\] \\[q = \\frac{2 \\pi r B_z}{L B_\\theta}\\] \\[q_a = \\frac{4 \\pi ^2 a^2 B_0}{\\mu_0 I_a}\\] \\[S = \\frac{r}{q} \\dv{q}{r}\\] \\[W = - \\frac{B_\\theta ^2}{B_\\theta ^2 \u0026#43; B_z ^2}\\] Stability\nShear:\n\\[S = 2 \\frac{ dq / q}{dV / V} = 2 \\frac{d \\ln q}{d \\ln V}\\] \\[q = \\frac{\\text{\\# long windings}}{\\text{\\# short windings}} = \\dv{\\psi_t}{\\psi_p}\\] Shear for toroid\n\\[q = \\frac{r B_\\phi}{R B_\\theta}\\] Shear for cylinder\n\\[q = 2 \\pi \\frac{r B_z}{L B_\\theta}\\] Well\n\\[W = \\frac{ d \\langle p \u0026#43; B^2 / 2 \\mu_0 \\rangle / \\langle B^2 / 2 \\mu_0 \\rangle}{dV / V}\\] For stabilization, \\( B^2 / 2 \\mu_0 \\) should increase faster than \\( p \\) decreases\n2D Equilibria # Grad-Shafranov Equation: Static toroidal equilibrium\n\\[\\grad p = \\vec j_\\theta \\cross \\vec B_\\phi \u0026#43; \\vec j_\\phi \\cross \\vec B_\\theta\\] \\[A_\\phi = \\frac{\\phi}{R} \\vu \\phi\\] \\[\\phi = \\frac{\\phi_p}{2 \\pi}\\] \\[\\vec B_\\theta = - \\frac{\\vu R}{R} \\pdv{\\psi}{z} \u0026#43; \\frac{ \\vu z}{R} \\pdv{\\psi}{R} = \\frac{ \\grad \\psi}{R} \\cross \\vu \\phi\\] \\[F \\equiv R B_\\phi\\] \\[\\Delta ^\\star \\equiv R \\pdv{}{R} \\frac{1}{R} \\pdv{}{R} \u0026#43; \\pdv{^2}{z^2}\\] \\[\\Delta ^\\star \\psi = \\pdv{^2 \\psi}{z^2} \u0026#43; \\pdv{^2 \\psi}{R^2} - \\frac{1}{R} \\pdv{\\psi}{R}\\] \\[\\vec j_\\phi = - \\frac{1}{\\mu_0 R} \\Delta ^\\star \\psi \\vu \\phi\\] \\[\\vec j_\\theta = \\frac{1}{\\mu_0 R} \\grad (F) \\cross \\vu \\phi\\] \\[R^2 \\mu_0 \\pdv{p}{\\psi} = - \\Delta ^\\star \\psi - F \\pdv{F}{\\psi}\\] \\[q(\\psi) = \\frac{F(\\psi)}{2 \\pi} \\oint_{p} \\frac{r \\dd \\theta}{R^2 B_\\theta}\\] Limits:\n\\[\\text{Force-free:} \\qquad \\vec j \\parallel \\vec B\\] \\[\\rightarrow \\Delta ^\\star \\psi \u0026#43; F F\u0026#39; = 0\\] \\[\\text{Connected $\\theta$ pinch:} \\qquad FF\u0026#39; \\gg \\Delta ^\\star \\psi\\] \\[\\rightarrow \\grad p \\approx \\vec j_\\theta \\cross \\vec B_\\phi\\] \\[\\text{Connected Z-pinch:} \\qquad FF\u0026#39; \\ll \\Delta ^\\star \\phi\\] \\[\\rightarrow \\grad p \\approx j_\\phi \\cross B_\\theta\\] MHD Stability # Linear stability\n\\[\\pdv{\\rho_1}{t} = - \\vec v_1 \\grad \\rho_0 - \\rho_0 \\div \\vec v_1\\] \\[\\pdv{\\vec B_1}{t} = \\curl ( \\vec v_1 \\cross \\vec B_0)\\] \\[\\rho_0 \\pdv{\\vec v_1}{t} = - \\grad p_1 \u0026#43; \\vec j_0 \\cross \\vec B_1 - \\vec j_1 \\cross \\vec B_0\\] \\[\\pdv{p_1}{t} = - \\vec v_1 \\cdot \\grad p_0 - \\gamma p_0 \\div \\vec v_1\\] For linear perturbation \\( \\vec \\xi = \\int_0 ^t \\vec v_1 \\dd t \\) the momentum equation becomes\n\\[\\rho_0 \\pdv{^2 \\xi}{t^2} = \\vec F(\\xi)\\] where\n\\[F(\\xi) = \\grad (\\xi \\cdot \\grad p_0 \u0026#43; \\gamma p_0 \\div \\xi) \u0026#43; \\frac{1}{\\mu_0} \\left[ ( \\curl \\vec B_0) \\cross \\curl (\\xi \\cross \\vec B_0) \u0026#43; \\curl \\curl (\\xi \\cross \\vec B_0) \\cross \\vec B_0 \\right]\\] Eigenvalues of \\( \\frac{1}{\\rho_0} \\vec F (\\xi) = \\omega^2 \\xi \\) are real and ordered. Only need to check \\( n=0 \\) to determine stability/instability of configuration.\n\\( \\delta W \\) Approach\n\\( \\delta W = \\) change in potential energy due to a displacement \\( \\xi \\)\n\\[\\delta W \u0026lt; 0 \\rightarrow \\text{instability}\\] \\[\\delta W = - \\frac{1}{2} \\int \\xi \\cdot F(\\xi) \\dd V = \\delta W_F \u0026#43; \\delta W_S \u0026#43; \\delta W_V\\] Surface term:\n\\[\\delta W_s = \\frac{1}{2} \\oint \\dd S (\\vu n \\cdot \\xi) ^2 \\left( \\vu n \\cdot \\grad p_0 \u0026#43; \\left[ \\vu n \\cdot \\grad \\frac{B_0 ^2}{2 \\mu_0} \\right]_{jump} \\right)\\] Vacuum term:\n\\[\\delta W_V = \\int_{vac} \\dd V \\frac{B_1 ^2}{\\mu_0}\\] Plasma (free) term:\n\\[\\delta W_F = \\frac{1}{2} \\int \\dd V \\frac{ |B_{1, \\perp}|^2}{\\mu_0} \\quad \\leftarrow \\text{Shear Alfven} \\\\ \u0026#43; \\mu_0 \\left| \\frac{B_{1, \\parallel}}{\\mu_0} - \\frac{B_0 \\xi \\cdot \\grad p_0}{B_0} ^2 \\right|^2 \\quad \\leftarrow \\text{Fast magnetosonic} \\\\ \u0026#43; \\Gamma p_0 |\\div \\xi|^2 \\quad \\leftarrow \\text{Acoustic}\\\\ \u0026#43; \\frac{\\vec j_0 \\cdot \\vec B_0}{B_0 ^2} (\\vec B_0 \\cross \\vec \\xi) \\cdot \\vec B_1 \\quad \\leftarrow \\text{Current-driven (kink)} \\\\ - 2 ( \\vec \\xi \\cdot \\grad p_0)(\\vec \\xi \\cdot \\vec \\kappa) \\quad \\leftarrow \\text{pressure-driven (interchange/balooning)}\\] Shear Alfven, fast magnetosonic, and acoustic modes are stabilizing. Current-driven and pressure-driven modes can lead to instability.\n"},{"id":89,"href":"/r/notes/UWAA560/formulary/","title":"Formulary","section":"Plasma Diagnostics","content":" \\( \\) Formulary # Kinetic Description\n\\[\\dv{\\vec v}{t} = \\frac{q_i}{m_i} (\\vec E \u0026#43; \\vec v_i \\cross \\vec B) \u0026#43; \\sum_{j \\neq i} \\left[ \\left. \\dv{\\vec v_{ij}}{t} \\right|_{coll} \\delta(\\vec r_i - \\vec r_j) \\right]\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\frac{1}{c^2} \\pdv{\\vec E}{t} = \\curl \\vec B - \\mu_0 \\sum_i q_i \\vec v_i \\delta (\\vec r - \\vec r_i)\\] \\[\\div \\vec{B} = 0\\] \\[\\div \\vec E = \\frac{1}{\\epsilon_0} \\sum_i q_i \\delta (\\vec r - \\vec r_i)\\] Klimontovich equation:\n\\[\\dv{N}{t} = 0 = \\pdv{N}{t} \u0026#43; \\pdv{}{q_i} \\cdot (\\dot{q_i} N) \\\\ N \\equiv \\sum_i \\delta (p - p_i) \\delta(q - q_i)\\] Plasma Fluid Description\nBoltzmann Equation\n\\[\\pdv{f_\\alpha}{t} \u0026#43; \\vec v \\cdot \\pdv{f_\\alpha}{t} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} (\\vec E \u0026#43; \\vec v \\cross \\vec B) \\cdot \\pdv{f_\\alpha}{\\vec v} = \\left. \\pdv{f_\\alpha}{t} \\right|_{coll} = \\sum_{\\beta \\neq \\alpha} C_{\\alpha \\beta}\\] Maxwellian distribution:\n\\[f_\\alpha (\\vec v) = n_\\alpha \\left( \\frac{m_\\alpha}{2 \\pi T} \\right)^{3/2} e^{- \\frac{m_\\alpha(\\vec v - \\vec v_\\alpha)^2}{2T}}\\] Moments of fluid model (moments of distribution \\( \\rightarrow \\) moments of Boltzmann equation:\n\\[\\text{Continuity:} \\qquad n_\\alpha = \\int f_\\alpha \\dd \\vec v \\\\ \\quad \\rightarrow \\pdv{n_\\alpha}{t} \u0026#43; \\div (n_\\alpha \\vec v_\\alpha) = 0\\] \\[\\text{Momentum:} \\qquad n_\\alpha \\vec v_\\alpha = \\int \\vec v f_\\alpha \\dd v \\\\ \\quad \\rightarrow \\quad \\pdv{}{t} (n_\\alpha \\vec v_\\alpha ) \u0026#43; \\div (n_\\alpha \\vec v_\\alpha \\vec v_\\alpha) \u0026#43; \\frac{1}{m_\\alpha} \\div \\vec P_\\alpha - \\frac{q_\\alpha}{m_\\alpha} n_\\alpha ( \\vec E \u0026#43; \\vec v _\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\int \\vec w C_{\\alpha \\beta} \\dd \\vec v\\] \\[\\rightarrow \\rho_\\alpha \\left(\\pdv{\\vec v_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad \\vec v_\\alpha \\right) \u0026#43; \\grad \\vec P_\\alpha \u0026#43; \\div \\vec \\Pi_\\alpha - q_\\alpha n_\\alpha (\\vec E \u0026#43; \\vec v_\\alpha \\cross \\vec B) = \\sum_{\\beta \\neq \\alpha} \\vec R_{\\alpha \\beta}\\] \\[\\text{Energy:} \\qquad \\int \\vec v \\vec v \\pdv{f_\\alpha}{t} \\dd \\vec v = \\pdv{}{t} \\int \\vec v \\vec v f_\\alpha \\dd \\vec v = \\pdv{}{t} \\vec E_\\alpha / m_\\alpha \\rightarrow \\pdv{}{t} \\vec P_\\alpha\\] \\[\\rightarrow \\quad \\frac{3}{2} n_\\alpha \\left( \\pdv{T_\\alpha}{t} \u0026#43; \\vec v_\\alpha \\cdot \\grad T_\\alpha \\right) \u0026#43; P_\\alpha \\div \\vec v_\\alpha \u0026#43; \\vec \\Pi_\\alpha \\cdot \\cdot \\grad \\vec v_\\alpha \u0026#43; \\div \\vec h_\\alpha = \\sum_{\\beta \\neq \\alpha} Q_{\\alpha \\beta}\\] Closure relations\n\\[\\vec h_\\alpha = - \\kappa \\grad T_\\alpha\\] \\[\\overline \\Pi_ \\alpha = \\nu \\grad \\vec v_\\alpha\\] Ideal MHD # Ideal MHD\nContinuity:\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0\\] Momentum:\n\\[\\rho \\left( \\pdv{\\vec v}{t} \u0026#43; \\vec v \\cdot \\grad \\vec v \\right) \u0026#43; \\grad p - \\vec j \\cross \\vec B = 0\\] Generalized Ohm\u0026rsquo;s Law\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = \\frac{1}{Zen}\\cancel{(\\vec j \\cross \\vec B - \\grad p_e)} = 0\\] Energy\n\\[\\dv{}{t} \\left( \\frac{p}{\\rho^\\gamma} \\right) = 0\\] \u0026ldquo;Lawson Criterion\u0026rdquo;\n\\[n \\tau_E \u0026gt; 10^4 s/m^3\\] Conservation Law Form of Ideal MHD\nContinuity:\n\\[\\pdv{\\rho}{t} \u0026#43; \\div (\\rho \\vec v) = 0\\] Momentum:\n\\[\\pdv{(\\rho \\vec v)}{t} \u0026#43; \\div \\left[ \\rho \\vec v \\vec v - \\frac{\\vec B \\vec B}{\\mu_0} \u0026#43; \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\overline{I} \\right] = 0\\] Energy:\n\\[\\pdv{\\epsilon}{t} \u0026#43; \\div \\left[ \\left( \\epsilon \u0026#43; p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) \\vec v - (\\vec B \\cdot \\vec v) \\frac{\\vec B}{\\mu_0} \\right] = 0\\] \\[\\pdv{\\vec B}{t} \u0026#43; \\div ( \\vec v \\vec B - \\vec B \\vec v) = 0\\] where\n\\[\\epsilon = \\frac{1}{\\gamma - 1} p \u0026#43; \\frac{1}{2} \\rho v^2 \u0026#43; \\frac{B^2}{2\\mu_0}\\] Static Equilibrium:\n\\[\\vec j \\cross \\vec B = \\grad p\\] \\[\\frac{B^2}{\\mu_0} \\vec K = \\grad_\\perp (p \u0026#43; \\frac{B^2}{2 \\mu_0})\\] \\[\\vec K \\equiv \\frac{\\vec B}{|B|} \\cdot \\grad \\frac{ \\vec B}{|B|}\\] Conservation of flux:\n\\[\\vec E \u0026#43; \\vec v \\cross \\vec B = 0\\] \\[\\pdv{\\vec B}{t} = - \\curl \\vec E\\] \\[\\rightarrow \\dv{}{t} \\left( \\frac{\\vec B}{\\rho} \\right) = \\frac{\\vec B}{\\rho} \\cdot \\grad \\vec v\\] 1D Equilibria # \\( \\theta \\)-pinch\n\\[B_\\theta = 0\\] \\[j_\\theta B_z = \\dv{p}{r}\\] \\[j_\\theta = - \\frac{1}{\\mu_0} \\dv{B_z}{r}\\] \\[\\rightarrow p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} = \\frac{B_0 ^2}{2 \\mu_0}\\] \\[\\langle \\beta \\rangle = \\frac{2}{a^2} \\int_0 ^a \\frac{r p}{B_0 ^2 / 2 \\mu_0} \\dd r\\] \\[q = \\infty\\] \\[W = \\frac{\\mu_0 r}{B_z ^2} \\dv{}{r} \\left( p \u0026#43; \\frac{B_z ^2}{2 \\mu_0} \\right) = 0\\] Z-pinch\n\\[B_z =0\\] \\[\\grad p = \\dv{p}{r} = - j_z B_\\theta\\] \\[- \\dv{}{r} \\left( p \u0026#43; \\frac{B_\\theta ^2}{2 \\mu_0} \\right) = \\frac{B_\\theta ^2}{\\mu_0 r}\\] \\[\\langle \\beta \\rangle = \\frac{2 \\mu_0}{B_0 ^2 \\pi a^2} \\int _0 ^a 2 \\pi r p \\dd r = 1 \\quad \\text{ if } \\quad p(a) = 0\\] \\[q = S = 0\\] \\[W = 1\\] Screw pinch\n\\[\\dv{}{r} \\left( p \u0026#43; \\frac{B^2}{2 \\mu_0} \\right) = - \\frac{B_\\theta ^2}{\\mu_0 r}\\] \\[\\beta_t = \\frac{2 \\mu_0}{B_z (a) ^2} \\left( \\frac{1}{\\pi a^2} \\int_0 ^a 2 \\pi r p \\dd r \\right)\\] \\[\\beta_p = \\left( 1 - \\frac{\\alpha_t}{\\beta _t} \\right)^{-1} \\qquad \\alpha_t \\equiv \\frac{2}{a^2} \\int_0 ^a \\left(1 - \\frac{B_z ^2}{B_0 ^2} \\right) r \\dd r\\] \\[q = \\frac{2 \\pi r B_z}{L B_\\theta}\\] \\[q_a = \\frac{4 \\pi ^2 a^2 B_0}{\\mu_0 I_a}\\] \\[S = \\frac{r}{q} \\dv{q}{r}\\] \\[W = - \\frac{B_\\theta ^2}{B_\\theta ^2 \u0026#43; B_z ^2}\\] Stability\nShear:\n\\[S = 2 \\frac{ dq / q}{dV / V} = 2 \\frac{d \\ln q}{d \\ln V}\\] \\[q = \\frac{\\text{\\# long windings}}{\\text{\\# short windings}} = \\dv{\\psi_t}{\\psi_p}\\] Shear for toroid\n\\[q = \\frac{r B_\\phi}{R B_\\theta}\\] Shear for cylinder\n\\[q = 2 \\pi \\frac{r B_z}{L B_\\theta}\\] Well\n\\[W = \\frac{ d \\langle p \u0026#43; B^2 / 2 \\mu_0 \\rangle / \\langle B^2 / 2 \\mu_0 \\rangle}{dV / V}\\] For stabilization, \\( B^2 / 2 \\mu_0 \\) should increase faster than \\( p \\) decreases\n2D Equilibria # Grad-Shafranov Equation: Static toroidal equilibrium\n\\[\\grad p = \\vec j_\\theta \\cross \\vec B_\\phi \u0026#43; \\vec j_\\phi \\cross \\vec B_\\theta\\] \\[A_\\phi = \\frac{\\phi}{R} \\vu \\phi\\] \\[\\phi = \\frac{\\phi_p}{2 \\pi}\\] \\[\\vec B_\\theta = - \\frac{\\vu R}{R} \\pdv{\\psi}{z} \u0026#43; \\frac{ \\vu z}{R} \\pdv{\\psi}{R} = \\frac{ \\grad \\psi}{R} \\cross \\vu \\phi\\] \\[F \\equiv R B_\\phi\\] \\[\\Delta ^\\star \\equiv R \\pdv{}{R} \\frac{1}{R} \\pdv{}{R} \u0026#43; \\pdv{^2}{z^2}\\] \\[\\Delta ^\\star \\psi = \\pdv{^2 \\psi}{z^2} \u0026#43; \\pdv{^2 \\psi}{R^2} - \\frac{1}{R} \\pdv{\\psi}{R}\\] \\[\\vec j_\\phi = - \\frac{1}{\\mu_0 R} \\Delta ^\\star \\psi \\vu \\phi\\] \\[\\vec j_\\theta = \\frac{1}{\\mu_0 R} \\grad (F) \\cross \\vu \\phi\\] \\[R^2 \\mu_0 \\pdv{p}{\\psi} = - \\Delta ^\\star \\psi - F \\pdv{F}{\\psi}\\] \\[q(\\psi) = \\frac{F(\\psi)}{2 \\pi} \\oint_{p} \\frac{r \\dd \\theta}{R^2 B_\\theta}\\] Limits:\n\\[\\text{Force-free:} \\qquad \\vec j \\parallel \\vec B\\] \\[\\rightarrow \\Delta ^\\star \\psi \u0026#43; F F\u0026#39; = 0\\] \\[\\text{Connected $\\theta$ pinch:} \\qquad FF\u0026#39; \\gg \\Delta ^\\star \\psi\\] \\[\\rightarrow \\grad p \\approx \\vec j_\\theta \\cross \\vec B_\\phi\\] \\[\\text{Connected Z-pinch:} \\qquad FF\u0026#39; \\ll \\Delta ^\\star \\phi\\] \\[\\rightarrow \\grad p \\approx j_\\phi \\cross B_\\theta\\] MHD Stability # Linear stability\n\\[\\pdv{\\rho_1}{t} = - \\vec v_1 \\grad \\rho_0 - \\rho_0 \\div \\vec v_1\\] \\[\\pdv{\\vec B_1}{t} = \\curl ( \\vec v_1 \\cross \\vec B_0)\\] \\[\\rho_0 \\pdv{\\vec v_1}{t} = - \\grad p_1 \u0026#43; \\vec j_0 \\cross \\vec B_1 - \\vec j_1 \\cross \\vec B_0\\] \\[\\pdv{p_1}{t} = - \\vec v_1 \\cdot \\grad p_0 - \\gamma p_0 \\div \\vec v_1\\] For linear perturbation \\( \\vec \\xi = \\int_0 ^t \\vec v_1 \\dd t \\) the momentum equation becomes\n\\[\\rho_0 \\pdv{^2 \\xi}{t^2} = \\vec F(\\xi)\\] where\n\\[F(\\xi) = \\grad (\\xi \\cdot \\grad p_0 \u0026#43; \\gamma p_0 \\div \\xi) \u0026#43; \\frac{1}{\\mu_0} \\left[ ( \\curl \\vec B_0) \\cross \\curl (\\xi \\cross \\vec B_0) \u0026#43; \\curl \\curl (\\xi \\cross \\vec B_0) \\cross \\vec B_0 \\right]\\] Eigenvalues of \\( \\frac{1}{\\rho_0} \\vec F (\\xi) = \\omega^2 \\xi \\) are real and ordered. Only need to check \\( n=0 \\) to determine stability/instability of configuration.\n\\( \\delta W \\) Approach\n\\( \\delta W = \\) change in potential energy due to a displacement \\( \\xi \\)\n\\[\\delta W \u0026lt; 0 \\rightarrow \\text{instability}\\] \\[\\delta W = - \\frac{1}{2} \\int \\xi \\cdot F(\\xi) \\dd V = \\delta W_F \u0026#43; \\delta W_S \u0026#43; \\delta W_V\\] Surface term:\n\\[\\delta W_s = \\frac{1}{2} \\oint \\dd S (\\vu n \\cdot \\xi) ^2 \\left( \\vu n \\cdot \\grad p_0 \u0026#43; \\left[ \\vu n \\cdot \\grad \\frac{B_0 ^2}{2 \\mu_0} \\right]_{jump} \\right)\\] Vacuum term:\n\\[\\delta W_V = \\int_{vac} \\dd V \\frac{B_1 ^2}{\\mu_0}\\] Plasma (free) term:\n\\[\\delta W_F = \\frac{1}{2} \\int \\dd V \\frac{ |B_{1, \\perp}|^2}{\\mu_0} \\quad \\leftarrow \\text{Shear Alfven} \\\\ \u0026#43; \\mu_0 \\left| \\frac{B_{1, \\parallel}}{\\mu_0} - \\frac{B_0 \\xi \\cdot \\grad p_0}{B_0} ^2 \\right|^2 \\quad \\leftarrow \\text{Fast magnetosonic} \\\\ \u0026#43; \\Gamma p_0 |\\div \\xi|^2 \\quad \\leftarrow \\text{Acoustic}\\\\ \u0026#43; \\frac{\\vec j_0 \\cdot \\vec B_0}{B_0 ^2} (\\vec B_0 \\cross \\vec \\xi) \\cdot \\vec B_1 \\quad \\leftarrow \\text{Current-driven (kink)} \\\\ - 2 ( \\vec \\xi \\cdot \\grad p_0)(\\vec \\xi \\cdot \\vec \\kappa) \\quad \\leftarrow \\text{pressure-driven (interchange/balooning)}\\] Shear Alfven, fast magnetosonic, and acoustic modes are stabilizing. Current-driven and pressure-driven modes can lead to instability.\n"},{"id":90,"href":"/r/notes/griffiths/ch10-1/","title":"Scalar and Vector Potentials","section":"Griffiths Introduction to Electrodynamics","content":" 10.0.1 Presentation/Paper notes # A few words about the paper/presentation\nIt\u0026rsquo;s supposed to be on a topic related to the class (classical electromagnetism) The topic can be wide-ranging. Some sample topics will be put it. It should be thematically related to what we\u0026rsquo;ve been talking about, and you should be making connections to things we\u0026rsquo;ve been talking about (waveguides, waves, generation of waves, etc.) There will be two class periods in the last week which are reserved for presentations of about 15 minutes each. Examples of possible topics:\nApplications of X-ray radiation Advanced light source Advanced photon source Waveguiding Optical fibers Applications of coaxial cables Microwave cavities ADMX experiment Atom-photon entanglement with Rydberg atoms in high finesse cavities Optical interferometry Fabry-Perot interferometer / cavity and applications LIGO and gravitational waves Microcavities, Photonic crystal cavities 10.1.1 Scalar and Vector Potentials # Ultimately the question of electromagnetism is given some sources \\( \\rho(\\vec r, t) \\) and \\( \\vec J ( \\vec r, t) \\), what are the resulting fields \\( \\vec E( \\vec r, t) \\) and \\( \\vec B ( \\vec r, t) \\)? In the static case, Coulomb\u0026rsquo;s law and the Biot-Savart law provide deterministic answers, so how do we then generalize to time-dependent configurations?\nIt turns out that once again, it will pay to represent the fields in terms of potentials. Just like in electrostatics, \\( \\curl \\vec E = 0 \\) allowed us to write \\( \\vec E \\) as the gradient of a scalar potential. We can\u0026rsquo;t do that any more, but we do still have a divergenceless \\( \\vec B \\), so\n\\[\\vec B = \\curl \\vec A\\] is still valid, as in magnetostatics. Plugging into Faraday\u0026rsquo;s law gives us\n\\[\\curl \\vec E = - \\pdv{\\vec B}{t} \\\\ = - \\pdv{}{t} \\left( \\curl \\vec A \\right) \\\\ = - \\curl \\left( \\pdv{\\vec{A}}{t} \\right) \\\\ \\rightarrow \\curl ( \\vec E \u0026#43; \\pdv{\\vec{A}}{t} ) = 0 \\\\ \\rightarrow \\vec E \u0026#43; \\pdv{\\vec{A}}{t} = - \\grad V\\] In terms of a scalar and a vector potential, then, we can write \\( \\vec E \\) as\n\\[\\vec E = - \\grad V - \\pdv{\\vec{A}}{t}\\] What happens with Gauss\u0026rsquo; law and the Ampere-Maxwell law?\n\\[\\div \\vec E = \\rho / \\epsilon _0 \\\\ \\rightarrow \\div ( - \\grad V - \\pdv{\\vec A}{t}) = \\rho / \\epsilon_0 \\\\ \\rightarrow \\laplacian V \u0026#43; \\pdv{}{t} \\left( \\div \\vec A \\right) = - \\rho / \\epsilon_0\\] \\[\\curl \\vec B = \\mu_0 \\vec J \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec E}{t} \\\\ \\curl ( \\curl \\vec A) = \\mu_0 \\vec J \u0026#43; \\mu_0 \\epsilon_0 \\pdv{}{t} \\left(- \\grad V - \\pdv{\\vec A}{t} \\right) \\\\ \\laplacian \\vec A - \\mu_0 \\epsilon_0 \\pdv{^2 \\vec A}{t^2} - \\grad ( \\div \\vec A \u0026#43; \\mu_0 \\epsilon_0 \\pdv{V}{t} )= - \\mu_0 \\vec J\\] Well now what! We\u0026rsquo;ve got a fairly complicated differential equation on our hands\u0026hellip; how can we make it simpler? The fields actually only care about the curl of the vector potential, and that allows us some freedom in the gauge of \\( \\vec A \\). We can add any curl-less function to modify the divergence \\(\\div \\vec A\\) and nothing at all about the real fields will change. In particular, wouldn\u0026rsquo;t it be nice if\n\\[\\div \\vec A = 0 \\quad \\text{(Coulomb gauge)}\\] so that solving for \\( V \\) just amounts to the Poisson equation:\n\\[\\laplacian V \u0026#43; \\pdv{}{t} (\\div \\vec A) = - \\rho / \\epsilon_0 \\\\ \\rightarrow \\laplacian V = - \\rho / \\epsilon_0 \\quad \\text{(Coulomb Gauge)}\\] But the Coulomb gauge still makes it very difficult to solve for \\( \\vec A \\). If we want to solve for the vector potential more easily, it\u0026rsquo;s pretty obvious that we should set\n\\[\\div \\vec A = - \\mu_0 \\epsilon_0 \\pdv{V}{t} \\quad \\text{(Lorenz gauge)}\\] so that\n\\[\\laplacian \\vec A - \\mu_0 \\epsilon_0 \\pdv{^2 \\vec A}{t^2} - \\grad ( \\div \\vec A \u0026#43; \\mu_0 \\epsilon_0 \\pdv{V}{t} )= - \\mu_0 \\vec J \\\\ \\rightarrow \\laplacian \\vec A - \\mu_0 \\epsilon_0 \\pdv{^2 \\vec A}{t^2} = - \\mu_0 \\vec J \\quad \\text{(Lorenz gauge)}\\] The result is an inhomogeneous wave equation with a \u0026ldquo;source\u0026rdquo; term on the right. It\u0026rsquo;s quite similar to the wave equations we\u0026rsquo;ve been solving for the past couple of chapters, but the sources have been re-introduced, and it\u0026rsquo;s important to remember that now the sources \\( \\rho \\) and \\( \\vec J \\) are allowed to vary in both space and time.\n"},{"id":91,"href":"/r/notes/griffiths/ch10-2/","title":"Retarded Potentials","section":"Griffiths Introduction to Electrodynamics","content":" 10.2.1 Retarded Potentials # In the Lorenz gauge, we want to find the potentials by solving\n\\[\\laplacian \\vec A - \\mu_0 \\epsilon_0 \\pdv{^2 \\vec A}{t^2} = - \\mu_0 \\vec J\\] Back in the static case, this reduces to Poisson\u0026rsquo;s equation\n\\[\\laplacian V = - \\frac{1}{\\epsilon_0} \\rho\\] \\[\\laplacian \\vec A = - \\mu_0 \\vec J\\] and we know how to solve these\n\\[V(\\vec r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec r\u0026#39;)}{\\gr} \\dd \\tau\u0026#39; \\] \\[\\vec A(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec J (\\vec r\u0026#39;)}{\\gr} \\dd \\tau\u0026#39; \\] We know that electromagnetic disturbances travel at the speed of light (at least in vacuum). So for general distributions of sources that may be changing in time, it\u0026rsquo;s not what the source is doing right now that matters - it\u0026rsquo;s what was happening at some earlier time (called the retarded time) when the \u0026ldquo;message\u0026rdquo; left. The information has traveled a distance \\( \\gr \\), so the delay is \\( \\gr / c \\), so the retarded time is\n\\[t_r \\equiv t - \\frac{\\gr }{c}\\] So we can immediately generalize our solutions for the potentials of static sources to the retarded potentials\n\\[V(\\vec r, t) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec r\u0026#39;, t_r)}{\\gr} \\dd \\tau\u0026#39;\\] \\[\\vec A(\\vec r, t) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec J(\\vec r \u0026#39;, t_r)}{\\gr} \\dd \\tau \u0026#39;\\] That wasn\u0026rsquo;t so bad! It can be shown that the retarded potentials satisfy the inhomogeneous wave equations and the Lorenz gauge condition, giving some much-needed credibility to our argument that EM \u0026ldquo;messages\u0026rdquo; travel at the speed of light. While the math involved might be quite nasty (remember that \\( \\gr \\) depends on \\( | \\vec r - \\vec r\u0026rsquo; | \\)), in principle it\u0026rsquo;s straightforward to determine the fields directly by\n\\[\\vec E = - \\grad V - \\pdv{\\vec A}{t} \\qquad \\vec B = \\curl \\vec A\\] Together, these are called the Jefimenko\u0026rsquo;s equations.\n"},{"id":92,"href":"/r/notes/griffiths/ch10-3/","title":"Point Charges","section":"Griffiths Introduction to Electrodynamics","content":" 10.3 Point Charges # 10.3.1 Moving Point Charges: Lienard-Wiechert Potentials # Suppose we have a point charge moving on some trajectory \\( \\vec w(t) \\). If we naively read off the retarded potential\n\\[V(\\vec r, t) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec r\u0026#39;, t_r)}{\\gr} \\dd \\tau\u0026#39;\\] might suggest that the potential is just\n\\[\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr}\\] But that\u0026rsquo;s wrong for a subtle reason: It is true that for a point source \\( \\gr \\) comes outside the integral, but the remainder\n\\[\\int \\rho(\\vec r\u0026#39;, t_r) \\dd \\tau\u0026#39;\\] is not equal to the charge of the particle, and depends on the location of the point \\( \\vec r \\). To calculate the total charge of a configuration, you must integrate \\( \\rho \\) over the entire distribution at one instant of time, but here the retardation \\( t_r = t - \\gr / c \\), obliges us to evaluate \\( \\rho \\) at different times for different parts of the configuration. If the source is moving, this will give a distorted picture of the total charge. You might think that this problem would disappear for point charges, but it doesn\u0026rsquo;t. In Maxwell\u0026rsquo;s electrodynamics, formulated as it is in terms of charge and current densities, a point charge must be regarded as the limit of an extended charge, when the size goes to zero. And for an extended particle, no matter how small, the retardation throws a factor \\( (1 - \\vu \\gr \\cdot \\vec v / c)^{-1} \\) where \\( \\vec v \\) is the velocity of the charge at the retarded time\n\\[\\int \\rho ( \\vec r \u0026#39;, t_r) \\dd \\tau\u0026#39; = \\frac{q}{1 - \\vu \\gr \\cdot \\vec v / c}\\] Theorem: \\( \\int \\rho ( \\vec r ', t_r) \\dd \\tau' = \\frac{q}{1 - \\vu \\gr \\cdot \\vec v / c} \\)\nThis is a purely geometrical effect, and it may help to tell the story in a less abstract context. You will not have noticed it, for obvious reasons, the the fact is that a train coming towards you looks a little longer than it really is, because the light you receive from the caboose left earlier than the light you receive simultaneously from the engine, and at that earlier time the train was farther away (Fig 10.6). In the interval it takes light from the caboose to travel the extra distance \\( L\u0026rsquo; \\), the train itself moves a distance \\( L\u0026rsquo; - L \\)\n\\[\\frac{L\u0026#39;}{c} = \\frac{L\u0026#39; - L}{v} \\quad \\text{or} \\quad L\u0026#39; = \\frac{L}{1-v/c} \\] So approaching trains appear longer, by a factor \\( (1 - v/c)^{-1} \\). By contrast, a train going away from you looks shorter, by a factor \\( (1 + v/c)^{-1} \\). In general, if the train\u0026rsquo;s velocity makes an angle \\( \\theta \\) with your line of sight, the extra distance light from the caboose must cover is \\( L\u0026rsquo; \\cos \\theta \\). In the time \\( L\u0026rsquo; \\cos \\theta / c \\), then, the train moves a distance \\( (L\u0026rsquo; - L) \\)\n\\[\\frac{L\u0026#39; \\cos \\theta}{c} = \\frac{L\u0026#39; - L}{v} \\quad \\text{or} \\quad L\u0026#39; = \\frac{L}{1 - v \\cos \\theta / c} \\] Since the effect does not distort the dimensions perpendicular to the motion, we can easily move to 3D as\n\\[L\u0026#39; = \\frac{l}{1 - \\vu \\gr \\cdot \\vec v / c} \\] and the apparent volume of the train \\( \\tau\u0026rsquo; \\) of the train is then related to the actual volume by\n\\[\\tau\u0026#39; = \\frac{\\tau}{1 - \\vu \\gr \\cdot \\vec v / c} \\] where \\( \\vu \\gr \\) is a unit vector from the train to the observer.\nBack to our moving point charge, the retarded time is determined implicitly by\n\\[| \\vec r - \\vec w(t_r)| = c(t - t_r)\\] \\[\\vec \\gr = \\vec r - \\vec w(t_r)\\] It follows that\n\\[V( \\vec r, t) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{qc}{(\\gr c - \\vec \\gr \\cdot \\vec v)}\\] where \\( \\vec v \\) is the velocity of the charge at the retarded time, and \\( \\vec \\gr \\) is the vector from the retarded position to the field point \\( \\vec r \\). Moreover, since the current density is \\( \\rho \\vec v \\), the vector potential is\n\\[\\begin{aligned} \\vec A ( \\vec r, t) \u0026amp; = \u0026amp; \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\rho(\\vec r\u0026#39;, t_r) \\vec v(t_r)}{\\gr} \\dd \\tau\u0026#39; \\\\ \u0026amp; = \u0026amp; \\frac{\\mu_0}{4 \\pi} \\frac{\\vec v}{\\gr} \\int \\rho(\\vec r\u0026#39;, t_r) \\dd \\tau\u0026#39; \\\\ \u0026amp; = \u0026amp; \\frac{\\mu_0}{4 \\pi} \\frac{qc \\vec v}{(\\gr c - \\vec \\gr \\cdot \\vec v)} \\\\ \u0026amp; = \u0026amp; \\frac{\\vec v}{c^2} V(\\vec r, t) \\end{aligned}\\] These are the Lienard-Wiechert potentials for a moving point charge.\n10.3.2: The Fields of a Moving Point Charge # After a great deal of vector calculus, starting from the Lienard-Wiechert potentials for a moving point charge, you can work out the fields\n\\[\\vec E = - \\grad V - \\pdv{\\vec{A}}{t} \\qquad \\vec B = \\curl \\vec A\\] Because of all the various dependencies on \\( \\vec r, \\vec r\u0026rsquo;, \\vec \\gr \\), the integration is tricky, but eventually the result is\n\\[\\vec E(\\vec r, t) = \\frac{q}{4 \\pi \\epsilon_0} \\frac{ \\gr}{(\\vec \\gr \\cdot \\vec u)^3} \\left[ (c^2 - v^2) \\vec u \u0026#43; \\gr \\cross ( \\vec u \\cross \\vec a) \\right]\\] where we introduce the vector\n\\[\\vec u \\equiv c \\vu \\gr - \\vec v\\] and \\( \\vec a = \\pdv{\\vec v}{t} \\) is the acceleration of the particle at the retarded time.\n"},{"id":93,"href":"/r/notes/griffiths/ch11-1/","title":"Dipole Radiation","section":"Griffiths Introduction to Electrodynamics","content":" 11.1 Dipole Radiation # 11.1.1 Electric Dipole Radiation # Consider a dipole which is \u0026ldquo;oscillating\u0026rdquo; at some particular frequency. This looks like two metal spheres separated by distance \\( d \\) and connected by a small wire; at time \\( t \\) the charge on the upper sphere is \\( q(t) \\) and the charge on the lower sphere is \\( -q(t) \\). We drive the charge back and forth through the wire from one sphere to the other at an angular frequency \\( \\omega \\)\n\\[q(t) = q_0 \\cos (\\omega t)\\] The dipole moment is\n\\[\\vec p(t) = p_0 \\cos (\\omega t) \\vu z \\qquad p_0 \\equiv q_0 d\\] Of course, we consider the harmonically oscillating dipole because we can build any other oscillations out of this basis.\nThe retarded potential of this oscillating dipole is the superposition of the two point charges:\n\\[V(\\vec r, t) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q_0 \\cos [\\omega(t - \\gr_\u0026#43; / c)]}{\\gr_\u0026#43;} - \\frac{q_0 \\cos[\\omega(t - \\gr_- / c)]}{\\gr_-} \\right]\\] where\n\\[\\gr_{\\pm} = \\sqrt{ r^2 \\mp r d \\cos \\theta \u0026#43; (d/2)^2}\\] To take this physical dipole towards a perfect dipole, we want\n\\[d \\ll r \\quad \\text{ while } \\quad q_0 d = \\text{const.}\\] This approximation gives us\n\\[\\gr_{\\pm} \\approx r \\left( 1 \\mp \\frac{d}{2r} \\cos \\theta \\right)\\] so that\n\\[\\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right)\\] and\n\\[\\begin{aligned} \\cos[\\omega(t - \\gr_{\\pm} / c)] \u0026amp; \\approx \\cos \\left[ \\omega(t - r/c) \\pm \\frac{\\omega d}{2c} \\cos \\theta \\right] \\\\ \u0026amp; = \\cos [ \\omega(t - r/c)] \\cos \\left( \\frac{\\omega d}{2c} \\cos \\theta \\right) \\\\ \u0026amp; \\quad \\pm \\sin[\\omega(t - r/c)] \\sin \\left( \\frac{\\omega d}{2c} \\cos \\theta \\right) \\end{aligned}\\] In the limit of a perfect dipole, we take the dipole approximation\n\\[d \\ll \\frac{c}{\\omega}\\] Under that condition,\n\\[\\cos [ \\omega(t - \\gr_{\\pm}/c)] \\approx \\cos [\\omega(t - r/c)] \\mp \\frac{\\omega d}{2c} \\cos \\theta \\sin [\\omega(t - r/c)]\\] Finally, we do not really care what happens near the origin. Rather, we are looking for the far-field behavior of the radiation, so we must consider fields that survive at large distances from the source:\n\\[r \\gg \\frac{c}{\\omega}\\] In this region, the retarded potential reduces to\n\\[V(r, \\theta, t) = - \\frac{p_0 \\omega}{4 \\pi \\epsilon_0 c} \\left( \\frac{\\cos \\theta}{r} \\right) \\sin [ \\omega(t - r/ c)]\\] What about the vector potential? In our model of spheres connected by a wire, it is determined by the current flowing in the wire:\n\\[\\vec I(t) = \\dv{q}{t} \\vu z = - q_0 \\omega \\sin(\\omega t) \\vu z\\] so\n\\[\\begin{aligned} \\vec A \u0026amp; = \u0026amp; \\frac{\\mu_0}{4 \\pi} \\int \\frac{ \\vec J(\\vec r\u0026#39;, t_r)}{\\gr} \\dd \\tau\u0026#39; \\\\ \u0026amp; = \u0026amp; \\frac{\\mu_0}{4 \\pi} \\int_{-d/2} ^{d/2} \\frac{- q_0 \\omega \\sin [ \\omega(t - \\gr / c)]\\vu z}{\\gr} \\dd z \\end{aligned}\\] Given our previous approximations, since the integration itself happens over the assumed short distance \\( d \\), we can replace the integrand by its value at the center and introduce a factor of \\( d \\)\n\\[\\vec A(r, \\theta, t) = - \\frac{\\mu_0 p_0 \\omega}{4 \\pi r} \\sin [\\omega(t - r/c)] \\vu z\\] Great, we\u0026rsquo;ve got the potentials! What are the fields in the radiation zone (far-field)?\n\\[\\begin{aligned} \\grad V \u0026amp; = \u0026amp; \\pdv{V}{r} \\vu r \u0026#43; \\frac{1}{r} \\pdv{V}{\\theta} \\vu \\theta \\\\ \u0026amp; = \u0026amp; - \\frac{p_0 \\omega}{4 \\pi \\epsilon_0 c} \\left[ \\cos \\theta \\left( - \\frac{1}{r^2} \\sin [\\omega (t - r/c)] - \\frac{\\omega}{rc} \\cos [\\omega(t - r/c)] \\right) \\vu r \\right. \\\\ \u0026amp; \u0026amp; \\qquad \\left. - \\frac{\\sin \\theta}{r^2} \\sin [\\omega(t - r/c)] \\vu \\theta \\right] \\\\ \u0026amp; \\approx \u0026amp; \\frac{p_0 \\omega^2}{4 \\pi \\epsilon_0 c^2} \\left( \\frac{\\cos \\theta}{r} \\right) \\cos [\\omega(t - r/c)] \\vu r \\end{aligned}\\] where we\u0026rsquo;ve dropped the first and last terms in accordance with our far-field approximation. Similarly,\n\\[\\pdv{\\vec A}{t} = - \\frac{\\mu_0 p_0 \\omega^2}{4 \\pi r} \\cos [\\omega(t - r/c)] (\\cos \\theta \\vu r - \\sin \\theta \\vu \\theta)\\] therefore\n\\[\\vec E = - \\grad V - \\pdv{\\vec A}{t} = - \\frac{\\mu_0 p_0 \\omega^2}{4 \\pi} \\left( \\frac{\\sin \\theta}{r} \\right) \\cos [ \\omega( t - r/c)] \\vu \\theta\\] What about the magnetic field?\n\\[\\curl \\vec A = \\frac{1}{r} \\left[ \\pdv{}{r} (r A_\\theta) - \\pdv{A_r}{\\theta} \\right] \\vu \\phi \\\\ = - \\frac{\\mu_0 p_0 \\omega}{4 \\pi r} \\left[ \\sin \\theta \\cos [ \\omega(t - r/c)] \u0026#43; \\frac{\\sin \\theta}{r} [\\omega(t - r/c)] \\right] \\vu \\phi\\] The second term can be eliminated in the far-field, so\n\\[\\vec B = \\curl \\vec A = - \\frac{\\mu_0 p_0 \\omega^2}{4 \\pi c} \\left( \\frac{\\sin \\theta}{r} \\right) \\cos [\\omega(t - r/c)]\\vu \\phi\\] These fields are in phase, mutually perpendicular, and transverse to the direction of propagation, and the ratio of their amplitudes is \\( E_0 / B_0 = c \\), exactly as we expect from electromagnetic waves. These are actually spherical waves, not plane waves, and their amplitude decreases like \\( 1/r \\) as they progress. But for large r, they are approximately plane over small regions.\nThe energy flux is determined by the Poynting vector\n\\[\\vec S(\\vec r, t) = \\frac{1}{\\mu_0} \\vec E \\cross \\vec B = \\frac{\\mu_0}{c} \\left[ \\frac{p_0 \\omega^2}{4 \\pi} \\left( \\frac{\\sin \\theta}{r} \\right) \\cos [\\omega(t - r/c)] \\right]^2 \\vu r\\] The intensity is the time average over a cycle\n\\[I = \\langle \\vec S \\rangle = \\left( \\frac{\\mu_0 p_0 ^2 \\omega^4}{32 \\pi^2 c} \\right) \\frac{\\sin^2 \\theta}{r^2} \\vu r\\] and the total power radiated is found by integrating \\( \\langle \\vec S \\rangle \\) over a sphere of radius \\( r \\):\n\\[\\langle P \\rangle = \\int \\langle \\vec S \\rangle \\cdot \\dd \\vec a = \\frac{\\mu_0 p_0 ^2 \\omega^4}{32 \\pi^2 c} \\int \\frac{\\sin^2 \\theta}{r^2} r^2 \\sin \\theta \\dd \\theta \\dd \\phi = \\frac{\\mu_0 p_0 ^2 \\omega^4}{12 \\pi c} \\] "},{"id":94,"href":"/r/notes/griffiths/problems-ch3/","title":"Solved Problems Ch3","section":"Griffiths Introduction to Electrodynamics","content":" Chapter 3 Solved Problems # \\[\\] Problem 3.24 # Q Solve Laplace\u0026rsquo;s equation by separation of variables in cylindrical coordinates, assuming there is no dependence on \\( z \\) (cylindrical symmetry). Make sure you find all solutions to the radial equation; in particular, your result must accomodate the case of an infinite line charge, for which we already know the answer. A Since we are in cylindrical coordinates, we will write Laplace\u0026rsquo;s equation in cylindrical coordinates \\( (s, \\phi, z) \\):\n\\[\\laplacian V = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{V}{s} \\right) \u0026#43; \\frac{1}{s^2} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\] We\u0026rsquo;ll try the method of separation of variables on s and \\( \\phi \\) by searching for solutions which are products of the form\n\\[V(s, \\phi) = S(s) \u0026#43; \\Phi(\\phi)\\] \\[\\frac{1}{s} \\Phi \\dv{}{s} \\left( s \\dv{S}{s} \\right) \u0026#43; \\frac{1}{s^2} S \\frac{d^2 \\Phi}{d\\phi ^2} = 0 \\] to separate the variables, we need to divide by V and multiply by \\( s^2 \\)\n\\[\\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) \u0026#43; \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} = 0\\] We define\n\\[f(s) = \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) \\] and\n\\[g(\\phi) = \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} \\] Since we have separated our independent variables and the sum is equal to zero, they must both be constant\n\\[f(s) = C_1 \\qquad g(\\phi) = C_2 \\qquad C_1 \u0026#43; C_2 = 0\\] Cylindrical symmetry implies that\n\\[\\text{ when } \\phi \\rightarrow \\phi \u0026#43; 2 \\pi : \\qquad \\Phi(\\phi \u0026#43; 2 \\pi) = \\Phi(\\phi)\\] So \\( C_2 \\) must be the positive one, since we know that will give us the periodic solutions to Laplace\u0026rsquo;s equation. We write our constant as \\( k^2 \\) so\n\\[\\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi \\\\ \\rightarrow \\Phi(\\phi) = A \\cos (k \\phi) \u0026#43; B \\sin (k \\phi), \\quad k = 0, 1, 2, 3, \\ldots\\] Back to the S part, we need a solution to\n\\[s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S\\] A convenient solution would be a power function, \\( S(s) = s^n \\) if we choose the power n appropriately\n\\[\\begin{aligned} s \\dv{}{s} \\left( s \\dv{s^n}{s} \\right) \u0026amp; = s \\dv{}{s} \\left( s n s^{n-1} \\right) \\\\ \u0026amp; = s \\dv{}{s} (n s^n) \\\\ \u0026amp; = s n^2 s^{n-1} \\\\ \u0026amp; = n^2 s^n \\\\ \u0026amp; = k^2 S = k^2 s^n \\\\ \u0026amp; \\rightarrow n = \\pm k \\end{aligned}\\] So, our general solution for S is\n\\[S(s) = C s^k \u0026#43; D s^{-k}\\] And our general solution will be an infinite series over k. But we have to now be careful, because previously we\u0026rsquo;ve expressed our general solution in terms of strictly non-zero k, but here we have \\( k = 0 \\), which gives us a constant solution\n\\[k = 0: \\qquad S(s) = C s^0 \u0026#43; D s^0 = \\text{const.}\\] But we should get two solutions for a second-order ordinary differential equation. If we go back to the differential equation for S,\n\\[s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S \\\\ \\rightarrow s \\dv{S}{s} = \\text{ const. } = C \\\\ \\rightarrow \\dv{S}{s} = \\frac{c}{s} \\\\ \\rightarrow \\dd S = C \\frac{ds}{s} \\\\ S(s) = C \\ln s \u0026#43; D \\] This gives us our second solution for S for \\( k = 0 \\).\nNow what about for \\( \\Phi \\)? Looking at the k = 0 case for the \\( \\Phi \\) ODE,\n\\[\\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi = 0 \\quad \\text{ for } k = 0 \\\\ \\frac{d \\Phi}{d \\phi} = \\text{ const. } = B \\\\ \\rightarrow \\Phi(\\phi) = B \\phi \u0026#43; A\\] But this doesn\u0026rsquo;t meet our periodicity requirement! This isn\u0026rsquo;t a physically acceptable solution. For k = 0, \\( \\Phi = B \\) is the only \u0026lsquo;physically acceptable\u0026rsquo; solution (we discard \\( B \\phi + A \\) out of hand.)\nFinally, our general solution looks like\n\\[V(s, \\phi) = a_0 \u0026#43; b_0 \\ln s \u0026#43; \\sum_{k=1} ^\\infty \\left[ s^k (a_k \\cos k \\phi \u0026#43; b_k \\sin k \\phi) \u0026#43; s^{-k} (a_k \\cos k \\phi \u0026#43; b_k \\sin k \\phi) \\right]\\] We\u0026rsquo;ve only been asked for the general solution in cylindrical coordinates (from which we can tell that our solution is independent of a), and we must be given boundary conditions in order to solve for the constants \\( a_k, b_k \\).\nProblem 3.27 # Q A sphere of radius R, centered at the origin, carries charge density\n\\[\\rho(r, \\theta) = k \\frac{R}{r^2} (R - 2r) \\sin \\theta\\] where k is a constant, and \\( r, \\theta \\) are the usual spherical coordinates. Find the approximate potential for points on the z axis, far from the sphere.\nA We are asked for the approximate potential for points on the z-axis far from the charge distribution, so we\u0026rsquo;ll calculate the terms of our potential from Eq 3.95, and stop when we find the first non-zero term, replacing \\( \\theta \\) for \\( \\alpha \\) and \\( z \\) for \\( r \\) as we go.\n\\[V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n\u0026#43;1)}} \\int (r\u0026#39;) P_n(\\cos \\alpha) \\rho(\\vec{r\u0026#39;}) \\dd{\\tau\u0026#39;}\\] Let\u0026rsquo;s start with the monopole term. The integral we have to calculate is simply the charge density integrated over the charge distribution\n\\[\\int \\rho(r) \\dd \\tau = k R \\int_0 ^R \\int _0 ^{\\pi} \\int_{0} ^{2 \\pi} \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta ) \\dd r \\dd \\theta \\dd \\phi \\\\ \\int _{0} ^R (R - 2r) \\dd r = \\left.(R r - r^2)\\right|_{0} ^R = 0\\] So the monopole term comes out to zero. Next, we try calculating the dipole term:\n\\[\\int r \\cos \\theta \\rho(r) \\dd \\tau = k R \\iiint r \\cos \\theta \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta) \\dd r \\dd \\theta \\dd \\phi\\] The \\( \\theta \\) integral will come out to\n\\[\\int_0 ^{\\pi} \\sin ^2 \\cos \\theta \\dd \\theta = \\int _0 ^\\pi \\sin ^2 \\theta \\dd (\\sin \\theta) = \\left. \\frac{1}{3} \\sin ^3 \\theta \\right|_0 ^\\pi = 0\\] Well dangit, we still don\u0026rsquo;t have the first non-zero term! On to the quadrupole term:\n\\[\\begin{aligned} \u0026amp; \\int r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\rho \\dd \\tau \\\\ = \u0026amp; \\iiint r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\frac{kR}{r^2} (R - 2r) \\sin \\theta r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\\\ = \u0026amp; \\frac{1}{2} kR \\iiint r^2 (3 \\cos ^2 \\theta - 1)(R - 2r) \\sin ^2 \\theta \\dd r \\dd \\theta \\dd \\phi \\end{aligned}\\] Thankfully we don\u0026rsquo;t have any cross-terms, so we can do the integrals separately. The integral in r is\n\\[\\int_0 ^R r^2 (R - 2r) \\dd r = - \\frac{R^4}{6} \\] The integral in \\( \\theta \\) is\n\\[\\int_0 ^\\pi (3 \\cos ^2 \\theta - 1) \\sin ^2 \\theta \\dd \\theta = \\int _0 ^\\pi \\left[ 3 (1 - \\sin ^2 \\theta) - 1 \\right] \\sin ^2 \\theta \\dd \\theta = - \\frac{\\pi}{8} \\] And we just get a \\( 2 \\pi \\) from the \\( \\phi \\) integral, so converting our r to z in our coordinate system, the whole quadrupole potential is\n\\[V(\\vec{r}) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{z^3} \\frac{1}{2} k R \\left( - \\frac{R^4}{6} \\right) \\left( - \\frac{\\pi}{8} \\right) (2 \\pi) \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\frac{k \\pi ^2 R ^5}{48 z^3} \\quad \\text{(Quadrupole)}\\] Problem 3.31 # Q In Ex. 3.9, we derived the exact potential for a spherical shell of radius R, which carries a surface charge\n\\[\\sigma = k \\cos \\theta\\] a) Calculate the dipole moment of this charge distribution.\nb) Find the approximate potential, at points far from the sphere, and compare the exact answer (Eq 3.87). What can you conclude about the higher multipoles?\nA By the symmetry of the problem, p is going to be in the z-direction: \\( \\vec{p} = p \\vu{z}; , p = \\int z \\rho \\dd \\tau \\rightarrow \\int z \\sigma \\dd a \\).\n\\[\\begin{aligned} p \u0026amp; = \\int (R \\cos \\theta)(k \\cos \\theta) R^3 \\sin \\theta \\dd \\theta \\dd \\phi \\\\ \u0026amp; = 2 \\pi R^3 k \\int _0 ^\\pi \\cos ^2 \\theta \\sin \\theta \\dd \\theta \\\\ \u0026amp; = 2 \\pi R^3 k \\left. \\left( - \\frac{\\cos ^3 \\theta}{3} \\right) \\right|_0 ^\\pi \\\\ \u0026amp; = \\frac{2}{3} \\pi R^3 k [ 1 - (-1) ] \\\\ \u0026amp; = \\frac{4 \\pi R^3 k}{3} \\end{aligned}\\] \\[\\tag{a} \\vec{p} = \\frac{4 \\pi R^3 k}{3} \\vu{z}\\] The associated dipole potential is just\n\\[V_{dip} \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vu{r} \\cdot \\vec{p}}{r^2} = \\frac{k R^3}{3 \\epsilon_0} \\frac{\\cos \\theta}{r^2} \\] Problem 3.33 # Q Show that the electric field of a \u0026lsquo;pure\u0026rsquo; dipole can be written in the coordinate-free form\n\\[\\vec{E_{dip}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ]\\] A We still assume the dipole is pointing in the z-direction and start with spherical coordinates, and then move to a coordinate-free system\n\\[\\vec{p} = p \\vu{z}\\] \\[\\vec{p} = p_r \\vu{r} \u0026#43; p_\\theta \\vu{\\theta} \u0026#43; p_{\\phi} \\vu{\\phi}\\] Since p is in the z-direction, we can safely say \\( p_\\phi = 0 \\)\n\\[p_r = \\vec{p} \\cdot \\vu{r} = p \\cos \\theta \\\\ p_\\theta = \\vec{p} \\cdot \\vu{\\theta} = - p \\sin \\theta \\\\ \\vec{p} = p \\cos \\theta \\vu{r} - p \\sin \\theta \\vu{\\theta}\\] So we can directly check this expression against the expression we got as Eqn 3.103 \\( (\\vec{E_{dip}}(r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta} ) \\):\n\\[\\begin{aligned} 3 ( \\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} = \u0026amp; 3 p \\cos \\theta \\vu{r} - p \\cos \\theta \\vu{r} \u0026#43; p \\sin \\theta \\vu{\\theta} \\\\ \u0026amp; = 2 p \\cos \\theta \\vu{r} \u0026#43; p \\sin \\theta \\vu{\\theta} \\\\ \\rightarrow \\vec{E_{dip}} \u0026amp; = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] \\end{aligned}\\] So it all checks out.\nProblem 3.34 # Q Three point charges are located as shown in Fig 3.38, each a distance a from the origin. Find the approximate electric field at points far from the origin. Express your answer in spherical coordinates, and include the two lowest orders in the multipole expansion.\nA We\u0026rsquo;ll get to the electric field by writing down the multipole expansion of the potential, and then using the approximate potential to get the electric field. The total charge is -q, so the monopole term will be\n\\[V_{mon} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r} (-q)\\] The dipole moment is given by\n\\[\\begin{aligned} \\vec{p} \u0026amp; = \\sum_{i=1} ^3 q_i \\vec{r_i} \\\\ \u0026amp; = (-q) a \\vu{y} \u0026#43; (-q) a (-\\vu{y}) \u0026#43; q a \\vu{z} \\\\ \u0026amp; = qa \\vu{z} \\end{aligned}\\] The dipole term in the multipole expansion of V is then\n\\[\\begin{aligned} V_{dip} \u0026amp; = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\\\ \u0026amp; = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\vu{z} \\cdot \\vu{r}}{r^2} \\\\ \u0026amp; = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\cos \\theta}{r^2} \\end{aligned}\\] \\[V(r, \\theta) \\approx \\frac{q}{4 \\pi \\epsilon_0 } \\left( - \\frac{1}{r} \u0026#43; \\frac{a \\cos \\theta}{r^2} \\right)\\] The field is given by\n\\[\\vec{E} = - \\grad V \\approx \\frac{q}{4 \\pi \\epsilon_0} \\left( - \\frac{1}{r^2} \\vu{r} \u0026#43; \\frac{2 a \\cos \\theta \\vu{r}}{r^3} \\vu{r} \u0026#43; \\frac{a}{r^3} \\sin \\theta \\vu{\\theta} \\right)\\] "},{"id":95,"href":"/r/notes/griffiths/problems-ch5/","title":"Solved Problems Ch5","section":"Griffiths Introduction to Electrodynamics","content":" Chapter 5 Solved Problems # \\[\\] Problem 5.5 # Q A current I flows down a wire of radius a. (a) If it is uniformly distributed over the surface, what is the surface current density K? (b) If it is distributed in such a way that the volume current density is inversely proportional to the distance from the axis, what is J(s)? A K is the current per unit width \\( \\perp \\) to the direction of the flow.\n\\[K = \\frac{I}{2 \\pi a} \\] Suppose instead the current is distributed somehow throughout the volume of the wire such that the current density is inversely proportional to the distance from the axis. Then\n\\[j = \\frac{\\text{current}}{\\text{unit area } \\perp \\text{ flow}} = \\frac{d I}{da_{\\perp}} \\] We suppose that j has the form\n\\[j = \\frac{\\text{const.}}{s} = \\frac{c}{s} \\] \\[\\begin{aligned} I \u0026amp; = \\int j \\dd a_{\\perp} \\\\ \u0026amp; = \\int_{0} ^a \\int_{0} ^{2 \\pi} \\frac{c}{s} s \\dd s \\dd \\phi \\\\ \u0026amp; = 2 \\pi c a \\end{aligned}\\] so\n\\[c = \\frac{I}{2 \\pi a} \\] and\n\\[j = \\frac{I}{2 \\pi a s} \\] Problem 5.11 # Q Find the magnetic field at point P on the axis of a tightly wound solenoid (helical coil) consisting of n turns per unit length wrapped around a cylindrical tube of radius a and carrying current I (Fig 5.25). Express your answer in terms of \\( \\theta_0 \\) and \\( \\theta_2 \\) (it\u0026rsquo;s easiest that way). Consider the turns to be essentially circular and use the result of Ex 5.6. What is the field on the axis of an infinite solenoid (infinite in both directions)?\nA If I have n turns per unit length, then I have \\( n \\dd z \\) turns along a length \\( \\dd z \\) (using the natural cylindrical coordinates of the problem). The total current of the resulting loop is \\( I n \\dd z \\). From Ex 5.6, we know the magnetic field due to a circular loop is\n\\[\\dd \\vec{B}(z) = \\frac{ \\mu_0 n I \\dd z}{2} \\frac{a^2}{(a^2 \u0026#43; z^2 )^{3/2}} \\vu{\\phi} \\] From the geometry of Fig 5.25,\n\\[\\tan \\theta = \\frac{a}{z} \\quad \\rightarrow \\quad z = \\frac{a}{\\tan \\theta} \\] \\[\\dd z = - \\frac{a}{\\sin ^2 \\theta} \\dd \\theta\\] \\[(a^2 \u0026#43; z^2)^{3/2} = \\left( a^2 \u0026#43; \\frac{a^2}{\\tan ^2 \\theta} \\right)^{3/2} = \\left( \\frac{a}{\\sin \\theta} \\right)^3\\] \\[\\begin{aligned} B(z) \u0026amp; = \\frac{\\mu_0 n I}{2} \\left( - \\frac{a}{\\sin ^2 \\theta} \\dd \\theta \\right)\\frac{a^2}{(a^3 / \\sin ^3 \\theta)} \\\\ \u0026amp; = - \\frac{\\mu_0 n I}{2} \\sin \\theta \\dd \\theta \\end{aligned}\\] \\[\\begin{aligned} B(z) \u0026amp; = - \\frac{\\mu_0 n I}{2} \\int _{\\theta_1} ^{\\theta_2} \\sin \\theta \\dd \\theta \\\\ \u0026amp; = \\frac{\\mu_0 n I}{2} (\\cos \\theta_2 - \\cos \\theta_1) \\end{aligned}\\] For an infinite solenoid, we get\n\\[B(z) = \\frac{\\mu_0 n I}{2} (\\cos(0) - \\cos \\theta_1)\\] Problem 5.23 # Q Find the magnetic vector potential of a finite segment of straight wire carrying a current I. [Put the wire on the z axis, from \\( z_1 \\) to \\( z_2 \\), and use Eq. 5.66.] Check that your answer is consistent with Eq. 5.37.\nA We will get our vector potential using Eq 5.66, as suggested\n\\[\\begin{aligned} \\vec{A} \u0026amp; = \\frac{\\mu_0 }{4 \\pi} \\int \\frac{I \\vu{z}}{\\gr} \\\\ \u0026amp; = \\frac{\\mu_0 I}{4 \\pi} \\vu{z} \\int _{z_1} ^{z_2} \\frac{dz}{\\sqrt{z^2 \u0026#43; s^2}} \\\\ \u0026amp; = \\left. \\frac{\\mu_0 I}{4 \\pi} \\vu{z} \\left[ \\ln \\left( z \u0026#43; \\sqrt{z^2 \u0026#43; s^2} \\right) \\right] \\right|_{z_1} ^{z_2} \\\\ \u0026amp; = \\frac{\\mu_0 I}{4 \\pi} \\ln \\left( \\frac{z_2 \u0026#43; \\sqrt{(z_2)^2 \u0026#43; s^2}}{z_1 \u0026#43; \\sqrt{(z_1)^2 \u0026#43; s^2}} \\right) \\vu{z} \\end{aligned}\\] To get the magnetic field, we need to take the curl of A. We can easily tell from the symmetry of the problem that the field will be \u0026ldquo;circumferential\u0026rdquo; (in the \\( \\vu{\\phi} \\) direction):\n\\[\\begin{aligned} \\vec{B} \u0026amp; = \\curl \\vec{A} = - \\pdv{A}{s} \\vu{\\phi} \\\\ \u0026amp; = - \\frac{\\mu_0 I}{4 \\pi} \\left( \\frac{1}{z_2 \u0026#43; \\sqrt{(z_2)^2 \u0026#43; s^2}} \\frac{s}{\\sqrt{(z_2)^2 \u0026#43; s^2}} - \\frac{1}{z_1 \u0026#43; \\sqrt{(z_1)^2 \u0026#43; s^2}} \\frac{s}{\\sqrt{(z_1)^2 \u0026#43; s^2}} \\right) \\vu{\\phi} \\\\ \u0026amp; = - \\frac{\\mu_0 I s}{4 \\pi} \\left( \\frac{z_2 - \\sqrt{z_2 ^2 \u0026#43; s^2}}{z_2 ^2 - (z_2 ^2 \u0026#43; s^2)} \\frac{1}{\\sqrt{z_2 ^2 \u0026#43; s^2}} - \\frac{z_1 - \\sqrt{z_1 ^2 \u0026#43; s^2}}{z_1 ^2 - (z_1 ^2 \u0026#43; s^2)} \\frac{1}{\\sqrt{z_1 ^2 \u0026#43; s^2}} \\right) \\vu{\\phi} \\\\ \u0026amp; = - \\frac{\\mu_0 I s}{4 \\pi} \\left( - \\frac{1}{s^2} \\right) \\left( \\frac{z_2}{\\sqrt{z_2 ^2 \u0026#43; s^2}} - 1 - \\frac{z_1}{\\sqrt{z_1 ^2 \u0026#43; s^2}} \u0026#43; 1 \\right) \\vu{\\phi} \\\\ \u0026amp; = \\frac{\\mu_0 I}{4 \\pi s} \\left( \\frac{z_2}{\\sqrt{(z_2) ^2 \u0026#43; s^2}} - \\frac{z_1}{\\sqrt{z_1 ^2 \u0026#43; s^2}} \\right) \\vu{\\phi} \\end{aligned}\\] or, in terms of the angles made between r and the axis of the wire,\n\\[\\sin \\theta_1 = \\frac{z_1}{\\sqrt{z_1 ^2 \u0026#43; s^2}} \\quad \\text{ and } \\quad \\sin \\theta_2 = \\frac{z_2}{\\sqrt{z_2 ^2 \u0026#43; s^2}} \\] \\[\\vec{B} = \\frac{\\mu_0 I}{4 \\pi s} (\\sin \\theta_2 - \\sin \\theta_1) \\vu{\\phi} \\] which is just what we got back in Eq. 5.37.\nProblem 5.26 # Q (a) By whatever means you can think of (short of looking it up), find the vector potential a distance \\( s \\) from an infinite straight wire carrying a current \\( I \\). Check that \\( \\div \\vec{A} = 0 \\) and \\( \\curl \\vec{A} = \\vec{B} \\).\n(b) Find the magnetic potential inside the wire, if it has radius R and the current is uniformly distributed.\nA (a) As we said, because the current distribution is infinite, we cannot use Eq. 5.65 to get A. So let\u0026rsquo;s use some symmetry. A must be parallel (or antiparallel) to I, and is a function of only s (the distance from the wire). In cylindrical coordinates, then, \\( \\vec{A} = A(s) \\vu{z} \\). We already calculated the magnetic field of an infinite straight wire via Biot-Savart:\n\\[\\vec{B} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi}\\] We can work backwards to get A from B in this case.\n\\[\\vec{B} = \\curl \\vec{A} = - \\pdv{A}{s} \\vu{\\phi} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi}\\] Therefore\n\\[\\pdv{A}{s} = -\\frac{\\mu_0 I}{2 \\pi s} \\quad \\rightarrow \\quad \\vec{A}(r) = - \\frac{\\mu_0 I}{2 \\pi} \\ln (s / a) \\vu{z}\\] There is an arbitrary constant a here which doesn\u0026rsquo;t actually affect our gauge at all:\n\\[\\div \\vec{A} = \\pdv{A_z}{z} = 0\\] \\[\\curl \\vec{A} = - \\pdv{A_z}{s} \\vu{\\phi} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} = \\vec{B}\\] (b) Ampere\u0026rsquo;s law in this case says\n\\[\\oint \\vec{B} \\cdot \\dd \\vec{l} = B 2 \\pi s = \\mu_0 I_{enc} = \\mu_0 J \\pi s^2 = \\mu_0 \\frac{I}{\\pi R^2} \\pi s^2 = \\frac{\\mu_0 I s^2}{R^2} \\] so, inside the wire,\n\\[\\vec{B} = \\frac{\\mu_0 I s}{2\\pi R^2} \\vu{\\phi}\\] From the definition of A,\n\\[\\pdv{A}{s} = - \\frac{\\mu_0 I}{2 \\pi} \\frac{s}{R^2} \\rightarrow \\vec{A} = -\\frac{\\mu_0 I}{2 \\pi R^2} \\int_{b} ^s s \\, \\dd s = - \\frac{\\mu_0 I}{4 \\pi R^2} (s^2 - b^2) \\vu{z}\\] Here, again, b is arbitrary, except that A must be continuous at R (we know that A is continuous!)\n\\[- \\frac{\\mu_0 I}{2 \\pi } \\ln (R / a) = - \\frac{\\mu_0 I}{4 \\pi R^2} (R^2 - b^2)\\] which means that we have to pick a and b such that\n\\[2 \\ln (R / b) = 1 - (b / R)^2\\] One such combination of a and b is \\( a = b = R \\). Then\n\\[\\vec{A} = \\begin{cases} - \\frac{\\mu_0 I}{4 \\pi R^2} (s^2 - R^2) \\vu{z} \u0026amp; \\quad \\text{ for } s \\leq R \\\\ - \\frac{\\mu_0 I}{2 \\pi} \\ln(s / R) \\vu{z}\u0026amp; \\quad \\text{ for } s \\geq R \\end{cases}\\] Problem 5.37 # Q (a) A phonograph record of radius R, carrying a uniform surface charge \\( sigma \\) is rotating at constant angular velocity \\( \\omega \\). Find its magnetic dipole moment.\n(b) Find the magnetic dipole moment of the spinning spherical shell in Example 5.11. Show that for points \\( r \u0026gt; R \\) the potential is that of a perfect dipole.\nA (a) We get the monopole moment by integrating over the disk of the record. For a ring at radius r, \\( m = I \\pi r^2 \\). In this case,\n\\[I \\rightarrow \\sigma v \\dd r = \\sigma \\omega r \\dd r\\] so\n\\[m = \\int _0 ^R \\pi r^2 \\sigma \\omega r \\dd r = \\pi \\sigma \\omega R^4 / 4\\] (b) To get the magnetic dipole moment of our sphere, we need to integrate over the surface of the sphere:\nThe total charge on the shaded ring is \\\\( \\dd q = \\sigma (2 \\pi R \\sin \\theta) R \\dd \\theta \\\\). The time to make one revolution is \\\\( \\dd t = 2 \\pi \\omega \\\\), so the current in the ring is \\[I = \\frac{dq}{dt} = \\sigma \\omega R^2 \\sin \\theta \\dd \\theta\\] The area of the ring is \\( \\pi (R \\sin \\theta)^2 \\), so the magnetic moment of the ring is\n\\[\\dd m = (\\sigma \\omega R^2 \\sin \\theta \\dd \\theta) \\pi R^2 \\sin ^2 \\theta\\] and the total dipole moment is\n\\[m = \\sigma \\omega \\pi R^4 \\int_0 ^\\pi \\sin ^3 \\theta \\dd \\theta = (4 / 3) \\sigma \\omega \\pi R^4\\] and we know that m points in the \\( \\vu{z} \\) direction (right-hand-rule), so\n\\[\\vec{m} = \\frac{4 \\pi}{3} \\sigma \\omega R^4 \\vu{z}\\] The dipole term in the multipole expansion for A is therefore\n\\[\\vec{A}_{dip} = \\frac{\\mu_0}{4 \\pi} \\frac{4 \\pi}{3} \\sigma \\omega R^4 \\frac{\\sin \\theta}{r^2} \\vu{\\phi} = \\frac{\\mu_0 \\sigma \\omega R^4}{3} \\frac{\\sin \\theta}{r^2} \\vu{\\phi}\\] This is actually the exact vector potential we calculated (Eq. 5.69); evidently a spinning sphere produces a perfect dipole field, with no higher multipole contributions.\n"},{"id":96,"href":"/r/notes/griffiths/problems-ch7/","title":"Solved Problems Ch7","section":"Griffiths Introduction to Electrodynamics","content":" Chapter 7 Solved Problems # \\[\\] Problem 7.7 # Q A metal bar of mass m slides frictionlessly on two parallel conducting rails a distance l apart. A resistor R is connected across the rails, and a uniform magnetic field B pointing into the page fills the region.\n(a) If the bar moves to the right at speed v, what is the current in the resistor?\n(b) What is the magnetic force on the bar? In what direction?\n(c) If the bar starts out with speed \\( v_0 \\) at time \\( t = 0 \\), and is left to slide, what is its speed at a later time t?\n(d) The initial kinetic energy of the bar was, of course, \\( 1/2 m v_0 ^2 \\). Check that the energy delivered to the resistor is exactly \\( 1/2 m v_0 ^2 \\)\nA (a) To get the current through the resistor, calculate the flux through the loop:\n\\[\\Phi_B = B l x \\\\ emf = - \\pdv{\\Phi}{t} = - Blv\\] We can always use good old Ohm\u0026rsquo;s law\n\\[I = V / R = -Blv / R\\] The induced magnetic flux opposes the change in flux, so the current flows down through the resistor.\n(b) Force on the bar? Just use Lorentz force law\n\\[F = I \\int \\dd \\vec{l} \\cross \\vec{B} = - \\frac{B^2 l^2 v}{R}\\] The direction opposes x (force is to the left).\n(c)\n\\[v(t = 0) = v_0 \\\\ F = m \\dv{v}{t} = - \\frac{B^2 l^2}{R} v \\\\ \\frac{\\dd v}{v} = - \\frac{B^2 l^2 }{m R } \\dd t \\\\ \\int_{v_0} ^v \\frac{\\dd v}{v} = - \\frac{B^2 l^2 }{m R } \\int_0 ^t \\dd t \\\\ \\ln \\frac{v}{v_0} = - \\frac{B^2 l^2 }{m R } t \\\\ v = v_0 e^{-\\frac{B^2 l^2 }{m R } t}\\] (d) Power dissipated in a resistor is\n\\[P = I^2 R \\] So energy delivered is\n\\[\\int_0 ^\\infty I^2 R \\dd t = \\frac{B^2 l^2 v^2}{R^2} R \\dd t \\\\ = \\int_0 ^\\infty \\frac{B^2 l^2}{R} v_0 ^2 e^{-2\\frac{B^2 l^2 }{m R } t} \\dd t \\\\ = - \\frac{B^2 l^2}{R} v_0 ^2 \\frac{m R}{2 B^2 l^2} [ 0 - 1 ] \\\\ = \\frac{1}{2} m v_0 ^2\\] Hooray!\nProblem 7.34 # Q A fat wire, radius a, carries constant current I, uniformly distributed over its cross section. A narrow gap in the wire of width \\( w \u0026laquo; a \\) forms a parallel-plate capacitor, as shown. Find the magnetic field in the gap, at a distance \\( s \u0026lt; a \\) from the axis. A Within the wire, you can draw an Amperian loop to find B within the wire with\n\\[\\int B \\cdot \\dd l = \\mu_0 I_{enc}\\] Within the gap, we need the Ampere\u0026rsquo;s correction term\n\\[\\curl \\vec{B} = \\mu_0 \\vec{J} \u0026#43; \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\\\ \\oint B \\cdot \\dd l = \\mu_0 \\epsilon_0 \\int \\pdv{\\vec{E}}{t} \\cdot \\dd l\\] Current is flowing to the end of the wire with nowhere to go, so it must build up there. We\u0026rsquo;ve got a parallel plate capacitor within the gap\n\\[B(s) \\cdot 2 \\pi s = \\mu_0 \\epsilon_0 \\dv{}{t} \\int \\frac{\\sigma (t) }{\\epsilon_0} \\dd a\\] \\[B(s) = \\frac{\\mu_0}{2} s \\pdv{\\sigma}{t} = \\frac{\\mu_0 s}{2} \\frac{I}{\\pi a^2}\\] where \\( \\pdv{\\sigma}{t} = \\dv{A}{t} \\frac{1}{\\pi a^2} \\)\n\\[B(s) = \\frac{\\mu_0 I}{2 \\pi a^2} s \\hat{\\phi}\\] "},{"id":97,"href":"/r/notes/griffiths/problems-ch9/","title":"Solved Problems Ch9","section":"Griffiths Introduction to Electrodynamics","content":" Chapter 9 Solved Problems # \\[\\] Review Problem 1 # Q Current-carrying wire with gap. A large diameter wire of cross-section area \\( A \\) carries current uniformly over its cross-section. There is a narrow gap of width \\( d \\), forming a parallel-plate capacitor. The current is zero for times \\( t \u0026lt; 0 \\) and the current is \\( I \\) at times \\( t \u0026gt; 0 \\). The charge on the capacitor is zero at \\( t = 0 \\). Neglect fringe fields.\n(a) Find the electric field in the gap.\n(b) Find the magnetic field in the gap.\nA (a)\nWith the normal Gauss\u0026rsquo; law approach to capacitors\n\\[\\vec E = \\frac{I t}{\\epsilon_0 A} \\vu z\\] (b)\nFaraday\u0026rsquo;s law here\n\\[\\int ( \\curl \\vec B) = \\int \\mu_0 \\epsilon _0 \\pdv{ \\vec E}{t} \\cdot \\dd \\vec{a}\\] \\[\\rightarrow \\oint \\vec B \\cdot \\dd \\vec l = \\mu_0 \\epsilon_0 \\frac{I}{\\epsilon_0 A} \\pi r^2\\] \\[B \\cdot 2 \\pi r = \\mu_0 \\frac{I \\pi r^2}{A} \\rightarrow \\vec B = \\frac{\\mu_0 I r}{2 A} \\vu \\phi\\] Review Problem 2 # Q Waves in non-conductors. A laser beam in vacuum has power 20GW and diameter 1 mm.\n(a) Find the magnitude and direction of the Poynting vector.\n(b) Find the peak values of the \\( \\vec E \\) and \\( \\vec B \\) fields.\n(c) The beam then enters lossless glass having index of refraction 1.6. Assuming no reflection at the air-glass interface and the glass is non-magnetic, find the peak values of the \\( \\vec E \\) and \\( \\vec B \\) fields. Hint: consider whether the Poynting vector changed.\nA To get the magnitude of the Poynting vector, we need to realize that the intensity of the beam is related to the time average of the Poynting vector\n\\[\\langle \\vec s \\rangle = I = \\frac{20 \\cdot 10^{9}}{\\pi (1\\text{mm} / 2)^2} \\approx 2.5 \\cdot 10^{16} \\text{W} / \\text{m}^2\\] As for the direction of the Poynting vector, it\u0026rsquo;s always orthogonal to both \\( \\vec E \\) and \\( \\vec B \\), in the direction of the laser beam (\\( \\vu k \\))\nTo get the peak values of \\( \\vec E \\) and \\( \\vec B \\), recall that for monochromatic plane waves (like lasers) the time average of the Poynting vector can be related to the peak value of the field by using the relationship \\( E_0 / B_0 = v \\) and\n\\[\\langle \\vec S \\rangle = \\frac{1}{2} c \\epsilon _0 E_0 ^2 \\vu {z}\\] \\[\\rightarrow E_0 \\approx 4.3 \\cdot 10^9 \\text{V} / \\text{m} \\] \\[B_0 = \\frac{E_0}{c} \\approx 14.3\\text{Teslas}\\] Now, the beam enters a linear medium without any reflection, so the intensity does not change but the electric field will be lessened by the polarization of the material. Note: you could assume that the problem is an artificial scenario (we crank up the intensity of the beam so that the transmission is the same as the original intensity), or we could suppose that the beam enters at the Brewster\u0026rsquo;s angle so that the reflected beam is quenched.\nWithin the linear medium, we just replace \\( c \\) with \\( v \\) and \\( \\epsilon_0 \\) with \\( \\epsilon \\)\n\\[\\langle \\vec S \\rangle = \\frac{1}{2} v \\epsilon (E_0 \u0026#39;) ^2\\] \\[\\rightarrow E_0 \u0026#39; = \\sqrt{\\frac{2In}{c \\epsilon } } = \\frac{E_0}{\\sqrt{n}}\\] \\[B_0 = \\frac{E_0 \u0026#39;}{v} = \\frac{E_0 / \\sqrt{n}}{c / n} = B_0 \\sqrt{n}\\] Review Problem 3 # Q Waves in conductors. Consider a 1MHz plane wave in a vacuum incident on a thick slab of copper. Copper is a good conductor, non-magnetic, and you can assume its conductivity is \\( 6 \\cdot 10^7 \\Omega ^{-1}/ m \\).\n(a) What is the wave\u0026rsquo;s skin depth in the copper?\n(b) What is the wavelength in vacuum?\n(c) What is the wavelength in copper?\n(d) What is the wave\u0026rsquo;s propagation velocity (phase velocity) in vacuum?\n(e) What is the wave\u0026rsquo;s propagation velocity (phase velocity) in the copper?\nA To get the skin depth, we want \\( d = 1 / \\kappa \\). For a good conductor, we simplify the situation with\n\\[\\kappa = k = \\sqrt{\\frac{\\omega \\sigma \\mu_0}{2}}\\] \\[\\rightarrow d = \\sqrt{\\frac{2}{\\omega \\sigma \\mu_0}} \\approx 65 \\mu\\text{m}\\] What wavelength does a 1MHz wave have in vacuum?\n\\[\\lambda = \\frac{c}{f} = \\frac{2 \\pi c}{\\omega} \\approx 300 \\text{m}\\] Now that we\u0026rsquo;re in copper, what is the wavelength?\n\\[\\lambda = \\frac{2 \\pi}{\\text{Re}[k]} = 2 \\pi d = 2 \\pi \\sqrt{ \\frac{2}{\\omega \\sigma \\mu_0}} \\approx 0.4 \\text{mm}\\] So the wavelength has shrunk by a huge factor. The speed of the wave will have dropped significantly in the conductor, while the frequency must match, so it makes sense that the wavelength must also shrink.\nThe phase velocity in vacuum is simply \\( c = \\omega / k \\) = \\( 3 \\cdot 10^8 \\) m/s. In the conductor,\n\\[v = \\frac{\\omega}{k} = \\omega d \\approx 400 \\text{m}/\\text{s}\\] Review Problem 4 # Q Dispersive Gaseous Medium. A dilute gaseous medium is found to exhibit a single optical resonance at frequency \\( \\omega_0 = 2\\pi \\cdot 10^{15} \\) Hz. The electric field of a plane wave at frequency \\( \\omega_0 \\) propagating through this medium is attenuated by a factor of two over a distance of 10 meters. The frequency width of the absorption resonance is \\( \\Delta \\omega \\).\n(a) What is the absorption coefficient \\( \\alpha \\) at resonance?\n(b) Arrange in ascending order the propagation velocities at frequencies \\( \\omega_0, \\omega_0 + \\Delta \\omega / 10 \\), and \\( \\omega_0 - \\Delta \\omega / 10 \\). Show your reasoning.\n(c) If there were no other resonances in the medium, what are the approximate numerical values of the index of refraction and the propagation velocity on resonance?\nA To recall, for a dilute medium\n\\[n = \\tilde{n} = 1 \u0026#43; \\frac{Nq^2}{2m\\epsilon_0} \\sum_j \\frac{f_j}{(\\omega _j ^2 - \\omega_0 ^2 ) - i \\gamma_j \\omega}\\] \\[\\alpha = 2 \\kappa \\approx \\frac{N q^2 \\omega^2}{m \\epsilon_0 c} \\sum_j \\frac{ f_j \\gamma _j}{(\\omega_j ^2 - \\omega ^2 ) ^2 \u0026#43; \\gamma _j ^2 \\omega ^2}\\] If the field drops by a factor of 2 over 10 meters, and the dissipation goes as \\( E \\propto e^{-\\kappa z} \\), so\n\\[e^{-\\kappa (10m)} = \\frac{1}{2} \\rightarrow \\alpha = 2 \\kappa \\approx 0.14 m^{-1}\\] To arrange the propagation velocities, we need to remember what the propagation velocity looks like:\n\\[v = \\frac{\\omega}{k} = \\frac{c}{n}\\] recalling that \\( \\dv{n}{\\omega} \u0026lt; 0 \\) within the resonance linewidth,\n\\[v( \\omega_0 - \\Delta \\omega / 10 ) \u0026lt; v(\\omega_0) \u0026lt; v( \\omega_0 \u0026#43; \\Delta \\omega / 10 ) \\] Assuming there are no other resonances, then exactly at resonance \\( n(\\omega_0) = 1 \\). Then, \\( v_p(\\omega) = \\frac{c}{n} = c \\).\n"},{"id":98,"href":"/r/notes/working/crews2018/","title":"Crews (2018)","section":"Working","content":" Development of a Collisionless Plasma Kinetic Solver and an Investigation of One-Dimensional Plasma Waves and Instabilities # Shielded potential of a test electron:\n\\[\\phi(r) = \\frac{-e}{4 \\pi \\epsilon_0 r} e ^{- r / \\lambda_D}\\] where the Debye length is \\( \\lambda_D = \\sqrt{\\frac{\\epsilon_0 T_e}{ n_e e}} \\) . The mean free path between large-angle collisions is estimated as\n\\[\\lambda_{mfp} \\sim \\frac{\\epsilon_0 T_e ^2}{\\phi_e n_e \\log ( \\Lambda)}\\] where \\(\\phi_e = e^2 / 4 \\pi \\epsilon_0\\) are the constants from the Coulomb force law.\nSmooth out the discreteness of particles via spatial average over small volumes:\n\\[\\rho \\rightarrow \\langle \\rho_c \\rangle \\qquad \\vec E \\rightarrow \\langle \\vec E \\rangle \u0026#43; \\delta \\vec E\\] The mean field \\(\\langle \\vec E \\rangle\\) is responsible for collective modes of plasma motion. Estimate the collisionality of the plasma by comparing the length scales \\(\\lambda_{mfp} / \\lambda_D\\) \\[\\frac{\\lambda_{mfp}}{\\lambda_D} \\sim \\frac{T_e ^{3/2}}{n_e ^{1/2}}\\] Plasma is seen to become collisionless as the temperature becomes high or the plasma becomes more rarified.\nPhase space mechanics # To arrive at a kinetic equation governing the collisionless mechanics, consider the one-dimensional motion of a single particle\n\\[\\dot{x} = v \\qquad \\dot v = F(x)\\] and define the phase space coordinates as \\(\\vec{\\dot r} = \\vec F \\equiv [ v, F(x) ]\\) . The flux vector is similar to the velocity field of a fluid flow. The streamlines of \\(\\vec F\\) are the streamlines which a particle will follow if the flux is constant in time. Phase flow is always analogous to that of an incompressible fluid because the flux divergence is zero:\n\\[\\div [v, F(x)] = \\pdv{v}{x} \u0026#43; \\pdv{F(x)}{v}\\] If the phase fluid density is given by a function \\(f(x, v, t)\\) where \\(t\\) is the time parameter, because any instantiation of a particle can not leave the phase plane, the probability density will be conserved. We can write a conservation law:\n\\[\\pdv{f(x, v, t)}{t} = - \\div ( f (x, v, t) \\vec F)\\] and due to the flow\u0026rsquo;s incompressibility\n\\[\\div (f \\vec F) = f ( \\div \\vec F) \u0026#43; \\vec F \\cdot \\grad f = \\vec F \\cdot \\grad f \\\\ \\rightarrow \\pdv{f}{t} = - \\left[ v, F(x) \\right] \\cdot \\left[ \\pdv{f}{x}, \\pdv{f}{v} \\right] \\\\ \\rightarrow \\pdv{f}{t} \u0026#43; v \\pdv{f}{x} \u0026#43; F(x) \\pdv{f}{v} = 0\\] the probability density \\( f \\) satisfies a simple advection equation in the phase space.\nVlasov Equation # Beginning from the probability density kinetic equation, one can arrive at the Boltzmann equation for the ensemble-averaged velocity distribution function \\( f(\\vec x, \\vec v, t) \\) of a gas or plasma\n\\[\\pdv{f}{t} \u0026#43; \\vec v \\cdot \\grad_x f \u0026#43; \\frac{1}{m} \\vec F \\cdot \\grad_v f = C(f)\\] where \\( C(f) \\) is in general an integral operator representing inter-particle correlations. We arrive at the Vlasov-Poisson system of equations if we entirely neglect correlations and only consider the electrostatic potential. Restricted to a single spatial dimension, it has the form\n\\[\\pdv{f _ \\alpha}{t} \u0026#43; v \\pdv{f _ \\alpha}{x} - \\frac{Z _ \\alpha}{m _ \\alpha} \\pdv{\\phi}{x} \\pdv{f _ \\alpha}{v} = 0 \\\\ \\dv{ ^2 \\phi}{x^2} = - \\frac{1}{\\epsilon_0} \\sum_\\alpha Z_\\alpha \\int_{-\\infty} ^\\infty f _ \\alpha(x, v, t) \\dd v\\] The most striking difference between the dynamics of neutral fluids and a collisionless plasma is the appearance of non-equilibrium velocity distributions in plasma as a result of collective behavior. The collision operator enforces local thermodynamic equilibrium, driving the velocity distribution to Maxwellian.\n\\[f(x, v) = n(x) \\sqrt{ \\frac{1}{2 \\pi v_{th} ^2 (x) }} \\exp \\left( - \\frac{(v - u(x))^2}{2 v_{th}^2 (x)} \\right)\\] where \\[n(x) = \\int_{-\\infty} ^\\infty f(x, v, t) \\\\ u(x) = \\frac{1}{n(x)} \\int_{-\\infty} ^\\infty v f(x, v, t) \\dd v \\\\ v_{th} ^2 (x) = \\frac{1}{n(x)} \\int_{-\\infty} ^\\infty (v - u(x))^2 f(x, v, t) \\dd v\\] In absence of collisions, there is no driver for the velocity distribution to posess normal statistics. The structure of the distribution function within the phase space is generated through wave-particle resonance, where the waves with phase velocity \\( v_{ph} = \\omega / k \\) resonate with particles traveling at the same velocity. This process is Landau resonance.\n"},{"id":99,"href":"/r/notes/working/datta2021/","title":"Datta (2021)","section":"Working","content":" \\[\\] A Domain-Hybridized Plasma Model Using Discontinuous Galerkin Finite Elements # Models # Keep in mind that the behavior of a plasma is primarily determined by the ratio between a spatial length of interest and the following reference lengths. The appropriate model for describing a plasma depends on the regime defined by these lengths.\nReference length Degree of magnetization Larmor radius \\( r_L \\) Charge separation Debye length \\( \\lambda_D \\) Collisionality Mean free path \\( \\lambda_{mfp} \\) Continuum Kinetic Model # The continuum kinetic model is the most complete description, taking into account the forces acting upon each particle:\n\\[\\dv{\\vec v_i}{t} = \\frac{q_i}{m_i} \\left( \\vec E^M \u0026#43; \\vec v_i \\cross \\vec B^M \\right)^\\prime\\] where \\( M \\) signifies the microscopic fields generated by particles and the \\( \\prime \\) signifies the force on particle \\( i \\) excludes the self-force. Given an ensemble of \\( \\overline{N_\\alpha} \\) particles, the density in phase space is\n\\[N_\\alpha (\\vec x, \\vec v, t) = \\sum_{1 \\leq i \\leq \\overline{N_\\alpha}} \\delta [\\vec x - \\vec x_i (t) ] \\delta [\\vec v - \\vec v_i(t)]\\] Taking the total derivative of \\( N_\\alpha \\) we get the Klimontovich equation\n\\[\\dv{N_\\alpha (\\vec x, \\vec v, t)}{t} = \\pdv{N_{\\alpha} (\\vec x, \\vec v, t)}{t} \u0026#43; \\vec v \\cdot \\pdv{N_\\alpha (\\vec x, \\vec v, t)}{\\vec x} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} \\left( \\vec E^M \u0026#43; \\vec v_i \\cross \\vec B^M \\right)^\\prime \\cdot \\pdv{N_\\alpha(\\vec x, \\vec v, t)}{\\vec v} = 0\\] A tractable kinetic model evolves the smooth probability distribution function \\( f_\\alpha \\) of species \\( \\alpha \\)\n\\[\\pdv{f_\\alpha}{t} \u0026#43; v_i \\pdv{f_\\alpha}{x_i} \u0026#43; \\frac{q_\\alpha}{m_\\alpha} (E_i \u0026#43; \\epsilon_{ijk} v_j B_k) \\pdv{f_\\alpha}{v_i} = \\left. \\pdv{f_\\alpha}{t} \\right| _C\\] Or, in conservative form:\n\\[\\pdv{f_\\alpha}{t} \u0026#43; \\pdv{}{x_i} (v_i f_\\alpha) \u0026#43; \\pdv{}{v_i} \\left[ \\frac{q_\\alpha}{m_\\alpha} (E_i \u0026#43; \\epsilon_{ijk} v_j B_k) f_\\alpha \\right] = \\left. \\pdv{f_\\alpha}{t} \\right| _C\\] 5N-moment Model # We take velocity-space moments of the Boltzmann equation to yield conservation laws for density, momentum, and energy\nContinuity equation # \\[\\pdv{\\rho _{\\alpha}}{t} \u0026#43; \\pdv{(\\rho v_{\\alpha i})}{x_i} = 0\\] Momentum equation # \\[\\pdv{\\rho_\\alpha v_{\\alpha i}}{t} \u0026#43; \\pdv{(\\rho_\\alpha v_{\\alpha i} v_{\\alpha j} \u0026#43; P_{\\alpha i j})}{x_j} = \\frac{q_\\alpha \\rho_\\alpha}{m_\\alpha} (E_i \u0026#43; \\epsilon_{ijk} v_{\\alpha j} B_k) \u0026#43; \\sum_{\\beta \\neq \\alpha} R_{\\alpha \\beta_i}\\] where \\( R_{\\alpha \\beta_i} \\) represents an interspecies momentum exchange (collisions) between species \\( \\beta \\neq \\alpha \\).\n"},{"id":100,"href":"/r/notes/working/goedbloed/","title":"Goedbloed (2019)","section":"Working","content":" \\[\\] Thermonuclear fusion and plasma confinement # Nuclear reactions release net energy if there is a mass defect, i.e. if\n\\[(m_A) \u0026#43; m_B) c ^2 \u0026gt; (m_C \u0026#43; m_D) c^2\\] In laboratory fusion, hydrogen isotopes are considered, where the deuterium-tritium reaction is the most promising one for future reactors:\n\\[D^2 \u0026#43; T^3 \\rightarrow He ^4 (3.5 MeV) \u0026#43; n (14.1 MeV)\\] This yields two kinds of products, viz. \\( \\alpha \\) particles which are charged so that they can be captured by a confining magnetic field, and neutrons which are electrically neutral so that they can escape from the magnetic configuration. The former contribute to the heating of the plasma (so-called \\( \\alpha \\) particle heating) and the latter have to be captured in a surrounding \\( Li ^6 \\) or \\( Li ^7 \\) blanket, which recovers the fusion energy and also breeds new \\( T^3 \\).\nDeuterium abounds in the oceans: out of 6500 molecules of water one molecule contains a deuteron and a proton instead of two protons. Thus, in principle, 1 litre of sea water contains \\( 10^{10} \\)J of deuterium fusion energy. This is a factor of about 300 more than the combustion energy of 1 litre of gasoline, which yields \\( 3 \\cdot 10^7 \\)J.\nA number of other reactions also occur, in particular reactions producing \\( T^3 \\) and \\( He ^3 \\) which may be burned again. Complete burn of all available \\( D^2 \\) would involve the following reactions:\n\\[\\begin{aligned} D^2 \u0026#43; D^2 \u0026amp; \\rightarrow \u0026amp; He ^3 (0.3 MeV) \u0026#43; n (2.5 MeV) \\\\ D^2 \u0026#43; D^2 \u0026amp; \\rightarrow \u0026amp; T^3 (1.0 MeV) \u0026#43; p (3.0 MeV) \\\\ D^2 \u0026#43; T^3 \u0026amp; \\rightarrow \u0026amp; He ^4 (3.5 MeV) \u0026#43; n (14.1 MeV) \\\\ D^2 \u0026#43; He ^3 \u0026amp; \\rightarrow \u0026amp; He ^4 (3.7 MeV) \u0026#43; p (14.6 MeV) \\end{aligned}\\] so that in effect\n\\[6D^2 \\rightarrow 2He^4 \u0026#43; 2p \u0026#43; 2n \u0026#43; 43.2 MeV\\] In the liquid Li blanket, fast neutrons are moderated, so that their kinetic energy is converted into heat, and the following reactions occur:\n\\[\\begin{aligned} n \u0026#43; Li ^6 \u0026amp; \\rightarrow \u0026amp; T^3 (2.1 MeV) \u0026#43; He ^4 (2.8 MeV) \\\\ n (2.5 MeV) \u0026#43; Li ^7 \u0026amp; \\rightarrow \u0026amp; T^3 \u0026#43; He ^4 \u0026#43; n \\end{aligned}\\] This provides the necessary tritium fuel for the main fusion reaction. Typical numbers associated with thermonuclear fusion reactors, as currently envisaged, are:\ntemperature \\( T \\sim 10^8 K (10 keV) \\) particle density \\( n \\sim 10^{21} m^{-3} \\) power density \\( \\sim 10 MW m^{-3} \\) time scale \\( \\tau \\sim 100s \\) Conditions for Fusion # Thermonuclear fusion happens when a gas of, e.g., deuterium and tritium atoms is sufficiently heated for the thermal motion of the nuclei to become so fast that they may overcome the repulsive Coulomb barrier and come close enough for the attractive nuclear forces to bring about the fusion reactions discussed above. This requires particle energies of \\( \\sim 10 keV \\), i.e. temperatures of about \\( 10^8 K \\). At these temperatures the electrons are completely stripped from the atoms (the ionization energy of hydrogen is \\( \\sim 14 eV \\)) so that a plasma rather than a gas is obtained.\nBecause the charged particles (occurring in about equal numbers of opposite charge) are freely moving and rarely collide at these high temperatures, a plasma may be considered as a perfectly conducting fluid for many purposes. In such fluids, electric currents are easily induced and the associated magnetic fields in turn interact with the plasma to confine or to accelerate it. The appropriate theoretical description of this state of matter is called magnetohydrodynamics (MHD), i.e. the dynamics of magneto-fluids.\nWhy are magnetic fields necessary? To understand this, we need to discuss the power requirements for fusion reactors. This involves three contributions:\n(a) The thermonuclear output power per unit volume:\n\\[P_T = n ^2 f (\\tilde{T})\\] \\[f(\\tilde{T}) \\equiv \\frac{1}{4} \\langle \\sigma v \\rangle E_T\\] \\[E_T \\approx 22.4 MeV\\] where \\( n \\) is the particle density, \\( \\sigma \\) is the cross-section of the D-T fusion reactions, \\( v \\) is the relative speed of the nuclei, \\( \\langle \\sigma v \\rangle \\) is the average nuclear reaction rate, which is a well-known function of temperature, and \\( E_T \\) is the average energy released in the fusion reactinos (i.e. more than the \\( 17.6 MeV \\) of the D-T reaction, but of course less than the \\( 43.2 MeV \\) released for the complete burn)\n(b) the power loss by Bremsstrahlung, i.e. the radiation due to electron-ion collisions\n\\[P_B = \\alpha n^2 \\tilde{T} ^{1/2}\\] \\[\\alpha \\approx 3.8 \\cdot 10^{-29} J^{1/2} m^3 s^{-1}\\] (c) the losses by heat transport through the plasma:\n\\[P_L = \\frac{3 n \\tilde{T}}{\\tau _E}\\] where \\( 2 n \\tilde{T} \\) is the total plasma kinetic energy density (electrons + ions) and \\( \\tau_E \\) is the energy confinement time (an empirical quantity). The latter estimates the usually anomalous (i.e. deviating from classical transport by Coulomb collisions between the charged particles) heat transport processes. Here, we have put a tilde on the temperature to indicate that energy units of keV are exploited:\n\\[\\tilde{T} (keV) = 8.62 \\cdot 10^{-8} T(K)\\] since \\( \\tilde{T} = 1 keV = 1.60 \\cdot 10^{-16} J \\) corresponds with \\( T = 1.16 \\cdot 10^{7} K \\) (using Boltzmann\u0026rsquo;s constant \\( k \\)).\nIf the three power contributions are considered to become externally available for conversion into electricity and back again into plasma heating, with efficiency \\( \\eta \\), the Lawson criterion\n\\[P_B \u0026#43; P_L = \\eta (P_T \u0026#43; P_B \u0026#43; P_L)\\] tells us that there should be power balance between the losses from the plasma (LHS) and what is obtained from plasma heating (RHS). Typically \\( \\eta \\approx 1/3 \\). Inserting the explicit expressions leads to a condition to be imposed on the product of the plasma density and the energy confinement time:\n\\[n \\tau _E = \\frac{3 \\tilde{T}}{[\\eta / (1-\\eta)] f(\\tilde{T}) - \\alpha \\tilde{T} ^{1/2}}\\] Since Bremsstrahlung losses dominate at lower temperatures and transport losses dominate at high temperatures, there si a minimum in the curve at about\n\\[n \\tau_E = 0.6 \\cdot 10^{20} m ^{-3} s \\quad \\text{for} \\tilde{T} = 25 keV\\] This should be considered to be the threshold for a fusion reactor under the given conditions.\nBy a rather different, more recent, approach of fusion conditions, ignition occurs when the total amount of power losses is balanced by the total amount of heating power. The latter consists of \\( \\alpha \\) particle heating \\( P_\\alpha \\) and additional heating power \\( P_H \\) e.g. by radio-frequency waves or neutral beam injection. The latter heating sources are only required to bring the plasma to the ignition point, when \\( \\alpha \\)-particle heating may take over. Hence, at ignition we may put \\( P_H = 0 \\) and the power balance becomes\n\\[P_B \u0026#43; P_L = P_\\alpha = \\frac{1}{4} \\langle \\sigma v \\rangle n^2 E_\\alpha \\quad E_\\alpha \\approx 3.5 MeV\\] Formally, this may be described by the same equation, taking now \\( \\eta \\approx 0.135 \\) so that a 2.5 times higher threshold for fusion is obtained:\n\\[n \\tau _E = 1.5 \\cdot 10^{20} m^{-3} s \\quad \\tilde{T} = 30 keV\\] Roughly speaking then, products of density and energy confinement time \\( n \\tau_E \\sim 10^{20} m ^{-3} s \\) and temperatures \\( \\tilde{T} \\sim 25 keV \\) or \\( T \\sim 3 \\cdot 10^{8} K \\), are required for fusion reactions. As a figure of merit for fusion experiments one frequently constructs the product of these two quantities, which should approach\n\\[n \\tau_E \\tilde{T} \\sim 3 \\cdot 10^{21} m^{-3} s ke V\\] for a fusion reactor. To get rid of the radioactive tritium component, one might consider pure D-D reactions in a more distant future. This would require yet another increase of the temperature by a factor of 10. Considering the kind of progress obtained over the past 40 years, one may hope that this difficulty eventually will turn out to be surmountable.\nReturning to our question on the magnetic fields: no material containers can hold plasmas with densities of \\( 10^{20} m^{-3} \\) and temperatures of \\( 100-300 \\) million K during times on the order of minutes, or at least seconds, without immediately extinguishing the \u0026lsquo;fire.\u0026rsquo; One way to solve this problem is to make use of the confining properties of magnetic fields, which may be viewed from quite different angles:\n(a) the charged particles of the plasma rapidly and tightly gyrate around the magnetic field lines (they \u0026lsquo;stick\u0026rsquo; to the field lines)\n(b) fluid and magnetic field move together (the magnetic field is frozen into the plasma), so that engineering the geometry of the magnetic field configuration also establishes the geometry of the plasma;\n(c) the thermal conductivity of plasmas is extremely anisotropic with respect to the magnetic field, \\( \\kappa_\\perp \\ll \\kappa _ \\parallel \\), so that heat is easily conducted along the field lines and the magnetic surfaces they map out, but not across.\nConsequently, what one needs foremost is a closed magnetic geometry facilitating stable, static plasma equilibrium with roughly bell-shaped pressure and density profiles and nested magnetic surfaces. This is the subject of the next section.\n"},{"id":101,"href":"/r/notes/griffiths/","title":"Griffiths Introduction to Electrodynamics","section":"Notes","content":" Griffiths - Introduction to Electrodynamics # This is basically just a web-friendly version of David Griffiths\u0026rsquo; Introduction to Electrodynamics, 4th Ed.. These are my class notes for the University of Washington\u0026rsquo;s PHYS 543. This is mostly an exercise for myself in learning about Mkdocs, MathJax, and physics!\nClick around in the sidebar to find a chapter to read, or follow the links at the bottom of the page to read in order! Don\u0026rsquo;t forget to try out the interface on mobile, it\u0026rsquo;s very slick ;)\nTable of Contents # 1 - Vector Analysis 1.1 - Vector Algebra 1.2 - Differential Calculus 1.3 - Integral Calculus 1.4 - Curvilinear Coordinates 1.5 - The Dirac Delta Function 1.6 - The Theory of Vector Fields 2 - Electrostatics 2.1 - The Electric Field 2.2 - Divergence and Curl of Electrostatic Fields 2.3 - Electric Potential 2.4 - Work and Energy in Electrostatics 2.5 - Conductors 3 - Potentials 3.1 - Laplace\u0026rsquo;s Equation 3.2 - The Method of Images 3.3 - Separation of Variables 3.4 - Multipole Expansion Chapter 3 Problems 4 - Electric Fields in Matter 4.1 - Polarization 4.2 - The Field of a Polarized Object 4.3 - The Electric Displacement 4.4 - Linear Dielectrics 5 - Magnetostatics 5.1 - The Lorentz Force Law 5.2 - The Biot-Savart Law 5.3 - The Divergence and Curl of B 5.4 - Magnetic Vector Potential Chapter 5 Problems 6 - Magnetic Fields in Matter 6.1 - Magnetization 6.2 - The Field of a Magnetized Object 6.3 - The Auxiliary Field 6.4 - Linear and Nonlinear Media 7 - Electrodynamics 7.1 - Electromotive Force 7.2 - Electromagnetic Induction 7.3 - Maxwell\u0026rsquo;s Equations Chapter 7 Problems 8 - Conservation Laws 8.0 - Phys 544 Introduction 8.1 - Charge and Energy 8.2 - Momentum 9 - Electromagnetic Waves 9.1 - Waves in One Dimension 9.2 - Electromagnetic Waves 9.3 - Electromagnetic Waves in Matter 9.4 - Absorption and Dispersion 9.5 - Guided Waves Chapter 9 Problems 10 - Potentials and Fields 10.1 - The Potential Fomulation 10.2 - Retarded Potentials 10.3 - Point Charges 11 - Radiation 11.1 - Dipole Radiation Internals # All content is written in Markdown and rendered to a static site using Hugo. The theme for the site is hugo-book. I use KaTeX to typeset any LaTeX in my source in the browser super fast (and in a mobile-friendly format!).\n"}]