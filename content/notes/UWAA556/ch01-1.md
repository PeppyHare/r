---
title: Mathematical review
weight: 10
bookToc: false
---

{{< katex display >}}

{{< /katex >}}

## Mathematical review

We're starting off with some mathematical foundation to ensure we're all on the same page before we start talking about waves and collisions.

### Fourier Series / Spectral Analysis

First, let's not get confused between Fourier series and Fourier transforms. A Fourier series is the breakdown of a function defined on a finite interval into "frequency components." For example, if we have \\( f(x) : x \in [a, b] \\), a Fourier series for \\( f(x) \\) pulls apart the **discrete** frequency components of the \\( e^{ikx} \\) modes that make up the function. Starting with a fundamental mode \\( f_0 \\), a first harmonic \\( f_1 \\), a second harmonic \\( f_2 \\), and so on and so forth. We won't end up with any energy in the frequencies between those modes, since it's a discrete series.

A Fourier transform is a transformation on a function on an infinite interval. When transformed, we end up with a continuous spectrum in frequency space. Same basic principles, but the important difference is a Fourier series is from a finite interval to a discrete spectrum.

There are also special geometries that don't follow the basic 1D Fourier transformation. For example, if a function is defined on a disk (polar coordinates), then the eigenfunctions of the transform are the Bessel functions, and we perform spectral analysis via a "Bessel transform." In a spherical geometry we have spherical Bessel functions.

Fourier's important contribution was this: any arbitrarily capricious graph can be represented in the limit as a sum of sines and cosines. Fourier series are applicable in every area of physics. Why? Why are these trigonometric functions so applicable?

The complex exponentials \\( e^{ikx} = \cos(kx) + i \sin(kx) \\) are the eigenfunctions of 1D intervals. Every time we draw a graph on a continuous interval, these eigenfunctions are the "best" available basis for representing that graph on that interval. This makes more sense if we try to summarize Sturm-Liouville theory.

### Sturm-Liouville Theory

Sturm-Liouville is a mature, complete theory of evolutionary equations. We've talked about various types of evolution equations in basically every other section in these notes.

For example,

- Heat equation (diffusion): \\( \pdv{u}{t} = D \pdv{^2u}{x} \\)
- Advection: \\( \pdv{u}{t} + a \pdv{u}{x} = 0 \\)
- Advection-Diffusion (both): \\( \pdv{u}{t} = \pdv{}{x}(D \pdv{u}{x}) + a \pdv{u}{x} \\)

Any generic evolution equation relates the time derivative to a differential operator

{{< katex display >}}
\pdv{u}{t} = A u
{{< /katex >}}

Sturm-Liouville is for all linear, 2nd-order operators \\( A \\). To make the notation a bit easier, let's use \\( u_t \equiv \pdv{u}{t} \\), \\( u_{xx} \equiv \pdv{^2u}{x^2} \\)

The standard form of a 2nd-order linear equation is

{{< katex display >}}
\begin{aligned}
u_t + f(t) u & = & a(x) u_{xx} + b(x) u_x + c(x) u \\
 & = & A u
\end{aligned}
{{< /katex >}}

To solve the equation, we use separation of variables, where we say our solution will be a product of a function \\( X(x) \\) with all the spatial information and a function \\( T(t) \\) with all of the temporal information. When we substitute the solution, we find

{{< katex display >}}
(T' + f(x) T )X = T A X \\
\rightarrow \frac{AX}{X} = \frac{T' + f(t) T}{T}
{{< /katex >}}

Because the left-hand-side depends only on \\( x \\) and the right-hand-side depends only on \\( t \\), they must be equal to some constant \\( \lambda \\). We get two equations, the first being the eigenvalue problem

{{< katex display >}}
AX = \lambda X
{{< /katex >}}

and a temporal eigenvalue problem

{{< katex display >}}
T' = (\lambda - f(t) ) T
{{< /katex >}}

We start with the the spatial one, because in general we will know the spatial boundary conditions for our problem (otherwise it'll be rather difficult to solve!). For the 2nd order eigenvalue problem given sufficient boundary conditions for \\( u \\)

{{< katex display >}}
a(x) X'' + b(x) X' + (c(x) - \lambda) X = 0
{{< /katex >}}

we can get to the standard form with an integrating factor

{{< katex display >}}
I(x) = \frac{1}{a(x)} \exp \left( \int ^x \frac{b(y)}{a(y)} \\dy \right)
{{< /katex >}}
{{< katex display >}}
\dv{}{x} \left( a(x) I \dv{X}{x} \right) + I(x) \left( c(x) - \lambda \right)X = 0
{{< /katex >}}

This is the standard Sturm-Liouville form. Or, on the internet we'll see the "standard Sturm-Liouville form" written slightly differently

{{< katex display >}}
\dv{}{x} \left( p(x) \dv{u}{x} \right) + q(x) u + \lambda w(x) u = 0
{{< /katex >}}

This form is important because of a miraculous theorem:

**Theorem** Give regular boundary conditions on \\( u(x) \\) for \\( x \in [a, b] \\), the Sturm-Liouville equation has

- Distinct eigenvalues \\( \lambda_n, n = 1, 2, \ldots \\)
- For each eigenvalue \\( \lambda_n \\) there is an eigenfunction \\( X_n \\).
- The eigenfunctions form a complete set of orthogonal functions: \\( \int _a ^b X_n X_m w(x) \dd x = 0, n \neq m \\) (the weighting function \\( w(x) \\) is the above function in the standard S-L form). Completeness is the statement that one can expand any function on \\( [a, b] \\) using the set of eigenfunctions.
- With the temporal eigenfunctions \\( T_n(t) \\), the solution to the S-L equation is the sum of all the eigenfunctions
{{< katex display >}}
u(x, t) = \sum_{n=1} ^{\infty} a_n X_n(x) T_n (t)
{{< /katex >}}

We start out with some temporal initial conditions

{{< katex display >}}
u_0 \equiv u(x, 0) = \sum_{n=1}^{\infty} a_n X_n(x)
{{< /katex >}}

We get the weighting coefficients \\( a_n \\) by taking the inner product of each eigenfunction with the initial conditions:

{{< katex display >}}
\langle X_m | u_0 \rangle = \sum_{n=1} ^{\infty} a_n \langle X_m | X_n \rangle \\
= a_m \langle X_m | X_m \rangle \\
\rightarrow a_m = \frac{\langle X_m | u_0 \rangle}{\langle X_m | X_m \rangle}
{{< /katex >}}

The completeness property of S-L gives families of orthogonal functions for any particular differential equation. The family we get ends up being a natural basis for physical situations.

Some household examples of S-L systems:

- \\( u'' - \lambda ^2 u = 0, [ -L / 2, L / 2] \\)

We've got \\( w(x) = 1 \\) and inner product \\( \langle X_n | X_m \rangle = \int _{-L/2} ^{L/2} X_n X_m \dd x \\)

The eigenfunctions are \\( \left[ \frac{1}{2}, \sin (\lambda _n x), \cos (\lambda_n x) \right] \\) and the eigenvalues are \\( \lambda_n = \frac{2 \pi}{L} n \\)

The orthogonality relation says
{{< katex display >}}
\langle u_n u_m \rangle = \int _{-L/2} ^{L} u_m (x) u_n (x) \dd x = 2 L \delta _{n m}
{{< /katex >}}

The Fourier basis is the **simplest** Sturm-Liouville system. It applies to systems that are on an interval and internally homogeneous.

There are other systems, like the Chebyshev, Hermite, and Laguerre polynomials. The only requirement is that the system satisfies a 2nd-order linear differential equation. But the Fourier basis satisfies the simplest possible 2nd-order S-L equation.

Back to Fourier series, let's get the Fourier theorem:

**Theorem**: Any periodic function can be decomposed into a potentially infinite series of sine and cosine functions. If period is \\( L \\) (\\( f(x + L) = f(x) \\)) then

{{< katex display >}}
f(x) = \frac{a_0}{2} + \sum _{n=1} ^{\infty} a_n \cos (k_n x) + b_n \sin (k_n x)
{{< /katex >}}

with frequencies/wavenumbers given by

{{< katex display >}}
k_n = \frac{2 \pi}{L} n, n = 1, 2, 3, \ldots
{{< /katex >}}

and Fourier coefficients / expansion coefficients given by

{{< katex display >}}
a_n = \frac{2}{L} \int_0 ^L f(x) \cos (k_n x) \dd x
{{< /katex >}}
{{< katex display >}}
b_n = \frac{2}{L} \int_0 ^L f(x) \sin (k_n x) \dd x
{{< /katex >}}

The set of coefficients \\( a_n, b_n \\) forms the spectrum of \\( f(x) \\).

For the Fourier series to be convergent, the spectrum must decay sufficiently quickly, so we'll see the coefficients getting smaller as \\( n \rightarrow \infty \\).

The more convenient complex form using the Euler identity
{{< katex display >}}
f(x) = \sum_{n = -\infty} ^{\infty} c_n e^{i k_n x}
{{< /katex >}}
In this form we only have a single set of coefficients given by the Fourier integral
{{< katex display >}}
c_n = \frac{1}{L} \int_0 ^L f(x) e ^{-i k_n x} \dd x
{{< /katex >}}

You can show that they are related to the previous ones by

{{< katex display >}}
 c_n = \frac{1}{2} (a_n - i b_n) \qquad n > 0 \\
 = \frac{1}{2} (a_n + i b_n) \qquad n < 0>
{{< /katex >}}

Let's build some intuition by looking into some examples of Fourier spectra.

**Finite spectra**

- Simple waveform \\( u = \sin (5x) \\). By inspection, 
{{< katex display >}}
\sin (5x) = \frac{1}{2i} \left( e^{i 5x} - e ^{-i 5 x} \right)
{{< /katex >}}
That was easy, that's the whole series. There are only two Fourier coefficients
{{< katex display >}}
c_5 = - \frac{i}{2} \qquad c_{-5} = \frac{i}{2}
{{< /katex >}}

- Beat wave
{{< katex display >}}
u = \cos (6x) \cos (x)
{{< /katex >}}

Expanding each of the cosines as complex exponentials
{{< katex display >}}
= \frac{1}{4} \left( e^{i 6 x} + e^{- i 6 x} \right) (e^{ix} + e^{-ix})
{{< /katex >}}
{{< katex display >}}
= \frac{1}{4} \left( e^{i 7 x} + e^{-i 7 x} + e^{i 5 x} + e^{-i 5 x} \right)
{{< /katex >}}
{{< katex display >}}
= \frac{1}{2} \left( \cos (7x) + \cos (5x) \right)
{{< /katex >}}
So we've got four coefficients
{{< katex display >}}
c_{\pm 5} = \frac{1}{4}, \qquad c_{\pm 7} = \frac{1}{4}
{{< /katex >}}

**Infinite Spectrum**

- Square wave

{{< katex display >}}
f(x) = \begin{cases} 1 & \qquad & 0 < x < T/2 \\
-1 & \qquad & -T/2 < x < 0
\end{cases}
{{< /katex >}}

This is about as discontinuous as you get. If you do the Fourier integrals, you get

{{< katex display >}}
f(x) = \sum_{n = \text{odd}} \frac{-i}{\pi n} e^{i k_n x}
{{< /katex >}}
{{< katex display >}}
|c_n| \sim \frac{1}{n}
{{< /katex >}}
The series has a slow convergence, and if you plot the spectrum you get a power law relationship.

- Triangle Wave

The points of the triangle wave where the function is continuous but not differentiable give us a similarly slow convergence of the Fourier spectrum. For a triangle wave it ends up that
{{< katex display >}}
| c_n| \sim \frac{1}{n^2}
{{< /katex >}}
so they converge an order faster than a square wave, but still very slowly.

- Elliptic cosine

Sort of like a cosine, except that it's defined on an ellipse instead of a circle. We give it some parameter \\( m \\) that relates to the ellipticity

{{< katex display >}}
f(x) = c_n (x | m)
{{< /katex >}}

It's periodic, continuous, and infinitely differentiable _but_ its Fourier coefficients behave like

{{< katex display >}}
|c_n | \sim \text{sech}(b_n (1 + 2n)) \sim e^{-n}
{{< /katex >}}
so the spectrum converges _fast_.

To summarize, functions with discontinuities and non-differentiable points had Fourier coefficients that decay with a power law. Continuous, differentiable functions converge very fast. A general conclusion is that non-differentiable functions (or functions that are non-smooth at some level) have infinite power law spectra.

## The Fourier Transform

The Fourier transform is a two-sided representation of a function defined everywhere on the real line \\( f \in (- \infty, \infty) \\). As we calculate the Fourier series for the bounded interval and take the limit of the bounded interval out to the real line, we obtain the Fourier transform.

Basically every function has a Fourier transform. As we extend to \\( \infty \\), we have a very different set of boundary conditions than before. For the Fourier transform to exist, the function must decay sufficiently rapidly for the Fourier integral to converge. If we admit generalized functions or delta functions (as we should!), even functions that don't decay have transforms.

First, we start with our Fourier series

{{< katex display >}}
f(x) = \sum_{n = - \infty} ^{\infty} c_n e^{i k _n x} \qquad k_n = \frac{2 \pi}{L} n \qquad c_n = \frac{1}{L} \int_0 ^{L} f(n) e^{- i k_n x} \dd x
{{< /katex >}}

we extend the limits of integration out from \\( [0, L] \\) to \\( (-\infty, \infty) \\). The previously discrete spectrum becomes continuous as the distance between successive \\( k_n \\) goes to 0

{{< katex display >}}
k_n = \frac{2 \pi}{L} n \qquad \Delta k_n = \frac{2 \pi}{L} \rightarrow 0
{{< /katex >}}

Taking the limit, we get the Fourier transform pair

{{< katex display >}}
f(x) = \int _{-\infty} ^{\infty} e^{i k x} \dd k
{{< /katex >}}
{{< katex display >}}
\hat {f} (x) = \frac{1}{2 \pi} \int _{-\infty} ^{\infty} f(x) e^{- i k x} \dd x
{{< /katex >}}

Fourier transforms are appropriate to analyze a linear, homogeneous medium, e.g. system where the same linear differential equation is satisfied everywhere in space.

## Laplace Transform

Also known as a one-sided Fourier transform, or a generalized Fourier transform.

Consists of a transform over just half of the infinite interval: \\( [0, \infty] \\). This makes it appropriate for time problems / initial value problems where we need to treat the time variable.

The time spectrum of \\( f(t) \\) is
{{< katex display >}}
\hat {f}(\omega) = \int _0 ^\infty f(t) e^{i \omega t} \dd t
{{< /katex >}}

with inverse transform
{{< katex display >}}
f(t) = \frac{1}{2 \pi} \int _{-\infty + i s} ^{\infty + i s} f(\omega) e ^{- i \omega t} \dd \omega
{{< /katex >}}

The inverse Laplace integral is an integral over the complex plane. The factor \\( i s \\) is there in order for the transform pair to be convergent. Contour \\( i s \\) has to be above all of the poles of \\( f(\omega) \\).

## Important Fourier Transforms

Representative of the most important features of the Fourier Transform:

- Gaussian: 
{{< katex display >}}
f(x) = e^{- \alpha x ^2} \qquad \hat {f} (k) = \frac{1}{\sqrt{2 \alpha}} e ^{- k ^2 / 4 \alpha}
{{< /katex >}}

Note that if \\( \alpha = 1/2 \\), it is its own transform pair. The Gaussian is one of the only eigenfunctions of the Fourier transform.

- Derivative
{{< katex display >}}
\dv{^n f}{x^n} \quad \leftrightarrow \quad (i k) ^n \hat{f}(k)
{{< /katex >}}

In this way, linear differential equations become simple algebraic equations in their Fourier transformed form.

- Wave
{{< katex display >}}
f(x) = e^{i k_0 x} \qquad \hat{f}(k) = \sqrt{2 \pi} \delta (k - k_0)
{{< /katex >}}

### Unitary Fourier Transform

The factor of \\( 1/ 2\pi \\) in front of either the Fourier transform or its inverse can be moved to either side by convention. The "unitary" Fourier transform convention splits it evenly across both:

{{< katex display >}}
f(x) = \frac{1}{\sqrt{2 \pi}} \int _{- \infty} ^{\infty} \hat{f} (k) e^{i k x} \dd x
{{< /katex >}}
{{< katex display >}}
\hat{f}(k) = \frac{1}{\sqrt{ 2 \pi}} \int _{- \infty} ^{\infty} f(x) e^{- i k x} \dd x
{{< /katex >}}

We'll use this notation going forward.

### Multi-dimensional Fourier Transforms

Extending to more dimensions, we can do multi-dimensional Fourier transforms by transforming each coordinate successively.

{{< katex display >}}
f(x, y) \rightarrow \hat{f}(k, l) = \frac{1}{2 \pi} \int _{- \infty} ^{\infty} \dd x \int _{- \infty} ^{\infty} \dd y \, e^{- i (k x + ly)} f(x, y)
{{< /katex >}}

Let vectors \\( \vec k = (k, l) \\), \\( \vec x = (x, y) \\), then the phase factor \\( k x + l y = \vec k \cdot \vec x \\).

An N-dimensional transform is just

{{< katex display >}}
\hat{f}(\vec k) = \frac{1}{(2 \pi)^{d/2}} \int _{- \infty} ^{\infty} \dd ^d x \, e^{-i \vec k \cdot \vec x} f(\vec x)
{{< /katex >}}

If you do it in cylindrical coordinates, you end up with a Bessel function as your transform.

### Space-time Fourier Transform

The space-time Fourier Transform is also useful, where we combine both the time and space transforms to deal with differential equations that are functions of both space and time. We pretend time extends in both directions for this; otherwise we would have the initial conditions we'd use for a Laplace transform and we would have an easier situation on our hands.

{{< katex display >}}
f(\vec x, t) \rightarrow \hat{f}(\vec k) = \frac{1}{4 \pi ^2} \int _{- \infty} ^{\infty}  \dd ^3 x \, e^{- i \vec k \cdot \vec x} \int _{- \infty} ^{\infty} \dd t \, e^{ i \omega t} f(\vec x, t)
{{< /katex >}}

Note the flipped "signature" of the time transform, where the phase of the complex exponential has an opposite sign. Time has an opposite "direction" to space, and this convention ends up being the most meaningful.

The inverse transform just integrates the other way around with the signs flipped.

{{< katex display >}}
f(\vec x, t) = \int _{- \infty} ^{\infty} \dd ^3 k \int _{- \infty} ^{\infty} \dd \omega \hat f (\vec k, \omega) e^{i (\vec k \cdot \vec x - \omega t)}
{{< /katex >}}

Looking at the two components of the transform, \\( \hat f (\vec k, \omega) \\) in a sense represents the equivalent of the Fourier coefficients / amplitudes. \\( e^{i (\vec k \cdot \vec x - \omega t)} \\) is a waveform with wave vector \\( \vec k \\) and frequency \\( \omega \\)

Important properties of the space-time Fourier transform are:

- The spatial part satisfies:
  - \\( \dv {^n f}{x^n} \rightarrow (i k) ^n \hat f \\)
  - \\( \grad f \rightarrow i \vec k \hat f \\)
  - \\( \div f \rightarrow i \vec k \cdot \hat f \\)
  - \\( \curl f \rightarrow i \vec k \cross \hat f \\)
  - \\( \grad ^2 f \rightarrow (i k)^2 \hat f \rightarrow - k^2 \hat f \\)

So there is a symbolic correspondence between \\( \grad \\) and \\( i \vec k \\)

- The time part satisfies
  - \\( \pdv {^n f}{t^n} \rightarrow (- i \omega)^n \hat f\\)

So:
{{< katex display >}}
(\vec x, t) \leftrightarrow (\vec k, \omega)
{{< /katex >}}
{{< katex display >}}
\grad \leftrightarrow i \vec k
{{< /katex >}}
{{< katex display >}}
\pdv{}{t} \leftrightarrow -i \omega
{{< /katex >}}

PDE's in space and time become algebraic equations that can be much more easily solved in the Fourier domain.