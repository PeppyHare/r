# 7.3: Maxwell's Equations

---

## 7.3.1: Electrodynamics Before Maxwell

So far, we have encountered the following laws, specifying the divergence and curl of electric and magnetic fields


{{< katex display >}}
\begin{aligned}
(\text{i}) & \quad \div \vec{E} = \frac{1}{\epsilon_0 } \rho \quad \text{(Gauss's law)} \\
(\text{ii}) & \quad \div \vec{B} = 0 \quad \text{(Ng's Law)} \\
(\text{iii}) & \quad \curl \vec{E} = - \pdv{\vec{B}}{t} \quad \text{(Faraday's Law}) \\
(\text{iv}) & \quad \curl \vec{B} = \mu_0 \vec{J} \quad \text{(Ampere's Law)}
\end{aligned}
{{< /katex >}}

These equations represent the state of electromagnetic theory in the mid-nineteenth century, when Maxwell began his work. They were not written in so compact a form, in those days, but their physical content was familiar. Now, it happens that there is a fatal inconsistency in these formulas. It has to do with the old rule that divergence of curl is always zero. If you apply the divergence to number (iii), everything works out:

{{< katex display >}}
\div (\curl \vec{E}) = \div \left( - \pdv{\vec{B}}{t} \right) = - \pdv{}{t} (\div \vec{B})
{{< /katex >}}

The left side is zero because divergence of curl is zero; the right side is zero by virtue of equation (ii). But when you do the same thing to number (iv), you get into trouble:

{{< katex display >}}
\div (\curl \vec{B}) = \mu_0 (\div \vec{J}) \tagl{7.36}
{{< /katex >}}

the left side must be zero, but the right side, in general, is not. For steady currents, the divergence of J is zero, but when we go beyond magnetostatics Ampere's law cannot be right.

<p align="center"> <img alt="Figure 7.43" src="/r/img/griffiths/7.43.png" /> </p>

There's another way to see that Ampere's law is bound to fail for non-steady currents. Suppose we're in the process of charging up a capacitor (Fig. 7.43). In integral form, Ampere's law reads

{{< katex display >}}
\oint \vec{B} \cdot \dd \vec{l} = \mu_0 I_{enc}
{{< /katex >}}

I want to apply it to the Amperian loop shown in the diagram. How do I deter- mine \\( I_{enc} \\) ? Well, it's the total current passing through the loop, or, more precisely, the current piercing a surface that has the loop for its boundary. In this case, the simplest surface lies in the plane of the loop - the wire punctures this surface, so \\( I_{enc} = I \\) . Fine - but what if I draw instead the balloon-shaped surface in Fig. 7.43? No current passes through this surface, and I conclude that \\( I_{enc} = 0 \\) ! We never had this problem in magnetostatics because the conflict arises only when charge is piling up somewhere (in this case, on the capacitor plates). But for nonsteady currents (such as this one) "the current enclosed by the loop" is an ill-defined notion; it depends entirely on what surface you use. (If this seems pedantic to you - "obviously one should use the plane surface" - remember that the Amperian loop could be some contorted shape that doesn't even lie in a plane.)

Of course, we had no right to expect Ampere's law to hold outside of magnetostatics; after all, we derived it from the Biot-Savart law. However, in Maxwell's time there was no experimental reason to doubt that Ampere's law was of wider validity. The flaw was a purely theoretical one, and Maxwell fixed it by purely theoretical arguments.

## 7.3.2: How Maxwell Fixed Ampere's Law

The problem is on the right side of Eq. 7.36, which should be zero, but isn't. Applying the continuity equation (5.29) and Gauss's law, the offending term can be rewritten:

{{< katex display >}}
\div \vec{J} = - \pdv{\rho}{t} = - \pdv{}{t} ( \epsilon_0 \div \vec{E}) = - \div \left( \epsilon_0 \pdv{\vec{E}}{t} \right)
{{< /katex >}}

If we were to combine \\( \epsilon_0 (\partial \vec{E} / \partial t) \\) with \\( \vec{J} \\), in Ampere's law, it would be just right to kill off the extra divergence:

{{< katex display >}}
\curl \vec{B} = \mu_0 \vec{J} + \mu_0 \epsilon_0 \pdv{\vec{E}}{t} \tagl{7.37}
{{< /katex >}}


(Maxwell himself had other reasons for wanting to add this quantity to Ampere's law. To him, the rescue of the continuity equation was a happy dividend rather than a primary motive. But today we recognize this argument as a far more compelling one than Maxwell's, which was based on a now-discredited model of the ether.)

Such a modification changes nothing, as far as magnetostatics is concerned: when \\( \vec{E} \\)  is constant, we still have \\( \curl \vec{B} = \mu_0 \vec{J} \\) . In fact, Maxwell's term is hard to detect in ordinary electromagnetic experiments, where it must compete for attention with \\( \vec{J} \\) - that's why Faraday and the others never discovered it in the laboratory. However, it plays a crucial role in the propagation of electromagnetic waves, as we'll see in Chapter 9.

Apart from curing the defect in Ampere's law, Maxwell's term has a certain aesthetic appeal: Just as a changing magnetic field induces an electric field (Faraday's law), so

{{< katex display >}}
\textbf{A changing electric field induces a magnetic field}
{{< /katex >}}

Of course, theoretical convenience and aesthetic consistency are only suggestive - there might, after all, be other ways to doctor up Ampere's law. The real confirmation of Maxwell's theory came in 1888 with Hertz's experiments on electromagnetic waves.

Maxwell called his extra term the __displacement current__:

{{< katex display >}}
\vec{J}_d = \epsilon_0 \pdv{\vec{E}}{t} \tagl{7.38}
{{< /katex >}}


(It's a misleading name; \\( \epsilon_0 (\partial \vec{E} / \partial t) \\)  has nothing to do with current, except that it adds to \\( \vec{J} \\)  in Ampere's law.) Let's see now how displacement current resolves the paradox of the charging capacitor (Fig. 7.43). If the capacitor plates are very close together (I didn't draw them that way, but the calculation is simpler if you assume this), then the electric field between them is


{{< katex display >}}
E = \frac{1}{\epsilon_0} \sigma = \frac{1}{\epsilon_0 } \frac{Q}{A} 
{{< /katex >}}

where Q is the charge on the plate and \\( A \\) is its area. Thus, between the plates

{{< katex display >}}
\pdv{E}{t} = \frac{1}{\epsilon_0 A} \dv{Q}{t} = \frac{1}{\epsilon_0 A} I
{{< /katex >}}

Now, Eq. 7.37 reads, in integral form

{{< katex display >}}
\oint \vec{B} \cdot \dd \vec{l} = \mu_0 I_{enc} + \mu_0 \epsilon_0 \int \left( \pdv{\vec{E}}{t} \right) \cdot \dd \vec{a} \tagl{7.39}
{{< /katex >}}


If we choose the flat surface, then \\( E = 0 \\)  and \\( I_{enc} = I \\) . If, on the other hand, we use the balloon-shaped surface, then \\( I_{enc} = 0 \\) , but \\( \int (\partial \vec{E} / \partial t) \cdot \dd \vec{a} = I / \epsilon_0 \\) . So we get the same answer for either surface, though in the first case it comes from the conduction current, and in the second from the displacement current.

#### Example 7.14

!!! question "Imagine two concentric metal spherical shells (Fig. 7.44). The inner one (radius \\( a \\) ) carries a charge \\( Q(t) \\) , and the outer one (radius \\( b \\) ) an opposite charge \\( -Q(t) \\) . The space between them is filled with Ohmic material of conductivity \\( \sigma \\) , so a radial current flows 
{{< katex display >}} \vec J = \sigma \vec E = \sigma \frac{1}{4 \pi \epsilon_0} \frac{Q}{r^2} \vu{r}; \quad I = - \dot{Q} = \int \vec J \cdot \dd \vec a = \frac{\sigma Q}{\epsilon_0}  
{{< katex display >}} This configuration is spherically symmetrical, so the magnetic field has to be zero (the only direction it could possibly point is radial, and \\( \div \vec B = 0 \rightarrow \oint \vec B \cdot \dd \vec a = B(4 \pi r^2) = 0 \\), so \\( B = 0 \\)). What? I thought currents produce magnetic fields! Isn't that what Biot-Savart and Ampere taught us? How can there be a \\( \vec J \\) with no accompanying \\( \vec B \\)?"

    __Solution__
    This is not a static configuration! \\( Q, \vec E, \vec J \\) are all functions of time; Ampere and Biot-Savart do not apply. The displacement current
    
{{< katex display >}}
    J_d = \epsilon_0 \pdv{\vec E}{t} = \frac{1}{4 \pi} \frac{\dot{Q}}{r^2} \vu{r} = - \sigma \frac{Q}{4 \pi \epsilon_0 r^2} \vu{r}
    {{< /katex >}}

    exactly cancels the conduction current (in Eq. 7.37), and the magnetic field (determined by \\( \div \vec{B} = 0 \\) and \\( \curl \vec{B} = 0 \\) is indeed zero.

## 7.3.3 Maxwell's Equations

In the last section we put the finishing touches on Maxwell's equations:

!!! info "Maxwell's Equations: {{< /katex >}}
 \begin{aligned} (\text{i}) & \quad \div \vec{E} = \frac{1}{\epsilon_0 } \rho \quad \text{(Gauss's law)} \\ (\text{ii}) & \quad \div \vec{B} = 0 \quad \text{(Ng's Law)} \\ (\text{iii}) & \quad \curl \vec{E} = - \pdv{\vec{B}}{t} \quad \text{(Faraday's Law}) \\ (\text{iv}) & \quad \curl \vec{B} = \mu_0 \vec{J} + \mu_0 \epsilon_0 \pdv{\vec{E}}{t} \quad \text{(Ampere's Law)} \end{aligned} \tagl{7.40} {{< /katex >}}
"

Together, with the force law,

{{< katex display >}}
\vec{F} q (\vec{E} + \vec{v} \cross \vec{B}) \tagl{7.41}
{{< /katex >}}

they summarize the entire theoretical content of classical electrodynamics (save for some special properties of matter, which we encountered in Chapters 4 and 6). Even the continuity equation,

{{< katex display >}}
\div \vec{J} = - \pdv{\rho}{t} \tagl{7.42}
{{< /katex >}}

which is the mathematical expression of conservation of charge, can be derived from Maxwell's equations by applying the divergence to number (iv).

I have written Maxwell's equations in the traditional way, which emphasizes that they specify the divergence and curl of \\( \vec{E} \\)  and \\( \vec{B} \\) . In this form, they reinforce the notion that electric fields can be produced either by charges (\\( \rho \\) ) or by changing magnetic fields (\\( \partial \vec{B} / \partial t \\) ), and magnetic fields can be produced either by currents (\\( \vec{J} \\) ) or by changing electric fields (\\( \partial \vec{E} / \partial t \\) ). Actually, this is misleading, because \\( \partial \vec{B} / \partial t \\)  and \\( \partial \vec{E} / \partial t \\)  are themselves due to charges and currents. I think it is logically preferable to write


{{< katex display >}}
\div \vec{E} = \frac{1}{\epsilon_0} \rho
{{< /katex >}}


{{< katex display >}}
\div \vec{B} = 0
{{< /katex >}}


{{< katex display >}}
\curl \vec{E} + \pdv{\vec{B}}{t} = 0
{{< /katex >}}


{{< katex display >}}
\curl \vec{B} - \mu_0 \epsilon_0 \pdv{\vec{E}}{t} = \mu_0 \vec{J}
{{< /katex >}}


with the fields (\\( \vec{E} \\)  and \\( \vec{B} \\) ) on the left and the sources (\\( \rho \\)  and \\( \vec{J} \\) ) on the right. This notation emphasizes that all electromagnetic fields are ultimately attributable to charges and currents. Maxwell's equations tell you how charges produce fields; reciprocally, the force law tells you how fields affect charges.

## 7.3.4: Magnetic Charge

There is a pleasing symmetry to Maxwell's equations; it is particularly striking in free space, where \\( \rho \\) and \\( \vec J \\) vanish

{{< katex display >}}
\div \vec E = 0 \qquad \curl \vec E = - \pdv{\vec B}{t} \\
\div \vec B = 0 \qquad \curl \vec B = - \mu_0 \epsilon_0 \pdv{\vec E}{t}
{{< /katex >}}


If you replace \\( \vec E \\) by \\( \vec B \\) and \\( \vec B \\) by \\( - \mu_0 \epsilon_0 \vec E \\), the first pair of equations turns into the second, and vice versa. This symmetry between \\( \vec E \\) and \\( \vec B \\) is spoiled, though, by the charge term in Gauss's law and the current term in Ampere's law. You can't help wondering why the corresponding quantities are "missing" from \\( \div \vec B = 0 \\) and \\( \curl \vec E = - \partial \vec B / \partial t \\) . What if we had


{{< katex display >}}
\begin{aligned}
(\text{i}) & \quad \div \vec{E} = \frac{1}{\epsilon_0 } \rho_e \\
(\text{ii}) & \quad \div \vec{B} = \mu_0 \rho_m \\
(\text{iii}) & \quad \curl \vec{E} = - \mu_0 \vec{J}_m  - \pdv{\vec{B}}{t} \\
(\text{iv}) & \quad \curl \vec{B} = \mu_0 \vec{J} + \mu_0 \epsilon_0 \pdv{\vec{E}}{t}
\end{aligned} \tagl{7.44}
{{< /katex >}}


Then \\( \rho_m \\) would represent the density of magnetic "charge", and \\( \rho_e \\) the density of electric charge; \\( \vec J_m \\) would be the current of magnetic charge, and \\( \vec J_e \\) the current of electric charge. Both charges would be conserved:

{{< katex display >}}
\div \vec{J}_m = - \pdv{\rho_m}{t} \quad \div \vec{J}_e = - \pdv{\rho_e}{t} \tagl{7.45}
{{< /katex >}}

The former follows by application of the divergence to (iii), the latter by taking the divergence of (iv).

In a sense, Maxwell's equations beg for magnetic charge to exist - it would fit in so nicely. And yet, in spite of a diligent search, no one has ever found any. As far as we know, \\( \rho_m \\)  is zero everywhere, and so is \\( \vec J_m \\) ; \\( \vec B \\)  is not on equal footing with \\( \vec E \\) : there exist stationary sources for \\( \vec E \\)  (electric charges) but none for \\( \vec B \\) . (This is reflected in the fact that magnetic multipole expansions have no monopole term, and magnetic dipoles consist of current loops, not separated north and south "poles.") In quantum electrodynamics, by the way, it's a more than merely aesthetic shame that magnetic charge does not seem to exist: Dirac showed that the existence of magnetic charge would explain why electric charge is quantized. 

## 7.3.5: Maxwell's Equations in Matter

Maxwell's equations in the form 7.40 are complete and correct as they stand. However, when you are working with materials that are subject to electric and magnetic polarization there is a more convenient way to write them. For inside polarized matter there will be accumulations of "bound" charge and current, over which you exert no direct control. It would be nice to reformulate Maxwell's equations so as to make explicit reference only to the "free" charges and currents.

We have already learned, from the static case, that an electric polarization \\( \vec{P} \\)  produces a bound charge density


{{< katex display >}}
\rho_b = - \div \vec{P} \tagl{7.47}
{{< /katex >}}

(Eq. 4.12). Likewise, a magnetic polarization (or "magnetization") \\( \vec{M} \\) results in a bound current

{{< katex display >}}
\vec{J}_b = \curl \vec{M} \tagl{7.48}
{{< /katex >}}

(Eq. 6.13). There's just one new feature to consider in the nonstatic case: Any change in the electric polarization involves a flow of (bound) charge (call it \\( \vec{J}_p \\) ), which must be included in the total current. For suppose we examine a tiny chunk of polarized material (Fig. 7.47). The polarization introduces a charge density \\( \sigma_b = P \\) at one end and \\( - \sigma_b \\) at the other (Eq. 4.11). If \\( P \\)  now increases a bit, the charge on each end increases accordingly, giving a net current


{{< katex display >}}
\dd I = \pdv{\sigma_b}{t} \dd a_{\perp} = \pdv{P}{t} \dd a_{\perp}
{{< /katex >}}

The current density, therefore, is

{{< katex display >}}
\vec{J}_p = \pdv{\vec{P}}{t} \tagl{7.49}
{{< /katex >}}


This __polarization current__ has nothing to do with the bound current \\( \vec{J}_b \\) . The latter is associated with magnetization of the material and involves the spin and orbital motion of electrons; \\( \vec{J}_p \\)  by contrast, is the result of the linear motion of charge when the electric polarization changes. If \\( P \\)  points to the right, and is increasing, then each plus charge moves a bit to the right and each minus charge to the left; the cumulative effect is the polarization current \\( \vec{J}_p \\)  . We ought to check that Eq. 7.49 is consistent with the continuity equation:

{{< katex display >}}
\div \vec{J}_p = \div \pdv{\vec{P}}{t} = \pdv{}{t} ( \div \vec{P}) = - \pdv{\rho_b}{t}
{{< /katex >}}

<p align="center"> <img alt="Figure 7.47" src="/r/img/griffiths/7.47.png" /> </p>
Yes: the continuity equation is satisfied: in fact \\( \vec{J}_p \\) is essential to ensure the conservation of bound charge. (Incidentally, a changing magnetization does not lead to any analogous accumulation of charge or current. The bound current \\( \vec{J}_b = \curl \vec{M} \\) varies in response to \\( \vec{M} \\) , to be sure, but that's about it.)

In view of all this, the total charge density can be separated into two parts:

{{< katex display >}}
\rho = \rho_f + \rho_b = \rho_f - \div \vec{P} \tagl{7.50}
{{< /katex >}}

and the current density into three parts

{{< katex display >}}
\vec{J} = \vec{J}_f + \vec{J}_b + \vec{J}_p = \vec{J}_f + \curl \vec{M} + \pdv{\vec{P}}{t} \tagl{7.51}
{{< /katex >}}

Gauss's law can now be written as

{{< katex display >}}
\div \vec{E} = \frac{1}{\epsilon_0} (\rho _f - \div \vec{P})
{{< /katex >}}

or

{{< katex display >}}
\div \vec{D} = \rho_f \tagl{7.52}
{{< /katex >}}

where, as in the static case

{{< katex display >}}
\vec{D} = \epsilon_0 \vec{E} + \vec{P} \tagl{7.53}
{{< /katex >}}


Meanwhile, Ampere's law (with Maxwell's term) becomes

{{< katex display >}}
\curl \vec{B} = \mu_0 \left( \vec{J}_f + \curl \vec{M} + \pdv{\vec{P}}{t} \right) + \mu_0 \epsilon_0 \pdv{\vec{E}}{t}
{{< /katex >}}

or

{{< katex display >}}
\curl \vec{H} = \vec{J}_f + \pdv{\vec{D}}{t} \tagl{7.54}
{{< /katex >}}

where, as before

{{< katex display >}}
\vec{H} \equiv \frac{1}{\mu_0} \vec{B} - \vec{M} \tagl{7.55}
{{< /katex >}}

Faraday's law and \\( \div \vec{B} = 0 \\) are not affected by our separation of charge and current into free and bound parts, since they do not involve \\( \rho \\) or \\( \vec{J} \\) . 

In terms of free charges and currents, then, Maxwell's equations read

!!! info "Maxwell's Equations in Matter: 
{{< katex display >}} \begin{aligned} (\text{i}) & \quad \div \vec{D} = \rho_f \quad \text{(Gauss's law)} \\ (\text{ii}) & \quad \div \vec{B} = 0 \quad \text{(Ng's Law)} \\ (\text{iii}) & \quad \curl \vec{E} = - \pdv{\vec{B}}{t} \quad \text{(Faraday's Law}) \\ (\text{iv}) & \quad \curl \vec{H} = \vec{J}_f + \pdv{\vec{D}}{t} \quad \text{(Ampere's Law)} \end{aligned}  \tagl{7.56} 
{{< katex display >}}"

Some people regard these as the "true" Maxwell's equations, but please understand that they are in no way more "general" than Eq. 7.40; they simply reflect a convenient division of charge and current into free and nonfree parts. And they have the disadvantage of hybrid notation, since they contain both \\( \vec{E} \\)  and \\( \vec{D} \\) , both \\( \vec{B} \\)  and \\( \vec{H} \\) . They must be supplemented, therefore, by appropriate __constitutive relations__, giving \\( \vec{D} \\)  and \\( \vec{H} \\)  in terms of \\( \vec{E} \\)  and \\( \vec{B} \\) . These depend on the nature of the material; for linear media

{{< katex display >}}
\vec{P} = \epsilon_0 \chi_e \vec{E} \qquad \text{ and } \qquad \vec{M} = \chi_m \vec{H} \tagl{7.57}
{{< /katex >}}

so

{{< katex display >}}
\vec{D} = \epsilon \vec{E} \qquad \text{ and } \qquad \vec{H} = \frac{1}{\mu} \vec{B} \tagl{7.58}
{{< /katex >}}

where \\( \epsilon = \epsilon_0 (1 + \chi_e) \\) and \\( \mu = \mu_0 (1 + \chi_m) \\). Incidentally, you'll remember that \\( \vec{D} \\) is called the electric "displacement"; that's why the second term in the Ampere/Maxwell equation came to be called the __displacement current__. In this context

{{< katex display >}}
\vec{J}_d \equiv \pdv{\vec{D}}{t} \tagl{7.59}
{{< /katex >}}


## 7.3.6: Boundary Conditions

In general, the fields \\( \vec{E}, \vec{B}, \vec{D}, \\) and \\( \vec{H} \\) will be discontinuous at a boundary between two different media, or at a surface that carries a surface charge density \\( \sigma \\) or a current density \\( \vec{K} \\). The explicity form of these discontinuities can be deduced from Maxwell's equations in their integral form


{{< katex display >}}
\begin{aligned}
(1) \quad & \oint_S \vec{D} \cdot \dd \vec{a} = Q_{f, enc} \\
(2) \quad & \oint _S \vec{B} \cdot \dd \vec{a} = 0 \\
(3) \quad & \oint _P \vec{E} \cdot \dd \vec{l} = - \dv{}{t} \int _S \vec{B} \cdot \dd \vec{a} \\
(4) \quad & \oint _P \vec{H} \cdot \dd \vec{l} = I_{f, enc} + \dv{}{t} \int_S \vec{D} \cdot \dd \vec a
\end{aligned}
{{< /katex >}}


<p align="center"> <img alt="Figure 7.48" src="/r/img/griffiths/7.48.png" /> </p>

Applying (1) to a tiny, wafer-thin Gaussian pillbox extending just slightly into the material on either side of the boundary (Fig 7.48), we obtain

{{< katex display >}}
\vec{D}_1 \cdot \vec{a} - \vec{D}_2 \cdot \vec a = \sigma_f a
{{< /katex >}}

(The positive direction for \\( \vec a  \\) is from 2 toward 1. The edge of the wafer contributes nothing in the limit as the thickness goes to zero; nor does any volume charge density.) Thus, the component of \\( \vec D \\) that is perpendicular to the interface is discontinuous in the amount

{{< katex display >}}
D_1 ^\perp - D_2 ^\perp = \sigma_f \tagl{7.60}
{{< /katex >}}

Identical reasoning, applied to equation (2) yields

{{< katex display >}}
B_1 ^\perp - B_2 ^\perp = 0 \tagl{7.61}
{{< /katex >}}

Turning to (3), a very thin Amperian loop straddling the surface gives

{{< katex display >}}
\vec{E_1} \cdot \vec{l} - \vec{E_2} \cdot \vec{l} = - \dv{}{t} \int_S \vec{B} \cdot \dd \vec a 
{{< /katex >}}

But in the limit as the width of the loop goes to zero, the flux vanishes. (I have already dropped the contribution of the two ends to \\( \oint \vec{E} \cdot \dd \vec l \\), on the same grounds) 

Therefore,

{{< katex display >}}
\vec{E}_1 ^\parallel - \vec{E}_2 ^\parallel = 0 \tagl{7.62}
{{< /katex >}}


That is, the components of \\( \vec E \\) parallel to the interface are continuous across the boundary. By the same token, (4) implies

{{< katex display >}}
\vec{H}_1 \cdot \vec l - \vec{H}_2 \cdot \vec l = I_{f, enc}
{{< /katex >}}

where \\( I_{f, enc} \\) is the free current passing through the Amperian loop. No volume current density will contribute (in the limit of infinitesimal width), but a surface current can. In fact, if \\( \vu{n} \\) is a unit vector perpendicular to the interface (pointing from 2 toward 1), so that \\( (\vu{n} \cross \vec l ) \\) is normal to the Amperian loop (Fig 7.49), the

{{< katex display >}}
I_{f, enc} = \vec{K_f} \cdot (\vu n \cdot \vec l) = (\vec{K_f} \cross \vu n) \cdot \vec l
{{< /katex >}}

<p align="center"> <img alt="Figure 7.49" src="/r/img/griffiths/7.49.png" /> </p>
and hence

{{< katex display >}}
\vec{H}_1 ^\parallel - \vec{H}_2 ^\parallel = \vec{K}_f \cross \vu n \tagl{7.63}
{{< /katex >}}

So the parallel components of \\( \vec H \\) are discontinuous by an amount proportional to the free surface current density.

Equations 7.60-63 are the general boundary conditions for electrodynamics. In the case of linear media, they can be expressed in terms of \\( \vec E \\) and \\( \vec B \\) alone


{{< katex display >}}
\begin{aligned}
(1) \quad & \epsilon_1 E_1 ^\perp - \epsilon_2 E_2 ^\perp = \sigma_f \\
(2) \quad & \vec{B}_1 ^\perp - \vec{B}_2 ^\perp = 0 \\
(3) \quad & \vec{E}_1 ^\parallel - \vec{E}_2 ^\parallel = 0 \\
(4) \quad & \frac{1}{\mu_1} \vec{B}_1 ^\parallel - \frac{1}{\mu_2} \vec{B}_2 ^\parallel = \vec{K}_f \cross \vu{n}
\end{aligned} \tagl{7.64}
{{< /katex >}}


In particular, if there is no free charge or free current at the interface, then

{{< katex display >}}
\begin{aligned}
(1) \quad & \epsilon_1 E_1 ^\perp - \epsilon_2 E_2 ^\perp = 0 \\
(2) \quad & \vec{B}_1 ^\perp - \vec{B}_2 ^\perp = 0 \\
(3) \quad & \vec{E}_1 ^\parallel - \vec{E}_2 ^\parallel = 0 \\
(4) \quad & \frac{1}{\mu_1} \vec{B}_1 ^\parallel - \frac{1}{\mu_2} \vec{B}_2 ^\parallel = 0
\end{aligned} \tagl{7.65}
{{< /katex >}}


These equations provide the basis for the theory of reflection and refraction.