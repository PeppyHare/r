---
title: Statistical Mechanics
bookToc: false
weight: 20
---

# **10-1** Statistical Mechanics

<!-- TA office: AERB 430 -->

## **10.1.1** Very Large Numbers

Before getting started with real plasma physics concepts, we need to quickly review some statistical mechanics with the goal of deriving the Maxwell-Boltzmann distribution.



As we all know, if we have \\( N \\) unique objects, there are \\( N! \\) ways of arranging them. If \\( n_1 \ldots n_k \\) are identical with N things, the number of combinations is 

{{< katex display >}}
\frac{N!}{n_1 ! \ldots n_k !}
{{< /katex >}}


What does probability have to do with a velocity distribution? Consider the one-dimensional random walk, in which we take \\( N \\) steps, and at each step we move in a random direction. We take \\( n_r \\) steps to the right and \\( N - n_r \\) steps to the left. For a random walk we assume the probability of going in each direction is the same

{{< katex display >}}
P_r = p = \frac{1}{2} \qquad P_l = q =  \frac{1}{2}
{{< /katex >}}

After taking \\( N \\)  steps, the probability of taking \\( n_r \\) steps to the right and \\( (N - n_r) \\) steps to the left is

{{< katex display >}}
P(n_r) = \frac{N !}{n_r ! (N - n_r)!} \cdot p^{n_r} q^{n_r}
{{< /katex >}}


We are also interested in the final destination, which is the net number of steps to the right

{{< katex display >}}
m_r = n_r - n_l = 2 n_r - N
{{< /katex >}}


If we have a bias to move in a particular direction, then \\( p \neq q \\) and the distribution \\( P(n_r) \\) will be shifted towards the bias.

Of course, if we have a very large \\( N \\) we aren't going to be able to compute \\( P(n_r) \\) directly. We've got our handy dandy natural logarithm to help us.


{{< katex display >}}
\ln N! = \ln N + \ln (N - 1) + \ldots  + 1
{{< /katex >}}



{{< katex display >}}
\ln (N + 1) ! = \ln (N + 1) + \ln N!
{{< /katex >}}



{{< katex display >}}
\pdv{\ln N!}{N} \approx \frac{\ln(N+1)! - \ln N!}{N - (N-1)} = \ln(N+1) \approx \ln N
{{< /katex >}}


Recall our expression for the probability distribution


{{< katex display >}}
P(n_r) = \frac{N!}{n_r !(N - n_r)!} p^{n_r} q^{N- n_r}
{{< /katex >}}


{{< katex display >}}
\rightarrow \quad \ln P(n_r) = \ln N! - \ln n_r ! - \ln ((N-n_r)!) + n_r \ln p + (N - n_r) \ln q
{{< /katex >}}


Now we apply the little log trick \\( \pdv{\ln N!}{N} \approx \ln N \\) 

{{< katex display >}}
\dv{\ln P(n_r)}{n_r} = - \ln r + \ln (N - n_r) + \ln p - \ln q = 0
{{< /katex >}}


{{< katex display >}}
\qquad \rightarrow \qquad \ln \left( \frac{N - n_r}{n_r} \frac{p}{q} \right) = 0
{{< /katex >}}


{{< katex display >}}
(N - n_r)p = n_r q
{{< /katex >}}


{{< katex display >}}
Np = n_r(p + q) 
{{< /katex >}}


{{< katex display >}}
\overline{n_r} = Np
{{< /katex >}}

Surprise surprise, the probability of getting a certain result is just the expectation value of that result. Can we also say anything about the width of the distribution? 
Taylor expand about \\( \overline{n_r} = N p \\) and define \\( \eta \\) by \\( n_r = N p + \eta \\) 

{{< katex display >}}
\ln (P(n_r)) = \ln(P(Np)) + B_1 \eta + \frac{1}{2} B_2 \eta^2 + \frac{1}{6} B_3 \eta^4
{{< /katex >}}


{{< katex display >}}
B_k = \frac{d^k \ln P(n_r)}{d n_r ^k} 
{{< /katex >}}


{{< katex display >}}
\dv{\ln P(n_r)}{n_r} = - \ln n_r + \ln (N - n_r) + \ln p - \ln q
{{< /katex >}}


{{< katex display >}}
B_1 = - \ln(n_r) + \ln (N - n_r) + \ln p - \ln q
{{< /katex >}}


{{< katex display >}}
= - \ln (Np) + \ln(N - Np) = 0
{{< /katex >}}


{{< katex display >}}
B_2 = - \frac{1}{n_r} - \frac{1}{N - n_r} = - \frac{1}{Np} - \frac{1}{N - Np} = -\frac{1}{Nqp}
{{< /katex >}}


{{< katex display >}}
B_3 = \frac{1}{n_r ^2} - \frac{1}{(N - n_r)^2} \approx \frac{1}{N^2}
{{< /katex >}}


{{< katex display >}}
B_4 \approx \frac{1}{N_3}
{{< /katex >}}


The expansion converges for \\( N >> \eta \\) 

{{< katex display >}}
\ln(P(n_r)) \approx \ln (P(Np)) - \frac{1}{2} \frac{1}{Nqp} \eta ^2
{{< /katex >}}

or

{{< katex display >}}
P(n_r) = P(Np) e^{- \frac{\eta ^2}{2 Npq}} \qquad P_0 = P(Np)
{{< /katex >}}


{{< katex display >}}
P(\eta) = P_0 e^{-\frac{\eta ^2}{2Npq}}
{{< /katex >}}


So what's the width?


{{< katex display >}}
\langle \eta ^2 \rangle = \frac{ \int_{-\infty} ^\infty \eta ^2 P(\eta) \dd \eta }{\int_{-infty}^\infty P(\eta) \dd \eta} = \frac{ \frac{ \sqrt{\pi}}{4 \left( \frac{1}{2 N pq} \right) ^{3/2}}}{\frac{ \sqrt{\pi}}{2 \left( \frac{1}{2 N pq} \right)^{1/2}}} = \frac{2 Npq}{2} = Npq
{{< /katex >}}


So the uncertainty is \\( \delta n_r = \sqrt{ N pq} \approx \frac{1}{2} \sqrt{N} \\). For a very large N, the relative uncertainty \\( \delta n_r / N \approx \frac{1}{2 \sqrt{N}} \\) diminishes and the distribution (centered at \\( Np \\) ) gets very narrow. The signal-to-noise will go as \\( \frac{1}{\sqrt{N}} \\).

## Maxwell-Boltzmann Distribution Function

Suppose we've got a large number N of particles with total energy \\( W \\). What is the energy distribution among the particles?

We call a "state" a possible distribution (permutation) of energy among the particles which satisfies the constraints.  The fundamental principle of statistical mechanics states that all states have an equal chance of being occupied, with the resulting combinatorial factor giving the distribution. Because the number of particles is very large, the distribution will be close to the distribution that has the largest number of states.

Label the particles by the energy they have. Let \\( n_i \\) be the number of particles with energy between \\( \epsilon_i \\) and \\( \epsilon_{i+1} = \epsilon_i + \delta \epsilon \\). Choose \\( k \\) different energy ranges:


{{< katex display >}}
\sum_{i = 0} ^k n_i = N \qquad \sum_{i=1} ^k \epsilon_i n_i = W = \text{ total energy}
{{< /katex >}}
 

The number of states with a given distribution is just a partitioning (indistinguishable!) of the energy among \\( k \\) bins, so


{{< katex display >}}
P = \frac{1}{N} \frac{N!}{n_1 ! \ldots n_k ! } = \frac{N-1!}{n_1 ! \ldots n_k ! } \approx \frac{N!}{n_1 ! \ldots n_k ! }
{{< /katex >}}


We know that the distribution will be closely centered about the maximum of P, subject to the conservation constraints of energy and particles


{{< katex display >}}
\delta N = 0 = \sum _1 ^ k \delta n_i
{{< /katex >}}



{{< katex display >}}
\delta W = 0 = \sum_1 ^k \epsilon_i \delta n_i 
{{< /katex >}}


Going back to our log trick to find the critical point

{{< katex display >}}
\delta \ln P = 0
{{< /katex >}}


{{< katex display >}}
\ln P = \ln N! - \sum_1 ^k \ln n_i !
{{< /katex >}}


{{< katex display >}}
\delta \ln P = - \sum_1 ^k \ln n_i \delta n_i = 0
{{< /katex >}}


Let \\( \lambda \\)  be the Lagrange multiplier for \\( \delta N \\) and \\( \beta \\) be the Lagrange multiplier for \\( \delta W \\), so


{{< katex display >}}
(\ln n_i + \lambda + \beta \epsilon_i) \delta n_i = 0
{{< /katex >}}


{{< katex display >}}
\ln n_i = 0 \lambda - \beta \epsilon_i
{{< /katex >}}


{{< katex display >}}
n_i = e^{-\lambda - \beta \epsilon_i} = n_\lambda e^{-\beta \epsilon_i}
{{< /katex >}}

Values of \\( n_\lambda \\) just come from the real constraints \\( N \\)  and \\( W \\) 


{{< katex display >}}
\int _0 ^\infty n_\lambda e^{- \beta \epsilon} \dd \epsilon = N
{{< /katex >}}


{{< katex display >}}
\int_0 ^\infty \epsilon n_\lambda e^{- \beta \epsilon} \dd \epsilon = W
{{< /katex >}}


Putting in our constraints, out pops the Maxwell-Boltzmann distribution


{{< katex display >}}
f(\vec v) \dd \vec v = n \left( \frac{m}{2 \pi k T} \right) ^{3/2} e^{- \epsilon / kT} \qquad \epsilon = \text{ KE + PE }
{{< /katex >}}

{{< hint info >}}
**Example: Distribution under Gravity**

If we have a bunch of particles under the influence of gravity, the energy is

{{< katex display >}}
\epsilon = \frac{1}{2} m v^2 + mgz
{{< /katex >}}

The distribution is

{{< katex display >}}
f(v) = n_0 \left( \frac{m}{2 \pi k T} \right) ^{3/2} e^{-( \frac{1}{2} m v^2 + mgz)/kT}
{{< /katex >}}


{{< katex display >}}
= n(z) \left( \frac{m}{2 \pi k T} \right) ^{3/2} e^{-\frac{1}{2} m v^2 / kT}
{{< /katex >}}


{{< katex display >}}
n(z) = n_0 e^{- mgz / kT}
{{< /katex >}}

In a collisionless picture, the velocity spread is the same at all heights (\\( kT \\) is not a function of z). However lower energy particles do not make it to a higher z. We end up with a density drop with increasing \\( z \\) .

{{< /hint >}}
