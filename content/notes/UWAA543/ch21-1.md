---
title: Finite Difference Algorithms
bookToc: false
weight: 60
---


# 21.1 Finite Difference Algorithms


## Definitions

By definition, **Finite Differencing** is a method to _approximate_ partial differential equations which we cannot solve, into a system of algebraic equations which we can.

Notation to simplify our representations:

 - Superscripts: We use superscripts to denote steps in the time domain \\( t^n = n \Delta t \\). Here \\( n = [0, N] \\) is the step index and \\( \Delta t \\) is the time-step \\( T/N \\)
 - Subscripts: \\( x_j = j \Delta x \\). Here \\( j = [0, J] \\) is the step index and \\( \Delta x = L/J \\) is the spatial step.
 - Together: \\( u_j ^n = u(t^n, x_j) \\)
 - **Explicit** algorithms: Use data that is already known at the present time to advance the solution to the next time step. They are easier and faster to implement, but they introduce stability constraints.
    
{{< katex display >}}
    u_j ^{n+1} = f(u_j ^n, u_{j+1} ^n , u_{j+2} ^n \ldots)
    {{< /katex >}}

 - **Implicit** algorithms: Use data from the next time step when advancing the solution. Leads to a system of equations that must be solved simultaneously
    
{{< katex display >}}
    u_{j} ^{n+1} = f(u_{j} ^n , u_{j+1} ^{n+1}, u_{j-1} ^{n+1}, \ldots)
    {{< /katex >}}


One look at the typical definition of the derivative suggests an algebraic approximation


{{< katex display >}}
\dv{f}{x} = \lim_{\Delta x \rightarrow 0} \frac{f(x + \Delta x) - f(x)}{\Delta x} \quad \rightarrow \quad \dv{f}{x} \approx \frac{f(x + \Delta x) - f(x) }{\Delta x}
{{< /katex >}}


Applying to our model equation, the linear advection equation


{{< katex display >}}
\pdv{u}{t} + c \pdv{u}{x} = 0
{{< /katex >}}


The first-order forward difference approximation for the temporal derivative is

{{< katex display >}}
\pdv{u}{t} \approx \frac{u_j ^{n+1} - u_j ^n}{\Delta t} \equiv \Delta _t u
{{< /katex >}}

And the first-order backward approximation for \\( \pdv{u}{x} \\) is

{{< katex display >}}
\pdv{u}{x} \approx \frac{u_{j} ^n - u_{j-1} ^n}{\Delta x} \equiv \nabla _x u
{{< /katex >}}

These are sometimes called Euler differencing, since they are first-order. Plugging in our approximations, we arrive at the "Forward Euler Algorithm"


{{< katex display >}}
\frac{u_{j} ^{n+1} u_j ^n}{\Delta t} + c \frac{u_{j} ^n - u_{j-1} ^n}{\Delta x} = 0
{{< /katex >}}

This is an explicit scheme: solve for \\( u^{n+1} _j  \\) 

{{< katex display >}}
u_j ^{n+1} = u_j ^n - \frac{c \Delta t}{\Delta x} (u_j ^n - u_{j-1} ^n)
{{< /katex >}}


The multiplier out front \\( \frac{c \Delta t}{\Delta x} \\) is very important for stability, so we call it


{{< katex display >}}
\frac{c \Delta t}{\Delta x} = \text{ Courant number (or CFL number)}
{{< /katex >}}


For the problem to be mathematically well-posed, we must know initial conditions and boundary conditions

{{< katex display >}}
u(t = 0, j) \qquad u(t, x = 0)
{{< /katex >}}


## Accuracy

We can be more precise with the error in our difference approximation: the error will be on the order of \\( \Delta x \\):


{{< katex display >}}
\dv{f}{x} \approx \frac{f(x + \Delta x) - f(x) }{\Delta x}
{{< /katex >}}



{{< katex display >}}
\rightarrow \dv{f}{x} = \frac{f(x + \Delta x) - f(x) }{\Delta x} + O(\Delta x)
{{< /katex >}}


As it turns out, we can improve the accuracy of the finite difference operators by using centered differences:


{{< katex display >}}
\left. \pdv{u}{x} \right|_{j} = \frac{u_{j+1} ^n - u_{j-1} ^n}{2 \Delta x} + O(\Delta x ^2)
{{< /katex >}}


So the forward-time, centered-space (FTCS) PDE becomes

{{< katex display >}}
\frac{u_j ^{n+1} - u_j ^n}{\Delta t} + c \frac{u_{j+1} ^n - u_{j-1} ^n }{2 \Delta x} = 0
{{< /katex >}}


This is more accurate than the Euler method in space. We can write the accuracy as \\( O(\Delta t, \Delta x^2) \\), or "it is first-order accurate in time and second-order accurate in space."

How do we get accuracy estimates? The accuracy is defined by using a Taylor series expansion for the finite-difference operators:


{{< katex display >}}
u_j ^{n+1} = u_j ^n + \pdv{u}{t} \Delta t + \frac{1}{2} \pdv{ ^2 u}{t^2} \Delta t^2 + \frac{1}{6} \pdv{ ^3 u}{t ^3} \Delta t^3 + \ldots
{{< /katex >}}


{{< katex display >}}
u_{j+1} ^n = u_j ^n + \pdv{u}{x} \Delta x + \frac{1}{2} \pdv{ ^2 u}{x^2} \Delta x^2 + \frac{1}{6} \pdv{ ^3 u }{x ^3} \Delta x ^3 + \ldots
{{< /katex >}}


{{< katex display >}}
u_{j-1} ^n = u_j ^n - \pdv{u}{x} \Delta x + \frac{1}{2} \pdv{ ^2 u }{x^2} \Delta x^2 - \frac{1}{6} \pdv{^3 u }{x^3} \Delta x ^3 + \ldots 
{{< /katex >}}


Substituting into the Forward Euler algorithm:


{{< katex display >}}
u_j ^n + \pdv{u}{t} \Delta t + \frac{1}{2} \pdv{^2 u }{t^2} \Delta t^2 + \frac{1}{6} \pdv{ ^3 u}{t ^3} \Delta t^3 + \ldots
{{< /katex >}}


{{< katex display >}}
= u_j ^n - \frac{c \Delta t}{\Delta x} \left( u_j ^n - \left[ u_j ^n - \pdv{u}{x} \Delta x + \pdv{^2 u }{x^2} \frac{\Delta x ^2}{2} \pm \ldots \right] \right)
{{< /katex >}}


After simplifying and re-arranging/canceling like terms, we get


{{< katex display >}}
\pdv{u}{t} + c \pdv{u}{x} = - \frac{1}{2} \pdv{^2 u}{t^2} \Delta t - \frac{1}{6} \Delta t^2 + \ldots + \frac{c}{2} \pdv{^2 u}{x^2} \Delta x - \frac{c}{6} \pdv{ ^3 u }{x ^3} \Delta x ^2 + \ldots
{{< /katex >}}


The left-hand side is the PDE we're trying to solve, so everything on the right-hand side is the error term introduced by our approximation. The solution we're going to get is actually the solution to the modified PDE with all of the error terms.  Reading off the lowest-order terms of \\( \Delta x \\) and \\( \Delta t \\) we see that the algorithm is first-order accurate in space and time.

## Algorithm Requirements

For an algorithm to work, it requires the following properties:

1. **Consistency**: Whatever finite difference operator \\( \delta_x u \\) we use, applied to our solution, has to approximate the derivative as the spacing goes to zero
    
{{< katex display >}}
    \delta _x u \rightarrow \pdv{u}{x} \quad \text{ as } \quad \Delta x \rightarrow 0
    {{< /katex >}}

2. **Stability**: The solution must be bounded, so for some initial condition \\( u \\) the norm goes to zero as the number of points goes to infinity
    
{{< katex display >}}
    |u_j - u| \rightarrow 0 \quad \text{ as } \quad I \rightarrow \infty
    {{< /katex >}}

3. **Accuracy**: The accuracy is bounded by the finite difference operator
    
{{< katex display >}}
    \pdv{u}{x} - \delta _x u = O(\Delta x ^m)
    {{< /katex >}}

4. **Convergence**: The numerical solution must approach the exact solution as the grid spacing goes to zero
    
{{< katex display >}}
    u_j \rightarrow u \quad \text{ as } \quad \Delta x \rightarrow 0
    {{< /katex >}}


A useful theorem is Lax's Theorem: _An algorithm that is consistent and stable will converge_. This means that we only have to prove consistency, stability, and accuracy.


### Consistency

Let's take our Forward Euler algorithm as an example: From analyzing the accuracy, we derived the modified PDE and noted the leading-order error terms. Forward Euler gave \\( O(\Delta x, \Delta t) \\). As \\( \Delta t, \Delta x \rightarrow 0 \\), we recover the original PDE. Therefore, FE is consistent.

Likewise the FTCS is also consistent, since \\( \Delta t, \Delta x \rightarrow 0 \\) also recovers the original PDE. In fact, because the accuracy of FTCS was \\( O(\Delta x ^2, \Delta t) \\) it is also consistent.

### Stability

The way to perform the stability analysis is to use Fourier transforms to convert from a discrete spatial domain to a continuous frequency domain. Broadly speaking, the mathematical tools we have at our disposal to analyze stability only apply to continuous functions - you can't take a derivative of a set of discrete points. By transforming to a continuous domain, we can use properties of our basis functions to examine the growth of errors over time. This process is called **Von Neumann Stability Analysis**.

Example: Stability analysis of FTCS


{{< katex display >}}
u_j ^{n+1} = u_j ^n - \frac{a \Delta t}{2 \Delta x} \left( u_{j+1} ^n - u_{j-1} ^n \right)
{{< /katex >}}


We define an error norm by subtracting the exact solution


{{< katex display >}}
\varepsilon_j ^n = u_j ^n - \overline{u}
{{< /katex >}}



{{< katex display >}}
\varepsilon_j ^{n+1} = \varepsilon_j ^n - \frac{a \Delta t}{2 \Delta x} \left( \varepsilon_{j+1} ^n - \varepsilon_{j-1} ^n \right)
{{< /katex >}}


To study the evolution of the errors, apply a Fourier transform to get to frequency space:


{{< katex display >}}
\varepsilon_j ^n = V^n e^{ik_x x} = V^n e^{ik_x (j \Delta x)}
{{< /katex >}}


{{< katex display >}}
V^n = \text{ wave amplitude}
{{< /katex >}}


{{< katex display >}}
k = \text{ wavenumber } = \frac{2 \pi}{\lambda_x}
{{< /katex >}}


Substituting, 

{{< katex display >}}
V^{n+1} e^{i k_x j \Delta x} = V^n e^{i k_x j \Delta x} - \frac{a \Delta t}{2 \Delta x} \left(V^n e^{i k_x (j+1) \Delta x } - V^n e^{i k_x (j-1) \Delta x} \right)
{{< /katex >}}


{{< katex display >}}
\rightarrow \frac{V^{n+1}}{V_n} = 1 - \frac{a \Delta t}{2 \Delta x} \left( e^{i k_x \Delta x} - e^{ik_x \Delta x} \right)
{{< /katex >}}

The ratio \\( V^{n+1}/V^n \\) tells us how the amplitudes of the errors will evolve in time. We define the amplification factor \\( G \\) 

{{< katex display >}}
G = \left| \frac{V^{n+1}}{V^n} \right|
{{< /katex >}}

For stability, we need the errors not to grow over time, so the Von Neumann stability criterion is:


{{< katex display >}}
G^m \leq 1 \quad \forall n
{{< /katex >}}


Continuing with our example


{{< katex display >}}
\frac{V^{n+1}}{V_n} = 1 - \frac{a \Delta t}{2 \Delta x} \left[ \cos (k_x \Delta x) + i \sin(k \Delta x) - \cos (-k_x \Delta x) - i \sin (- k_x \Delta x) \right]
{{< /katex >}}


{{< katex display >}}
= 1 - i \frac{ a \Delta t}{\Delta x} \sin (k_x \Delta x)
{{< /katex >}}

To get the amplification factor, we need the norm

{{< katex display >}}
G = \left[ \left(1 - i \frac{ a \Delta t}{\Delta x} \sin (k_x \Delta x) \right) \left( 1 + i \frac{ a \Delta t}{\Delta x} \sin (k_x \Delta x)\right) \right] ^{1/2}
{{< /katex >}}


{{< katex display >}}
= \sqrt{ 1 + \left( \frac{a \Delta t}{\Delta x} ^2 \sin ^2 (k_x \Delta x)\right)}
{{< /katex >}}

But that's always greater than 1! So FTCS is unconditionally unstable.

Performing the same analysis for Forward Euler, we get a stability condition:


{{< katex display >}}
0 \leq 1 + 2\frac{a \Delta t}{\Delta x} \left( \frac{a \Delta t}{ \Delta x} -1 \right) (1 - \cos (k_x \Delta x)) \leq 1
{{< /katex >}}


<p align="center"> <img alt="Figure 20.4" src="/r/img/20.4.png" /> </p>

We can see that the CFL number must be


{{< katex display >}}
0 \leq \frac{a \Delta t}{\Delta x} \leq 1
{{< /katex >}}

Possible values of \\( k_x \Delta x = \frac{2 \pi}{\lambda} \Delta x \\) . The minimum of \\( k_x \Delta x \\) occurs at \\( \lambda = \infty \\), uniform error throughout the domain. This only occurs if the error is everywhere zero, since the boundary conditions are presumed accurate. In practice, the maximum wavelength is equal to twice the length of the domain

{{< katex display >}}
\lambda_{max} = 2L
{{< /katex >}}


{{< katex display >}}
(k_x \Delta x)_{max} = \frac{\pi}{J-1}
{{< /katex >}}


The maximum of \\( k_x \Delta x \\) is for \\( \lambda = 0 \\), but this is not possible on a grid with finite grid points. The Nyquist limit tells us that \\( \lambda_{min} = 2 \Delta x \\). That means that


{{< katex display >}}
(k_x \Delta x)_{max} = \pi
{{< /katex >}}


When you violate a stability condition, what do we expect to see? The error growth is largest for the largest value of \\( k_x \Delta x \\), which corresponds to a wavelength equal to the grid spacing. In practice, that looks like errors that blow up from point to point.

### Convergence

We want to show that the numerical solution approaches the exact solution to the original PDE.

Lax's equivalence theorem states:

If an algorithm is consistent and stability requirements are satisfied, the numerical solution will converge to the solution of the original PDE.

This is why consistency and stability is so important; if you have both, then you have convergence, which tells you that the problem you're solving is the problem you are actually trying to solve.

### Accuracy

The solution exists at discrete locations for multiple variables, e.g \\( \rho, p, u, v, \ldots \\), so we want a convenient measure of solution accuracy.

The general p-norm is defined as


{{< katex display >}}
\text{p-norm} = L_p =  \left[\sum_{j=1} ^J |z|^p \right]^{1/p}
{{< /katex >}}


In practice, the most common norms that we use are the 1, 2, or \\( \infty \\)-norms.


{{< katex display >}}
L_1 = \sum |z| \quad \text{(average)}
{{< /katex >}}



{{< katex display >}}
L_2 = \left[ \sum |z|^2 \right]^{1/2} \quad \text{(variance)}
{{< /katex >}}



{{< katex display >}}
L_{\infty} = \text{max}|z| \quad \text{(max-norm)}
{{< /katex >}}


In practice, the 2-norm is the most rigorous definition of the error. Specifically, we define the error norm in the form


{{< katex display >}}
\text{Error norm} = \left[ \sum_{j=1} ^{J} \Delta x ( \varepsilon) ^2 \right] ^{1/2} = \sqrt{\Delta x} || \varepsilon ||_2
{{< /katex >}}


{{< katex display >}}
\varepsilon_j = u_j - \overline{u}
{{< /katex >}}


### Sources of Errors?

Where do errors come from? In order to reduce our model's error, we must first understand where the errors come from.

**Truncation errors** - Result from the terms in the Taylor series difference approximation that are neglected. They can be reduced by using higher accuracy difference operators.

**Round-off errors** - Result of the limited machine accuracy (accuracy of floating point representation). It can be reduced by using higher precision to store values, _or_ by performing fewer calculations.

**Bugs** - Don't have bugs. ha. ha. ha. But really, we can minimize programming errors through practices like good planning, good comments, etc.

